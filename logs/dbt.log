[0m13:41:23.236193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024967BFF750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002496827B950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024967BFFED0>]}


============================== 13:41:23.241189 | 3bfda309-b260-4c25-97ba-77a3c03d24e8 ==============================
[0m13:41:23.241189 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:41:23.242154 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:41:23.243176 [warn ] [MainThread]: [ConfigFolderDirectory]: Unable to parse dict {'dir': WindowsPath('C:/Users/Marisa/.dbt')}
[0m13:41:23.244148 [info ] [MainThread]: Creating dbt configuration folder at 
[0m13:41:23.247164 [debug] [MainThread]: Starter project path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\include\starter_project
[0m14:16:01.288520 [error] [MainThread]: Encountered an error:

[0m14:16:01.302795 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 86, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 71, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\main.py", line 457, in init
    results = task.run()
              ^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\task\init.py", line 307, in run
    self.create_profile_from_target(adapter, profile_name=project_name)
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\task\init.py", line 180, in create_profile_from_target
    self.create_profile_from_profile_template(profile_template, profile_name)
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\task\init.py", line 164, in create_profile_from_profile_template
    target = self.generate_target_from_input(prompts, initial_target)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\task\init.py", line 117, in generate_target_from_input
    target = self.generate_target_from_input(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\task\init.py", line 130, in generate_target_from_input
    target[key] = click.prompt(
                  ^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\click\termui.py", line 168, in prompt
    value = prompt_func(prompt)
            ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\click\termui.py", line 151, in prompt_func
    raise Abort() from None
click.exceptions.Abort

[0m14:16:01.306660 [debug] [MainThread]: Command `dbt init` failed at 14:16:01.306660 after 2078.08 seconds
[0m14:16:01.307662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249684D7F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024967BC2450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024967F33F10>]}
[0m14:16:01.308656 [debug] [MainThread]: Flushing usage events
[0m14:16:46.275243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F01436B2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F014343050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F014976850>]}


============================== 14:16:46.282797 | 0e2333a4-1eb8-4a3f-bdc2-ba44463f01c8 ==============================
[0m14:16:46.282797 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:16:46.283808 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:16:46.286930 [info ] [MainThread]: A project called projeto_health_insights already exists here.
[0m14:16:46.287929 [debug] [MainThread]: Command `dbt init` succeeded at 14:16:46.287929 after 0.02 seconds
[0m14:16:46.288926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F014322AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0145EFD50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F00E17C7D0>]}
[0m14:16:46.288926 [debug] [MainThread]: Flushing usage events
[0m14:21:48.487386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0F950FA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0F9A67AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0F97C6050>]}


============================== 14:21:48.492373 | d6bc8a52-691a-4f99-a519-fd12c5333eba ==============================
[0m14:21:48.492373 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:21:48.494370 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:21:48.496358 [info ] [MainThread]: A project called projeto_health_insights already exists here.
[0m14:21:48.497366 [debug] [MainThread]: Command `dbt init` succeeded at 14:21:48.497366 after 0.02 seconds
[0m14:21:48.498352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0F95D1250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0F9858B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0F359C990>]}
[0m14:21:48.498352 [debug] [MainThread]: Flushing usage events
[0m14:32:03.785940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020494E28B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020492419110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020494E6EC90>]}


============================== 14:32:03.791066 | 7ecb6f07-0a36-4471-8610-bfa6975e512a ==============================
[0m14:32:03.791066 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:32:03.793084 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:32:03.793084 [info ] [MainThread]: dbt version: 1.5.2
[0m14:32:03.794214 [info ] [MainThread]: python version: 3.11.8
[0m14:32:03.795213 [info ] [MainThread]: python path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Scripts\python.exe
[0m14:32:03.796209 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m14:32:03.796209 [info ] [MainThread]: Using profiles.yml file at C:\Users\Marisa\.dbt\profiles.yml
[0m14:32:03.797229 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Marisa\Desktop\projeto_health_insights\dbt_project.yml
[0m14:32:03.798205 [info ] [MainThread]: Configuration:
[0m14:32:05.584384 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:32:05.586352 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m14:32:05.586352 [info ] [MainThread]: Required dependencies:
[0m14:32:05.587349 [debug] [MainThread]: Executing "git --help"
[0m14:32:05.667348 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:32:05.668346 [debug] [MainThread]: STDERR: "b''"
[0m14:32:05.668346 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:32:05.669342 [info ] [MainThread]: Connection:
[0m14:32:05.670340 [info ] [MainThread]:   host: <https://dbc-605a8848-8c13.cloud.databricks.com>
[0m14:32:05.671343 [info ] [MainThread]:   http_path: <jdbc:databricks://dbc-605a8848-8c13.cloud.databricks.com:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/warehouses/9d3cc00bfedf4aa8;>
[0m14:32:05.673332 [info ] [MainThread]:   catalog: hive_metastore
[0m14:32:05.674329 [info ] [MainThread]:   schema: default
[0m14:32:05.675270 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:32:05.676271 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m14:32:05.677295 [debug] [MainThread]: Using databricks connection "debug"
[0m14:32:05.677295 [debug] [MainThread]: On debug: select 1 as id
[0m14:32:05.678289 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:32:05.703956 [debug] [MainThread]: Databricks adapter: Error while running:
select 1 as id
[0m14:32:05.703956 [debug] [MainThread]: Databricks adapter: Database Error
  HTTPSConnectionPool(host='<https', port=None): Max retries exceeded with url: //dbc-605a8848-8c13.cloud.databricks.com%3E:443/%3Cjdbc:databricks://dbc-605a8848-8c13.cloud.databricks.com:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/warehouses/9d3cc00bfedf4aa8;%3E (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000002049E5D2C90>: Failed to resolve '<https' ([Errno 11001] getaddrinfo failed)"))
[0m14:32:05.704955 [debug] [MainThread]: On debug: No close available on handle
[0m14:32:05.705952 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m14:32:05.708944 [info ] [MainThread]: [31m2 checks failed:[0m
[0m14:32:05.709942 [info ] [MainThread]: Could not load dbt_project.yml

[0m14:32:05.710948 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    HTTPSConnectionPool(host='<https', port=None): Max retries exceeded with url: //dbc-605a8848-8c13.cloud.databricks.com%3E:443/%3Cjdbc:databricks://dbc-605a8848-8c13.cloud.databricks.com:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/warehouses/9d3cc00bfedf4aa8;%3E (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000002049E5D2C90>: Failed to resolve '<https' ([Errno 11001] getaddrinfo failed)"))

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m14:32:05.712934 [debug] [MainThread]: Command `dbt debug` failed at 14:32:05.712934 after 1.95 seconds
[0m14:32:05.713931 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:32:05.713931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020494D8FF10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204953D6C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204953D5D10>]}
[0m14:32:05.714928 [debug] [MainThread]: Flushing usage events
[0m14:38:02.648088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C93C2A0D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C93CB07E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C93C234050>]}


============================== 14:38:02.655294 | 43de1777-c255-4e42-a0b4-052513660248 ==============================
[0m14:38:02.655294 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:38:02.656258 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:38:02.657258 [info ] [MainThread]: dbt version: 1.5.2
[0m14:38:02.658254 [info ] [MainThread]: python version: 3.11.8
[0m14:38:02.659889 [info ] [MainThread]: python path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Scripts\python.exe
[0m14:38:02.660403 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m14:38:02.661394 [info ] [MainThread]: Using profiles.yml file at C:\Users\Marisa\.dbt\profiles.yml
[0m14:38:02.663390 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Marisa\Desktop\projeto_health_insights\dbt_project.yml
[0m14:38:02.666386 [info ] [MainThread]: Configuration:
[0m14:38:04.066563 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:38:04.068543 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m14:38:04.069543 [info ] [MainThread]: Required dependencies:
[0m14:38:04.070672 [debug] [MainThread]: Executing "git --help"
[0m14:38:04.113936 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:38:04.114961 [debug] [MainThread]: STDERR: "b''"
[0m14:38:04.115931 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:38:04.116927 [info ] [MainThread]: Connection:
[0m14:38:04.119044 [info ] [MainThread]:   host: https://dbc-605a8848-8c13.cloud.databricks.com
[0m14:38:04.120057 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
[0m14:38:04.121039 [info ] [MainThread]:   catalog: hive_metastore
[0m14:38:04.122044 [info ] [MainThread]:   schema: default
[0m14:38:04.123034 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:38:04.124031 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m14:38:04.124031 [debug] [MainThread]: Using databricks connection "debug"
[0m14:38:04.125028 [debug] [MainThread]: On debug: select 1 as id
[0m14:38:04.125028 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:38:06.420711 [debug] [MainThread]: Databricks adapter: Error while running:
select 1 as id
[0m14:38:06.421711 [debug] [MainThread]: Databricks adapter: Database Error
  HTTPSConnectionPool(host='https', port=None): Max retries exceeded with url: //dbc-605a8848-8c13.cloud.databricks.com:443/sql/1.0/warehouses/9d3cc00bfedf4aa8 (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001C945D07510>: Failed to resolve 'https' ([Errno 11001] getaddrinfo failed)"))
[0m14:38:06.421711 [debug] [MainThread]: On debug: No close available on handle
[0m14:38:06.422709 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m14:38:06.423706 [info ] [MainThread]: [31m2 checks failed:[0m
[0m14:38:06.424703 [info ] [MainThread]: Could not load dbt_project.yml

[0m14:38:06.425701 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    HTTPSConnectionPool(host='https', port=None): Max retries exceeded with url: //dbc-605a8848-8c13.cloud.databricks.com:443/sql/1.0/warehouses/9d3cc00bfedf4aa8 (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001C945D07510>: Failed to resolve 'https' ([Errno 11001] getaddrinfo failed)"))

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m14:38:06.428693 [debug] [MainThread]: Command `dbt debug` failed at 14:38:06.427695 after 3.80 seconds
[0m14:38:06.428693 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:38:06.429689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C93C5511D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C945CF9DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C93CB04A10>]}
[0m14:38:06.429689 [debug] [MainThread]: Flushing usage events
[0m14:42:49.100112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC2AB72850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC2ABFA790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC2AC1E450>]}


============================== 14:42:49.105411 | af84c286-8c71-4de4-b7aa-7489e6ca56c0 ==============================
[0m14:42:49.105411 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:42:49.106423 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:42:49.107405 [info ] [MainThread]: dbt version: 1.5.2
[0m14:42:49.108403 [info ] [MainThread]: python version: 3.11.8
[0m14:42:49.109406 [info ] [MainThread]: python path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Scripts\python.exe
[0m14:42:49.110397 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m14:42:49.110397 [info ] [MainThread]: Using profiles.yml file at C:\Users\Marisa\.dbt\profiles.yml
[0m14:42:49.111395 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Marisa\Desktop\projeto_health_insights\dbt_project.yml
[0m14:42:49.113390 [info ] [MainThread]: Configuration:
[0m14:42:50.613494 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:42:50.614492 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m14:42:50.615567 [info ] [MainThread]: Required dependencies:
[0m14:42:50.616588 [debug] [MainThread]: Executing "git --help"
[0m14:42:50.662152 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:42:50.662152 [debug] [MainThread]: STDERR: "b''"
[0m14:42:50.663178 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:42:50.664147 [info ] [MainThread]: Connection:
[0m14:42:50.665145 [info ] [MainThread]:   host: dbc-605a8848-8c13.cloud.databricks.com
[0m14:42:50.665145 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
[0m14:42:50.666233 [info ] [MainThread]:   catalog: hive_metastore
[0m14:42:50.667231 [info ] [MainThread]:   schema: default
[0m14:42:50.667231 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:42:50.668229 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m14:42:50.669254 [debug] [MainThread]: Using databricks connection "debug"
[0m14:42:50.671222 [debug] [MainThread]: On debug: select 1 as id
[0m14:42:50.671222 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:42:51.792307 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0786c-ef3a-1527-acba-30f61039f837
[0m14:42:57.595247 [debug] [MainThread]: Databricks adapter: Error while running:
select 1 as id
[0m14:42:57.596245 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UC_HIVE_METASTORE_DISABLED_EXCEPTION] The operation attempted to use Hive Metastore, which is disabled due to legacy access being turned off in your account or workspace. Please double check the default catalog in current session and default namespace setting. If you need to access the Hive Metastore, please ask your admin to set up Hive Metastore federation through Unity Catalog. SQLSTATE: 0A000
[0m14:42:57.598239 [debug] [MainThread]: Databricks adapter: diagnostic-info: None
[0m14:42:57.598751 [debug] [MainThread]: Databricks adapter: operation-id: 01f0786c-ef7d-1c6f-aef5-441bf124763b
[0m14:42:57.598751 [debug] [MainThread]: On debug: Close
[0m14:42:57.599772 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0786c-ef3a-1527-acba-30f61039f837
[0m14:42:57.825914 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m14:42:57.829902 [info ] [MainThread]: [31m2 checks failed:[0m
[0m14:42:57.832136 [info ] [MainThread]: Could not load dbt_project.yml

[0m14:42:57.835125 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  [UC_HIVE_METASTORE_DISABLED_EXCEPTION] The operation attempted to use Hive Metastore, which is disabled due to legacy access being turned off in your account or workspace. Please double check the default catalog in current session and default namespace setting. If you need to access the Hive Metastore, please ask your admin to set up Hive Metastore federation through Unity Catalog. SQLSTATE: 0A000

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m14:42:57.844099 [debug] [MainThread]: Command `dbt debug` failed at 14:42:57.844099 after 8.76 seconds
[0m14:42:57.846097 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:42:57.848093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC2AC5C2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC34412AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC287D0C90>]}
[0m14:42:57.849086 [debug] [MainThread]: Flushing usage events
[0m14:46:05.586729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000130668EF190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001306439ADD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001306698D5D0>]}


============================== 14:46:05.590718 | 19ecff82-2bba-4ed0-80ba-164054b851bc ==============================
[0m14:46:05.590718 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:46:05.592685 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:46:05.593682 [info ] [MainThread]: dbt version: 1.5.2
[0m14:46:05.595303 [info ] [MainThread]: python version: 3.11.8
[0m14:46:05.596310 [info ] [MainThread]: python path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Scripts\python.exe
[0m14:46:05.597299 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m14:46:05.598298 [info ] [MainThread]: Using profiles.yml file at C:\Users\Marisa\.dbt\profiles.yml
[0m14:46:05.599296 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Marisa\Desktop\projeto_health_insights\dbt_project.yml
[0m14:46:05.600291 [info ] [MainThread]: Configuration:
[0m14:46:07.334437 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:46:07.335435 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m14:46:07.336435 [info ] [MainThread]: Required dependencies:
[0m14:46:07.338427 [debug] [MainThread]: Executing "git --help"
[0m14:46:07.388294 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:46:07.388294 [debug] [MainThread]: STDERR: "b''"
[0m14:46:07.389291 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:46:07.390289 [info ] [MainThread]: Connection:
[0m14:46:07.391286 [info ] [MainThread]:   host: dbc-605a8848-8c13.cloud.databricks.com
[0m14:46:07.392287 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
[0m14:46:07.393285 [info ] [MainThread]:   catalog: main
[0m14:46:07.395275 [info ] [MainThread]:   schema: default
[0m14:46:07.396272 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:46:07.397270 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m14:46:07.398267 [debug] [MainThread]: Using databricks connection "debug"
[0m14:46:07.399264 [debug] [MainThread]: On debug: select 1 as id
[0m14:46:07.399264 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:46:08.304485 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0786d-6479-1550-bfd4-cb02ca2e8f47
[0m14:46:10.785410 [debug] [MainThread]: SQL status: OK in 3.390000104904175 seconds
[0m14:46:10.787492 [debug] [MainThread]: On debug: Close
[0m14:46:10.787492 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0786d-6479-1550-bfd4-cb02ca2e8f47
[0m14:46:11.043568 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:46:11.045270 [info ] [MainThread]: [31m1 check failed:[0m
[0m14:46:11.046266 [info ] [MainThread]: Could not load dbt_project.yml

[0m14:46:11.049258 [debug] [MainThread]: Command `dbt debug` failed at 14:46:11.049258 after 5.48 seconds
[0m14:46:11.050256 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:46:11.051253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013066B02550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000130702CF8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000130606E8910>]}
[0m14:46:11.053248 [debug] [MainThread]: Flushing usage events
[0m14:49:12.216244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A865CF610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A865CF210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A865FB150>]}


============================== 14:49:12.221229 | df0a3d47-5bd5-43ed-9710-e4fc29db4c58 ==============================
[0m14:49:12.221229 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:49:12.223372 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:49:12.224370 [info ] [MainThread]: dbt version: 1.5.2
[0m14:49:12.225367 [info ] [MainThread]: python version: 3.11.8
[0m14:49:12.225367 [info ] [MainThread]: python path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Scripts\python.exe
[0m14:49:12.227032 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m14:49:12.228030 [info ] [MainThread]: Using profiles.yml file at C:\Users\Marisa\.dbt\profiles.yml
[0m14:49:12.229027 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Marisa\Desktop\projeto_health_insights\dbt_project.yml
[0m14:49:12.230025 [info ] [MainThread]: Configuration:
[0m14:49:13.631337 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:49:13.633303 [info ] [MainThread]:   dbt_project.yml file [[31mERROR not found[0m]
[0m14:49:13.634300 [info ] [MainThread]: Required dependencies:
[0m14:49:13.635055 [debug] [MainThread]: Executing "git --help"
[0m14:49:13.678280 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:49:13.679306 [debug] [MainThread]: STDERR: "b''"
[0m14:49:13.680275 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:49:13.681287 [info ] [MainThread]: Connection:
[0m14:49:13.681287 [info ] [MainThread]:   host: dbc-605a8848-8c13.cloud.databricks.com
[0m14:49:13.682294 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
[0m14:49:13.683294 [info ] [MainThread]:   catalog: main
[0m14:49:13.684099 [info ] [MainThread]:   schema: default
[0m14:49:13.685100 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:49:13.686097 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m14:49:13.686097 [debug] [MainThread]: Using databricks connection "debug"
[0m14:49:13.687094 [debug] [MainThread]: On debug: select 1 as id
[0m14:49:13.687094 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:49:14.548021 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0786d-d37d-1499-b372-8d979d836017
[0m14:49:14.946918 [debug] [MainThread]: SQL status: OK in 1.2599999904632568 seconds
[0m14:49:14.948910 [debug] [MainThread]: On debug: Close
[0m14:49:14.948910 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0786d-d37d-1499-b372-8d979d836017
[0m14:49:15.228126 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:49:15.231123 [info ] [MainThread]: [31m1 check failed:[0m
[0m14:49:15.233149 [info ] [MainThread]: Could not load dbt_project.yml

[0m14:49:15.236104 [debug] [MainThread]: Command `dbt debug` failed at 14:49:15.236104 after 3.04 seconds
[0m14:49:15.238101 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:49:15.240094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A86C07510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A86C07D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A800F8A90>]}
[0m14:49:15.243087 [debug] [MainThread]: Flushing usage events
[0m15:08:19.783951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019BBA605D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019BBED07750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019BBEB9DBD0>]}


============================== 15:08:19.788937 | 8d924134-5e30-47b8-8b1c-ebe5a7968624 ==============================
[0m15:08:19.788937 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:08:19.789934 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:08:19.790932 [info ] [MainThread]: dbt version: 1.5.2
[0m15:08:19.791929 [info ] [MainThread]: python version: 3.11.8
[0m15:08:19.792927 [info ] [MainThread]: python path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Scripts\python.exe
[0m15:08:19.792927 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m15:08:19.793924 [info ] [MainThread]: Using profiles.yml file at C:\Users\Marisa\.dbt\profiles.yml
[0m15:08:19.794921 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Marisa\Desktop\projeto_health_insights\dbt_project.yml
[0m15:08:19.794921 [info ] [MainThread]: Configuration:
[0m15:08:21.538181 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:08:21.570856 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:08:21.571854 [info ] [MainThread]: Required dependencies:
[0m15:08:21.572852 [debug] [MainThread]: Executing "git --help"
[0m15:08:21.617768 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:08:21.617768 [debug] [MainThread]: STDERR: "b''"
[0m15:08:21.618825 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:08:21.618825 [info ] [MainThread]: Connection:
[0m15:08:21.618825 [info ] [MainThread]:   host: dbc-605a8848-8c13.cloud.databricks.com
[0m15:08:21.618825 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
[0m15:08:21.618825 [info ] [MainThread]:   catalog: main
[0m15:08:21.624839 [info ] [MainThread]:   schema: default
[0m15:08:21.624839 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m15:08:21.624839 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m15:08:21.624839 [debug] [MainThread]: Using databricks connection "debug"
[0m15:08:21.624839 [debug] [MainThread]: On debug: select 1 as id
[0m15:08:21.624839 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:08:23.037478 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07870-7fe7-1c1b-96ad-05c3fd778801
[0m15:08:41.342311 [debug] [MainThread]: SQL status: OK in 19.719999313354492 seconds
[0m15:08:41.638539 [debug] [MainThread]: On debug: Close
[0m15:08:41.639535 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07870-7fe7-1c1b-96ad-05c3fd778801
[0m15:08:41.867082 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:08:41.869075 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:08:41.870043 [debug] [MainThread]: Command `dbt debug` succeeded at 15:08:41.870043 after 22.11 seconds
[0m15:08:41.871216 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:08:41.872247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019BBE680850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019BB8A9DF50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019BB87FBCD0>]}
[0m15:08:41.873238 [debug] [MainThread]: Flushing usage events
[0m15:39:24.663059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000257C7CBA1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000257C7450ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000257C73E7E50>]}


============================== 15:39:24.663059 | 3d890688-4a2b-4b38-a147-a523e13cb3ec ==============================
[0m15:39:24.663059 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:39:24.663059 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:39:24.663059 [info ] [MainThread]: dbt version: 1.5.2
[0m15:39:24.663059 [info ] [MainThread]: python version: 3.11.8
[0m15:39:24.663059 [info ] [MainThread]: python path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Scripts\python.exe
[0m15:39:24.663059 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m15:39:24.663059 [info ] [MainThread]: Using profiles.yml file at C:\Users\Marisa\.dbt\profiles.yml
[0m15:39:24.663059 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Marisa\Desktop\projeto_health_insights\dbt_project.yml
[0m15:39:24.676068 [info ] [MainThread]: Configuration:
[0m15:39:24.681776 [error] [MainThread]: Encountered an error:
Runtime Error
  
  dbt encountered an error while trying to read your profiles.yml file.
  
  Runtime Error
    Syntax error near line 12
    ------------------------------
    9  |       http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
    10 |       token: dapiaa3ea14b95e9f79fda0a4748a63da06a
    11 |       threads: 1
    12 |        connect_timeout: 30
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 12, column 23
  
[0m15:39:24.686618 [debug] [MainThread]: Command `dbt debug` failed at 15:39:24.685635 after 0.04 seconds
[0m15:39:24.686618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000257C765CFD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000257C73AF650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000257C7CBBE90>]}
[0m15:39:24.687622 [debug] [MainThread]: Flushing usage events
[0m15:39:28.698615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8503011D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8502B9390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8502DF850>]}


============================== 15:39:28.698615 | 5dc9fd3c-a03d-411c-b9c4-c371cf3aa46e ==============================
[0m15:39:28.698615 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:39:28.698615 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:39:28.698615 [error] [MainThread]: Encountered an error:
Runtime Error
  
  dbt encountered an error while trying to read your profiles.yml file.
  
  Runtime Error
    Syntax error near line 12
    ------------------------------
    9  |       http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
    10 |       token: dapiaa3ea14b95e9f79fda0a4748a63da06a
    11 |       threads: 1
    12 |        connect_timeout: 30
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 12, column 23
  
[0m15:39:28.711468 [debug] [MainThread]: Command `dbt run` failed at 15:39:28.710470 after 0.02 seconds
[0m15:39:28.712352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8508A5310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E850013050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E8502F10D0>]}
[0m15:39:28.713284 [debug] [MainThread]: Flushing usage events
[0m15:39:32.644141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000164DFA832D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000164DFAE7910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000164DFB69510>]}


============================== 15:39:32.649150 | 48a16add-ec3a-4a42-abe9-795d666f354f ==============================
[0m15:39:32.649150 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:39:32.650131 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:39:32.653138 [error] [MainThread]: Encountered an error:
Runtime Error
  
  dbt encountered an error while trying to read your profiles.yml file.
  
  Runtime Error
    Syntax error near line 12
    ------------------------------
    9  |       http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
    10 |       token: dapiaa3ea14b95e9f79fda0a4748a63da06a
    11 |       threads: 1
    12 |        connect_timeout: 30
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 12, column 23
  
[0m15:39:32.656901 [debug] [MainThread]: Command `dbt test` failed at 15:39:32.655822 after 0.02 seconds
[0m15:39:32.657943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000164DF799F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000164D989DED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000164DF7AFA50>]}
[0m15:39:32.658944 [debug] [MainThread]: Flushing usage events
[0m15:39:36.605606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE5CBF8590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE5CBBFE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE5C93C990>]}


============================== 15:39:36.605606 | 6296979a-50ef-486d-adc5-75623813eff5 ==============================
[0m15:39:36.605606 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:39:36.605606 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:39:36.621200 [error] [MainThread]: Encountered an error:
Runtime Error
  
  dbt encountered an error while trying to read your profiles.yml file.
  
  Runtime Error
    Syntax error near line 12
    ------------------------------
    9  |       http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
    10 |       token: dapiaa3ea14b95e9f79fda0a4748a63da06a
    11 |       threads: 1
    12 |        connect_timeout: 30
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 12, column 23
  
[0m15:39:36.625907 [debug] [MainThread]: Command `dbt docs generate` failed at 15:39:36.624907 after 0.03 seconds
[0m15:39:36.625907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE5CBC6C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE5CC93150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE5CBBFE90>]}
[0m15:39:36.626923 [debug] [MainThread]: Flushing usage events
[0m15:39:40.697096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B930436950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B930191DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B930A63650>]}


============================== 15:39:40.697096 | 7cc64651-91d2-4e3f-a5a0-0eef61e65341 ==============================
[0m15:39:40.697096 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:39:40.697096 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:39:40.697096 [error] [MainThread]: Encountered an error:
Runtime Error
  
  dbt encountered an error while trying to read your profiles.yml file.
  
  Runtime Error
    Syntax error near line 12
    ------------------------------
    9  |       http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
    10 |       token: dapiaa3ea14b95e9f79fda0a4748a63da06a
    11 |       threads: 1
    12 |        connect_timeout: 30
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 12, column 23
  
[0m15:39:40.713235 [debug] [MainThread]: Command `dbt docs serve` failed at 15:39:40.712727 after 0.03 seconds
[0m15:39:40.714233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B9301A2D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B930827E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B93017E750>]}
[0m15:39:40.715230 [debug] [MainThread]: Flushing usage events
[0m15:40:09.855355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027704EFFB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027705286690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027702B1A610>]}


============================== 15:40:09.855355 | 0dfaa955-db1d-4725-b47a-07d9c7691e06 ==============================
[0m15:40:09.855355 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:40:09.855355 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:40:09.855355 [info ] [MainThread]: dbt version: 1.5.2
[0m15:40:09.855355 [info ] [MainThread]: python version: 3.11.8
[0m15:40:09.855355 [info ] [MainThread]: python path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Scripts\python.exe
[0m15:40:09.870977 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m15:40:09.870977 [info ] [MainThread]: Using profiles.yml file at C:\Users\Marisa\.dbt\profiles.yml
[0m15:40:09.872995 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Marisa\Desktop\projeto_health_insights\dbt_project.yml
[0m15:40:09.874175 [info ] [MainThread]: Configuration:
[0m15:40:11.167031 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:40:11.206095 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:40:11.206095 [info ] [MainThread]: Required dependencies:
[0m15:40:11.206095 [debug] [MainThread]: Executing "git --help"
[0m15:40:11.247693 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:40:11.247693 [debug] [MainThread]: STDERR: "b''"
[0m15:40:11.247693 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:40:11.247693 [info ] [MainThread]: Connection:
[0m15:40:11.247693 [info ] [MainThread]:   host: dbc-605a8848-8c13.cloud.databricks.com
[0m15:40:11.262141 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
[0m15:40:11.263140 [info ] [MainThread]:   catalog: main
[0m15:40:11.264376 [info ] [MainThread]:   schema: default
[0m15:40:11.264376 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m15:40:11.268138 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m15:40:11.268138 [debug] [MainThread]: Using databricks connection "debug"
[0m15:40:11.269136 [debug] [MainThread]: On debug: select 1 as id
[0m15:40:11.269136 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:40:12.399009 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07874-f1fa-1856-be9c-89b53c0aff34
[0m15:40:22.020849 [debug] [MainThread]: SQL status: OK in 10.75 seconds
[0m15:40:22.348690 [debug] [MainThread]: On debug: Close
[0m15:40:22.348690 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07874-f1fa-1856-be9c-89b53c0aff34
[0m15:40:22.568971 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:40:22.568971 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:40:22.568971 [debug] [MainThread]: Command `dbt debug` succeeded at 15:40:22.568971 after 12.74 seconds
[0m15:40:22.568971 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:40:22.568971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027704EB8C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277051C8050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002777ECABCD0>]}
[0m15:40:22.568971 [debug] [MainThread]: Flushing usage events
[0m15:40:26.776473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000135142D7950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000135142D7D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013513CA2690>]}


============================== 15:40:26.776473 | 82eef08f-d794-40a6-9622-89a3f395eaaa ==============================
[0m15:40:26.776473 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:40:26.776473 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:40:28.030624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '82eef08f-d794-40a6-9622-89a3f395eaaa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013513D990D0>]}
[0m15:40:28.046245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '82eef08f-d794-40a6-9622-89a3f395eaaa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001351D4F9610>]}
[0m15:40:28.046245 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m15:40:28.084947 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:40:28.084947 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:40:28.084947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '82eef08f-d794-40a6-9622-89a3f395eaaa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001351D4E9310>]}
[0m15:40:29.868051 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_table_default" in the project
  "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros\materializations\models\table\table.sql
      - macros\materializations\models\table.sql
[0m15:40:29.868051 [debug] [MainThread]: Command `dbt run` failed at 15:40:29.868051 after 3.12 seconds
[0m15:40:29.868051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013513AAA5D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000135142E4A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000135139FCFD0>]}
[0m15:40:29.868051 [debug] [MainThread]: Flushing usage events
[0m15:40:33.929685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF421AD790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF42252A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF4221C490>]}


============================== 15:40:33.929685 | caac5d56-1564-412f-a3c4-26ca402ceeac ==============================
[0m15:40:33.929685 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:40:33.929685 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:40:35.204920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'caac5d56-1564-412f-a3c4-26ca402ceeac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF41EDF510>]}
[0m15:40:35.231832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'caac5d56-1564-412f-a3c4-26ca402ceeac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF41EDF510>]}
[0m15:40:35.232830 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m15:40:35.237416 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:40:35.237416 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:40:35.237416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'caac5d56-1564-412f-a3c4-26ca402ceeac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF427DD090>]}
[0m15:40:35.801518 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_table_default" in the project
  "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros\materializations\models\table\table.sql
      - macros\materializations\models\table.sql
[0m15:40:35.801518 [debug] [MainThread]: Command `dbt test` failed at 15:40:35.801518 after 1.89 seconds
[0m15:40:35.801518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF4221C490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF4CA11090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF4CA10110>]}
[0m15:40:35.801518 [debug] [MainThread]: Flushing usage events
[0m15:40:39.905910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204495C8C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002044674AD50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020448D80E90>]}


============================== 15:40:39.921531 | fd39cbb9-43b6-4d26-8bea-eb971bb6f7de ==============================
[0m15:40:39.921531 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:40:39.921531 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:40:41.169658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fd39cbb9-43b6-4d26-8bea-eb971bb6f7de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020448C99110>]}
[0m15:40:41.185281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fd39cbb9-43b6-4d26-8bea-eb971bb6f7de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204495C8C90>]}
[0m15:40:41.185281 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m15:40:41.211580 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:40:41.211580 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:40:41.211580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fd39cbb9-43b6-4d26-8bea-eb971bb6f7de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204527EC1D0>]}
[0m15:40:41.770091 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_table_default" in the project
  "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros\materializations\models\table\table.sql
      - macros\materializations\models\table.sql
[0m15:40:41.770091 [debug] [MainThread]: Command `dbt docs generate` failed at 15:40:41.770091 after 1.88 seconds
[0m15:40:41.770091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020449066750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020453814ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020448F528D0>]}
[0m15:40:41.770091 [debug] [MainThread]: Flushing usage events
[0m15:40:45.819884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FD0491A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FD0AD7B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FD0490E90>]}


============================== 15:40:45.824933 | 45ca1246-e41d-426b-9611-a3a1a2abbbac ==============================
[0m15:40:45.824933 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:40:45.825903 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:40:47.105544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '45ca1246-e41d-426b-9611-a3a1a2abbbac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FD0589090>]}
[0m15:40:47.121165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '45ca1246-e41d-426b-9611-a3a1a2abbbac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FD0589090>]}
[0m15:40:47.121165 [error] [MainThread]: Encountered an error:
[WinError 2] O sistema não pode encontrar o arquivo especificado: 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\target'
[0m15:40:47.121165 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 86, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 71, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 142, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 215, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\main.py", line 299, in docs_serve
    results = task.run()
              ^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\task\serve.py", line 15, in run
    os.chdir(self.config.project_target_path)
FileNotFoundError: [WinError 2] O sistema não pode encontrar o arquivo especificado: 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\target'

[0m15:40:47.134617 [debug] [MainThread]: Command `dbt docs serve` failed at 15:40:47.134617 after 1.33 seconds
[0m15:40:47.135646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FD0588C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FD0AD5A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024FD0ADA590>]}
[0m15:40:47.136612 [debug] [MainThread]: Flushing usage events
[0m15:47:50.481404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE0958B2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE095A0AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE099DA450>]}


============================== 15:47:50.500191 | 82813bd7-0d75-4612-9d33-77f8b9f42156 ==============================
[0m15:47:50.500191 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:47:50.501165 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:47:51.894534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '82813bd7-0d75-4612-9d33-77f8b9f42156', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE09516550>]}
[0m15:47:51.910155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '82813bd7-0d75-4612-9d33-77f8b9f42156', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE09516550>]}
[0m15:47:51.910155 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m15:47:51.925748 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:47:51.925748 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:47:51.941370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '82813bd7-0d75-4612-9d33-77f8b9f42156', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE12FF2AD0>]}
[0m15:47:52.507587 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_table_default" in the project
  "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros\materializations\models\table\table.sql
      - macros\materializations\models\table.sql
[0m15:47:52.507587 [debug] [MainThread]: Command `dbt run` failed at 15:47:52.507587 after 2.04 seconds
[0m15:47:52.507587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE095B0B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE09DF4810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE097779D0>]}
[0m15:47:52.507587 [debug] [MainThread]: Flushing usage events
[0m15:51:21.239434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81EE779D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81E8337D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81E5A1D10>]}


============================== 15:51:21.244808 | a586bd88-3395-491a-8e0c-0695731aba69 ==============================
[0m15:51:21.244808 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:51:21.245806 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:51:21.272756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a586bd88-3395-491a-8e0c-0695731aba69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81E833790>]}
[0m15:51:21.272756 [debug] [MainThread]: Command `dbt clean` succeeded at 15:51:21.272756 after 0.06 seconds
[0m15:51:21.272756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81D835650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81847BCD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E81E578550>]}
[0m15:51:21.272756 [debug] [MainThread]: Flushing usage events
[0m15:51:25.618600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002001A2FDFD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002001A2FF710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002001A3A11D0>]}


============================== 15:51:25.624875 | 2362c9c0-ec1b-478c-93b2-f65585bc2d1b ==============================
[0m15:51:25.624875 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:51:25.625873 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:51:25.660130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2362c9c0-ec1b-478c-93b2-f65585bc2d1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002001A653790>]}
[0m15:51:25.661128 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m15:51:25.662125 [debug] [MainThread]: Command `dbt deps` succeeded at 15:51:25.662125 after 0.06 seconds
[0m15:51:25.663122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002001A31DC90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002001AC28090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002001A30F550>]}
[0m15:51:25.664119 [debug] [MainThread]: Flushing usage events
[0m15:51:37.288731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A228614150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A227DABE50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A22806C550>]}


============================== 15:51:37.304352 | d07c3a5c-db84-4c76-b011-cc25a8ed3b33 ==============================
[0m15:51:37.304352 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:51:37.304352 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:51:37.304352 [info ] [MainThread]: dbt version: 1.5.2
[0m15:51:37.304352 [info ] [MainThread]: python version: 3.11.8
[0m15:51:37.304352 [info ] [MainThread]: python path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Scripts\python.exe
[0m15:51:37.304352 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m15:51:37.304352 [info ] [MainThread]: Using profiles.yml file at C:\Users\Marisa\.dbt\profiles.yml
[0m15:51:37.311639 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Marisa\Desktop\projeto_health_insights\dbt_project.yml
[0m15:51:37.312643 [info ] [MainThread]: Configuration:
[0m15:51:38.602202 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m15:51:38.634089 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m15:51:38.635088 [info ] [MainThread]: Required dependencies:
[0m15:51:38.636079 [debug] [MainThread]: Executing "git --help"
[0m15:51:38.687015 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:51:38.687015 [debug] [MainThread]: STDERR: "b''"
[0m15:51:38.687015 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m15:51:38.687015 [info ] [MainThread]: Connection:
[0m15:51:38.687015 [info ] [MainThread]:   host: dbc-605a8848-8c13.cloud.databricks.com
[0m15:51:38.687015 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
[0m15:51:38.687015 [info ] [MainThread]:   catalog: main
[0m15:51:38.687015 [info ] [MainThread]:   schema: default
[0m15:51:38.687015 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m15:51:38.687015 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m15:51:38.687015 [debug] [MainThread]: Using databricks connection "debug"
[0m15:51:38.699632 [debug] [MainThread]: On debug: select 1 as id
[0m15:51:38.699632 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:51:39.748316 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07876-8bb2-185b-9334-1ffcb7cca2f4
[0m15:51:49.228276 [debug] [MainThread]: SQL status: OK in 10.529999732971191 seconds
[0m15:51:49.556312 [debug] [MainThread]: On debug: Close
[0m15:51:49.556312 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07876-8bb2-185b-9334-1ffcb7cca2f4
[0m15:51:49.773711 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m15:51:49.789166 [info ] [MainThread]: [32mAll checks passed![0m
[0m15:51:49.789166 [debug] [MainThread]: Command `dbt debug` succeeded at 15:51:49.789166 after 12.51 seconds
[0m15:51:49.789166 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m15:51:49.789166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A227DABE50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A23178FA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A221BABCD0>]}
[0m15:51:49.789166 [debug] [MainThread]: Flushing usage events
[0m15:53:21.724324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020745F89750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020745CD7750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020746065910>]}


============================== 15:53:21.724324 | ee62f23f-2e2d-4c9c-8007-94aaad8a4de9 ==============================
[0m15:53:21.724324 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:53:21.724324 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:53:23.027542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ee62f23f-2e2d-4c9c-8007-94aaad8a4de9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020745CBCA50>]}
[0m15:53:23.045641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ee62f23f-2e2d-4c9c-8007-94aaad8a4de9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020745CBCA50>]}
[0m15:53:23.045641 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m15:53:23.063702 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m15:53:23.065696 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:53:23.067692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ee62f23f-2e2d-4c9c-8007-94aaad8a4de9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002074F752FD0>]}
[0m15:53:23.634590 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_table_default" in the project
  "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros\materializations\models\table\table.sql
      - macros\materializations\models\table.sql
[0m15:53:23.634590 [debug] [MainThread]: Command `dbt compile` failed at 15:53:23.634590 after 1.93 seconds
[0m15:53:23.634590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020745D54710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020745CD4990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020745CD6E90>]}
[0m15:53:23.634590 [debug] [MainThread]: Flushing usage events
[0m15:54:19.273344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002007CE332D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002007ABF7750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002007CE99510>]}


============================== 15:54:19.289864 | 88f97800-e7e2-4304-b96f-f47174a0d38c ==============================
[0m15:54:19.289864 [info ] [MainThread]: Running with dbt=1.5.2
[0m15:54:19.289864 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:54:19.313576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '88f97800-e7e2-4304-b96f-f47174a0d38c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002007CAFE5D0>]}
[0m15:54:19.324095 [debug] [MainThread]: Command `dbt clean` succeeded at 15:54:19.324095 after 0.06 seconds
[0m15:54:19.324095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002007CB11CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002007692BCD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002007CD825D0>]}
[0m15:54:19.324095 [debug] [MainThread]: Flushing usage events
[0m16:05:56.687679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A1686A090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A16BBCC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A168A08D0>]}


============================== 16:05:56.703301 | 75138e22-407d-4e1f-b170-b7dffa1e88fd ==============================
[0m16:05:56.703301 [info ] [MainThread]: Running with dbt=1.5.2
[0m16:05:56.703301 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:05:57.955875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '75138e22-407d-4e1f-b170-b7dffa1e88fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A16849110>]}
[0m16:05:57.971496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '75138e22-407d-4e1f-b170-b7dffa1e88fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A16849110>]}
[0m16:05:57.971496 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m16:05:57.993528 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m16:05:57.993528 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:05:57.993528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '75138e22-407d-4e1f-b170-b7dffa1e88fd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A20392B90>]}
[0m16:05:58.549293 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_table_default" in the project
  "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros\materializations\models\table\table.sql
      - macros\materializations\models\table.sql
[0m16:05:58.549293 [debug] [MainThread]: Command `dbt compile` failed at 16:05:58.549293 after 1.87 seconds
[0m16:05:58.549293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A16868F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A171658D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A168FB7D0>]}
[0m16:05:58.549293 [debug] [MainThread]: Flushing usage events
[0m16:13:42.380354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BCE7DBF950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BCE7ADEED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BCE7D94C10>]}


============================== 16:13:42.380354 | b9723e72-343b-4e17-b26a-1f910aecbee5 ==============================
[0m16:13:42.380354 [info ] [MainThread]: Running with dbt=1.5.2
[0m16:13:42.380354 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:13:43.685062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9723e72-343b-4e17-b26a-1f910aecbee5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BCE83EA9D0>]}
[0m16:13:43.699672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9723e72-343b-4e17-b26a-1f910aecbee5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BCE83EA9D0>]}
[0m16:13:43.699672 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m16:13:43.722379 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m16:13:43.722379 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:13:43.722379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b9723e72-343b-4e17-b26a-1f910aecbee5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BCF25B4210>]}
[0m16:13:44.278256 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_view_default" in the project
  "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros\materializations\models\view\view.sql
      - macros\materializations\models\view.sql
[0m16:13:44.278256 [debug] [MainThread]: Command `dbt compile` failed at 16:13:44.278256 after 1.92 seconds
[0m16:13:44.278256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BCE7AC4ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BCE7B7B5D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BCE84106D0>]}
[0m16:13:44.278256 [debug] [MainThread]: Flushing usage events
[0m16:15:19.386326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218CB08B050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218CA77EB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218CAB26850>]}


============================== 16:15:19.401712 | fa9500da-34d0-4e6b-bcc8-f976a7d1d93e ==============================
[0m16:15:19.401712 [info ] [MainThread]: Running with dbt=1.5.2
[0m16:15:19.402558 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:15:20.704742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fa9500da-34d0-4e6b-bcc8-f976a7d1d93e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218CB08C390>]}
[0m16:15:20.720362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fa9500da-34d0-4e6b-bcc8-f976a7d1d93e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218CB08C390>]}
[0m16:15:20.720362 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m16:15:20.735954 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m16:15:20.735954 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:15:20.735954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fa9500da-34d0-4e6b-bcc8-f976a7d1d93e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218D52478D0>]}
[0m16:15:21.448160 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "drop_relation" in the project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros\relations\drop.sql
      - macros\adapters\relation.sql
[0m16:15:21.448160 [debug] [MainThread]: Command `dbt compile` failed at 16:15:21.448160 after 2.07 seconds
[0m16:15:21.448160 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218CA4AF210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218CA76E8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218CA76FCD0>]}
[0m16:15:21.448160 [debug] [MainThread]: Flushing usage events
[0m16:16:27.764139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231ACC0B210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231ACCB1ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231AAA37290>]}


============================== 16:16:27.764139 | bf679c54-8ea4-4556-8fb9-d585f1acd04f ==============================
[0m16:16:27.764139 [info ] [MainThread]: Running with dbt=1.5.2
[0m16:16:27.779760 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:16:29.088520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bf679c54-8ea4-4556-8fb9-d585f1acd04f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231ACBC2A50>]}
[0m16:16:29.104171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bf679c54-8ea4-4556-8fb9-d585f1acd04f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231B63C7050>]}
[0m16:16:29.104171 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m16:16:29.127672 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m16:16:29.127672 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:16:29.127672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bf679c54-8ea4-4556-8fb9-d585f1acd04f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231B6448690>]}
[0m16:16:29.773112 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "get_table_columns_and_constraints" in the project
  "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros\relations\column\columns_spec_ddl.sql
      - macros\materializations\models\table\columns_spec_ddl.sql
[0m16:16:29.774605 [debug] [MainThread]: Command `dbt compile` failed at 16:16:29.774605 after 2.02 seconds
[0m16:16:29.774605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231AA3CA5D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231AC91EC90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231ACBDF290>]}
[0m16:16:29.774605 [debug] [MainThread]: Flushing usage events
[0m16:16:52.561252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205E7F7B2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205E7EA9D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205E87C7E90>]}


============================== 16:16:52.561252 | 0e69138b-9903-4708-90a1-96f44dfbc5b9 ==============================
[0m16:16:52.561252 [info ] [MainThread]: Running with dbt=1.5.2
[0m16:16:52.561252 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:16:53.868652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0e69138b-9903-4708-90a1-96f44dfbc5b9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205E8276410>]}
[0m16:16:53.899895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0e69138b-9903-4708-90a1-96f44dfbc5b9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205E8276410>]}
[0m16:16:53.899895 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m16:16:53.918870 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m16:16:53.920865 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:16:53.922127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0e69138b-9903-4708-90a1-96f44dfbc5b9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205F1977190>]}
[0m16:16:54.397225 [error] [MainThread]: Encountered an error:
Compilation Error
  In dispatch: No macro named 'test_unique' found
      Searched for: 'projeto_health_insights.databricks__test_unique', 'projeto_health_insights.spark__test_unique', 'projeto_health_insights.default__test_unique', 'dbt.databricks__test_unique', 'dbt.spark__test_unique', 'dbt.default__test_unique'
[0m16:16:54.397225 [debug] [MainThread]: Command `dbt compile` failed at 16:16:54.397225 after 1.87 seconds
[0m16:16:54.397225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205E8440E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205F19C4E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205E1F5CA90>]}
[0m16:16:54.397225 [debug] [MainThread]: Flushing usage events
[0m16:19:14.158656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002077A439210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002077A45EDD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020777EFAF10>]}


============================== 16:19:14.162779 | 9280db9d-6e3d-4312-b9d3-ff43f67329d4 ==============================
[0m16:19:14.162779 [info ] [MainThread]: Running with dbt=1.5.2
[0m16:19:14.165010 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:19:15.448567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9280db9d-6e3d-4312-b9d3-ff43f67329d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002077AD66410>]}
[0m16:19:15.468059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9280db9d-6e3d-4312-b9d3-ff43f67329d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002077AD66410>]}
[0m16:19:15.468974 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m16:19:15.485465 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m16:19:15.488408 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:19:15.488408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9280db9d-6e3d-4312-b9d3-ff43f67329d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020703FD6E90>]}
[0m16:19:16.110362 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "get_table_columns_and_constraints" in the project
  "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros\relations\column\columns_spec_ddl.sql
      - macros\materializations\models\table\columns_spec_ddl.sql
[0m16:19:16.110362 [debug] [MainThread]: Command `dbt compile` failed at 16:19:16.110362 after 1.98 seconds
[0m16:19:16.110362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002077AD65BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020705040110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002077A490AD0>]}
[0m16:19:16.110362 [debug] [MainThread]: Flushing usage events
[0m16:21:15.554704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D191B32D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D1946DFD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D195142D0>]}


============================== 16:21:15.554704 | 1bb1768b-4d9c-4d57-9aec-8b4ad35bef26 ==============================
[0m16:21:15.554704 [info ] [MainThread]: Running with dbt=1.5.2
[0m16:21:15.554704 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:21:16.874051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1bb1768b-4d9c-4d57-9aec-8b4ad35bef26', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D19709F90>]}
[0m16:21:16.889698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1bb1768b-4d9c-4d57-9aec-8b4ad35bef26', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D19709F90>]}
[0m16:21:16.889698 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m16:21:16.910811 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m16:21:16.910811 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:21:16.910811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1bb1768b-4d9c-4d57-9aec-8b4ad35bef26', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D20671ED0>]}
[0m16:21:17.533028 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "get_create_table_as_sql" in the project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros\relations\table\create.sql
      - macros\materializations\models\table\create_table_as.sql
[0m16:21:17.533028 [debug] [MainThread]: Command `dbt compile` failed at 16:21:17.533028 after 2.00 seconds
[0m16:21:17.533028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D19682550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D19D85A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D19492450>]}
[0m16:21:17.533028 [debug] [MainThread]: Flushing usage events
[0m16:22:19.900038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252CD192290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252CD177750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252CD7B7590>]}


============================== 16:22:19.905029 | 53993967-3bce-4bfb-84e1-1a5f5030a3bd ==============================
[0m16:22:19.905029 [info ] [MainThread]: Running with dbt=1.5.2
[0m16:22:19.905999 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:22:21.157163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '53993967-3bce-4bfb-84e1-1a5f5030a3bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252CD7CFED0>]}
[0m16:22:21.172785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '53993967-3bce-4bfb-84e1-1a5f5030a3bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252CD7CFED0>]}
[0m16:22:21.172785 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m16:22:21.188378 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m16:22:21.204000 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:22:21.205492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '53993967-3bce-4bfb-84e1-1a5f5030a3bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252D79A5090>]}
[0m16:22:22.396769 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.projeto_health_insights
[0m16:22:22.396769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '53993967-3bce-4bfb-84e1-1a5f5030a3bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252D7A22B90>]}
[0m16:22:22.413670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '53993967-3bce-4bfb-84e1-1a5f5030a3bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252D7985310>]}
[0m16:22:22.414667 [info ] [MainThread]: Found 0 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m16:22:22.415538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '53993967-3bce-4bfb-84e1-1a5f5030a3bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252C9ED4690>]}
[0m16:22:22.416550 [info ] [MainThread]: 
[0m16:22:22.417565 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m16:22:22.419558 [debug] [MainThread]: Command end result
[0m16:22:22.429503 [debug] [MainThread]: Command `dbt compile` succeeded at 16:22:22.428507 after 2.55 seconds
[0m16:22:22.429503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252CD7B6750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252D7A21090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252C6D8BD10>]}
[0m16:22:22.430910 [debug] [MainThread]: Flushing usage events
[0m16:36:23.739107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5B9FF32D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5BA571E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5BA571510>]}


============================== 16:36:23.743096 | b86e9416-8258-4f69-9acb-34a51c274047 ==============================
[0m16:36:23.743096 [info ] [MainThread]: Running with dbt=1.5.2
[0m16:36:23.744779 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:36:25.111854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b86e9416-8258-4f69-9acb-34a51c274047', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5BA603ED0>]}
[0m16:36:25.129448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b86e9416-8258-4f69-9acb-34a51c274047', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5BA603ED0>]}
[0m16:36:25.129448 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m16:36:25.152540 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m16:36:25.263215 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:36:25.264214 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:36:25.264214 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.projeto_health_insights
[0m16:36:25.269582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b86e9416-8258-4f69-9acb-34a51c274047', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5C4EB1390>]}
[0m16:36:25.279329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b86e9416-8258-4f69-9acb-34a51c274047', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5C4DA6ED0>]}
[0m16:36:25.281564 [info ] [MainThread]: Found 0 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m16:36:25.282562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b86e9416-8258-4f69-9acb-34a51c274047', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5C133C510>]}
[0m16:36:25.282562 [warn ] [MainThread]: The selection criterion 'stg_hello_world' does not match any nodes
[0m16:36:25.283738 [info ] [MainThread]: 
[0m16:36:25.285430 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m16:36:25.286474 [debug] [MainThread]: Command end result
[0m16:36:25.295124 [debug] [MainThread]: Command `dbt run` succeeded at 16:36:25.295124 after 1.58 seconds
[0m16:36:25.296156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5BABA79D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5BA3767D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5BA2CC2D0>]}
[0m16:36:25.296156 [debug] [MainThread]: Flushing usage events
[0m16:52:15.659843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C525532D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C501A0890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C528C2F10>]}


============================== 16:52:15.659843 | b657c909-9c57-42c7-92ad-0c17ee36bd1d ==============================
[0m16:52:15.659843 [info ] [MainThread]: Running with dbt=1.5.2
[0m16:52:15.659843 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:52:17.034554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b657c909-9c57-42c7-92ad-0c17ee36bd1d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C5251A990>]}
[0m16:52:17.054422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b657c909-9c57-42c7-92ad-0c17ee36bd1d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C5251A990>]}
[0m16:52:17.054422 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m16:52:17.074679 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m16:52:17.169680 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:52:17.169680 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:52:17.169680 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.projeto_health_insights
[0m16:52:17.169680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b657c909-9c57-42c7-92ad-0c17ee36bd1d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C5D1481D0>]}
[0m16:52:17.188636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b657c909-9c57-42c7-92ad-0c17ee36bd1d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C5CF87650>]}
[0m16:52:17.188820 [info ] [MainThread]: Found 0 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics, 0 groups
[0m16:52:17.190815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b657c909-9c57-42c7-92ad-0c17ee36bd1d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C5923A410>]}
[0m16:52:17.191804 [warn ] [MainThread]: The selection criterion 'stg_hello_world' does not match any nodes
[0m16:52:17.191804 [info ] [MainThread]: 
[0m16:52:17.191804 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m16:52:17.191804 [debug] [MainThread]: Command end result
[0m16:52:17.207479 [debug] [MainThread]: Command `dbt run` succeeded at 16:52:17.206482 after 1.56 seconds
[0m16:52:17.207479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C5253D190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C527CA690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C528E6750>]}
[0m16:52:17.209475 [debug] [MainThread]: Flushing usage events
[0m17:23:01.118564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204879E0D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204879E0F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204879AD9D0>]}


============================== 17:23:01.128763 | cb036177-8670-4adf-9e99-014886c523cd ==============================
[0m17:23:01.128763 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:23:01.128763 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:23:02.558554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cb036177-8670-4adf-9e99-014886c523cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020487996B90>]}
[0m17:23:02.581760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cb036177-8670-4adf-9e99-014886c523cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020487996B90>]}
[0m17:23:02.581760 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m17:23:02.602957 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:23:02.753040 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 7 files added, 0 files changed.
[0m17:23:02.753040 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\staging\stg_atendimento.sql
[0m17:23:02.753040 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\marts\dim_doenca.sql
[0m17:23:02.753040 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\staging\stg_doenca.sql
[0m17:23:02.753040 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\marts\fato_atendimento_hospitalar.sql
[0m17:23:02.753040 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\marts\dim_localidade.sql
[0m17:23:02.753040 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\staging\stg_localidade.sql
[0m17:23:02.758657 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\intermediate\int_atendimento.sql
[0m17:23:02.789078 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_atendimento.sql
[0m17:23:02.798909 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_doenca.sql
[0m17:23:02.808900 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_doenca.sql
[0m17:23:02.808900 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m17:23:02.818547 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m17:23:02.818547 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_localidade.sql
[0m17:23:02.828559 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_atendimento.sql
[0m17:23:02.828559 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.projeto_health_insights.stg_atendimento' (models\staging\stg_atendimento.sql) depends on a source named 'raw.atendimento' which was not found
[0m17:23:02.828559 [debug] [MainThread]: Command `dbt run` failed at 17:23:02.828559 after 1.73 seconds
[0m17:23:02.828559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020487A4F090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204882B2090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020487CA8490>]}
[0m17:23:02.828559 [debug] [MainThread]: Flushing usage events
[0m17:28:26.891435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A074A8DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A077AE790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A074F32D0>]}


============================== 17:28:26.891435 | b270f242-1fde-42e5-a24c-bd2caf0dd1ca ==============================
[0m17:28:26.891435 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:28:26.899542 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:28:28.301174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b270f242-1fde-42e5-a24c-bd2caf0dd1ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A07817490>]}
[0m17:28:28.321986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b270f242-1fde-42e5-a24c-bd2caf0dd1ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A07817490>]}
[0m17:28:28.322983 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m17:28:28.342095 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:28:28.448729 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 8 files added, 0 files changed.
[0m17:28:28.449334 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\staging\stg_localidade.sql
[0m17:28:28.449334 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\marts\fato_atendimento_hospitalar.sql
[0m17:28:28.449334 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\intermediate\int_atendimento.sql
[0m17:28:28.449334 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\marts\dim_doenca.sql
[0m17:28:28.449334 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\staging\stg_doenca.sql
[0m17:28:28.449334 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\marts\dim_localidade.sql
[0m17:28:28.449334 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\sources\raw.yaml
[0m17:28:28.449334 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\staging\stg_atendimento.sql
[0m17:28:28.479368 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_localidade.sql
[0m17:28:28.489135 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m17:28:28.499214 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_atendimento.sql
[0m17:28:28.499214 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_doenca.sql
[0m17:28:28.499214 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_doenca.sql
[0m17:28:28.510153 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m17:28:28.510153 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_atendimento.sql
[0m17:28:28.609516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b270f242-1fde-42e5-a24c-bd2caf0dd1ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A06873310>]}
[0m17:28:28.619355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b270f242-1fde-42e5-a24c-bd2caf0dd1ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A121C1550>]}
[0m17:28:28.619355 [info ] [MainThread]: Found 7 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 5 sources, 0 exposures, 0 metrics, 0 groups
[0m17:28:28.619355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b270f242-1fde-42e5-a24c-bd2caf0dd1ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A0E28BB50>]}
[0m17:28:28.619355 [info ] [MainThread]: 
[0m17:28:28.629191 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:28:28.629191 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main_default'
[0m17:28:28.629191 [debug] [ThreadPool]: Using databricks connection "list_main_default"
[0m17:28:28.629191 [debug] [ThreadPool]: On list_main_default: GetTables(database=main, schema=default, identifier=None)
[0m17:28:28.629191 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:28:29.809728 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07884-12b3-1291-8664-da94cb793a5c
[0m17:28:30.029625 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetTables'), ('session-id', b'\x01\xf0x\x84\x12\xb3\x12\x91\x86d\xda\x94\xcby:\\'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.21016860008239746/900.0')])
[0m17:28:35.229508 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetTables'), ('session-id', b'\x01\xf0x\x84\x12\xb3\x12\x91\x86d\xda\x94\xcby:\\'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '2/30'), ('elapsed-seconds', '5.410051345825195/900.0')])
[0m17:28:42.709664 [debug] [ThreadPool]: SQL status: OK in 14.079999923706055 seconds
[0m17:28:42.739514 [debug] [ThreadPool]: On list_main_default: Close
[0m17:28:42.739514 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07884-12b3-1291-8664-da94cb793a5c
[0m17:28:42.969599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b270f242-1fde-42e5-a24c-bd2caf0dd1ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A12060CD0>]}
[0m17:28:42.969599 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:28:42.979586 [info ] [MainThread]: 
[0m17:28:43.009168 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m17:28:43.012447 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m17:28:43.012447 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m17:28:43.019508 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m17:28:43.019508 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 17:28:43.012447 => 17:28:43.019508
[0m17:28:43.025656 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m17:28:43.025656 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 17:28:43.025656 => 17:28:43.025656
[0m17:28:43.025656 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m17:28:43.029173 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m17:28:43.029173 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m17:28:43.029173 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m17:28:43.029173 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m17:28:43.029173 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 17:28:43.029173 => 17:28:43.029173
[0m17:28:43.029173 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m17:28:43.029173 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 17:28:43.029173 => 17:28:43.029173
[0m17:28:43.029173 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m17:28:43.029173 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m17:28:43.039676 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m17:28:43.039676 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m17:28:43.043582 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m17:28:43.045378 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 17:28:43.039676 => 17:28:43.045378
[0m17:28:43.046378 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m17:28:43.047375 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 17:28:43.046378 => 17:28:43.046378
[0m17:28:43.048372 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m17:28:43.049369 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m17:28:43.050369 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.dim_doenca)
[0m17:28:43.051393 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m17:28:43.054384 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m17:28:43.056350 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 17:28:43.051393 => 17:28:43.055376
[0m17:28:43.056350 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m17:28:43.057377 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 17:28:43.057377 => 17:28:43.057377
[0m17:28:43.059394 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m17:28:43.060337 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m17:28:43.061221 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m17:28:43.062248 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m17:28:43.065240 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m17:28:43.066232 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 17:28:43.062248 => 17:28:43.066232
[0m17:28:43.067230 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m17:28:43.068233 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 17:28:43.067230 => 17:28:43.067230
[0m17:28:43.069231 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m17:28:43.069231 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m17:28:43.070360 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m17:28:43.071360 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m17:28:43.074943 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m17:28:43.076967 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 17:28:43.071360 => 17:28:43.076967
[0m17:28:43.077958 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m17:28:43.078961 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 17:28:43.077958 => 17:28:43.077958
[0m17:28:43.079978 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m17:28:43.081040 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m17:28:43.082063 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m17:28:43.082063 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m17:28:43.085057 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m17:28:43.086889 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 17:28:43.083060 => 17:28:43.086889
[0m17:28:43.087890 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m17:28:43.088887 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 17:28:43.087890 => 17:28:43.087890
[0m17:28:43.089884 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m17:28:43.090902 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:28:43.091906 [debug] [MainThread]: Connection 'list_main_default' was properly closed.
[0m17:28:43.091906 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_atendimento_hospitalar' was properly closed.
[0m17:28:43.092973 [debug] [MainThread]: Command end result
[0m17:28:43.099578 [debug] [MainThread]: Command `dbt compile` succeeded at 17:28:43.099578 after 16.23 seconds
[0m17:28:43.099578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A07DC9310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A07759C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021A0154FF90>]}
[0m17:28:43.099578 [debug] [MainThread]: Flushing usage events
[0m17:30:41.029571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277FEEEDED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277FEFBB150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277FECCF750>]}


============================== 17:30:41.036513 | 069fe26f-e883-4c34-8fb7-53038a12c37b ==============================
[0m17:30:41.036513 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:30:41.037486 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:30:42.349522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '069fe26f-e883-4c34-8fb7-53038a12c37b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277FF5DC990>]}
[0m17:30:42.369181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '069fe26f-e883-4c34-8fb7-53038a12c37b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277FF5DC990>]}
[0m17:30:42.379253 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m17:30:42.389581 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:30:42.504324 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:30:42.504324 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:30:42.509389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '069fe26f-e883-4c34-8fb7-53038a12c37b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002778996F590>]}
[0m17:30:42.519480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '069fe26f-e883-4c34-8fb7-53038a12c37b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277888C9110>]}
[0m17:30:42.519480 [info ] [MainThread]: Found 7 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 5 sources, 0 exposures, 0 metrics, 0 groups
[0m17:30:42.519480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '069fe26f-e883-4c34-8fb7-53038a12c37b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277FBCE4810>]}
[0m17:30:42.519480 [info ] [MainThread]: 
[0m17:30:42.519480 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:30:42.531780 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m17:30:42.531780 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m17:30:42.532803 [debug] [ThreadPool]: On list_main: GetSchemas(database=`main`, schema=None)
[0m17:30:42.533776 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:30:43.379527 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07884-6265-12f3-9a33-37285dea52e6
[0m17:30:43.809651 [debug] [ThreadPool]: SQL status: OK in 1.2799999713897705 seconds
[0m17:30:43.819384 [debug] [ThreadPool]: On list_main: Close
[0m17:30:43.819384 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07884-6265-12f3-9a33-37285dea52e6
[0m17:30:44.062446 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_main, now create_main_default)
[0m17:30:44.064840 [debug] [ThreadPool]: Creating schema "database: "main"
schema: "default"
"
[0m17:30:44.079586 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:30:44.079586 [debug] [ThreadPool]: Using databricks connection "create_main_default"
[0m17:30:44.079586 [debug] [ThreadPool]: On create_main_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_main_default"} */
create schema if not exists `main`.`default`
  
[0m17:30:44.079586 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:30:44.927493 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07884-634e-1e4c-bcc1-bceb3f8b64a8
[0m17:30:47.409792 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_main_default"} */
create schema if not exists `main`.`default`
  
[0m17:30:47.409792 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m17:30:47.409792 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createSchemaProto$1(ManagedCatalogClientImpl.scala:901)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.createSchemaProto(ManagedCatalogClientImpl.scala:874)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createSchema$3(ManagedCatalogClientImpl.scala:932)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.createSchema(ManagedCatalogClientImpl.scala:923)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.createSchemaInUnityCatalog(ManagedCatalogCommon.scala:623)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.$anonfun$createSchema$1(ManagedCatalogCommon.scala:536)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.withSchemaCacheInvalidated(ManagedCatalogCommon.scala:942)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.withSchemaCacheInvalidated(ManagedCatalogCommon.scala:934)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.createSchema(ManagedCatalogCommon.scala:528)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createSchema$1(ProfiledManagedCatalog.scala:133)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createSchema(ProfiledManagedCatalog.scala:133)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createDatabase(ManagedCatalogSessionCatalog.scala:766)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.$anonfun$createNamespace$1(UnityCatalogV2Proxy.scala:182)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.$anonfun$createNamespace$1$adapted(UnityCatalogV2Proxy.scala:168)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.assertSingleNamespace(UnityCatalogV2Proxy.scala:131)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createNamespace(UnityCatalogV2Proxy.scala:168)
	at org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:49)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:658)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createSchemaProto$1(ManagedCatalogClientImpl.scala:901)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.createSchemaProto(ManagedCatalogClientImpl.scala:874)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createSchema$3(ManagedCatalogClientImpl.scala:932)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.createSchema(ManagedCatalogClientImpl.scala:923)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.createSchemaInUnityCatalog(ManagedCatalogCommon.scala:623)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.$anonfun$createSchema$1(ManagedCatalogCommon.scala:536)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.withSchemaCacheInvalidated(ManagedCatalogCommon.scala:942)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.withSchemaCacheInvalidated(ManagedCatalogCommon.scala:934)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.createSchema(ManagedCatalogCommon.scala:528)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createSchema$1(ProfiledManagedCatalog.scala:133)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createSchema(ProfiledManagedCatalog.scala:133)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createDatabase(ManagedCatalogSessionCatalog.scala:766)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.$anonfun$createNamespace$1(UnityCatalogV2Proxy.scala:182)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.$anonfun$createNamespace$1$adapted(UnityCatalogV2Proxy.scala:168)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.assertSingleNamespace(UnityCatalogV2Proxy.scala:131)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createNamespace(UnityCatalogV2Proxy.scala:168)
		at org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:49)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 51 more

[0m17:30:47.409792 [debug] [ThreadPool]: Databricks adapter: operation-id: 01f07884-637e-13ea-8e38-c04032162c7e
[0m17:30:47.409792 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro create_schema
[0m17:30:47.409792 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m17:30:47.409792 [debug] [ThreadPool]: On create_main_default: ROLLBACK
[0m17:30:47.409792 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:30:47.409792 [debug] [ThreadPool]: On create_main_default: Close
[0m17:30:47.419298 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07884-634e-1e4c-bcc1-bceb3f8b64a8
[0m17:30:47.659615 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:30:47.659615 [debug] [MainThread]: Connection 'create_main_default' was properly closed.
[0m17:30:47.659615 [info ] [MainThread]: 
[0m17:30:47.669277 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 5.14 seconds (5.14s).
[0m17:30:47.669277 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    [NO_SUCH_CATALOG_EXCEPTION] Catalog 'main' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m17:30:47.672738 [debug] [MainThread]: Command `dbt run` failed at 17:30:47.672738 after 6.66 seconds
[0m17:30:47.672738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277FEF8EF10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277FEF8EB50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277FECB9BD0>]}
[0m17:30:47.673764 [debug] [MainThread]: Flushing usage events
[0m17:40:10.447210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273D2259E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273D2461250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273D1FB0550>]}


============================== 17:40:10.447210 | bab0bc4e-b656-467d-9ca8-0bae597feeb8 ==============================
[0m17:40:10.447210 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:40:10.447210 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:40:10.447210 [info ] [MainThread]: dbt version: 1.5.2
[0m17:40:10.447210 [info ] [MainThread]: python version: 3.11.8
[0m17:40:10.447210 [info ] [MainThread]: python path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Scripts\python.exe
[0m17:40:10.447210 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m17:40:10.447210 [info ] [MainThread]: Using profiles.yml file at C:\Users\Marisa\.dbt\profiles.yml
[0m17:40:10.447210 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Marisa\Desktop\projeto_health_insights\dbt_project.yml
[0m17:40:10.461134 [info ] [MainThread]: Configuration:
[0m17:40:11.791089 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m17:40:11.818456 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m17:40:11.818456 [info ] [MainThread]: Required dependencies:
[0m17:40:11.818456 [debug] [MainThread]: Executing "git --help"
[0m17:40:11.873699 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:40:11.874806 [debug] [MainThread]: STDERR: "b''"
[0m17:40:11.874806 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m17:40:11.874806 [info ] [MainThread]: Connection:
[0m17:40:11.874806 [info ] [MainThread]:   host: dbc-605a8848-8c13.cloud.databricks.com
[0m17:40:11.874806 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
[0m17:40:11.874806 [info ] [MainThread]:   catalog: catalog
[0m17:40:11.874806 [info ] [MainThread]:   schema: default
[0m17:40:11.874806 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m17:40:11.874806 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m17:40:11.874806 [debug] [MainThread]: Using databricks connection "debug"
[0m17:40:11.874806 [debug] [MainThread]: On debug: select 1 as id
[0m17:40:11.883988 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:40:12.823909 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07885-b5cf-14d5-afbe-409ff989048b
[0m17:40:14.429832 [debug] [MainThread]: SQL status: OK in 2.559999942779541 seconds
[0m17:40:14.429832 [debug] [MainThread]: On debug: Close
[0m17:40:14.429832 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07885-b5cf-14d5-afbe-409ff989048b
[0m17:40:14.682386 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m17:40:14.684409 [info ] [MainThread]: [32mAll checks passed![0m
[0m17:40:14.686405 [debug] [MainThread]: Command `dbt debug` succeeded at 17:40:14.686405 after 4.26 seconds
[0m17:40:14.688043 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m17:40:14.689595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273D2327250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273DBAA42D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273CC0BC9D0>]}
[0m17:40:14.690517 [debug] [MainThread]: Flushing usage events
[0m17:40:33.551605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BE35B5350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BE36A9810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BE0D9AC50>]}


============================== 17:40:33.551605 | ebf431c4-3cef-47f8-a2d0-283a895ec173 ==============================
[0m17:40:33.551605 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:40:33.551605 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:40:34.853267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ebf431c4-3cef-47f8-a2d0-283a895ec173', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BE3BF9E10>]}
[0m17:40:34.872343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ebf431c4-3cef-47f8-a2d0-283a895ec173', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BE3BF9E10>]}
[0m17:40:34.873341 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m17:40:34.891946 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:40:34.897105 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m17:40:34.897105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ebf431c4-3cef-47f8-a2d0-283a895ec173', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BEDF1C950>]}
[0m17:40:36.144894 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_atendimento.sql
[0m17:40:36.159863 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_doenca.sql
[0m17:40:36.165291 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m17:40:36.169278 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m17:40:36.173243 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_atendimento.sql
[0m17:40:36.178494 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_doenca.sql
[0m17:40:36.182485 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_localidade.sql
[0m17:40:36.262920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ebf431c4-3cef-47f8-a2d0-283a895ec173', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BECDE6490>]}
[0m17:40:36.272887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ebf431c4-3cef-47f8-a2d0-283a895ec173', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BEDF2BC10>]}
[0m17:40:36.273863 [info ] [MainThread]: Found 7 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 5 sources, 0 exposures, 0 metrics, 0 groups
[0m17:40:36.274529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ebf431c4-3cef-47f8-a2d0-283a895ec173', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BEA39EB10>]}
[0m17:40:36.276448 [info ] [MainThread]: 
[0m17:40:36.276448 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:40:36.276448 [debug] [ThreadPool]: Acquiring new databricks connection 'list_catalog'
[0m17:40:36.276448 [debug] [ThreadPool]: Using databricks connection "list_catalog"
[0m17:40:36.276448 [debug] [ThreadPool]: On list_catalog: GetSchemas(database=`catalog`, schema=None)
[0m17:40:36.276448 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:40:37.082497 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07885-c447-144b-ba8d-d13d86725476
[0m17:40:37.642165 [debug] [ThreadPool]: SQL status: OK in 1.3700000047683716 seconds
[0m17:40:37.645628 [debug] [ThreadPool]: On list_catalog: Close
[0m17:40:37.645628 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07885-c447-144b-ba8d-d13d86725476
[0m17:40:37.868879 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_catalog, now create_catalog_default)
[0m17:40:37.868879 [debug] [ThreadPool]: Creating schema "database: "catalog"
schema: "default"
"
[0m17:40:37.884486 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:40:37.884486 [debug] [ThreadPool]: Using databricks connection "create_catalog_default"
[0m17:40:37.884486 [debug] [ThreadPool]: On create_catalog_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_catalog_default"} */
create schema if not exists `catalog`.`default`
  
[0m17:40:37.884486 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:40:38.709580 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07885-c53c-1374-aa04-815299d4a35d
[0m17:40:39.547927 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_catalog_default"} */
create schema if not exists `catalog`.`default`
  
[0m17:40:39.547927 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'catalog' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m17:40:39.547927 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [NO_SUCH_CATALOG_EXCEPTION] org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'catalog' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchCatalogException: [NO_SUCH_CATALOG_EXCEPTION] Catalog 'catalog' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createSchemaProto$1(ManagedCatalogClientImpl.scala:901)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.createSchemaProto(ManagedCatalogClientImpl.scala:874)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createSchema$3(ManagedCatalogClientImpl.scala:932)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.createSchema(ManagedCatalogClientImpl.scala:923)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.createSchemaInUnityCatalog(ManagedCatalogCommon.scala:623)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.$anonfun$createSchema$1(ManagedCatalogCommon.scala:536)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.withSchemaCacheInvalidated(ManagedCatalogCommon.scala:942)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.withSchemaCacheInvalidated(ManagedCatalogCommon.scala:934)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.createSchema(ManagedCatalogCommon.scala:528)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createSchema$1(ProfiledManagedCatalog.scala:133)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createSchema(ProfiledManagedCatalog.scala:133)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createDatabase(ManagedCatalogSessionCatalog.scala:766)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.$anonfun$createNamespace$1(UnityCatalogV2Proxy.scala:182)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.$anonfun$createNamespace$1$adapted(UnityCatalogV2Proxy.scala:168)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.assertSingleNamespace(UnityCatalogV2Proxy.scala:131)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createNamespace(UnityCatalogV2Proxy.scala:168)
	at org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:49)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:658)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createSchemaProto$1(ManagedCatalogClientImpl.scala:901)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.createSchemaProto(ManagedCatalogClientImpl.scala:874)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$createSchema$3(ManagedCatalogClientImpl.scala:932)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$2(ManagedCatalogClientImpl.scala:7398)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.$anonfun$recordAndWrapExceptionBase$1(ManagedCatalogClientImpl.scala:7397)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:66)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.createSchema(ManagedCatalogClientImpl.scala:923)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.createSchemaInUnityCatalog(ManagedCatalogCommon.scala:623)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.$anonfun$createSchema$1(ManagedCatalogCommon.scala:536)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.withSchemaCacheInvalidated(ManagedCatalogCommon.scala:942)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.withSchemaCacheInvalidated(ManagedCatalogCommon.scala:934)
		at com.databricks.sql.managedcatalog.ManagedCatalogCommon.createSchema(ManagedCatalogCommon.scala:528)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createSchema$1(ProfiledManagedCatalog.scala:133)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createSchema(ProfiledManagedCatalog.scala:133)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createDatabase(ManagedCatalogSessionCatalog.scala:766)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.$anonfun$createNamespace$1(UnityCatalogV2Proxy.scala:182)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.$anonfun$createNamespace$1$adapted(UnityCatalogV2Proxy.scala:168)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.assertSingleNamespace(UnityCatalogV2Proxy.scala:131)
		at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.createNamespace(UnityCatalogV2Proxy.scala:168)
		at org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:49)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 51 more

[0m17:40:39.547927 [debug] [ThreadPool]: Databricks adapter: operation-id: 01f07885-c55e-1fcf-908e-f8a986666e99
[0m17:40:39.547927 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro create_schema
[0m17:40:39.547927 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  [NO_SUCH_CATALOG_EXCEPTION] Catalog 'catalog' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m17:40:39.547927 [debug] [ThreadPool]: On create_catalog_default: ROLLBACK
[0m17:40:39.547927 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:40:39.547927 [debug] [ThreadPool]: On create_catalog_default: Close
[0m17:40:39.547927 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07885-c53c-1374-aa04-815299d4a35d
[0m17:40:39.787379 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:40:39.787379 [debug] [MainThread]: Connection 'create_catalog_default' was properly closed.
[0m17:40:39.787379 [info ] [MainThread]: 
[0m17:40:39.787379 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 3.51 seconds (3.51s).
[0m17:40:39.787379 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    [NO_SUCH_CATALOG_EXCEPTION] Catalog 'catalog' was not found. Please verify the catalog name and then retry the query or command again. SQLSTATE: 42704
[0m17:40:39.787379 [debug] [MainThread]: Command `dbt run` failed at 17:40:39.787379 after 6.26 seconds
[0m17:40:39.787379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BE32D8890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BE3BF4390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BE3BF4DD0>]}
[0m17:40:39.787379 [debug] [MainThread]: Flushing usage events
[0m17:42:15.531073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A2D92850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A2C41090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A2B68E50>]}


============================== 17:42:15.531073 | bd99da50-43b9-47c4-bf8f-c4e33869cfb0 ==============================
[0m17:42:15.531073 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:42:15.531073 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:42:16.880789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bd99da50-43b9-47c4-bf8f-c4e33869cfb0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A348AED0>]}
[0m17:42:16.900910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bd99da50-43b9-47c4-bf8f-c4e33869cfb0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AC623010>]}
[0m17:42:16.900910 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m17:42:16.925470 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:42:16.951607 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m17:42:16.953602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bd99da50-43b9-47c4-bf8f-c4e33869cfb0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AD7B05D0>]}
[0m17:42:18.200603 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_atendimento.sql
[0m17:42:18.215597 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_doenca.sql
[0m17:42:18.220667 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m17:42:18.225672 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m17:42:18.230714 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_atendimento.sql
[0m17:42:18.235719 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_doenca.sql
[0m17:42:18.235719 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_localidade.sql
[0m17:42:18.310684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bd99da50-43b9-47c4-bf8f-c4e33869cfb0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AC6544D0>]}
[0m17:42:18.325777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bd99da50-43b9-47c4-bf8f-c4e33869cfb0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AD738C10>]}
[0m17:42:18.325777 [info ] [MainThread]: Found 7 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 5 sources, 0 exposures, 0 metrics, 0 groups
[0m17:42:18.325777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd99da50-43b9-47c4-bf8f-c4e33869cfb0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002409FB946D0>]}
[0m17:42:18.330889 [info ] [MainThread]: 
[0m17:42:18.330889 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:42:18.330889 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:42:18.330889 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:42:18.330889 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m17:42:18.330889 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:42:19.185228 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07886-0120-1e8e-9a7e-805ca98d9127
[0m17:42:19.510787 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m17:42:19.510787 [debug] [ThreadPool]: On list_workspace: Close
[0m17:42:19.510787 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07886-0120-1e8e-9a7e-805ca98d9127
[0m17:42:19.771043 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m17:42:19.781028 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m17:42:19.790666 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:42:19.790666 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m17:42:19.790666 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m17:42:19.790666 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:42:20.595630 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07886-01f9-164d-8be7-786c1b62ff3e
[0m17:42:21.125165 [debug] [ThreadPool]: SQL status: OK in 1.3300000429153442 seconds
[0m17:42:21.126163 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m17:42:21.126163 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m17:42:21.127160 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:42:21.128157 [debug] [ThreadPool]: On create_workspace_default: Close
[0m17:42:21.128157 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07886-01f9-164d-8be7-786c1b62ff3e
[0m17:42:21.360868 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m17:42:21.370662 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m17:42:21.370662 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m17:42:21.370662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:42:22.201035 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07886-02e8-147b-8d0b-a1adbdc12336
[0m17:42:22.659587 [debug] [ThreadPool]: SQL status: OK in 1.2899999618530273 seconds
[0m17:42:22.665633 [debug] [ThreadPool]: On list_workspace_default: Close
[0m17:42:22.666631 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07886-02e8-147b-8d0b-a1adbdc12336
[0m17:42:22.900765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd99da50-43b9-47c4-bf8f-c4e33869cfb0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002409FB946D0>]}
[0m17:42:22.903407 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:42:22.904441 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:42:22.908045 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:42:22.910009 [info ] [MainThread]: 
[0m17:42:22.920687 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m17:42:22.921714 [info ] [Thread-1 (]: 1 of 7 START sql view model default.stg_atendimento ............................ [RUN]
[0m17:42:22.924662 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m17:42:22.924662 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m17:42:22.930768 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m17:42:22.930768 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 17:42:22.924662 => 17:42:22.930768
[0m17:42:22.930768 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m17:42:22.964344 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m17:42:22.972168 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:42:22.973190 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m17:42:22.974540 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    -- stg_atendimento.sql
-- Preparação da tabela de atendimentos hospitalares
select
    id_atendimento,
    id_paciente,
    id_doenca,
    id_localidade,
    data_atendimento,
    procedimento,
    valor_total
from `workspace`.`raw`.`atendimento`

[0m17:42:22.974540 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:42:23.890904 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07886-03ec-17d5-a1b9-87eee9a05700
[0m17:42:24.795682 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    -- stg_atendimento.sql
-- Preparação da tabela de atendimentos hospitalares
select
    id_atendimento,
    id_paciente,
    id_doenca,
    id_localidade,
    data_atendimento,
    procedimento,
    valor_total
from `workspace`.`raw`.`atendimento`

[0m17:42:24.795682 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`atendimento` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 5
[0m17:42:24.795682 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`atendimento` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`atendimento` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more

[0m17:42:24.795682 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07886-0411-1db1-bbb1-fad7d43f8297
[0m17:42:24.795682 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 17:42:22.930768 => 17:42:24.795682
[0m17:42:24.795682 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m17:42:24.800720 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m17:42:24.800720 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m17:42:24.800720 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07886-03ec-17d5-a1b9-87eee9a05700
[0m17:42:25.061762 [debug] [Thread-1 (]: Runtime Error in model stg_atendimento (models\staging\stg_atendimento.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`atendimento` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 5
[0m17:42:25.063466 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd99da50-43b9-47c4-bf8f-c4e33869cfb0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AD831C10>]}
[0m17:42:25.064466 [error] [Thread-1 (]: 1 of 7 ERROR creating sql view model default.stg_atendimento ................... [[31mERROR[0m in 2.14s]
[0m17:42:25.065455 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m17:42:25.066455 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m17:42:25.067452 [info ] [Thread-1 (]: 2 of 7 START sql view model default.stg_doenca ................................. [RUN]
[0m17:42:25.067942 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m17:42:25.068966 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m17:42:25.073929 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m17:42:25.075617 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 17:42:25.068966 => 17:42:25.075268
[0m17:42:25.076265 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m17:42:25.080893 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m17:42:25.080893 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:42:25.080893 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m17:42:25.080893 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    -- stg_doenca.sql
-- Preparação da tabela de doenças (CID-10)
select
    id_doenca,
    nome_doenca,
    codigo_cid
from `workspace`.`raw`.`doenca`

[0m17:42:25.080893 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:42:25.910659 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07886-0521-12fd-8cc5-a14c1e75ac4e
[0m17:42:26.300667 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    -- stg_doenca.sql
-- Preparação da tabela de doenças (CID-10)
select
    id_doenca,
    nome_doenca,
    codigo_cid
from `workspace`.`raw`.`doenca`

[0m17:42:26.305679 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`doenca` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5
[0m17:42:26.305679 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`doenca` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`doenca` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more

[0m17:42:26.305679 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07886-0547-190f-aa4c-66fc75e0f7f6
[0m17:42:26.305679 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 17:42:25.076265 => 17:42:26.305679
[0m17:42:26.305679 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m17:42:26.305679 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m17:42:26.310724 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m17:42:26.310724 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07886-0521-12fd-8cc5-a14c1e75ac4e
[0m17:42:26.550620 [debug] [Thread-1 (]: Runtime Error in model stg_doenca (models\staging\stg_doenca.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`doenca` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5
[0m17:42:26.550620 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd99da50-43b9-47c4-bf8f-c4e33869cfb0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AD7808D0>]}
[0m17:42:26.550620 [error] [Thread-1 (]: 2 of 7 ERROR creating sql view model default.stg_doenca ........................ [[31mERROR[0m in 1.48s]
[0m17:42:26.550620 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m17:42:26.550620 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m17:42:26.550620 [info ] [Thread-1 (]: 3 of 7 START sql view model default.stg_localidade ............................. [RUN]
[0m17:42:26.550620 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m17:42:26.550620 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m17:42:26.563862 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m17:42:26.565411 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 17:42:26.550620 => 17:42:26.565411
[0m17:42:26.566409 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m17:42:26.571049 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m17:42:26.571049 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:42:26.571049 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m17:42:26.571049 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    -- stg_localidade.sql
-- Preparação da tabela de localidade (município, estado)
select
    id_localidade,
    municipio,
    estado,
    regiao
from `workspace`.`raw`.`localidade`

[0m17:42:26.571049 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:42:27.350986 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07886-0600-14a9-a1d6-305e41125769
[0m17:42:27.760645 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    -- stg_localidade.sql
-- Preparação da tabela de localidade (município, estado)
select
    id_localidade,
    municipio,
    estado,
    regiao
from `workspace`.`raw`.`localidade`

[0m17:42:27.760645 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`localidade` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 13 pos 5
[0m17:42:27.765759 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`localidade` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 13 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`localidade` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 13 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more

[0m17:42:27.765759 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07886-0621-13e0-bd76-d451e26dc573
[0m17:42:27.765759 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 17:42:26.566409 => 17:42:27.765759
[0m17:42:27.765759 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m17:42:27.765759 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m17:42:27.765759 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m17:42:27.770817 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07886-0600-14a9-a1d6-305e41125769
[0m17:42:28.010696 [debug] [Thread-1 (]: Runtime Error in model stg_localidade (models\staging\stg_localidade.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`localidade` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 13 pos 5
[0m17:42:28.010696 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd99da50-43b9-47c4-bf8f-c4e33869cfb0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AD782650>]}
[0m17:42:28.010696 [error] [Thread-1 (]: 3 of 7 ERROR creating sql view model default.stg_localidade .................... [[31mERROR[0m in 1.46s]
[0m17:42:28.010696 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m17:42:28.010696 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m17:42:28.010696 [info ] [Thread-1 (]: 4 of 7 SKIP relation default.dim_doenca ........................................ [[33mSKIP[0m]
[0m17:42:28.010696 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m17:42:28.010696 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m17:42:28.010696 [info ] [Thread-1 (]: 5 of 7 SKIP relation default.dim_localidade .................................... [[33mSKIP[0m]
[0m17:42:28.010696 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m17:42:28.010696 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m17:42:28.020557 [info ] [Thread-1 (]: 6 of 7 SKIP relation default.int_atendimento ................................... [[33mSKIP[0m]
[0m17:42:28.021538 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m17:42:28.023756 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m17:42:28.024756 [info ] [Thread-1 (]: 7 of 7 SKIP relation default.fato_atendimento_hospitalar ....................... [[33mSKIP[0m]
[0m17:42:28.025752 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m17:42:28.028530 [debug] [MainThread]: On master: ROLLBACK
[0m17:42:28.028530 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:42:28.860762 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07886-06e6-1ce4-9956-7d3ac85e95ff
[0m17:42:28.860762 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:42:28.860762 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:42:28.860762 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:42:28.860762 [debug] [MainThread]: On master: ROLLBACK
[0m17:42:28.868887 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:42:28.868887 [debug] [MainThread]: On master: Close
[0m17:42:28.872632 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07886-06e6-1ce4-9956-7d3ac85e95ff
[0m17:42:29.110930 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:42:29.110930 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m17:42:29.110930 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m17:42:29.110930 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_localidade' was properly closed.
[0m17:42:29.110930 [info ] [MainThread]: 
[0m17:42:29.115938 [info ] [MainThread]: Finished running 7 view models in 0 hours 0 minutes and 10.78 seconds (10.78s).
[0m17:42:29.115938 [debug] [MainThread]: Command end result
[0m17:42:29.121029 [info ] [MainThread]: 
[0m17:42:29.121029 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:42:29.121029 [info ] [MainThread]: 
[0m17:42:29.130810 [error] [MainThread]: [33mRuntime Error in model stg_atendimento (models\staging\stg_atendimento.sql)[0m
[0m17:42:29.132201 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`atendimento` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m17:42:29.133396 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m17:42:29.135193 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 16 pos 5
[0m17:42:29.135193 [info ] [MainThread]: 
[0m17:42:29.135193 [error] [MainThread]: [33mRuntime Error in model stg_doenca (models\staging\stg_doenca.sql)[0m
[0m17:42:29.135193 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`doenca` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m17:42:29.135193 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m17:42:29.135193 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5
[0m17:42:29.140754 [info ] [MainThread]: 
[0m17:42:29.140754 [error] [MainThread]: [33mRuntime Error in model stg_localidade (models\staging\stg_localidade.sql)[0m
[0m17:42:29.140754 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`localidade` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m17:42:29.140754 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m17:42:29.146019 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 13 pos 5
[0m17:42:29.147016 [info ] [MainThread]: 
[0m17:42:29.148014 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=4 TOTAL=7
[0m17:42:29.149476 [debug] [MainThread]: Command `dbt run` failed at 17:42:29.149476 after 13.64 seconds
[0m17:42:29.150496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A2D92650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A2C280D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A2E3FB90>]}
[0m17:42:29.150496 [debug] [MainThread]: Flushing usage events
[0m17:51:40.470949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015097FA2990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001509A790590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001509AAF3150>]}


============================== 17:51:40.470949 | 610e471b-f164-4bd6-ac70-d1a47f027fc4 ==============================
[0m17:51:40.470949 [info ] [MainThread]: Running with dbt=1.5.2
[0m17:51:40.470949 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m17:51:41.840505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '610e471b-f164-4bd6-ac70-d1a47f027fc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001509B083ED0>]}
[0m17:51:41.860658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '610e471b-f164-4bd6-ac70-d1a47f027fc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001509A76F1D0>]}
[0m17:51:41.860658 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m17:51:41.881265 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m17:51:41.990660 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:51:41.990660 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:51:41.995694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '610e471b-f164-4bd6-ac70-d1a47f027fc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150A1864E90>]}
[0m17:51:42.000811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '610e471b-f164-4bd6-ac70-d1a47f027fc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150A427AFD0>]}
[0m17:51:42.000811 [info ] [MainThread]: Found 7 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 5 sources, 0 exposures, 0 metrics, 0 groups
[0m17:51:42.010776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '610e471b-f164-4bd6-ac70-d1a47f027fc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150A1829CD0>]}
[0m17:51:42.010776 [info ] [MainThread]: 
[0m17:51:42.010776 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:51:42.020432 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m17:51:42.021905 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m17:51:42.022978 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m17:51:42.022978 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:51:42.921266 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07887-5123-11e9-818c-fa3543ec583d
[0m17:51:43.540940 [debug] [ThreadPool]: SQL status: OK in 1.5199999809265137 seconds
[0m17:51:43.540940 [debug] [ThreadPool]: On list_workspace: Close
[0m17:51:43.540940 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07887-5123-11e9-818c-fa3543ec583d
[0m17:51:43.770916 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m17:51:43.770916 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m17:51:43.788876 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:51:43.790972 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m17:51:43.790972 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m17:51:43.790972 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:51:44.710792 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07887-5231-1a4a-a195-41ebe366c4ba
[0m17:51:45.200944 [debug] [ThreadPool]: SQL status: OK in 1.409999966621399 seconds
[0m17:51:45.200944 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m17:51:45.200944 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m17:51:45.200944 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:51:45.200944 [debug] [ThreadPool]: On create_workspace_default: Close
[0m17:51:45.200944 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07887-5231-1a4a-a195-41ebe366c4ba
[0m17:51:45.450790 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m17:51:45.520869 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m17:51:45.520869 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m17:51:45.520869 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:51:46.316108 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07887-5326-1e4f-aaef-7faf3bca1afc
[0m17:51:46.671019 [debug] [ThreadPool]: SQL status: OK in 1.149999976158142 seconds
[0m17:51:46.676025 [debug] [ThreadPool]: On list_workspace_default: Close
[0m17:51:46.676025 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07887-5326-1e4f-aaef-7faf3bca1afc
[0m17:51:46.901213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '610e471b-f164-4bd6-ac70-d1a47f027fc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015097774810>]}
[0m17:51:46.910974 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:51:46.910974 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:51:46.910974 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:51:46.910974 [info ] [MainThread]: 
[0m17:51:46.921090 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m17:51:46.921090 [info ] [Thread-1 (]: 1 of 7 START sql view model default.stg_atendimento ............................ [RUN]
[0m17:51:46.921090 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m17:51:46.925018 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m17:51:46.929221 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m17:51:46.931216 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 17:51:46.925018 => 17:51:46.930246
[0m17:51:46.931216 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m17:51:46.961046 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m17:51:46.967192 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:51:46.967770 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m17:51:46.967770 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    -- stg_atendimento.sql
-- Preparação da tabela de atendimentos hospitalares
select
    id_atendimento,
    id_paciente,
    id_doenca,
    id_localidade,
    data_atendimento,
    procedimento,
    valor_total
from `workspace`.`raw`.`atendimento`

[0m17:51:46.967770 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:51:47.781308 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07887-540a-1239-b85a-cd2d5f409866
[0m17:51:49.669500 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    -- stg_atendimento.sql
-- Preparação da tabela de atendimentos hospitalares
select
    id_atendimento,
    id_paciente,
    id_doenca,
    id_localidade,
    data_atendimento,
    procedimento,
    valor_total
from `workspace`.`raw`.`atendimento`

[0m17:51:49.670664 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_atendimento` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `procedimento`, `paciente`, `id`]. SQLSTATE: 42703; line 9 pos 4
[0m17:51:49.670664 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_atendimento` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `procedimento`, `paciente`, `id`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_atendimento` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `procedimento`, `paciente`, `id`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more

[0m17:51:49.670664 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07887-542c-125a-86d9-1a98adbd5576
[0m17:51:49.670664 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 17:51:46.932256 => 17:51:49.670664
[0m17:51:49.670664 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m17:51:49.670664 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m17:51:49.670664 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m17:51:49.670664 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07887-540a-1239-b85a-cd2d5f409866
[0m17:51:49.911134 [debug] [Thread-1 (]: Runtime Error in model stg_atendimento (models\staging\stg_atendimento.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_atendimento` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `procedimento`, `paciente`, `id`]. SQLSTATE: 42703; line 9 pos 4
[0m17:51:49.911134 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '610e471b-f164-4bd6-ac70-d1a47f027fc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015096B3FA10>]}
[0m17:51:49.911134 [error] [Thread-1 (]: 1 of 7 ERROR creating sql view model default.stg_atendimento ................... [[31mERROR[0m in 2.99s]
[0m17:51:49.920992 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m17:51:49.920992 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m17:51:49.920992 [info ] [Thread-1 (]: 2 of 7 START sql view model default.stg_doenca ................................. [RUN]
[0m17:51:49.920992 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m17:51:49.920992 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m17:51:49.920992 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m17:51:49.920992 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 17:51:49.920992 => 17:51:49.920992
[0m17:51:49.931039 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m17:51:49.937487 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m17:51:49.938046 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:51:49.938046 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m17:51:49.938046 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    -- stg_doenca.sql
-- Preparação da tabela de doenças (CID-10)
select
    id_doenca,
    nome_doenca,
    codigo_cid
from `workspace`.`raw`.`doenca`

[0m17:51:49.941131 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:51:50.761037 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07887-55ce-190c-aad7-73573bc30ea7
[0m17:51:51.160908 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    -- stg_doenca.sql
-- Preparação da tabela de doenças (CID-10)
select
    id_doenca,
    nome_doenca,
    codigo_cid
from `workspace`.`raw`.`doenca`

[0m17:51:51.160908 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_doenca` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 9 pos 4
[0m17:51:51.165920 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_doenca` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_doenca` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more

[0m17:51:51.165920 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07887-55f1-112a-b8bc-4376b93e0d8b
[0m17:51:51.171024 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 17:51:49.931039 => 17:51:51.171024
[0m17:51:51.171024 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m17:51:51.176034 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m17:51:51.176034 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m17:51:51.176034 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07887-55ce-190c-aad7-73573bc30ea7
[0m17:51:51.419519 [debug] [Thread-1 (]: Runtime Error in model stg_doenca (models\staging\stg_doenca.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_doenca` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 9 pos 4
[0m17:51:51.420488 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '610e471b-f164-4bd6-ac70-d1a47f027fc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150A52D0E10>]}
[0m17:51:51.421392 [error] [Thread-1 (]: 2 of 7 ERROR creating sql view model default.stg_doenca ........................ [[31mERROR[0m in 1.50s]
[0m17:51:51.421392 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m17:51:51.421392 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m17:51:51.421392 [info ] [Thread-1 (]: 3 of 7 START sql view model default.stg_localidade ............................. [RUN]
[0m17:51:51.421392 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m17:51:51.421392 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m17:51:51.421392 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m17:51:51.430889 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 17:51:51.421392 => 17:51:51.421392
[0m17:51:51.430889 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m17:51:51.435894 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m17:51:51.435894 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m17:51:51.437903 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m17:51:51.438809 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    -- stg_localidade.sql
-- Preparação da tabela de localidade (município, estado)
select
    id_localidade,
    municipio,
    estado,
    regiao
from `workspace`.`raw`.`localidade`

[0m17:51:51.439111 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m17:51:52.250990 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07887-56b3-1f2a-b715-8b052d545d44
[0m17:51:52.672796 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    -- stg_localidade.sql
-- Preparação da tabela de localidade (município, estado)
select
    id_localidade,
    municipio,
    estado,
    regiao
from `workspace`.`raw`.`localidade`

[0m17:51:52.672796 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_localidade` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 9 pos 4
[0m17:51:52.672796 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_localidade` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_localidade` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more

[0m17:51:52.672796 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07887-56d5-1531-ae51-5cf494f2fc0b
[0m17:51:52.672796 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 17:51:51.430889 => 17:51:52.672796
[0m17:51:52.672796 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m17:51:52.672796 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m17:51:52.672796 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m17:51:52.672796 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07887-56b3-1f2a-b715-8b052d545d44
[0m17:51:52.920764 [debug] [Thread-1 (]: Runtime Error in model stg_localidade (models\staging\stg_localidade.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_localidade` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 9 pos 4
[0m17:51:52.920764 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '610e471b-f164-4bd6-ac70-d1a47f027fc4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150A529AB50>]}
[0m17:51:52.920764 [error] [Thread-1 (]: 3 of 7 ERROR creating sql view model default.stg_localidade .................... [[31mERROR[0m in 1.50s]
[0m17:51:52.920764 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m17:51:52.920764 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m17:51:52.920764 [info ] [Thread-1 (]: 4 of 7 SKIP relation default.dim_doenca ........................................ [[33mSKIP[0m]
[0m17:51:52.920764 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m17:51:52.920764 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m17:51:52.920764 [info ] [Thread-1 (]: 5 of 7 SKIP relation default.dim_localidade .................................... [[33mSKIP[0m]
[0m17:51:52.930641 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m17:51:52.930641 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m17:51:52.931982 [info ] [Thread-1 (]: 6 of 7 SKIP relation default.int_atendimento ................................... [[33mSKIP[0m]
[0m17:51:52.932980 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m17:51:52.933895 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m17:51:52.934893 [info ] [Thread-1 (]: 7 of 7 SKIP relation default.fato_atendimento_hospitalar ....................... [[33mSKIP[0m]
[0m17:51:52.935891 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m17:51:52.937884 [debug] [MainThread]: On master: ROLLBACK
[0m17:51:52.938882 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:51:53.731002 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07887-5796-18d3-9d1b-1dc7c08f36b4
[0m17:51:53.740977 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:51:53.740977 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:51:53.740977 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:51:53.745989 [debug] [MainThread]: On master: ROLLBACK
[0m17:51:53.745989 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:51:53.745989 [debug] [MainThread]: On master: Close
[0m17:51:53.745989 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07887-5796-18d3-9d1b-1dc7c08f36b4
[0m17:51:53.991536 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:51:53.993169 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m17:51:53.994168 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m17:51:53.996163 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_localidade' was properly closed.
[0m17:51:53.999513 [info ] [MainThread]: 
[0m17:51:54.001473 [info ] [MainThread]: Finished running 7 view models in 0 hours 0 minutes and 11.99 seconds (11.99s).
[0m17:51:54.005464 [debug] [MainThread]: Command end result
[0m17:51:54.028321 [info ] [MainThread]: 
[0m17:51:54.030865 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:51:54.030865 [info ] [MainThread]: 
[0m17:51:54.030865 [error] [MainThread]: [33mRuntime Error in model stg_atendimento (models\staging\stg_atendimento.sql)[0m
[0m17:51:54.030865 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_atendimento` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `procedimento`, `paciente`, `id`]. SQLSTATE: 42703; line 9 pos 4
[0m17:51:54.030865 [info ] [MainThread]: 
[0m17:51:54.030865 [error] [MainThread]: [33mRuntime Error in model stg_doenca (models\staging\stg_doenca.sql)[0m
[0m17:51:54.030865 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_doenca` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 9 pos 4
[0m17:51:54.030865 [info ] [MainThread]: 
[0m17:51:54.030865 [error] [MainThread]: [33mRuntime Error in model stg_localidade (models\staging\stg_localidade.sql)[0m
[0m17:51:54.030865 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_localidade` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 9 pos 4
[0m17:51:54.040668 [info ] [MainThread]: 
[0m17:51:54.041655 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=4 TOTAL=7
[0m17:51:54.043650 [debug] [MainThread]: Command `dbt run` failed at 17:51:54.043650 after 13.59 seconds
[0m17:51:54.044647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001509AD32290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001509A820050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000150A42ACD50>]}
[0m17:51:54.045644 [debug] [MainThread]: Flushing usage events
[0m18:05:49.007021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245BF82FE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245BFB71E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245BFB726D0>]}


============================== 18:05:49.007021 | 202eed42-9858-4f0c-ad98-2b66674631c2 ==============================
[0m18:05:49.007021 [info ] [MainThread]: Running with dbt=1.5.2
[0m18:05:49.007021 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m18:05:50.324452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245C012C4D0>]}
[0m18:05:50.340104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245C012C4D0>]}
[0m18:05:50.340104 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m18:05:50.363786 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m18:05:50.454958 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m18:05:50.454958 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_localidade.sql
[0m18:05:50.454958 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_atendimento.sql
[0m18:05:50.454958 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_doenca.sql
[0m18:05:50.486203 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_localidade.sql
[0m18:05:50.501826 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_atendimento.sql
[0m18:05:50.501826 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_doenca.sql
[0m18:05:50.564316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245BEBCAA90>]}
[0m18:05:50.579933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245CA32E790>]}
[0m18:05:50.579933 [info ] [MainThread]: Found 7 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 5 sources, 0 exposures, 0 metrics, 0 groups
[0m18:05:50.579933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245BC8346D0>]}
[0m18:05:50.579933 [info ] [MainThread]: 
[0m18:05:50.579933 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:05:50.579933 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m18:05:50.579933 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m18:05:50.579933 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m18:05:50.595525 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:05:51.550825 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07889-4ae9-148d-b04f-6bb322c98521
[0m18:05:51.800439 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0x\x89J\xe9\x14\x8d\xb0Ok\xb3"\xc9\x85!'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.24961328506469727/900.0')])
[0m18:05:59.634005 [debug] [ThreadPool]: SQL status: OK in 9.050000190734863 seconds
[0m18:05:59.634005 [debug] [ThreadPool]: On list_workspace: Close
[0m18:05:59.634005 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07889-4ae9-148d-b04f-6bb322c98521
[0m18:05:59.880091 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m18:05:59.883482 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m18:05:59.883482 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m18:05:59.883482 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m18:05:59.883482 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m18:05:59.883482 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:06:00.743617 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07889-506d-1ce1-8815-d06941298a07
[0m18:06:02.528632 [debug] [ThreadPool]: SQL status: OK in 2.6500000953674316 seconds
[0m18:06:02.528632 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m18:06:02.528632 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m18:06:02.528632 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m18:06:02.528632 [debug] [ThreadPool]: On create_workspace_default: Close
[0m18:06:02.528632 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07889-506d-1ce1-8815-d06941298a07
[0m18:06:02.777284 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m18:06:02.792732 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m18:06:02.792732 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m18:06:02.792732 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:06:03.632289 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07889-5224-12bf-8da2-b5f4a425c9ed
[0m18:06:04.161863 [debug] [ThreadPool]: SQL status: OK in 1.3700000047683716 seconds
[0m18:06:04.164827 [debug] [ThreadPool]: On list_workspace_default: Close
[0m18:06:04.164827 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07889-5224-12bf-8da2-b5f4a425c9ed
[0m18:06:04.411138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245BF03D2D0>]}
[0m18:06:04.411138 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:06:04.411138 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:06:04.411138 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:06:04.411138 [info ] [MainThread]: 
[0m18:06:04.426542 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m18:06:04.429022 [info ] [Thread-1 (]: 1 of 7 START sql view model default.stg_atendimento ............................ [RUN]
[0m18:06:04.431607 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m18:06:04.431607 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m18:06:04.436922 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m18:06:04.438873 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 18:06:04.433681 => 18:06:04.437935
[0m18:06:04.438873 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m18:06:04.472314 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m18:06:04.472314 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:06:04.472314 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m18:06:04.472314 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m18:06:04.472314 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:06:05.319343 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07889-5324-1ea4-8fe0-0636e5c1c5b0
[0m18:06:07.229575 [debug] [Thread-1 (]: SQL status: OK in 2.759999990463257 seconds
[0m18:06:07.245022 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 18:06:04.438873 => 18:06:07.245022
[0m18:06:07.245022 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m18:06:07.245022 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:06:07.245022 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m18:06:07.245022 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07889-5324-1ea4-8fe0-0636e5c1c5b0
[0m18:06:07.488780 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245CA3A79D0>]}
[0m18:06:07.488780 [info ] [Thread-1 (]: 1 of 7 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 3.06s]
[0m18:06:07.488780 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m18:06:07.488780 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m18:06:07.488780 [info ] [Thread-1 (]: 2 of 7 START sql view model default.stg_doenca ................................. [RUN]
[0m18:06:07.488780 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m18:06:07.488780 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m18:06:07.488780 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m18:06:07.504408 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 18:06:07.488780 => 18:06:07.504408
[0m18:06:07.504408 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m18:06:07.509954 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m18:06:07.510951 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:06:07.511958 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m18:06:07.511958 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m18:06:07.511958 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:06:08.290354 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07889-54ef-1f3c-b5fb-b128172dff0d
[0m18:06:09.004523 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m18:06:09.004523 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 18:06:07.504408 => 18:06:09.004523
[0m18:06:09.004523 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m18:06:09.004523 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:06:09.004523 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m18:06:09.004523 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07889-54ef-1f3c-b5fb-b128172dff0d
[0m18:06:09.235011 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245CA36E210>]}
[0m18:06:09.235011 [info ] [Thread-1 (]: 2 of 7 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.75s]
[0m18:06:09.235011 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m18:06:09.235011 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m18:06:09.235011 [info ] [Thread-1 (]: 3 of 7 START sql view model default.stg_localidade ............................. [RUN]
[0m18:06:09.235011 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m18:06:09.235011 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m18:06:09.235011 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m18:06:09.250446 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 18:06:09.235011 => 18:06:09.250446
[0m18:06:09.250446 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m18:06:09.257211 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m18:06:09.259207 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:06:09.260050 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m18:06:09.260050 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m18:06:09.260050 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:06:10.086907 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07889-5600-12e0-aac8-1a1482c2e08a
[0m18:06:10.707571 [debug] [Thread-1 (]: SQL status: OK in 1.4500000476837158 seconds
[0m18:06:10.707571 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 18:06:09.250446 => 18:06:10.707571
[0m18:06:10.707571 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m18:06:10.707571 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:06:10.707571 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m18:06:10.707571 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07889-5600-12e0-aac8-1a1482c2e08a
[0m18:06:10.935510 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245CA445350>]}
[0m18:06:10.935510 [info ] [Thread-1 (]: 3 of 7 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.70s]
[0m18:06:10.935510 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m18:06:10.935510 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m18:06:10.950912 [info ] [Thread-1 (]: 4 of 7 START sql view model default.dim_doenca ................................. [RUN]
[0m18:06:10.950912 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.dim_doenca)
[0m18:06:10.950912 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m18:06:10.950912 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m18:06:10.950912 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 18:06:10.950912 => 18:06:10.950912
[0m18:06:10.950912 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m18:06:10.950912 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m18:06:10.966537 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:06:10.966537 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m18:06:10.968845 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- dim_doenca.sql
select distinct
    codigo_cid,
    nome_doenca
from `workspace`.`default`.`stg_doenca`

[0m18:06:10.970130 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:06:11.756588 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07889-5700-1c12-bedd-d71ce3223c63
[0m18:06:12.207091 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- dim_doenca.sql
select distinct
    codigo_cid,
    nome_doenca
from `workspace`.`default`.`stg_doenca`

[0m18:06:12.207091 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `codigo_cid` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 8 pos 4
[0m18:06:12.207091 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `codigo_cid` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 8 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `codigo_cid` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 8 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more

[0m18:06:12.207091 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07889-5722-1193-9a09-3ca92de95081
[0m18:06:12.207091 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 18:06:10.950912 => 18:06:12.207091
[0m18:06:12.207091 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m18:06:12.222473 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:06:12.222473 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m18:06:12.222473 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07889-5700-1c12-bedd-d71ce3223c63
[0m18:06:12.471378 [debug] [Thread-1 (]: Runtime Error in model dim_doenca (models\marts\dim_doenca.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `codigo_cid` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 8 pos 4
[0m18:06:12.471378 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245CA40D6D0>]}
[0m18:06:12.471378 [error] [Thread-1 (]: 4 of 7 ERROR creating sql view model default.dim_doenca ........................ [[31mERROR[0m in 1.52s]
[0m18:06:12.471378 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m18:06:12.471378 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m18:06:12.471378 [info ] [Thread-1 (]: 5 of 7 START sql view model default.dim_localidade ............................. [RUN]
[0m18:06:12.471378 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m18:06:12.471378 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m18:06:12.486985 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m18:06:12.490503 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 18:06:12.471378 => 18:06:12.490110
[0m18:06:12.491163 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m18:06:12.499052 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m18:06:12.500050 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:06:12.501068 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m18:06:12.501068 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- dim_localidade.sql
select distinct
    id_localidade,
    municipio,
    estado,
    regiao
from `workspace`.`default`.`stg_localidade`

[0m18:06:12.502072 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:06:13.336152 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07889-57eb-1fcc-b7a9-f90ce2512863
[0m18:06:13.696880 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- dim_localidade.sql
select distinct
    id_localidade,
    municipio,
    estado,
    regiao
from `workspace`.`default`.`stg_localidade`

[0m18:06:13.696880 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_localidade` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 8 pos 4
[0m18:06:13.696880 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_localidade` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 8 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_localidade` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 8 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more

[0m18:06:13.696880 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07889-5811-149f-9d61-dae4b9bddb79
[0m18:06:13.696880 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 18:06:12.491511 => 18:06:13.696880
[0m18:06:13.696880 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m18:06:13.696880 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:06:13.696880 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m18:06:13.696880 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07889-57eb-1fcc-b7a9-f90ce2512863
[0m18:06:13.938444 [debug] [Thread-1 (]: Runtime Error in model dim_localidade (models\marts\dim_localidade.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_localidade` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 8 pos 4
[0m18:06:13.938444 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245CA574110>]}
[0m18:06:13.938444 [error] [Thread-1 (]: 5 of 7 ERROR creating sql view model default.dim_localidade .................... [[31mERROR[0m in 1.47s]
[0m18:06:13.938444 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m18:06:13.938444 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m18:06:13.938444 [info ] [Thread-1 (]: 6 of 7 START sql view model default.int_atendimento ............................ [RUN]
[0m18:06:13.938444 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m18:06:13.938444 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m18:06:13.957037 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m18:06:13.959392 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 18:06:13.938444 => 18:06:13.958390
[0m18:06:13.959392 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m18:06:13.964980 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m18:06:13.967488 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:06:13.968485 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m18:06:13.968485 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- int_atendimento.sql
-- Transformações intermediárias para análise
select
    a.id_atendimento,
    a.id_paciente,
    d.nome_doenca,
    d.codigo_cid,
    l.municipio,
    l.estado,
    a.data_atendimento,
    a.procedimento,
    a.valor_total
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d on a.id_doenca = d.id_doenca
left join `workspace`.`default`.`stg_localidade` l on a.id_localidade = l.id_localidade

[0m18:06:13.969485 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:06:14.746893 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07889-58c9-1341-81b1-1a7750da118c
[0m18:06:15.176473 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- int_atendimento.sql
-- Transformações intermediárias para análise
select
    a.id_atendimento,
    a.id_paciente,
    d.nome_doenca,
    d.codigo_cid,
    l.municipio,
    l.estado,
    a.data_atendimento,
    a.procedimento,
    a.valor_total
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d on a.id_doenca = d.id_doenca
left join `workspace`.`default`.`stg_localidade` l on a.id_localidade = l.id_localidade

[0m18:06:15.176473 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`id_doenca` cannot be resolved. Did you mean one of the following? [`a`.`id`, `d`.`id`, `a`.`paciente`, `d`.`paciente`, `a`.`procedimento`]. SQLSTATE: 42703; line 19 pos 50
[0m18:06:15.176473 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`id_doenca` cannot be resolved. Did you mean one of the following? [`a`.`id`, `d`.`id`, `a`.`paciente`, `d`.`paciente`, `a`.`procedimento`]. SQLSTATE: 42703; line 19 pos 50
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`id_doenca` cannot be resolved. Did you mean one of the following? [`a`.`id`, `d`.`id`, `a`.`paciente`, `d`.`paciente`, `a`.`procedimento`]. SQLSTATE: 42703; line 19 pos 50
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more

[0m18:06:15.192094 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07889-58ea-130e-94c5-cbddc4dbde07
[0m18:06:15.192094 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 18:06:13.960387 => 18:06:15.192094
[0m18:06:15.192094 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m18:06:15.192094 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:06:15.192094 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m18:06:15.192094 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07889-58c9-1341-81b1-1a7750da118c
[0m18:06:15.455798 [debug] [Thread-1 (]: Runtime Error in model int_atendimento (models\intermediate\int_atendimento.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`id_doenca` cannot be resolved. Did you mean one of the following? [`a`.`id`, `d`.`id`, `a`.`paciente`, `d`.`paciente`, `a`.`procedimento`]. SQLSTATE: 42703; line 19 pos 50
[0m18:06:15.455798 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '202eed42-9858-4f0c-ad98-2b66674631c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245CA566FD0>]}
[0m18:06:15.455798 [error] [Thread-1 (]: 6 of 7 ERROR creating sql view model default.int_atendimento ................... [[31mERROR[0m in 1.52s]
[0m18:06:15.455798 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m18:06:15.455798 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:06:15.455798 [info ] [Thread-1 (]: 7 of 7 SKIP relation default.fato_atendimento_hospitalar ....................... [[33mSKIP[0m]
[0m18:06:15.455798 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:06:15.455798 [debug] [MainThread]: On master: ROLLBACK
[0m18:06:15.455798 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:06:16.285981 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07889-59b3-19c1-9ec1-2c5ded298d69
[0m18:06:16.285981 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:06:16.285981 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:06:16.285981 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:06:16.285981 [debug] [MainThread]: On master: ROLLBACK
[0m18:06:16.285981 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:06:16.285981 [debug] [MainThread]: On master: Close
[0m18:06:16.285981 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07889-59b3-19c1-9ec1-2c5ded298d69
[0m18:06:16.543984 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:06:16.545978 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m18:06:16.546972 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m18:06:16.546972 [debug] [MainThread]: Connection 'model.projeto_health_insights.int_atendimento' was properly closed.
[0m18:06:16.547969 [info ] [MainThread]: 
[0m18:06:16.548968 [info ] [MainThread]: Finished running 7 view models in 0 hours 0 minutes and 25.97 seconds (25.97s).
[0m18:06:16.551202 [debug] [MainThread]: Command end result
[0m18:06:16.561146 [info ] [MainThread]: 
[0m18:06:16.562316 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m18:06:16.563317 [info ] [MainThread]: 
[0m18:06:16.564313 [error] [MainThread]: [33mRuntime Error in model dim_doenca (models\marts\dim_doenca.sql)[0m
[0m18:06:16.565222 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `codigo_cid` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 8 pos 4
[0m18:06:16.566220 [info ] [MainThread]: 
[0m18:06:16.566399 [error] [MainThread]: [33mRuntime Error in model dim_localidade (models\marts\dim_localidade.sql)[0m
[0m18:06:16.567317 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id_localidade` cannot be resolved. Did you mean one of the following? [`id`, `paciente`, `procedimento`, `data_atendimento`]. SQLSTATE: 42703; line 8 pos 4
[0m18:06:16.568421 [info ] [MainThread]: 
[0m18:06:16.568421 [error] [MainThread]: [33mRuntime Error in model int_atendimento (models\intermediate\int_atendimento.sql)[0m
[0m18:06:16.568421 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`id_doenca` cannot be resolved. Did you mean one of the following? [`a`.`id`, `d`.`id`, `a`.`paciente`, `d`.`paciente`, `a`.`procedimento`]. SQLSTATE: 42703; line 19 pos 50
[0m18:06:16.568421 [info ] [MainThread]: 
[0m18:06:16.568421 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=3 SKIP=1 TOTAL=7
[0m18:06:16.568421 [debug] [MainThread]: Command `dbt run` failed at 18:06:16.568421 after 27.58 seconds
[0m18:06:16.568421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245BF8BF790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245BFB182D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000245BF842550>]}
[0m18:06:16.568421 [debug] [MainThread]: Flushing usage events
[0m18:12:12.484518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF12350ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF1484F090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF1484F190>]}


============================== 18:12:12.493387 | d8072910-e429-43da-a0a3-4f242e3b9e68 ==============================
[0m18:12:12.493387 [info ] [MainThread]: Running with dbt=1.5.2
[0m18:12:12.493387 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m18:12:13.894224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF14B1E6D0>]}
[0m18:12:13.910997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF14B1E6D0>]}
[0m18:12:13.910997 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m18:12:13.926711 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m18:12:14.043565 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m18:12:14.043565 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_atendimento_hospitalar.sql
[0m18:12:14.043565 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\dim_localidade.sql
[0m18:12:14.043565 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\dim_doenca.sql
[0m18:12:14.043565 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\intermediate\int_atendimento.sql
[0m18:12:14.077595 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m18:12:14.096263 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m18:12:14.096263 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_doenca.sql
[0m18:12:14.110870 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_atendimento.sql
[0m18:12:14.175561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF1281F1D0>]}
[0m18:12:14.191182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF1E594950>]}
[0m18:12:14.191182 [info ] [MainThread]: Found 7 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 5 sources, 0 exposures, 0 metrics, 0 groups
[0m18:12:14.191182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF11B24690>]}
[0m18:12:14.191182 [info ] [MainThread]: 
[0m18:12:14.191182 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:12:14.191182 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m18:12:14.191182 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m18:12:14.191182 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m18:12:14.191182 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:12:15.109345 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0788a-2f92-126f-a736-e9bd54571d50
[0m18:12:15.630030 [debug] [ThreadPool]: SQL status: OK in 1.440000057220459 seconds
[0m18:12:15.646032 [debug] [ThreadPool]: On list_workspace: Close
[0m18:12:15.647030 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0788a-2f92-126f-a736-e9bd54571d50
[0m18:12:15.880069 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m18:12:15.880069 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m18:12:15.895641 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m18:12:15.895641 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m18:12:15.895641 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m18:12:15.895641 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:12:16.695322 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0788a-3083-127d-89c3-f125570adfa8
[0m18:12:17.092561 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m18:12:17.092561 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m18:12:17.092561 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m18:12:17.092561 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m18:12:17.092561 [debug] [ThreadPool]: On create_workspace_default: Close
[0m18:12:17.092561 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0788a-3083-127d-89c3-f125570adfa8
[0m18:12:17.336075 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m18:12:17.336075 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m18:12:17.336075 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m18:12:17.336075 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:12:18.159411 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0788a-3163-152c-bee7-b2d56e7e81a7
[0m18:12:18.515585 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m18:12:18.515585 [debug] [ThreadPool]: On list_workspace_default: Close
[0m18:12:18.515585 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0788a-3163-152c-bee7-b2d56e7e81a7
[0m18:12:18.758638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF1F63AB90>]}
[0m18:12:18.758638 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:12:18.758638 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:12:18.758638 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:12:18.758638 [info ] [MainThread]: 
[0m18:12:18.758638 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m18:12:18.758638 [info ] [Thread-1 (]: 1 of 7 START sql view model default.stg_atendimento ............................ [RUN]
[0m18:12:18.758638 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m18:12:18.774219 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m18:12:18.779924 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m18:12:18.780908 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 18:12:18.775067 => 18:12:18.780908
[0m18:12:18.782468 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m18:12:18.814816 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m18:12:18.816198 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:12:18.816198 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m18:12:18.816198 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m18:12:18.816198 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:12:19.622526 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-3242-19f7-b2cd-2328840cf911
[0m18:12:21.407441 [debug] [Thread-1 (]: SQL status: OK in 2.5899999141693115 seconds
[0m18:12:21.423042 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 18:12:18.782468 => 18:12:21.423042
[0m18:12:21.423042 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m18:12:21.423042 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:12:21.423042 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m18:12:21.423042 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-3242-19f7-b2cd-2328840cf911
[0m18:12:21.667340 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF15415AD0>]}
[0m18:12:21.667340 [info ] [Thread-1 (]: 1 of 7 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 2.91s]
[0m18:12:21.667340 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m18:12:21.667340 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m18:12:21.667340 [info ] [Thread-1 (]: 2 of 7 START sql view model default.stg_doenca ................................. [RUN]
[0m18:12:21.682768 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m18:12:21.682768 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m18:12:21.682768 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m18:12:21.682768 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 18:12:21.682768 => 18:12:21.682768
[0m18:12:21.682768 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m18:12:21.702619 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m18:12:21.704613 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:12:21.704613 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m18:12:21.705642 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m18:12:21.705642 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:12:22.490893 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-33fa-158b-af37-7d975caf3a11
[0m18:12:23.195326 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m18:12:23.195326 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 18:12:21.682768 => 18:12:23.195326
[0m18:12:23.195326 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m18:12:23.195326 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:12:23.195326 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m18:12:23.195326 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-33fa-158b-af37-7d975caf3a11
[0m18:12:23.417127 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF1F72DC90>]}
[0m18:12:23.432547 [info ] [Thread-1 (]: 2 of 7 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.73s]
[0m18:12:23.432547 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m18:12:23.432547 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m18:12:23.432547 [info ] [Thread-1 (]: 3 of 7 START sql view model default.stg_localidade ............................. [RUN]
[0m18:12:23.432547 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m18:12:23.448907 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m18:12:23.453851 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m18:12:23.456175 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 18:12:23.449918 => 18:12:23.453851
[0m18:12:23.456175 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m18:12:23.461205 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m18:12:23.461205 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:12:23.461205 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m18:12:23.461205 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m18:12:23.461205 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:12:24.250702 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-3505-1ee4-8f59-f2ec98d270be
[0m18:12:24.954990 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m18:12:24.954990 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 18:12:23.457201 => 18:12:24.954990
[0m18:12:24.954990 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m18:12:24.954990 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:12:24.954990 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m18:12:24.954990 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-3505-1ee4-8f59-f2ec98d270be
[0m18:12:25.196790 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF15415AD0>]}
[0m18:12:25.196790 [info ] [Thread-1 (]: 3 of 7 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.76s]
[0m18:12:25.196790 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m18:12:25.196790 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m18:12:25.196790 [info ] [Thread-1 (]: 4 of 7 START sql view model default.dim_doenca ................................. [RUN]
[0m18:12:25.212188 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.dim_doenca)
[0m18:12:25.212188 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m18:12:25.212188 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m18:12:25.212188 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 18:12:25.212188 => 18:12:25.212188
[0m18:12:25.212188 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m18:12:25.212188 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m18:12:25.212188 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:12:25.212188 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m18:12:25.212188 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m18:12:25.212188 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:12:26.021569 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-3614-1433-af1e-786dccb688b3
[0m18:12:26.603651 [debug] [Thread-1 (]: SQL status: OK in 1.3899999856948853 seconds
[0m18:12:26.619052 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 18:12:25.212188 => 18:12:26.619052
[0m18:12:26.619052 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m18:12:26.619052 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:12:26.619052 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m18:12:26.623159 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-3614-1433-af1e-786dccb688b3
[0m18:12:26.850025 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF1F85CD10>]}
[0m18:12:26.850025 [info ] [Thread-1 (]: 4 of 7 OK created sql view model default.dim_doenca ............................ [[32mOK[0m in 1.64s]
[0m18:12:26.850025 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m18:12:26.850025 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m18:12:26.850025 [info ] [Thread-1 (]: 5 of 7 START sql view model default.dim_localidade ............................. [RUN]
[0m18:12:26.850025 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m18:12:26.850025 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m18:12:26.850025 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m18:12:26.850025 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 18:12:26.850025 => 18:12:26.850025
[0m18:12:26.850025 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m18:12:26.867710 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m18:12:26.869444 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:12:26.869833 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m18:12:26.870522 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m18:12:26.870522 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:12:27.652159 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-370c-1be3-8ab6-a981ec8d07a7
[0m18:12:28.219586 [debug] [Thread-1 (]: SQL status: OK in 1.350000023841858 seconds
[0m18:12:28.219586 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 18:12:26.850025 => 18:12:28.219586
[0m18:12:28.219586 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m18:12:28.219586 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:12:28.219586 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m18:12:28.219586 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-370c-1be3-8ab6-a981ec8d07a7
[0m18:12:28.472573 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF1F6BAFD0>]}
[0m18:12:28.472573 [info ] [Thread-1 (]: 5 of 7 OK created sql view model default.dim_localidade ........................ [[32mOK[0m in 1.62s]
[0m18:12:28.472573 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m18:12:28.472573 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m18:12:28.472573 [info ] [Thread-1 (]: 6 of 7 START sql view model default.int_atendimento ............................ [RUN]
[0m18:12:28.472573 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m18:12:28.472573 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m18:12:28.472573 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m18:12:28.472573 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 18:12:28.472573 => 18:12:28.472573
[0m18:12:28.472573 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m18:12:28.491113 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m18:12:28.493490 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:12:28.493490 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m18:12:28.494477 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m18:12:28.494477 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:12:29.272121 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-3804-19eb-9b67-4edfb283ab54
[0m18:12:29.942806 [debug] [Thread-1 (]: SQL status: OK in 1.4500000476837158 seconds
[0m18:12:29.942806 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 18:12:28.472573 => 18:12:29.942806
[0m18:12:29.942806 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m18:12:29.942806 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:12:29.942806 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m18:12:29.942806 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-3804-19eb-9b67-4edfb283ab54
[0m18:12:30.168797 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF1F85CCD0>]}
[0m18:12:30.168797 [info ] [Thread-1 (]: 6 of 7 OK created sql view model default.int_atendimento ....................... [[32mOK[0m in 1.70s]
[0m18:12:30.168797 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m18:12:30.168797 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:12:30.168797 [info ] [Thread-1 (]: 7 of 7 START sql view model default.fato_atendimento_hospitalar ................ [RUN]
[0m18:12:30.184191 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m18:12:30.187213 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:12:30.194629 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m18:12:30.194629 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 18:12:30.188211 => 18:12:30.194629
[0m18:12:30.194629 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:12:30.194629 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m18:12:30.194629 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:12:30.194629 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m18:12:30.194629 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.doenca_id,
    l.localidade_id
from `workspace`.`default`.`int_atendimento` a
left join `workspace`.`default`.`dim_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`dim_localidade` l
    on a.paciente = l.paciente

[0m18:12:30.194629 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:12:30.997810 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-390a-1493-bd08-d288f00617e8
[0m18:12:31.516599 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.doenca_id,
    l.localidade_id
from `workspace`.`default`.`int_atendimento` a
left join `workspace`.`default`.`dim_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`dim_localidade` l
    on a.paciente = l.paciente

[0m18:12:31.516599 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`id` cannot be resolved. Did you mean one of the following? [`a`.`doenca_id`, `a`.`paciente`, `d`.`doenca_id`, `l`.`paciente`, `d`.`paciente`]. SQLSTATE: 42703; line 8 pos 4
[0m18:12:31.516599 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`id` cannot be resolved. Did you mean one of the following? [`a`.`doenca_id`, `a`.`paciente`, `d`.`doenca_id`, `l`.`paciente`, `d`.`paciente`]. SQLSTATE: 42703; line 8 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`id` cannot be resolved. Did you mean one of the following? [`a`.`doenca_id`, `a`.`paciente`, `d`.`doenca_id`, `l`.`paciente`, `d`.`paciente`]. SQLSTATE: 42703; line 8 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more

[0m18:12:31.516599 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0788a-392b-1ff7-ad7f-65eb51d4dfe6
[0m18:12:31.516599 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 18:12:30.194629 => 18:12:31.516599
[0m18:12:31.516599 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m18:12:31.516599 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:12:31.516599 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m18:12:31.516599 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-390a-1493-bd08-d288f00617e8
[0m18:12:31.771429 [debug] [Thread-1 (]: Runtime Error in model fato_atendimento_hospitalar (models\marts\fato_atendimento_hospitalar.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`id` cannot be resolved. Did you mean one of the following? [`a`.`doenca_id`, `a`.`paciente`, `d`.`doenca_id`, `l`.`paciente`, `d`.`paciente`]. SQLSTATE: 42703; line 8 pos 4
[0m18:12:31.771429 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8072910-e429-43da-a0a3-4f242e3b9e68', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF1F652FD0>]}
[0m18:12:31.771429 [error] [Thread-1 (]: 7 of 7 ERROR creating sql view model default.fato_atendimento_hospitalar ....... [[31mERROR[0m in 1.59s]
[0m18:12:31.771429 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:12:31.771429 [debug] [MainThread]: On master: ROLLBACK
[0m18:12:31.787078 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:12:32.566956 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0788a-39fa-1f9d-b72b-df3fb461bc6a
[0m18:12:32.566956 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:12:32.566956 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:12:32.566956 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:12:32.582393 [debug] [MainThread]: On master: ROLLBACK
[0m18:12:32.582393 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:12:32.582393 [debug] [MainThread]: On master: Close
[0m18:12:32.584610 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0788a-39fa-1f9d-b72b-df3fb461bc6a
[0m18:12:32.823882 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:12:32.823882 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m18:12:32.823882 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m18:12:32.823882 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_atendimento_hospitalar' was properly closed.
[0m18:12:32.823882 [info ] [MainThread]: 
[0m18:12:32.823882 [info ] [MainThread]: Finished running 7 view models in 0 hours 0 minutes and 18.63 seconds (18.63s).
[0m18:12:32.823882 [debug] [MainThread]: Command end result
[0m18:12:32.845408 [info ] [MainThread]: 
[0m18:12:32.846412 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:12:32.847366 [info ] [MainThread]: 
[0m18:12:32.847366 [error] [MainThread]: [33mRuntime Error in model fato_atendimento_hospitalar (models\marts\fato_atendimento_hospitalar.sql)[0m
[0m18:12:32.847366 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `a`.`id` cannot be resolved. Did you mean one of the following? [`a`.`doenca_id`, `a`.`paciente`, `d`.`doenca_id`, `l`.`paciente`, `d`.`paciente`]. SQLSTATE: 42703; line 8 pos 4
[0m18:12:32.850842 [info ] [MainThread]: 
[0m18:12:32.851405 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
[0m18:12:32.852406 [debug] [MainThread]: Command `dbt run` failed at 18:12:32.852406 after 20.38 seconds
[0m18:12:32.853403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF15403D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF14EA3A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0E998910>]}
[0m18:12:32.853403 [debug] [MainThread]: Flushing usage events
[0m18:15:57.948024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197EFF4B2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197F0181490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197F0181510>]}


============================== 18:15:57.963645 | 0224d32a-4f7a-4040-b9f1-0914f1c26360 ==============================
[0m18:15:57.963645 [info ] [MainThread]: Running with dbt=1.5.2
[0m18:15:57.963645 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m18:15:59.241941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197F0180710>]}
[0m18:15:59.257559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197F0180710>]}
[0m18:15:59.257559 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m18:15:59.273181 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m18:15:59.380162 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:15:59.380162 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_atendimento_hospitalar.sql
[0m18:15:59.411408 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m18:15:59.489513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197EFA37F10>]}
[0m18:15:59.489513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197EF9E9110>]}
[0m18:15:59.489513 [info ] [MainThread]: Found 7 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 5 sources, 0 exposures, 0 metrics, 0 groups
[0m18:15:59.505114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197ECEC4690>]}
[0m18:15:59.505114 [info ] [MainThread]: 
[0m18:15:59.505114 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:15:59.505114 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m18:15:59.505114 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m18:15:59.505114 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m18:15:59.505114 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:16:00.325827 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0788a-b5cf-1fe1-b8e6-4c7dd5b05d37
[0m18:16:00.712555 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m18:16:00.728646 [debug] [ThreadPool]: On list_workspace: Close
[0m18:16:00.729026 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0788a-b5cf-1fe1-b8e6-4c7dd5b05d37
[0m18:16:00.957226 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m18:16:00.957226 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m18:16:00.972668 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m18:16:00.972668 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m18:16:00.972668 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m18:16:00.972668 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:16:01.739487 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0788a-b6a6-1aa1-a72e-d050be4646bf
[0m18:16:02.146102 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m18:16:02.146102 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m18:16:02.146102 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m18:16:02.146102 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m18:16:02.146102 [debug] [ThreadPool]: On create_workspace_default: Close
[0m18:16:02.146102 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0788a-b6a6-1aa1-a72e-d050be4646bf
[0m18:16:02.427759 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m18:16:02.427759 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m18:16:02.427759 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m18:16:02.427759 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:16:03.220187 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0788a-b787-172a-9685-486bb66dfbaa
[0m18:16:03.607592 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m18:16:03.611559 [debug] [ThreadPool]: On list_workspace_default: Close
[0m18:16:03.612557 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0788a-b787-172a-9685-486bb66dfbaa
[0m18:16:03.847027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197EF258090>]}
[0m18:16:03.847027 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:16:03.847027 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:16:03.847027 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:16:03.847027 [info ] [MainThread]: 
[0m18:16:03.862659 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m18:16:03.862659 [info ] [Thread-1 (]: 1 of 7 START sql view model default.stg_atendimento ............................ [RUN]
[0m18:16:03.862659 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m18:16:03.862659 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m18:16:03.873966 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m18:16:03.874963 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 18:16:03.870341 => 18:16:03.874963
[0m18:16:03.875960 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m18:16:03.904309 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m18:16:03.904309 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:16:03.904309 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m18:16:03.904309 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m18:16:03.904309 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:16:04.731520 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-b871-1467-9837-1760ca5a14bd
[0m18:16:05.567528 [debug] [Thread-1 (]: SQL status: OK in 1.659999966621399 seconds
[0m18:16:05.583141 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 18:16:03.875960 => 18:16:05.583141
[0m18:16:05.583141 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m18:16:05.583141 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:16:05.583141 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m18:16:05.583141 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-b871-1467-9837-1760ca5a14bd
[0m18:16:05.795481 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197F99B0A50>]}
[0m18:16:05.795481 [info ] [Thread-1 (]: 1 of 7 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.93s]
[0m18:16:05.795481 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m18:16:05.795481 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m18:16:05.810892 [info ] [Thread-1 (]: 2 of 7 START sql view model default.stg_doenca ................................. [RUN]
[0m18:16:05.810892 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m18:16:05.810892 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m18:16:05.810892 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m18:16:05.810892 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 18:16:05.810892 => 18:16:05.810892
[0m18:16:05.810892 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m18:16:05.810892 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m18:16:05.810892 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:16:05.810892 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m18:16:05.810892 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m18:16:05.810892 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:16:06.619905 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-b991-1412-aded-1e971bed73a2
[0m18:16:07.325593 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m18:16:07.325593 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 18:16:05.810892 => 18:16:07.325593
[0m18:16:07.325593 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m18:16:07.325593 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:16:07.325593 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m18:16:07.325593 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-b991-1412-aded-1e971bed73a2
[0m18:16:07.569093 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197EF153B90>]}
[0m18:16:07.569093 [info ] [Thread-1 (]: 2 of 7 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.76s]
[0m18:16:07.569093 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m18:16:07.569093 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m18:16:07.569093 [info ] [Thread-1 (]: 3 of 7 START sql view model default.stg_localidade ............................. [RUN]
[0m18:16:07.569093 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m18:16:07.569093 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m18:16:07.582883 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m18:16:07.584605 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 18:16:07.569093 => 18:16:07.584214
[0m18:16:07.585991 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m18:16:07.590202 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m18:16:07.592094 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:16:07.593120 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m18:16:07.593120 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m18:16:07.594117 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:16:08.389343 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-ba9d-132d-a895-45759ddddb4a
[0m18:16:09.084942 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m18:16:09.084942 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 18:16:07.585991 => 18:16:09.084942
[0m18:16:09.084942 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m18:16:09.084942 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:16:09.084942 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m18:16:09.084942 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-ba9d-132d-a895-45759ddddb4a
[0m18:16:09.302903 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197F99A7510>]}
[0m18:16:09.318335 [info ] [Thread-1 (]: 3 of 7 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.73s]
[0m18:16:09.318335 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m18:16:09.318335 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m18:16:09.318335 [info ] [Thread-1 (]: 4 of 7 START sql view model default.dim_doenca ................................. [RUN]
[0m18:16:09.318335 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.dim_doenca)
[0m18:16:09.318335 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m18:16:09.318335 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m18:16:09.318335 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 18:16:09.318335 => 18:16:09.318335
[0m18:16:09.318335 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m18:16:09.336744 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m18:16:09.337760 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:16:09.338761 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m18:16:09.339742 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m18:16:09.339742 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:16:10.157707 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-bbab-1858-a311-0c30bbae030e
[0m18:16:10.841746 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m18:16:10.841746 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 18:16:09.318335 => 18:16:10.841746
[0m18:16:10.841746 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m18:16:10.841746 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:16:10.841746 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m18:16:10.841746 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-bbab-1858-a311-0c30bbae030e
[0m18:16:11.100728 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197FAA6ED10>]}
[0m18:16:11.100728 [info ] [Thread-1 (]: 4 of 7 OK created sql view model default.dim_doenca ............................ [[32mOK[0m in 1.78s]
[0m18:16:11.100728 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m18:16:11.100728 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m18:16:11.100728 [info ] [Thread-1 (]: 5 of 7 START sql view model default.dim_localidade ............................. [RUN]
[0m18:16:11.100728 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m18:16:11.100728 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m18:16:11.118062 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m18:16:11.120099 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 18:16:11.100728 => 18:16:11.120099
[0m18:16:11.121121 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m18:16:11.127460 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m18:16:11.129611 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:16:11.130606 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m18:16:11.130606 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m18:16:11.131623 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:16:11.956711 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-bcc0-14ad-aea5-1b4c0b870b01
[0m18:16:12.647055 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m18:16:12.649050 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 18:16:11.121121 => 18:16:12.649050
[0m18:16:12.650047 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m18:16:12.651044 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:16:12.651044 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m18:16:12.652043 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-bcc0-14ad-aea5-1b4c0b870b01
[0m18:16:12.869716 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197FABF4750>]}
[0m18:16:12.885119 [info ] [Thread-1 (]: 5 of 7 OK created sql view model default.dim_localidade ........................ [[32mOK[0m in 1.77s]
[0m18:16:12.885119 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m18:16:12.885119 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m18:16:12.885119 [info ] [Thread-1 (]: 6 of 7 START sql view model default.int_atendimento ............................ [RUN]
[0m18:16:12.885119 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m18:16:12.885119 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m18:16:12.885119 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m18:16:12.885119 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 18:16:12.885119 => 18:16:12.885119
[0m18:16:12.885119 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m18:16:12.900734 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m18:16:12.902849 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:16:12.903775 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m18:16:12.904139 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m18:16:12.905118 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:16:13.677152 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-bdc6-1059-ab86-cdf5bf277d26
[0m18:16:14.435767 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m18:16:14.435767 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 18:16:12.885119 => 18:16:14.435767
[0m18:16:14.435767 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m18:16:14.435767 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:16:14.435767 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m18:16:14.435767 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-bdc6-1059-ab86-cdf5bf277d26
[0m18:16:14.663720 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197FABF4150>]}
[0m18:16:14.663720 [info ] [Thread-1 (]: 6 of 7 OK created sql view model default.int_atendimento ....................... [[32mOK[0m in 1.78s]
[0m18:16:14.663720 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m18:16:14.663720 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:16:14.663720 [info ] [Thread-1 (]: 7 of 7 START sql view model default.fato_atendimento_hospitalar ................ [RUN]
[0m18:16:14.663720 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m18:16:14.663720 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:16:14.679328 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m18:16:14.681500 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 18:16:14.663720 => 18:16:14.679328
[0m18:16:14.681500 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:16:14.686504 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m18:16:14.687551 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m18:16:14.687551 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m18:16:14.687551 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m18:16:14.687551 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m18:16:15.441549 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0788a-bed4-1b97-a2de-722b84b58a22
[0m18:16:16.193376 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m18:16:16.193376 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 18:16:14.682723 => 18:16:16.193376
[0m18:16:16.193376 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m18:16:16.193376 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m18:16:16.193376 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m18:16:16.209022 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0788a-bed4-1b97-a2de-722b84b58a22
[0m18:16:16.454198 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0224d32a-4f7a-4040-b9f1-0914f1c26360', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197FAA83750>]}
[0m18:16:16.454198 [info ] [Thread-1 (]: 7 of 7 OK created sql view model default.fato_atendimento_hospitalar ........... [[32mOK[0m in 1.79s]
[0m18:16:16.454198 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:16:16.454198 [debug] [MainThread]: On master: ROLLBACK
[0m18:16:16.454198 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:16:17.277131 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0788a-bfe9-1faf-ba5d-1a8a8a5e9e76
[0m18:16:17.277131 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:16:17.277131 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:16:17.277131 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:16:17.277131 [debug] [MainThread]: On master: ROLLBACK
[0m18:16:17.277131 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:16:17.277131 [debug] [MainThread]: On master: Close
[0m18:16:17.277131 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0788a-bfe9-1faf-ba5d-1a8a8a5e9e76
[0m18:16:17.524728 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:16:17.524728 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m18:16:17.524728 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m18:16:17.524728 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_atendimento_hospitalar' was properly closed.
[0m18:16:17.524728 [info ] [MainThread]: 
[0m18:16:17.524728 [info ] [MainThread]: Finished running 7 view models in 0 hours 0 minutes and 18.02 seconds (18.02s).
[0m18:16:17.524728 [debug] [MainThread]: Command end result
[0m18:16:17.540173 [info ] [MainThread]: 
[0m18:16:17.540173 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:16:17.540173 [info ] [MainThread]: 
[0m18:16:17.552723 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m18:16:17.553722 [debug] [MainThread]: Command `dbt run` succeeded at 18:16:17.553722 after 19.61 seconds
[0m18:16:17.555251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197EFF3FDD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197EFEEC290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000197E9FDDF50>]}
[0m18:16:17.555840 [debug] [MainThread]: Flushing usage events
[0m18:52:32.171067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE6BD8B2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE6BDA79D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE6BFCC890>]}


============================== 18:52:32.177576 | a5b2804e-2044-43c9-847c-649267d85fec ==============================
[0m18:52:32.177576 [info ] [MainThread]: Running with dbt=1.5.2
[0m18:52:32.178545 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m18:52:33.570435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a5b2804e-2044-43c9-847c-649267d85fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE756F8050>]}
[0m18:52:33.589384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a5b2804e-2044-43c9-847c-649267d85fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE756F8050>]}
[0m18:52:33.590382 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m18:52:33.614344 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m18:52:33.724029 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:52:33.724029 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:52:33.731010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a5b2804e-2044-43c9-847c-649267d85fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE7594CA10>]}
[0m18:52:33.733005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a5b2804e-2044-43c9-847c-649267d85fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE758FBE90>]}
[0m18:52:33.734003 [info ] [MainThread]: Found 7 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 5 sources, 0 exposures, 0 metrics, 0 groups
[0m18:52:33.734539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a5b2804e-2044-43c9-847c-649267d85fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE72AB77D0>]}
[0m18:52:33.736558 [info ] [MainThread]: 
[0m18:52:33.737534 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:52:33.740553 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m18:52:33.747507 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m18:52:33.748516 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m18:52:33.749520 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:52:34.790384 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0788f-d1b4-1511-a27d-46560390eaef
[0m18:52:34.992971 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetTables'), ('session-id', b'\x01\xf0x\x8f\xd1\xb4\x15\x11\xa2}FV\x03\x90\xea\xef'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.20059847831726074/900.0')])
[0m18:52:42.587550 [debug] [ThreadPool]: SQL status: OK in 8.84000015258789 seconds
[0m18:52:42.592838 [debug] [ThreadPool]: On list_workspace_default: Close
[0m18:52:42.592838 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0788f-d1b4-1511-a27d-46560390eaef
[0m18:52:42.812837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a5b2804e-2044-43c9-847c-649267d85fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE75793710>]}
[0m18:52:42.814807 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:52:42.816799 [info ] [MainThread]: 
[0m18:52:42.824534 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m18:52:42.825534 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m18:52:42.826530 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m18:52:42.829551 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m18:52:42.830529 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 18:52:42.826530 => 18:52:42.830529
[0m18:52:42.831547 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m18:52:42.832517 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 18:52:42.831547 => 18:52:42.831547
[0m18:52:42.833540 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m18:52:42.833540 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m18:52:42.834537 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m18:52:42.836505 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m18:52:42.841519 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m18:52:42.842489 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 18:52:42.836505 => 18:52:42.842489
[0m18:52:42.843515 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m18:52:42.844512 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 18:52:42.843515 => 18:52:42.843515
[0m18:52:42.845511 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m18:52:42.845511 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m18:52:42.846508 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m18:52:42.847504 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m18:52:42.852462 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m18:52:42.854457 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 18:52:42.848473 => 18:52:42.854457
[0m18:52:42.855454 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m18:52:42.856452 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 18:52:42.856452 => 18:52:42.856452
[0m18:52:42.857449 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m18:52:42.858455 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m18:52:42.859443 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.dim_doenca)
[0m18:52:42.860469 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m18:52:42.863460 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m18:52:42.865450 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 18:52:42.860469 => 18:52:42.864438
[0m18:52:42.865450 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m18:52:42.866425 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 18:52:42.866425 => 18:52:42.866425
[0m18:52:42.867421 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m18:52:42.868419 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m18:52:42.930253 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m18:52:42.931251 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m18:52:42.935241 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m18:52:42.936239 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 18:52:42.931251 => 18:52:42.936239
[0m18:52:42.937235 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m18:52:42.938265 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 18:52:42.938265 => 18:52:42.938265
[0m18:52:42.939231 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m18:52:42.940256 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m18:52:42.941225 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m18:52:42.942252 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m18:52:42.946240 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m18:52:42.948206 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 18:52:42.942252 => 18:52:42.948206
[0m18:52:42.948206 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m18:52:42.949235 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 18:52:42.949235 => 18:52:42.949235
[0m18:52:42.951227 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m18:52:42.952196 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:52:42.953194 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m18:52:42.954190 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:52:42.958180 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m18:52:42.959176 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 18:52:42.954190 => 18:52:42.959176
[0m18:52:42.960174 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:52:42.960174 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 18:52:42.960174 => 18:52:42.960174
[0m18:52:42.962197 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m18:52:42.962902 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:52:42.963902 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m18:52:42.963902 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_atendimento_hospitalar' was properly closed.
[0m18:52:42.965925 [debug] [MainThread]: Command end result
[0m18:52:43.329690 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m18:52:43.330667 [info ] [MainThread]: Building catalog
[0m18:52:43.333548 [debug] [ThreadPool]: Acquiring new databricks connection 'default'
[0m18:52:43.348079 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m18:52:43.348079 [debug] [ThreadPool]: Using databricks connection "default"
[0m18:52:43.349110 [debug] [ThreadPool]: On default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "default"} */

      select current_catalog()
  
[0m18:52:43.349110 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:52:44.231924 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0788f-d765-1ea4-bd0e-66f859811419
[0m18:52:46.616641 [debug] [ThreadPool]: SQL status: OK in 3.2699999809265137 seconds
[0m18:52:46.627611 [debug] [ThreadPool]: Using databricks connection "default"
[0m18:52:46.628608 [debug] [ThreadPool]: On default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "default"} */
show table extended in `workspace`.`default` like 'int_atendimento|stg_atendimento|stg_localidade|stg_doenca|fato_atendimento_hospitalar|dim_doenca|dim_localidade'
  
[0m18:52:48.229254 [debug] [ThreadPool]: SQL status: OK in 1.600000023841858 seconds
[0m18:52:48.237233 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`dim_doenca`
[0m18:52:48.238230 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`dim_localidade`
[0m18:52:48.239233 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`fato_atendimento_hospitalar`
[0m18:52:48.239233 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`int_atendimento`
[0m18:52:48.240230 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_atendimento`
[0m18:52:48.241222 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_doenca`
[0m18:52:48.241222 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_localidade`
[0m18:52:48.244215 [debug] [ThreadPool]: On default: ROLLBACK
[0m18:52:48.245217 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m18:52:48.245217 [debug] [ThreadPool]: On default: Close
[0m18:52:48.246208 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0788f-d765-1ea4-bd0e-66f859811419
[0m18:52:48.484841 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly default, now raw)
[0m18:52:48.488829 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m18:52:48.488829 [debug] [ThreadPool]: Using databricks connection "raw"
[0m18:52:48.489826 [debug] [ThreadPool]: On raw: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "raw"} */

      select current_catalog()
  
[0m18:52:48.489826 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:52:49.368020 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0788f-da78-176a-b223-2b983da26afd
[0m18:52:49.760738 [debug] [ThreadPool]: SQL status: OK in 1.2699999809265137 seconds
[0m18:52:49.767749 [debug] [ThreadPool]: Using databricks connection "raw"
[0m18:52:49.768745 [debug] [ThreadPool]: On raw: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "raw"} */
show table extended in `workspace`.`raw` like 'faixa_etaria_sexo|atendimento|doenca|localidade|procedimento'
  
[0m18:52:50.400996 [debug] [ThreadPool]: SQL status: OK in 0.6299999952316284 seconds
[0m18:52:50.403960 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`atendimento`
[0m18:52:50.403960 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`doenca`
[0m18:52:50.405956 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`localidade`
[0m18:52:50.406980 [debug] [ThreadPool]: On raw: ROLLBACK
[0m18:52:50.406980 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m18:52:50.407977 [debug] [ThreadPool]: On raw: Close
[0m18:52:50.407977 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0788f-da78-176a-b223-2b983da26afd
[0m18:52:50.656164 [info ] [MainThread]: Catalog written to C:\Users\Marisa\Desktop\projeto_health_insights\target\catalog.json
[0m18:52:50.658131 [debug] [MainThread]: Command `dbt docs generate` succeeded at 18:52:50.657162 after 18.51 seconds
[0m18:52:50.658131 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m18:52:50.659129 [debug] [MainThread]: Connection 'raw' was properly closed.
[0m18:52:50.659129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE6BD7E490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE6BCED810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EE65E4CB90>]}
[0m18:52:50.660136 [debug] [MainThread]: Flushing usage events
[0m18:52:55.063865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F0DC2F350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F0DC1FB50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F0DC2DED0>]}


============================== 18:52:55.068852 | d3a69868-b271-436c-9f73-ff843c9a841e ==============================
[0m18:52:55.068852 [info ] [MainThread]: Running with dbt=1.5.2
[0m18:52:55.069824 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m18:52:56.395579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd3a69868-b271-436c-9f73-ff843c9a841e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F0DBF9E90>]}
[0m18:52:56.414528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd3a69868-b271-436c-9f73-ff843c9a841e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F0DBF9E90>]}
[0m21:05:05.066775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CB4F67510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CB4BF6010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CB4CD52D0>]}


============================== 21:05:05.073831 | d570f9ef-8e26-471f-b73e-627e9353dc71 ==============================
[0m21:05:05.073831 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:05:05.074828 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:05:05.075841 [info ] [MainThread]: dbt version: 1.5.2
[0m21:05:05.076822 [info ] [MainThread]: python version: 3.11.8
[0m21:05:05.077828 [info ] [MainThread]: python path: C:\Users\Marisa\Desktop\projeto_health_insights\venv\Scripts\python.exe
[0m21:05:05.077828 [info ] [MainThread]: os info: Windows-10-10.0.19045-SP0
[0m21:05:05.078841 [info ] [MainThread]: Using profiles.yml file at C:\Users\Marisa\.dbt\profiles.yml
[0m21:05:05.079830 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\Marisa\Desktop\projeto_health_insights\dbt_project.yml
[0m21:05:05.080812 [info ] [MainThread]: Configuration:
[0m21:05:06.838516 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m21:05:06.871811 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m21:05:06.872778 [info ] [MainThread]: Required dependencies:
[0m21:05:06.873787 [debug] [MainThread]: Executing "git --help"
[0m21:05:06.925172 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m21:05:06.926170 [debug] [MainThread]: STDERR: "b''"
[0m21:05:06.926170 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m21:05:06.927168 [info ] [MainThread]: Connection:
[0m21:05:06.928165 [info ] [MainThread]:   host: dbc-605a8848-8c13.cloud.databricks.com
[0m21:05:06.929162 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/9d3cc00bfedf4aa8
[0m21:05:06.930188 [info ] [MainThread]:   catalog: workspace
[0m21:05:06.931179 [info ] [MainThread]:   schema: default
[0m21:05:06.932154 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:05:06.933843 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m21:05:06.933843 [debug] [MainThread]: Using databricks connection "debug"
[0m21:05:06.934799 [debug] [MainThread]: On debug: select 1 as id
[0m21:05:06.934799 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:05:08.107116 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a2-5622-185d-8cfb-17a4e60ee4b2
[0m21:05:16.707027 [debug] [MainThread]: SQL status: OK in 9.770000457763672 seconds
[0m21:05:16.991157 [debug] [MainThread]: On debug: Close
[0m21:05:16.992155 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a2-5622-185d-8cfb-17a4e60ee4b2
[0m21:05:17.222293 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m21:05:17.223307 [info ] [MainThread]: [32mAll checks passed![0m
[0m21:05:17.225285 [debug] [MainThread]: Command `dbt debug` succeeded at 21:05:17.225285 after 12.18 seconds
[0m21:05:17.226281 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m21:05:17.227279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CB521F210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CB4D4A310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CB4CAE110>]}
[0m21:05:17.228277 [debug] [MainThread]: Flushing usage events
[0m21:11:48.883278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8DEB968D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8DE4BDD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8DE58AB90>]}


============================== 21:11:48.888236 | fe08dbfe-0f85-461d-a78d-aa289367c9ce ==============================
[0m21:11:48.888236 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:11:48.890517 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:11:50.287196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8DE541ED0>]}
[0m21:11:50.307142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8DE541ED0>]}
[0m21:11:50.308140 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:11:50.325562 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:11:50.459842 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:11:50.460843 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:11:50.467797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E7D69450>]}
[0m21:11:50.478788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E7DB9C90>]}
[0m21:11:50.479763 [info ] [MainThread]: Found 7 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 5 sources, 0 exposures, 0 metrics, 0 groups
[0m21:11:50.480807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8DE2D2C90>]}
[0m21:11:50.482754 [info ] [MainThread]: 
[0m21:11:50.484749 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:11:50.488245 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:11:50.488245 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:11:50.489242 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:11:50.489242 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:11:51.452041 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a3-46aa-1c37-b480-8c3f0653e0b3
[0m21:11:52.538411 [debug] [ThreadPool]: SQL status: OK in 2.049999952316284 seconds
[0m21:11:52.565391 [debug] [ThreadPool]: On list_workspace: Close
[0m21:11:52.566389 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a3-46aa-1c37-b480-8c3f0653e0b3
[0m21:11:52.796655 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:11:52.798732 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:11:52.812582 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:11:52.813579 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:11:52.814577 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:11:52.814577 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:11:53.937968 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a3-4828-106c-9298-bfd45cb704fd
[0m21:11:54.663784 [debug] [ThreadPool]: SQL status: OK in 1.850000023841858 seconds
[0m21:11:54.664781 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:11:54.664781 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:11:54.666248 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:11:54.666248 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:11:54.667229 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a3-4828-106c-9298-bfd45cb704fd
[0m21:11:54.938750 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:11:55.006598 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:11:55.006598 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:11:55.007596 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:11:55.826221 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a3-4947-10b1-b18f-1169ae734b23
[0m21:11:56.306376 [debug] [ThreadPool]: SQL status: OK in 1.2999999523162842 seconds
[0m21:11:56.312390 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:11:56.313387 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a3-4947-10b1-b18f-1169ae734b23
[0m21:11:56.549617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8DDABDA10>]}
[0m21:11:56.551573 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:11:56.553606 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:11:56.556560 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:11:56.558555 [info ] [MainThread]: 
[0m21:11:56.573962 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m21:11:56.574960 [info ] [Thread-1 (]: 1 of 7 START sql view model default.stg_atendimento ............................ [RUN]
[0m21:11:56.576979 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m21:11:56.576979 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m21:11:56.580972 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m21:11:56.581971 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 21:11:56.577979 => 21:11:56.581971
[0m21:11:56.582965 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m21:11:56.621832 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m21:11:56.622830 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:11:56.623830 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m21:11:56.624825 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m21:11:56.624825 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:11:57.506029 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a3-4a4a-1242-9d13-9446034976c5
[0m21:12:01.012534 [debug] [Thread-1 (]: SQL status: OK in 4.389999866485596 seconds
[0m21:12:01.031443 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 21:11:56.583964 => 21:12:01.031443
[0m21:12:01.032439 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m21:12:01.032439 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:12:01.033438 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m21:12:01.034435 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a3-4a4a-1242-9d13-9446034976c5
[0m21:12:01.284383 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E8DCA110>]}
[0m21:12:01.286376 [info ] [Thread-1 (]: 1 of 7 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 4.71s]
[0m21:12:01.288372 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m21:12:01.289369 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m21:12:01.290365 [info ] [Thread-1 (]: 2 of 7 START sql view model default.stg_doenca ................................. [RUN]
[0m21:12:01.292343 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m21:12:01.293340 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m21:12:01.299292 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m21:12:01.301288 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 21:12:01.294338 => 21:12:01.300329
[0m21:12:01.302284 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m21:12:01.309296 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m21:12:01.311289 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:12:01.312259 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m21:12:01.313256 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m21:12:01.314253 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:12:02.136068 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a3-4d0b-1fab-8a80-6ba35dbcc03f
[0m21:12:03.245408 [debug] [Thread-1 (]: SQL status: OK in 1.9299999475479126 seconds
[0m21:12:03.248399 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 21:12:01.302284 => 21:12:03.248399
[0m21:12:03.249397 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m21:12:03.249397 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:12:03.250394 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m21:12:03.250394 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a3-4d0b-1fab-8a80-6ba35dbcc03f
[0m21:12:03.497123 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E8E1C090>]}
[0m21:12:03.499117 [info ] [Thread-1 (]: 2 of 7 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 2.21s]
[0m21:12:03.501080 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m21:12:03.502104 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m21:12:03.504103 [info ] [Thread-1 (]: 3 of 7 START sql view model default.stg_localidade ............................. [RUN]
[0m21:12:03.506094 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m21:12:03.507096 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m21:12:03.517036 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m21:12:03.519032 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 21:12:03.508062 => 21:12:03.518033
[0m21:12:03.520029 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m21:12:03.527009 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m21:12:03.528006 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:12:03.529003 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m21:12:03.529003 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m21:12:03.530001 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:12:04.307529 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a3-4e57-1bc1-9d09-e330571bdaf4
[0m21:12:05.270010 [debug] [Thread-1 (]: SQL status: OK in 1.7400000095367432 seconds
[0m21:12:05.281008 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 21:12:03.521026 => 21:12:05.280021
[0m21:12:05.282005 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m21:12:05.283970 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:12:05.284996 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m21:12:05.285987 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a3-4e57-1bc1-9d09-e330571bdaf4
[0m21:12:05.523708 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E7D5F290>]}
[0m21:12:05.525706 [info ] [Thread-1 (]: 3 of 7 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 2.02s]
[0m21:12:05.527723 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m21:12:05.527723 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m21:12:05.528726 [info ] [Thread-1 (]: 4 of 7 START sql view model default.dim_doenca ................................. [RUN]
[0m21:12:05.530714 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.dim_doenca)
[0m21:12:05.530714 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m21:12:05.534676 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m21:12:05.535673 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 21:12:05.531708 => 21:12:05.535673
[0m21:12:05.536671 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m21:12:05.544652 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m21:12:05.546645 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:12:05.546645 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m21:12:05.547669 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m21:12:05.548639 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:12:06.646652 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a3-4fa1-1ecf-9684-5863fd0fac7b
[0m21:12:07.407318 [debug] [Thread-1 (]: SQL status: OK in 1.8600000143051147 seconds
[0m21:12:07.410310 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 21:12:05.536671 => 21:12:07.410310
[0m21:12:07.411307 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m21:12:07.411307 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:12:07.412304 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m21:12:07.412304 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a3-4fa1-1ecf-9684-5863fd0fac7b
[0m21:12:07.646020 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E8E5EDD0>]}
[0m21:12:07.647015 [info ] [Thread-1 (]: 4 of 7 OK created sql view model default.dim_doenca ............................ [[32mOK[0m in 2.12s]
[0m21:12:07.648981 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m21:12:07.648981 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m21:12:07.650344 [info ] [Thread-1 (]: 5 of 7 START sql view model default.dim_localidade ............................. [RUN]
[0m21:12:07.652343 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m21:12:07.653340 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m21:12:07.658355 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m21:12:07.659355 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 21:12:07.653340 => 21:12:07.659355
[0m21:12:07.660322 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m21:12:07.664341 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m21:12:07.666305 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:12:07.666305 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m21:12:07.667331 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m21:12:07.668300 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:12:08.459058 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a3-50d1-142a-ac60-1dbd29b498b1
[0m21:12:09.131307 [debug] [Thread-1 (]: SQL status: OK in 1.4600000381469727 seconds
[0m21:12:09.135298 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 21:12:07.660322 => 21:12:09.135298
[0m21:12:09.136294 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m21:12:09.137295 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:12:09.138258 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m21:12:09.139254 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a3-50d1-142a-ac60-1dbd29b498b1
[0m21:12:09.371334 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E8FB4290>]}
[0m21:12:09.372331 [info ] [Thread-1 (]: 5 of 7 OK created sql view model default.dim_localidade ........................ [[32mOK[0m in 1.72s]
[0m21:12:09.373241 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m21:12:09.374270 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m21:12:09.375239 [info ] [Thread-1 (]: 6 of 7 START sql view model default.int_atendimento ............................ [RUN]
[0m21:12:09.376236 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m21:12:09.377233 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m21:12:09.381252 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m21:12:09.382248 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 21:12:09.377233 => 21:12:09.382248
[0m21:12:09.383216 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m21:12:09.388232 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m21:12:09.389202 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:12:09.390199 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m21:12:09.390199 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m21:12:09.392193 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:12:10.231910 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a3-51da-1df7-aec4-4787cb209dd1
[0m21:12:11.083359 [debug] [Thread-1 (]: SQL status: OK in 1.690000057220459 seconds
[0m21:12:11.086322 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 21:12:09.383216 => 21:12:11.086322
[0m21:12:11.086322 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m21:12:11.087346 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:12:11.088316 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m21:12:11.088316 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a3-51da-1df7-aec4-4787cb209dd1
[0m21:12:11.327083 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E8FB96D0>]}
[0m21:12:11.330022 [info ] [Thread-1 (]: 6 of 7 OK created sql view model default.int_atendimento ....................... [[32mOK[0m in 1.95s]
[0m21:12:11.334982 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m21:12:11.337941 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:12:11.340968 [info ] [Thread-1 (]: 7 of 7 START sql view model default.fato_atendimento_hospitalar ................ [RUN]
[0m21:12:11.344921 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m21:12:11.347913 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:12:11.357882 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:12:11.359876 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 21:12:11.349907 => 21:12:11.359876
[0m21:12:11.360874 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:12:11.367884 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:12:11.369850 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:12:11.369850 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:12:11.370847 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m21:12:11.371844 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:12:12.148067 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a3-5304-123c-aad7-af839af371b6
[0m21:12:12.955791 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m21:12:12.960777 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 21:12:11.361901 => 21:12:12.959780
[0m21:12:12.961745 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m21:12:12.961745 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:12:12.962772 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m21:12:12.963769 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a3-5304-123c-aad7-af839af371b6
[0m21:12:13.206411 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe08dbfe-0f85-461d-a78d-aa289367c9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8E7D41D50>]}
[0m21:12:13.208435 [info ] [Thread-1 (]: 7 of 7 OK created sql view model default.fato_atendimento_hospitalar ........... [[32mOK[0m in 1.86s]
[0m21:12:13.211429 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:12:13.214389 [debug] [MainThread]: On master: ROLLBACK
[0m21:12:13.215386 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:12:14.253393 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a3-5444-1f15-8fe9-88a43ef8cc4a
[0m21:12:14.257422 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:12:14.259378 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:12:14.261375 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:12:14.263368 [debug] [MainThread]: On master: ROLLBACK
[0m21:12:14.264366 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:12:14.266362 [debug] [MainThread]: On master: Close
[0m21:12:14.268391 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a3-5444-1f15-8fe9-88a43ef8cc4a
[0m21:12:14.509394 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:12:14.510390 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:12:14.511388 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:12:14.512386 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_atendimento_hospitalar' was properly closed.
[0m21:12:14.513382 [info ] [MainThread]: 
[0m21:12:14.515378 [info ] [MainThread]: Finished running 7 view models in 0 hours 0 minutes and 24.03 seconds (24.03s).
[0m21:12:14.518401 [debug] [MainThread]: Command end result
[0m21:12:14.535353 [info ] [MainThread]: 
[0m21:12:14.536321 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:12:14.537319 [info ] [MainThread]: 
[0m21:12:14.538315 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m21:12:14.539313 [debug] [MainThread]: Command `dbt run` succeeded at 21:12:14.539313 after 25.67 seconds
[0m21:12:14.539313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8DE29F090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8DE820110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A8D813BB10>]}
[0m21:12:14.540340 [debug] [MainThread]: Flushing usage events
[0m21:20:47.586527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223344276D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022333B69910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022333B14ED0>]}


============================== 21:20:47.591542 | 475481a7-523a-4681-a073-b7980f1ad1db ==============================
[0m21:20:47.591542 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:20:47.592511 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:20:49.025349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '475481a7-523a-4681-a073-b7980f1ad1db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022333B577D0>]}
[0m21:20:49.045263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '475481a7-523a-4681-a073-b7980f1ad1db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022333B577D0>]}
[0m21:20:49.046260 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:20:49.064212 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:20:49.181035 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 5 files added, 0 files changed.
[0m21:20:49.182032 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m21:20:49.182032 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\marts\dim_tempo.sql
[0m21:20:49.183029 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\intermediate\int_nascimento.sql
[0m21:20:49.183029 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m21:20:49.184026 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\staging\stg_tempo.sql
[0m21:20:49.220044 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m21:20:49.234995 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m21:20:49.240009 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m21:20:49.243970 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m21:20:49.248986 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_tempo.sql
[0m21:20:49.324755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '475481a7-523a-4681-a073-b7980f1ad1db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002233D47FFD0>]}
[0m21:20:49.337229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '475481a7-523a-4681-a073-b7980f1ad1db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002233E641CD0>]}
[0m21:20:49.338227 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 5 sources, 0 exposures, 0 metrics, 0 groups
[0m21:20:49.339224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '475481a7-523a-4681-a073-b7980f1ad1db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002233ABEE210>]}
[0m21:20:49.341218 [info ] [MainThread]: 
[0m21:20:49.342216 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:20:49.345236 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:20:49.345236 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:20:49.346205 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:20:49.347203 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:20:50.323139 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a4-87dc-1d50-a1cc-dc99ad400d2e
[0m21:20:50.826854 [debug] [ThreadPool]: SQL status: OK in 1.4800000190734863 seconds
[0m21:20:50.829844 [debug] [ThreadPool]: On list_workspace: Close
[0m21:20:50.830841 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a4-87dc-1d50-a1cc-dc99ad400d2e
[0m21:20:51.083218 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:20:51.086210 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:20:51.098176 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:20:51.099201 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:20:51.100171 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:20:51.100171 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:20:51.936143 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a4-88d3-1ace-8a7c-2bd7eadec806
[0m21:20:52.333759 [debug] [ThreadPool]: SQL status: OK in 1.2300000190734863 seconds
[0m21:20:52.337326 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:20:52.338293 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:20:52.339321 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:20:52.340318 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:20:52.342313 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a4-88d3-1ace-8a7c-2bd7eadec806
[0m21:20:52.567057 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:20:52.576059 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:20:52.577058 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:20:52.577058 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:20:53.345047 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a4-89aa-1f95-af98-0c32b4aa9f5c
[0m21:20:53.736666 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m21:20:53.742647 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:20:53.743642 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a4-89aa-1f95-af98-0c32b4aa9f5c
[0m21:20:53.974631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '475481a7-523a-4681-a073-b7980f1ad1db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022333652AD0>]}
[0m21:20:53.975659 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:20:53.976651 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:20:53.977649 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:20:53.978618 [info ] [MainThread]: 
[0m21:20:53.986044 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m21:20:53.987042 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m21:20:53.988468 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m21:20:53.989464 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m21:20:53.993480 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m21:20:53.994451 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 21:20:53.990462 => 21:20:53.994451
[0m21:20:53.995462 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m21:20:54.030384 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m21:20:54.031368 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:20:54.032350 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m21:20:54.032350 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m21:20:54.033347 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:20:54.824580 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a4-8a8c-1ced-878b-ac7fc10e6933
[0m21:20:55.584311 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m21:20:55.613264 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 21:20:53.995462 => 21:20:55.612266
[0m21:20:55.614231 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m21:20:55.614231 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:20:55.615259 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m21:20:55.615259 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a4-8a8c-1ced-878b-ac7fc10e6933
[0m21:20:55.844075 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '475481a7-523a-4681-a073-b7980f1ad1db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002233E69FD10>]}
[0m21:20:55.845071 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.86s]
[0m21:20:55.847067 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m21:20:55.848096 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m21:20:55.850088 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m21:20:55.852055 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m21:20:55.853051 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m21:20:55.860033 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m21:20:55.862059 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 21:20:55.854049 => 21:20:55.862059
[0m21:20:55.863058 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m21:20:55.871030 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m21:20:55.873002 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:20:55.873002 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m21:20:55.873995 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m21:20:55.874992 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:20:56.717883 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a4-8bad-116a-8a12-4a20f7fcb572
[0m21:20:57.440208 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m21:20:57.444198 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 21:20:55.864053 => 21:20:57.443200
[0m21:20:57.444198 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m21:20:57.445195 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:20:57.446199 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m21:20:57.447189 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a4-8bad-116a-8a12-4a20f7fcb572
[0m21:20:57.676576 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '475481a7-523a-4681-a073-b7980f1ad1db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002233E692350>]}
[0m21:20:57.677575 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.82s]
[0m21:20:57.679569 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m21:20:57.679569 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m21:20:57.680566 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m21:20:57.682561 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m21:20:57.682561 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m21:20:57.688555 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m21:20:57.690540 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 21:20:57.683558 => 21:20:57.689541
[0m21:20:57.690540 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m21:20:57.696524 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m21:20:57.698518 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:20:57.698518 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m21:20:57.699515 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m21:20:57.700513 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:20:58.488170 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a4-8cba-1e46-9f9a-2e196e092bd8
[0m21:20:59.185840 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m21:20:59.188833 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 21:20:57.691560 => 21:20:59.188833
[0m21:20:59.189830 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m21:20:59.190827 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:20:59.190827 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m21:20:59.191839 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a4-8cba-1e46-9f9a-2e196e092bd8
[0m21:20:59.421736 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '475481a7-523a-4681-a073-b7980f1ad1db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002233E704490>]}
[0m21:20:59.423731 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.74s]
[0m21:20:59.424728 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m21:20:59.425726 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:20:59.426723 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m21:20:59.428718 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m21:20:59.428718 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m21:20:59.433705 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:20:59.435699 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 21:20:59.429715 => 21:20:59.434702
[0m21:20:59.436697 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m21:20:59.443677 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:20:59.444703 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:20:59.445673 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:20:59.446670 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(id AS STRING) AS nascimento_id,
    CAST(data_nascimento AS DATE) AS data_nascimento,
    municipio,
    uf,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas
FROM sinasc_2022_sc_clean
WHERE uf = 'SC'

[0m21:20:59.447684 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:21:00.287294 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a4-8dc9-1f3c-92fb-db6b87e3a25b
[0m21:21:01.023710 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(id AS STRING) AS nascimento_id,
    CAST(data_nascimento AS DATE) AS data_nascimento,
    municipio,
    uf,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas
FROM sinasc_2022_sc_clean
WHERE uf = 'SC'

[0m21:21:01.023710 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 18 pos 6
[0m21:21:01.024709 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m21:21:01.025704 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078a4-8def-1762-8a0e-2eba3c87bb06
[0m21:21:01.026702 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 21:20:59.436697 => 21:21:01.026702
[0m21:21:01.027699 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m21:21:01.027699 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:21:01.028717 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m21:21:01.028717 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a4-8dc9-1f3c-92fb-db6b87e3a25b
[0m21:21:01.281094 [debug] [Thread-1 (]: Runtime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 18 pos 6
[0m21:21:01.282092 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '475481a7-523a-4681-a073-b7980f1ad1db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002233E885410>]}
[0m21:21:01.282092 [error] [Thread-1 (]: 4 of 5 ERROR creating sql view model default.stg_nascidos_vivos ................ [[31mERROR[0m in 1.85s]
[0m21:21:01.284086 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:21:01.285083 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m21:21:01.286105 [info ] [Thread-1 (]: 5 of 5 SKIP relation default.stg_tempo ......................................... [[33mSKIP[0m]
[0m21:21:01.286724 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m21:21:01.288722 [debug] [MainThread]: On master: ROLLBACK
[0m21:21:01.289720 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:21:02.106763 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a4-8ee3-1c39-bd76-172694d8ad41
[0m21:21:02.109754 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:21:02.111750 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:21:02.112748 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:21:02.113744 [debug] [MainThread]: On master: ROLLBACK
[0m21:21:02.115739 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:21:02.116735 [debug] [MainThread]: On master: Close
[0m21:21:02.117732 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a4-8ee3-1c39-bd76-172694d8ad41
[0m21:21:02.351480 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:21:02.353436 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:21:02.355468 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:21:02.356428 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_nascidos_vivos' was properly closed.
[0m21:21:02.359455 [info ] [MainThread]: 
[0m21:21:02.362413 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 13.02 seconds (13.02s).
[0m21:21:02.365402 [debug] [MainThread]: Command end result
[0m21:21:02.375374 [info ] [MainThread]: 
[0m21:21:02.377373 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:21:02.378367 [info ] [MainThread]: 
[0m21:21:02.379365 [error] [MainThread]: [33mRuntime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m21:21:02.381373 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 18 pos 6
[0m21:21:02.384352 [info ] [MainThread]: 
[0m21:21:02.385349 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m21:21:02.386345 [debug] [MainThread]: Command `dbt run` failed at 21:21:02.386345 after 14.82 seconds
[0m21:21:02.386345 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022333E8C710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022334427D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022333E23050>]}
[0m21:21:02.387343 [debug] [MainThread]: Flushing usage events
[0m21:30:04.211983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A775EFA350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A775FA2A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A775ED9C90>]}


============================== 21:30:04.216968 | df172862-3161-4af5-b93c-ce06eb9c2ff1 ==============================
[0m21:30:04.216968 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:30:04.217944 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:30:05.486899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'df172862-3161-4af5-b93c-ce06eb9c2ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A775BF5790>]}
[0m21:30:05.505848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'df172862-3161-4af5-b93c-ce06eb9c2ff1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A775BF5790>]}
[0m21:30:05.505848 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:30:05.522098 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:30:05.635387 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:30:05.636370 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m21:30:05.663299 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m21:30:05.676250 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.projeto_health_insights.stg_nascidos_vivos' (models\staging\stg_nascidos_vivos.sql) depends on a source named 'raw.sinasc_2022_sc_clean' which was not found
[0m21:30:05.678095 [debug] [MainThread]: Command `dbt run` failed at 21:30:05.677067 after 1.48 seconds
[0m21:30:05.678095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A775F6CC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A775CAF7D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A77F797850>]}
[0m21:30:05.679092 [debug] [MainThread]: Flushing usage events
[0m21:30:34.481873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEA1CCD090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEA1959E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEA1C0AD10>]}


============================== 21:30:34.486573 | 2caf4ec3-9cca-466a-b527-25d1050e869a ==============================
[0m21:30:34.486573 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:30:34.487543 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:30:35.753250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2caf4ec3-9cca-466a-b527-25d1050e869a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEA2294ED0>]}
[0m21:30:35.771202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2caf4ec3-9cca-466a-b527-25d1050e869a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEA2294ED0>]}
[0m21:30:35.772172 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:30:35.790133 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:30:35.885895 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:30:35.886865 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m21:30:35.912818 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m21:30:35.925789 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.projeto_health_insights.stg_nascidos_vivos' (models\staging\stg_nascidos_vivos.sql) depends on a source named 'raw.sinasc_2022_sc_clean' which was not found
[0m21:30:35.927756 [debug] [MainThread]: Command `dbt run` failed at 21:30:35.927756 after 1.46 seconds
[0m21:30:35.928753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEA1D02F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEA1C0AB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AEA1C0A9D0>]}
[0m21:30:35.928753 [debug] [MainThread]: Flushing usage events
[0m21:33:41.839993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3D0B99210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3D0E7F090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3D0975ED0>]}


============================== 21:33:41.844979 | bb8a0ffc-2b8d-4884-b777-0713cd9c6483 ==============================
[0m21:33:41.844979 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:33:41.845958 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:33:43.127077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bb8a0ffc-2b8d-4884-b777-0713cd9c6483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3D0C43510>]}
[0m21:33:43.145029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bb8a0ffc-2b8d-4884-b777-0713cd9c6483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3D0C43510>]}
[0m21:33:43.146031 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:33:43.164489 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:33:43.263226 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m21:33:43.264234 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\staging\_sources.yaml
[0m21:33:43.264234 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m21:33:43.291112 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m21:33:43.390848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bb8a0ffc-2b8d-4884-b777-0713cd9c6483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3DA3E5E50>]}
[0m21:33:43.401827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bb8a0ffc-2b8d-4884-b777-0713cd9c6483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3DA3716D0>]}
[0m21:33:43.401827 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:33:43.403152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bb8a0ffc-2b8d-4884-b777-0713cd9c6483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3D7956C50>]}
[0m21:33:43.405177 [info ] [MainThread]: 
[0m21:33:43.406147 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:33:43.408148 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:33:43.409147 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:33:43.410145 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:33:43.410145 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:33:44.611525 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a6-5545-140c-898e-b5f602d83557
[0m21:33:44.811633 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0x\xa6UE\x14\x0c\x89\x8e\xb5\xf6\x02\xd85W'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.19811487197875977/900.0')])
[0m21:33:50.042794 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0x\xa6UE\x14\x0c\x89\x8e\xb5\xf6\x02\xd85W'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '2/30'), ('elapsed-seconds', '5.4292778968811035/900.0')])
[0m21:33:57.119948 [debug] [ThreadPool]: SQL status: OK in 13.710000038146973 seconds
[0m21:33:57.123947 [debug] [ThreadPool]: On list_workspace: Close
[0m21:33:57.124944 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a6-5545-140c-898e-b5f602d83557
[0m21:33:57.389350 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:33:57.393339 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:33:57.416275 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:33:57.416275 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:33:57.417272 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:33:57.417272 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:33:58.220209 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a6-5d7b-14ee-b2ad-8f8dbab72ead
[0m21:33:59.963929 [debug] [ThreadPool]: SQL status: OK in 2.549999952316284 seconds
[0m21:33:59.965921 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:33:59.965921 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:33:59.966918 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:33:59.966918 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:33:59.967915 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a6-5d7b-14ee-b2ad-8f8dbab72ead
[0m21:34:00.209007 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:34:00.214982 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:34:00.215979 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:34:00.215979 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:34:01.025530 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a6-5f23-1bf4-a39b-72cf1646a10a
[0m21:34:01.634220 [debug] [ThreadPool]: SQL status: OK in 1.4199999570846558 seconds
[0m21:34:01.640210 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:34:01.640210 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a6-5f23-1bf4-a39b-72cf1646a10a
[0m21:34:01.873770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bb8a0ffc-2b8d-4884-b777-0713cd9c6483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3DA3ED450>]}
[0m21:34:01.874736 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:34:01.874736 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:34:01.875762 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:34:01.876739 [info ] [MainThread]: 
[0m21:34:01.884376 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:34:01.885376 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m21:34:01.886920 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_nascidos_vivos'
[0m21:34:01.886920 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m21:34:01.891932 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:34:01.892943 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 21:34:01.887946 => 21:34:01.892943
[0m21:34:01.893912 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m21:34:01.926838 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:34:01.928805 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:34:01.928805 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:34:01.929802 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(ORIGEM AS STRING) AS nascimento_id,
    TO_DATE(CAST(DTNASC AS STRING), 'yyyyMMdd') AS data_nascimento,
    CAST(CODMUNNASC AS STRING) AS cod_municipio_nasc,
    CAST(CODMUNRES AS STRING) AS cod_municipio_res,
    CAST(SEXO AS INT) AS sexo,
    CAST(PESO AS INT) AS peso_gramas,
    CAST(IDADEMAE AS INT) AS idade_mae,
    CAST(GESTACAO AS INT) AS gestacao_semanas
FROM `workspace`.`raw`.`sinasc_2022_sc_clean`
WHERE CAST(CODMUNRES AS STRING) LIKE '42%'  -- códigos IBGE que começam com 42 = SC

[0m21:34:01.930799 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:34:02.741924 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a6-602a-1edd-9757-a8ceb359d000
[0m21:34:03.875251 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(ORIGEM AS STRING) AS nascimento_id,
    TO_DATE(CAST(DTNASC AS STRING), 'yyyyMMdd') AS data_nascimento,
    CAST(CODMUNNASC AS STRING) AS cod_municipio_nasc,
    CAST(CODMUNRES AS STRING) AS cod_municipio_res,
    CAST(SEXO AS INT) AS sexo,
    CAST(PESO AS INT) AS peso_gramas,
    CAST(IDADEMAE AS INT) AS idade_mae,
    CAST(GESTACAO AS INT) AS gestacao_semanas
FROM `workspace`.`raw`.`sinasc_2022_sc_clean`
WHERE CAST(CODMUNRES AS STRING) LIKE '42%'  -- códigos IBGE que começam com 42 = SC

[0m21:34:03.876244 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`sinasc_2022_sc_clean` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
[0m21:34:03.877242 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`sinasc_2022_sc_clean` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`sinasc_2022_sc_clean` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m21:34:03.878238 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078a6-604e-1044-a64b-fe77fb1ef5d9
[0m21:34:03.879237 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 21:34:01.893912 => 21:34:03.878238
[0m21:34:03.879237 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m21:34:03.880233 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:34:03.880233 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m21:34:03.881231 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a6-602a-1edd-9757-a8ceb359d000
[0m21:34:04.124635 [debug] [Thread-1 (]: Runtime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`sinasc_2022_sc_clean` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
[0m21:34:04.125633 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb8a0ffc-2b8d-4884-b777-0713cd9c6483', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3DB4ECE50>]}
[0m21:34:04.126610 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stg_nascidos_vivos ................ [[31mERROR[0m in 2.24s]
[0m21:34:04.127599 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:34:04.129594 [debug] [MainThread]: On master: ROLLBACK
[0m21:34:04.130591 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:34:04.962735 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a6-6180-1552-a4a6-570e604254c3
[0m21:34:04.964697 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:34:04.964697 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:34:04.965694 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:34:04.965694 [debug] [MainThread]: On master: ROLLBACK
[0m21:34:04.966719 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:34:04.966719 [debug] [MainThread]: On master: Close
[0m21:34:04.967717 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a6-6180-1552-a4a6-570e604254c3
[0m21:34:05.200439 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:34:05.202440 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:34:05.204400 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:34:05.205431 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_nascidos_vivos' was properly closed.
[0m21:34:05.210413 [info ] [MainThread]: 
[0m21:34:05.212374 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 21.80 seconds (21.80s).
[0m21:34:05.215397 [debug] [MainThread]: Command end result
[0m21:34:05.235311 [info ] [MainThread]: 
[0m21:34:05.235961 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:34:05.236962 [info ] [MainThread]: 
[0m21:34:05.237959 [error] [MainThread]: [33mRuntime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m21:34:05.238989 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`raw`.`sinasc_2022_sc_clean` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m21:34:05.239990 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m21:34:05.241984 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
[0m21:34:05.242982 [info ] [MainThread]: 
[0m21:34:05.244281 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m21:34:05.245280 [debug] [MainThread]: Command `dbt run` failed at 21:34:05.245280 after 23.42 seconds
[0m21:34:05.246279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3D0B7C450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3CA76BC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3D0B98650>]}
[0m21:34:05.246279 [debug] [MainThread]: Flushing usage events
[0m21:38:06.219730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C61FC7F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C61A67B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C60847B50>]}


============================== 21:38:06.224722 | 83b9e889-51a4-463d-927d-36c80d03d807 ==============================
[0m21:38:06.224722 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:38:06.225696 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:38:07.462414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '83b9e889-51a4-463d-927d-36c80d03d807', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C616B63D0>]}
[0m21:38:07.481362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '83b9e889-51a4-463d-927d-36c80d03d807', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C616B63D0>]}
[0m21:38:07.481362 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:38:07.501310 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:38:07.610119 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:38:07.611116 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\_sources.yaml
[0m21:38:07.639059 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m21:38:07.653525 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m21:38:07.657544 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m21:38:07.662531 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_tempo.sql
[0m21:38:07.666520 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m21:38:07.755255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '83b9e889-51a4-463d-927d-36c80d03d807', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C1C7450>]}
[0m21:38:07.766248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '83b9e889-51a4-463d-927d-36c80d03d807', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C1B5AD0>]}
[0m21:38:07.767254 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:38:07.768221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '83b9e889-51a4-463d-927d-36c80d03d807', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C68497D90>]}
[0m21:38:07.769239 [info ] [MainThread]: 
[0m21:38:07.771240 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:38:07.773479 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:38:07.773479 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:38:07.774476 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:38:07.775474 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:38:08.628461 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a6-f2b8-16dc-82f9-0381940063d4
[0m21:38:08.986303 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m21:38:08.998271 [debug] [ThreadPool]: On list_workspace: Close
[0m21:38:08.999267 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a6-f2b8-16dc-82f9-0381940063d4
[0m21:38:09.279513 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:38:09.280543 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:38:09.290514 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:38:09.291512 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:38:09.292481 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:38:09.292481 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:38:10.110423 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a6-f39a-17c6-9e0c-8c733cafcb94
[0m21:38:10.575751 [debug] [ThreadPool]: SQL status: OK in 1.2799999713897705 seconds
[0m21:38:10.576748 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:38:10.577746 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:38:10.577746 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:38:10.578743 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:38:10.578743 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a6-f39a-17c6-9e0c-8c733cafcb94
[0m21:38:10.814821 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:38:10.819794 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:38:10.820790 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:38:10.820790 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:38:11.627863 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a6-f485-1a46-b887-2968e1875cb7
[0m21:38:11.991030 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m21:38:11.994022 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:38:11.995019 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a6-f485-1a46-b887-2968e1875cb7
[0m21:38:12.232824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '83b9e889-51a4-463d-927d-36c80d03d807', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C304A90>]}
[0m21:38:12.233821 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:38:12.234789 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:38:12.234789 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:38:12.235813 [info ] [MainThread]: 
[0m21:38:12.242927 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:38:12.243923 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m21:38:12.246921 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_nascidos_vivos'
[0m21:38:12.247928 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m21:38:12.252900 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:38:12.253897 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 21:38:12.247928 => 21:38:12.253897
[0m21:38:12.254913 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m21:38:12.289829 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:38:12.290820 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:38:12.291825 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:38:12.291825 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(ORIGEM AS STRING) AS nascimento_id,
    TO_DATE(CAST(DTNASC AS STRING), 'yyyyMMdd') AS data_nascimento,
    CAST(CODMUNNASC AS STRING) AS cod_municipio_nasc,
    CAST(CODMUNRES AS STRING) AS cod_municipio_res,
    CAST(SEXO AS INT) AS sexo,
    CAST(PESO AS INT) AS peso_gramas,
    CAST(IDADEMAE AS INT) AS idade_mae,
    CAST(GESTACAO AS INT) AS gestacao_semanas
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CAST(CODMUNRES AS STRING) LIKE '42%'  -- códigos IBGE que começam com 42 = SC

[0m21:38:12.292821 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:38:13.118621 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a6-f569-19cc-893e-0a49e7a81e48
[0m21:38:15.797116 [debug] [Thread-1 (]: SQL status: OK in 3.5 seconds
[0m21:38:15.809084 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 21:38:12.255892 => 21:38:15.808087
[0m21:38:15.809084 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m21:38:15.810081 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:38:15.810081 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m21:38:15.811079 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a6-f569-19cc-893e-0a49e7a81e48
[0m21:38:16.044678 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '83b9e889-51a4-463d-927d-36c80d03d807', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C6C161FD0>]}
[0m21:38:16.046669 [info ] [Thread-1 (]: 1 of 1 OK created sql view model default.stg_nascidos_vivos .................... [[32mOK[0m in 3.80s]
[0m21:38:16.047680 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:38:16.049662 [debug] [MainThread]: On master: ROLLBACK
[0m21:38:16.050660 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:38:16.849648 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a6-f7a3-172b-9f61-80231f9fb231
[0m21:38:16.850645 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:38:16.851642 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:38:16.851642 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:38:16.852639 [debug] [MainThread]: On master: ROLLBACK
[0m21:38:16.852639 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:38:16.853637 [debug] [MainThread]: On master: Close
[0m21:38:16.854634 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a6-f7a3-172b-9f61-80231f9fb231
[0m21:38:17.084702 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:38:17.086659 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:38:17.086659 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:38:17.087654 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_nascidos_vivos' was properly closed.
[0m21:38:17.088680 [info ] [MainThread]: 
[0m21:38:17.089649 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 9.32 seconds (9.32s).
[0m21:38:17.090653 [debug] [MainThread]: Command end result
[0m21:38:17.104608 [info ] [MainThread]: 
[0m21:38:17.106604 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:38:17.107603 [info ] [MainThread]: 
[0m21:38:17.108627 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:38:17.110619 [debug] [MainThread]: Command `dbt run` succeeded at 21:38:17.109623 after 10.91 seconds
[0m21:38:17.110619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C61FC7990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C61D8B090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C616EF290>]}
[0m21:38:17.111599 [debug] [MainThread]: Flushing usage events
[0m21:39:06.008700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EBD70FAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EBD63A110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EBD9DE1D0>]}


============================== 21:39:06.013685 | 19bbcd61-e642-482e-8bcf-1106b9ca0433 ==============================
[0m21:39:06.013685 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:39:06.014660 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:39:07.286647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EBDA9B810>]}
[0m21:39:07.304599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EBDA9B810>]}
[0m21:39:07.305597 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:39:07.324546 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:39:07.430265 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:39:07.431261 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:39:07.438242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC830EC50>]}
[0m21:39:07.448216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC820FFD0>]}
[0m21:39:07.449186 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:39:07.450182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC47ACD10>]}
[0m21:39:07.452206 [info ] [MainThread]: 
[0m21:39:07.454200 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:39:07.456172 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:39:07.457192 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:39:07.458162 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:39:07.459158 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:39:08.363525 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a7-1656-1267-8400-509ef37810a1
[0m21:39:08.888174 [debug] [ThreadPool]: SQL status: OK in 1.4299999475479126 seconds
[0m21:39:08.893261 [debug] [ThreadPool]: On list_workspace: Close
[0m21:39:08.893261 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a7-1656-1267-8400-509ef37810a1
[0m21:39:09.110947 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:39:09.111945 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:39:09.172810 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:09.173781 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:39:09.173781 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:39:09.174778 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:39:09.985394 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a7-174d-1f7e-aa57-e854e193694e
[0m21:39:10.357993 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m21:39:10.359985 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:39:10.359985 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:39:10.360982 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:39:10.360982 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:39:10.361983 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a7-174d-1f7e-aa57-e854e193694e
[0m21:39:10.601978 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:39:10.608960 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:39:10.609957 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:39:10.610954 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:39:11.399076 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a7-1825-156f-94df-beb0343244fd
[0m21:39:11.778165 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m21:39:11.784150 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:39:11.785147 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a7-1825-156f-94df-beb0343244fd
[0m21:39:12.012029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC83344D0>]}
[0m21:39:12.013014 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:12.013984 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:39:12.013984 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:39:12.015010 [info ] [MainThread]: 
[0m21:39:12.022623 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m21:39:12.023622 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m21:39:12.025639 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m21:39:12.025639 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m21:39:12.031601 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m21:39:12.032599 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 21:39:12.026615 => 21:39:12.032599
[0m21:39:12.033627 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m21:39:12.067505 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m21:39:12.068502 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:12.069499 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m21:39:12.069499 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m21:39:12.070496 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:39:12.852345 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-1903-1756-8377-7bde9efffa1e
[0m21:39:13.688655 [debug] [Thread-1 (]: SQL status: OK in 1.6200000047683716 seconds
[0m21:39:13.704611 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 21:39:12.033627 => 21:39:13.704611
[0m21:39:13.705609 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m21:39:13.705609 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:39:13.706605 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m21:39:13.706605 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-1903-1756-8377-7bde9efffa1e
[0m21:39:13.943591 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC824A290>]}
[0m21:39:13.944591 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.92s]
[0m21:39:13.945604 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m21:39:13.946585 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m21:39:13.947584 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m21:39:13.948578 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m21:39:13.949577 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m21:39:13.953568 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m21:39:13.954576 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 21:39:13.949577 => 21:39:13.954576
[0m21:39:13.955560 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m21:39:13.961545 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m21:39:13.962542 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:13.963554 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m21:39:13.963554 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m21:39:13.964539 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:39:14.778362 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-1a29-1b89-bee1-9d52582102de
[0m21:39:15.471660 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m21:39:15.475649 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 21:39:13.956558 => 21:39:15.475649
[0m21:39:15.476645 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m21:39:15.477642 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:39:15.478640 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m21:39:15.479638 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-1a29-1b89-bee1-9d52582102de
[0m21:39:15.766796 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC82CBED0>]}
[0m21:39:15.768790 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.82s]
[0m21:39:15.770049 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m21:39:15.771078 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m21:39:15.772049 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m21:39:15.773941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m21:39:15.773941 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m21:39:15.776933 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m21:39:15.777914 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 21:39:15.774911 => 21:39:15.777914
[0m21:39:15.778901 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m21:39:15.784911 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m21:39:15.786878 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:15.786878 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m21:39:15.787875 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m21:39:15.788872 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:39:16.597312 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-1b3e-12ed-a437-a7a75c515d41
[0m21:39:17.443134 [debug] [Thread-1 (]: SQL status: OK in 1.659999966621399 seconds
[0m21:39:17.446126 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 21:39:15.778901 => 21:39:17.446126
[0m21:39:17.447124 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m21:39:17.447124 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:39:17.448119 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m21:39:17.448119 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-1b3e-12ed-a437-a7a75c515d41
[0m21:39:17.663989 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC8420410>]}
[0m21:39:17.664985 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.89s]
[0m21:39:17.667352 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m21:39:17.668354 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:39:17.669459 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m21:39:17.671618 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m21:39:17.672585 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m21:39:17.679599 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:39:17.681560 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 21:39:17.672585 => 21:39:17.681560
[0m21:39:17.682589 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m21:39:17.687543 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:39:17.689537 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:17.690543 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:39:17.690543 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(ORIGEM AS STRING) AS nascimento_id,
    TO_DATE(CAST(DTNASC AS STRING), 'yyyyMMdd') AS data_nascimento,
    CAST(CODMUNNASC AS STRING) AS cod_municipio_nasc,
    CAST(CODMUNRES AS STRING) AS cod_municipio_res,
    CAST(SEXO AS INT) AS sexo,
    CAST(PESO AS INT) AS peso_gramas,
    CAST(IDADEMAE AS INT) AS idade_mae,
    CAST(GESTACAO AS INT) AS gestacao_semanas
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CAST(CODMUNRES AS STRING) LIKE '42%'  -- códigos IBGE que começam com 42 = SC

[0m21:39:17.691532 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:39:18.489140 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-1c5c-1c8c-924a-c9c93e0f91f8
[0m21:39:19.209089 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m21:39:19.212079 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 21:39:17.683554 => 21:39:19.212079
[0m21:39:19.212079 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m21:39:19.213075 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:39:19.213075 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m21:39:19.214073 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-1c5c-1c8c-924a-c9c93e0f91f8
[0m21:39:19.447270 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC82CBD50>]}
[0m21:39:19.450263 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.78s]
[0m21:39:19.454282 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:39:19.456277 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m21:39:19.457267 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m21:39:19.459466 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m21:39:19.460464 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m21:39:19.464452 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m21:39:19.466448 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 21:39:19.460464 => 21:39:19.465450
[0m21:39:19.466448 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m21:39:19.471434 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m21:39:19.472432 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:19.473429 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m21:39:19.473429 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m21:39:19.474426 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:39:20.322378 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-1d77-1d75-87b6-41bba21c109a
[0m21:39:21.048863 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m21:39:21.051852 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 21:39:19.467444 => 21:39:21.051852
[0m21:39:21.052857 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m21:39:21.052857 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:39:21.053847 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m21:39:21.053847 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-1d77-1d75-87b6-41bba21c109a
[0m21:39:21.300294 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC8267890>]}
[0m21:39:21.303283 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.84s]
[0m21:39:21.303850 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m21:39:21.304879 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m21:39:21.305847 [info ] [Thread-1 (]: 6 of 12 START sql view model default.dim_localidade ............................ [RUN]
[0m21:39:21.306845 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m21:39:21.307873 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m21:39:21.310862 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m21:39:21.311859 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 21:39:21.307873 => 21:39:21.311859
[0m21:39:21.312856 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m21:39:21.317843 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m21:39:21.318843 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:21.319823 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m21:39:21.319823 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m21:39:21.320819 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:39:22.105392 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-1e87-1a94-9733-f91da78ef0d0
[0m21:39:22.759873 [debug] [Thread-1 (]: SQL status: OK in 1.440000057220459 seconds
[0m21:39:22.762862 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 21:39:21.312856 => 21:39:22.762862
[0m21:39:22.762862 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m21:39:22.763860 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:39:22.764857 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m21:39:22.764857 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-1e87-1a94-9733-f91da78ef0d0
[0m21:39:22.997066 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC8403650>]}
[0m21:39:22.998064 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.dim_localidade ....................... [[32mOK[0m in 1.69s]
[0m21:39:22.999061 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m21:39:23.000059 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m21:39:23.000059 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m21:39:23.002339 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m21:39:23.002339 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m21:39:23.008330 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m21:39:23.010296 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 21:39:23.003327 => 21:39:23.009299
[0m21:39:23.010296 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m21:39:23.015282 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m21:39:23.016279 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:23.016279 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m21:39:23.017277 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m21:39:23.017277 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:39:23.835321 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-1f8e-1ac5-bab1-cd7f6e53f3ef
[0m21:39:24.664728 [debug] [Thread-1 (]: SQL status: OK in 1.649999976158142 seconds
[0m21:39:24.667719 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 21:39:23.011293 => 21:39:24.667719
[0m21:39:24.668717 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m21:39:24.668717 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:39:24.669714 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m21:39:24.669714 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-1f8e-1ac5-bab1-cd7f6e53f3ef
[0m21:39:24.899269 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC8397D90>]}
[0m21:39:24.902274 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.90s]
[0m21:39:24.902840 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m21:39:24.903870 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m21:39:24.904851 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m21:39:24.905844 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.stg_tempo)
[0m21:39:24.906860 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m21:39:24.910849 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m21:39:24.911819 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 21:39:24.906860 => 21:39:24.911819
[0m21:39:24.912815 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m21:39:24.920823 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m21:39:24.921820 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:24.922789 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m21:39:24.922789 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(FORMAT_DATE(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    YEAR(data_nascimento) AS ano,
    MONTH(data_nascimento) AS mes,
    DAY(data_nascimento) AS dia,
    QUARTER(data_nascimento) AS trimestre,
    DAYOFWEEK(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m21:39:24.923816 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:39:25.747774 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-20b4-10b8-8699-3c20477ec17b
[0m21:39:26.485854 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(FORMAT_DATE(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    YEAR(data_nascimento) AS ano,
    MONTH(data_nascimento) AS mes,
    DAY(data_nascimento) AS dia,
    QUARTER(data_nascimento) AS trimestre,
    DAYOFWEEK(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m21:39:26.486851 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_ROUTINE] Cannot resolve routine `FORMAT_DATE` on search path [`system`.`builtin`, `system`.`session`, `workspace`.`default`].
Verify the spelling of `FORMAT_DATE`, check that the routine exists, and confirm you have `USE` privilege on the catalog and schema, and EXECUTE on the routine. SQLSTATE: 42883; line 10 pos 9
[0m21:39:26.488846 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_ROUTINE] org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve routine `FORMAT_DATE` on search path [`system`.`builtin`, `system`.`session`, `workspace`.`default`].
Verify the spelling of `FORMAT_DATE`, check that the routine exists, and confirm you have `USE` privilege on the catalog and schema, and EXECUTE on the routine. SQLSTATE: 42883; line 10 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve routine `FORMAT_DATE` on search path [`system`.`builtin`, `system`.`session`, `workspace`.`default`].
Verify the spelling of `FORMAT_DATE`, check that the routine exists, and confirm you have `USE` privilege on the catalog and schema, and EXECUTE on the routine. SQLSTATE: 42883; line 10 pos 9
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedRoutineError(QueryCompilationErrors.scala:1239)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2705)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2682)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:189)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:248)
	at scala.collection.immutable.List.map(List.scala:251)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:248)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:189)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:160)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:308)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:307)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.plans.logical.Distinct.mapChildren(basicLogicalOperators.scala:2479)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1360)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1357)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1496)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:307)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:304)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:279)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:277)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2682)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2676)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)
	at scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedRoutineError(QueryCompilationErrors.scala:1239)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2705)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2682)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:189)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:248)
		at scala.collection.immutable.List.map(List.scala:251)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:248)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:189)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:160)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:308)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:307)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.plans.logical.Distinct.mapChildren(basicLogicalOperators.scala:2479)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1360)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1357)
		at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1496)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:307)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:304)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:279)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:277)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2682)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2676)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)
		at scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 60 more

[0m21:39:26.491838 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078a7-20d3-1f9b-97f1-f9aabcd2d734
[0m21:39:26.491838 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 21:39:24.912815 => 21:39:26.491838
[0m21:39:26.492835 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m21:39:26.493836 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:39:26.493836 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m21:39:26.494830 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-20b4-10b8-8699-3c20477ec17b
[0m21:39:26.738609 [debug] [Thread-1 (]: Runtime Error in model stg_tempo (models\staging\stg_tempo.sql)
  [UNRESOLVED_ROUTINE] Cannot resolve routine `FORMAT_DATE` on search path [`system`.`builtin`, `system`.`session`, `workspace`.`default`].
  Verify the spelling of `FORMAT_DATE`, check that the routine exists, and confirm you have `USE` privilege on the catalog and schema, and EXECUTE on the routine. SQLSTATE: 42883; line 10 pos 9
[0m21:39:26.739606 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC847FE90>]}
[0m21:39:26.740604 [error] [Thread-1 (]: 8 of 12 ERROR creating sql view model default.stg_tempo ........................ [[31mERROR[0m in 1.83s]
[0m21:39:26.741280 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m21:39:26.742305 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:39:26.743186 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m21:39:26.744072 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m21:39:26.745096 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:39:26.749085 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:39:26.750059 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 21:39:26.745096 => 21:39:26.750059
[0m21:39:26.751071 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:39:26.755046 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:39:26.757051 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:26.758037 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:39:26.759034 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m21:39:26.759034 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:39:27.570032 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-21c8-1893-a517-0009e7540d06
[0m21:39:28.396025 [debug] [Thread-1 (]: SQL status: OK in 1.6399999856948853 seconds
[0m21:39:28.398050 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 21:39:26.751071 => 21:39:28.398050
[0m21:39:28.399044 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m21:39:28.399044 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:39:28.400043 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m21:39:28.401011 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-21c8-1893-a517-0009e7540d06
[0m21:39:28.639470 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19bbcd61-e642-482e-8bcf-1106b9ca0433', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EC8303E50>]}
[0m21:39:28.640466 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.90s]
[0m21:39:28.640962 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:39:28.641990 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m21:39:28.642731 [info ] [Thread-1 (]: 10 of 12 SKIP relation default.dim_tempo ....................................... [[33mSKIP[0m]
[0m21:39:28.643442 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m21:39:28.644443 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m21:39:28.644443 [info ] [Thread-1 (]: 11 of 12 SKIP relation default.int_nascimento .................................. [[33mSKIP[0m]
[0m21:39:28.645486 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m21:39:28.646465 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m21:39:28.647460 [info ] [Thread-1 (]: 12 of 12 SKIP relation default.fato_nascimento ................................. [[33mSKIP[0m]
[0m21:39:28.648431 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m21:39:28.651424 [debug] [MainThread]: On master: ROLLBACK
[0m21:39:28.651424 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:39:29.468204 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a7-22ec-10df-bb24-3fa98b483bb4
[0m21:39:29.469205 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:39:29.469205 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:39:29.470200 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:39:29.470200 [debug] [MainThread]: On master: ROLLBACK
[0m21:39:29.471197 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:39:29.472195 [debug] [MainThread]: On master: Close
[0m21:39:29.472195 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a7-22ec-10df-bb24-3fa98b483bb4
[0m21:39:29.696648 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:39:29.698615 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:39:29.700638 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:39:29.701641 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_atendimento_hospitalar' was properly closed.
[0m21:39:29.705625 [info ] [MainThread]: 
[0m21:39:29.708587 [info ] [MainThread]: Finished running 10 view models, 2 table models in 0 hours 0 minutes and 22.25 seconds (22.25s).
[0m21:39:29.715596 [debug] [MainThread]: Command end result
[0m21:39:29.731554 [info ] [MainThread]: 
[0m21:39:29.733517 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:39:29.734538 [info ] [MainThread]: 
[0m21:39:29.735686 [error] [MainThread]: [33mRuntime Error in model stg_tempo (models\staging\stg_tempo.sql)[0m
[0m21:39:29.736686 [error] [MainThread]:   [UNRESOLVED_ROUTINE] Cannot resolve routine `FORMAT_DATE` on search path [`system`.`builtin`, `system`.`session`, `workspace`.`default`].
[0m21:39:29.737685 [error] [MainThread]:   Verify the spelling of `FORMAT_DATE`, check that the routine exists, and confirm you have `USE` privilege on the catalog and schema, and EXECUTE on the routine. SQLSTATE: 42883; line 10 pos 9
[0m21:39:29.738681 [info ] [MainThread]: 
[0m21:39:29.739678 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=1 SKIP=3 TOTAL=12
[0m21:39:29.741697 [debug] [MainThread]: Command `dbt run` failed at 21:39:29.740704 after 23.75 seconds
[0m21:39:29.741697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EBDC74190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EB78BDE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025EBD79ED90>]}
[0m21:39:29.742685 [debug] [MainThread]: Flushing usage events
[0m21:44:18.140177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289AAD932D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289AAD47F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289AAB66C50>]}


============================== 21:44:18.144166 | 2905e31b-7ab7-4836-b44f-61db96b93fd2 ==============================
[0m21:44:18.144166 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:44:18.145135 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:44:19.403764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2905e31b-7ab7-4836-b44f-61db96b93fd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289AAADFA10>]}
[0m21:44:19.422712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2905e31b-7ab7-4836-b44f-61db96b93fd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289AAADFA10>]}
[0m21:44:19.423711 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:44:19.444654 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:44:19.540402 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:44:19.540402 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_tempo.sql
[0m21:44:19.567323 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_tempo.sql
[0m21:44:19.653098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2905e31b-7ab7-4836-b44f-61db96b93fd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289B4392350>]}
[0m21:44:19.665037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2905e31b-7ab7-4836-b44f-61db96b93fd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289B55F5810>]}
[0m21:44:19.665037 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:44:19.666064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2905e31b-7ab7-4836-b44f-61db96b93fd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289B1B7E810>]}
[0m21:44:19.668057 [info ] [MainThread]: 
[0m21:44:19.669030 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:44:19.672041 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:44:19.672041 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:44:19.673037 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:44:19.673037 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:44:20.693578 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a7-d07e-15d8-b8cd-4f291bfce8f0
[0m21:44:21.228025 [debug] [ThreadPool]: SQL status: OK in 1.5499999523162842 seconds
[0m21:44:21.231684 [debug] [ThreadPool]: On list_workspace: Close
[0m21:44:21.231684 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a7-d07e-15d8-b8cd-4f291bfce8f0
[0m21:44:21.469052 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:44:21.472045 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:44:21.482011 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:44:21.483009 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:44:21.483009 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:44:21.484006 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:44:22.332930 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a7-d178-1665-b137-d7aa58a8cfec
[0m21:44:22.784773 [debug] [ThreadPool]: SQL status: OK in 1.2999999523162842 seconds
[0m21:44:22.786765 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:44:22.786765 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:44:22.787762 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:44:22.787762 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:44:22.788759 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a7-d178-1665-b137-d7aa58a8cfec
[0m21:44:23.021383 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:44:23.032358 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:44:23.033355 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:44:23.033355 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:44:23.854535 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a7-d25f-1a28-b550-770bcc7ed3e4
[0m21:44:24.214892 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m21:44:24.217884 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:44:24.218881 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a7-d25f-1a28-b550-770bcc7ed3e4
[0m21:44:24.461216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2905e31b-7ab7-4836-b44f-61db96b93fd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289AB3EBDD0>]}
[0m21:44:24.462222 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:44:24.462222 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:44:24.463212 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:44:24.464209 [info ] [MainThread]: 
[0m21:44:24.471442 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m21:44:24.472441 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m21:44:24.474153 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m21:44:24.475151 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m21:44:24.480165 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m21:44:24.482145 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 21:44:24.476157 => 21:44:24.481162
[0m21:44:24.483129 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m21:44:24.516069 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m21:44:24.518046 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:44:24.518046 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m21:44:24.519032 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m21:44:24.519032 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:44:25.316868 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-d341-12f6-98f6-49660634814b
[0m21:44:26.070542 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m21:44:26.084505 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 21:44:24.483129 => 21:44:26.084505
[0m21:44:26.085502 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m21:44:26.085502 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:44:26.086500 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m21:44:26.087497 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-d341-12f6-98f6-49660634814b
[0m21:44:26.321691 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2905e31b-7ab7-4836-b44f-61db96b93fd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289B45EBC10>]}
[0m21:44:26.324684 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.85s]
[0m21:44:26.327971 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m21:44:26.329929 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m21:44:26.331925 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m21:44:26.333824 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m21:44:26.333824 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m21:44:26.337831 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m21:44:26.338834 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 21:44:26.334840 => 21:44:26.338834
[0m21:44:26.338834 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m21:44:26.343817 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m21:44:26.345783 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:44:26.345783 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m21:44:26.346781 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m21:44:26.347776 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:44:27.148854 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-d458-173a-92ab-222257ae8c84
[0m21:44:27.849952 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m21:44:27.852942 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 21:44:26.339827 => 21:44:27.852942
[0m21:44:27.853939 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m21:44:27.853939 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:44:27.854936 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m21:44:27.855934 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-d458-173a-92ab-222257ae8c84
[0m21:44:28.088536 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2905e31b-7ab7-4836-b44f-61db96b93fd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289B56D6950>]}
[0m21:44:28.090533 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.76s]
[0m21:44:28.091911 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m21:44:28.092939 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m21:44:28.093908 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m21:44:28.094908 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m21:44:28.095911 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m21:44:28.099919 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m21:44:28.100920 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 21:44:28.095911 => 21:44:28.100920
[0m21:44:28.101887 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m21:44:28.106888 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m21:44:28.107873 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:44:28.108868 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m21:44:28.108868 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m21:44:28.109865 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:44:28.904573 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-d564-14ed-95aa-ee89b38b007e
[0m21:44:29.630188 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m21:44:29.633180 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 21:44:28.101887 => 21:44:29.633180
[0m21:44:29.634177 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m21:44:29.634177 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:44:29.635183 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m21:44:29.635183 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-d564-14ed-95aa-ee89b38b007e
[0m21:44:29.868815 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2905e31b-7ab7-4836-b44f-61db96b93fd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289B456F410>]}
[0m21:44:29.870806 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.77s]
[0m21:44:29.871803 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m21:44:29.872800 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:44:29.872800 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m21:44:29.874795 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m21:44:29.874795 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m21:44:29.879782 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:44:29.880779 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 21:44:29.875793 => 21:44:29.880779
[0m21:44:29.881776 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m21:44:29.888757 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:44:29.890752 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:44:29.891750 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:44:29.891750 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(ORIGEM AS STRING) AS nascimento_id,
    TO_DATE(CAST(DTNASC AS STRING), 'yyyyMMdd') AS data_nascimento,
    CAST(CODMUNNASC AS STRING) AS cod_municipio_nasc,
    CAST(CODMUNRES AS STRING) AS cod_municipio_res,
    CAST(SEXO AS INT) AS sexo,
    CAST(PESO AS INT) AS peso_gramas,
    CAST(IDADEMAE AS INT) AS idade_mae,
    CAST(GESTACAO AS INT) AS gestacao_semanas
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CAST(CODMUNRES AS STRING) LIKE '42%'  -- códigos IBGE que começam com 42 = SC

[0m21:44:29.892759 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:44:30.702216 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-d677-1ca9-8a5a-dae5edcc2617
[0m21:44:31.390174 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m21:44:31.393167 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 21:44:29.881776 => 21:44:31.393167
[0m21:44:31.393167 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m21:44:31.394164 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:44:31.395166 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m21:44:31.395166 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-d677-1ca9-8a5a-dae5edcc2617
[0m21:44:31.637786 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2905e31b-7ab7-4836-b44f-61db96b93fd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289B5826710>]}
[0m21:44:31.638782 [info ] [Thread-1 (]: 4 of 5 OK created sql view model default.stg_nascidos_vivos .................... [[32mOK[0m in 1.76s]
[0m21:44:31.639377 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:44:31.640405 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m21:44:31.641404 [info ] [Thread-1 (]: 5 of 5 START sql view model default.stg_tempo .................................. [RUN]
[0m21:44:31.642371 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.stg_tempo)
[0m21:44:31.643399 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m21:44:31.647359 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m21:44:31.648384 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 21:44:31.643399 => 21:44:31.648384
[0m21:44:31.649353 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m21:44:31.654339 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m21:44:31.655336 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:44:31.656334 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m21:44:31.656334 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m21:44:31.657331 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:44:32.454932 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-d782-1b6e-8be9-6d3374b9c216
[0m21:44:33.066897 [debug] [Thread-1 (]: SQL status: OK in 1.409999966621399 seconds
[0m21:44:33.068889 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 21:44:31.649353 => 21:44:33.068889
[0m21:44:33.069886 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m21:44:33.070883 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:44:33.070883 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m21:44:33.071881 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-d782-1b6e-8be9-6d3374b9c216
[0m21:44:33.315991 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2905e31b-7ab7-4836-b44f-61db96b93fd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289B580F450>]}
[0m21:44:33.316998 [info ] [Thread-1 (]: 5 of 5 OK created sql view model default.stg_tempo ............................. [[32mOK[0m in 1.67s]
[0m21:44:33.317986 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m21:44:33.321007 [debug] [MainThread]: On master: ROLLBACK
[0m21:44:33.321007 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:44:34.152230 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a7-d886-1425-9283-722734a0a780
[0m21:44:34.153906 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:44:34.154923 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:44:34.155935 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:44:34.156929 [debug] [MainThread]: On master: ROLLBACK
[0m21:44:34.157927 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:44:34.157927 [debug] [MainThread]: On master: Close
[0m21:44:34.158917 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a7-d886-1425-9283-722734a0a780
[0m21:44:34.403317 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:44:34.404315 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:44:34.405308 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:44:34.405308 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_tempo' was properly closed.
[0m21:44:34.407303 [info ] [MainThread]: 
[0m21:44:34.407745 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 14.74 seconds (14.74s).
[0m21:44:34.409771 [debug] [MainThread]: Command end result
[0m21:44:34.419747 [info ] [MainThread]: 
[0m21:44:34.420735 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:44:34.421711 [info ] [MainThread]: 
[0m21:44:34.423706 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m21:44:34.424703 [debug] [MainThread]: Command `dbt run` succeeded at 21:44:34.424703 after 16.30 seconds
[0m21:44:34.425700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289AA756D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289A48CBE10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289A48CBDD0>]}
[0m21:44:34.426697 [debug] [MainThread]: Flushing usage events
[0m21:44:52.434420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A47CB116D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A47C831890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A47D154D50>]}


============================== 21:44:52.440404 | 819f7c10-6ac5-47eb-aa71-81bf81102f90 ==============================
[0m21:44:52.440404 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:44:52.442375 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:44:53.762492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '819f7c10-6ac5-47eb-aa71-81bf81102f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A47C8FB910>]}
[0m21:44:53.781440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '819f7c10-6ac5-47eb-aa71-81bf81102f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A405421B10>]}
[0m21:44:53.781440 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:44:53.798594 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:44:53.904312 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:44:53.905280 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:44:53.911292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '819f7c10-6ac5-47eb-aa71-81bf81102f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4062A8250>]}
[0m21:44:53.922235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '819f7c10-6ac5-47eb-aa71-81bf81102f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4063C98D0>]}
[0m21:44:53.922235 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:44:53.923400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '819f7c10-6ac5-47eb-aa71-81bf81102f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A47F8F9090>]}
[0m21:44:53.925425 [info ] [MainThread]: 
[0m21:44:53.926395 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:44:53.928401 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:44:53.929415 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:44:53.930385 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:44:53.931382 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:44:54.720279 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a7-e4c8-17a9-84cb-ed14672f3f16
[0m21:44:55.020901 [debug] [ThreadPool]: SQL status: OK in 1.090000033378601 seconds
[0m21:44:55.025176 [debug] [ThreadPool]: On list_workspace: Close
[0m21:44:55.026174 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a7-e4c8-17a9-84cb-ed14672f3f16
[0m21:44:55.266008 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:44:55.269997 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:44:55.333823 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:44:55.334820 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:44:55.334820 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:44:55.335819 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:44:56.118290 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a7-e59d-1145-80fd-5937d33c77fb
[0m21:44:56.495277 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m21:44:56.496267 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:44:56.497273 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:44:56.497273 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:44:56.498272 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:44:56.498272 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a7-e59d-1145-80fd-5937d33c77fb
[0m21:44:56.736661 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:44:56.751619 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:44:56.753586 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:44:56.754582 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:44:57.554823 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a7-e677-1cb3-a257-d18246f3819e
[0m21:44:57.895739 [debug] [ThreadPool]: SQL status: OK in 1.1399999856948853 seconds
[0m21:44:57.898726 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:44:57.899724 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a7-e677-1cb3-a257-d18246f3819e
[0m21:44:58.143921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '819f7c10-6ac5-47eb-aa71-81bf81102f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A47D160810>]}
[0m21:44:58.145913 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:44:58.145913 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:44:58.146907 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:44:58.148164 [info ] [MainThread]: 
[0m21:44:58.154807 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m21:44:58.155815 [info ] [Thread-1 (]: 1 of 2 START sql view model default.int_atendimento ............................ [RUN]
[0m21:44:58.157781 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.int_atendimento'
[0m21:44:58.158778 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m21:44:58.164789 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m21:44:58.165760 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 21:44:58.159786 => 21:44:58.165760
[0m21:44:58.166784 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m21:44:58.200694 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m21:44:58.201664 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:44:58.202660 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m21:44:58.202660 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m21:44:58.203675 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:44:58.973329 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-e751-1c8e-b1c8-536907947e45
[0m21:44:59.750669 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m21:44:59.765654 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 21:44:58.166784 => 21:44:59.765654
[0m21:44:59.766651 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m21:44:59.766651 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:44:59.767621 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m21:44:59.768618 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-e751-1c8e-b1c8-536907947e45
[0m21:45:00.000238 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '819f7c10-6ac5-47eb-aa71-81bf81102f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4063D09D0>]}
[0m21:45:00.001231 [info ] [Thread-1 (]: 1 of 2 OK created sql view model default.int_atendimento ....................... [[32mOK[0m in 1.84s]
[0m21:45:00.002270 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m21:45:00.003301 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m21:45:00.004281 [info ] [Thread-1 (]: 2 of 2 START sql view model default.int_nascimento ............................. [RUN]
[0m21:45:00.005265 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m21:45:00.006291 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m21:45:00.010279 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m21:45:00.011249 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 21:45:00.006291 => 21:45:00.011249
[0m21:45:00.012246 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m21:45:00.017232 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m21:45:00.019247 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:45:00.020238 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m21:45:00.020238 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

SELECT
    n.nascimento_id,
    t.data_id,
    n.municipio,
    n.uf,
    n.sexo,
    n.peso,
    n.idade_mae,
    n.gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m21:45:00.021244 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:45:00.837559 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a7-e86d-1848-a4ca-e84d86798c37
[0m21:45:01.274063 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

SELECT
    n.nascimento_id,
    t.data_id,
    n.municipio,
    n.uf,
    n.sexo,
    n.peso,
    n.idade_mae,
    n.gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m21:45:01.274063 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`municipio` cannot be resolved. Did you mean one of the following? [`t`.`dia`, `t`.`ano`, `t`.`data`, `t`.`mes`, `n`.`sexo`]. SQLSTATE: 42703; line 11 pos 4
[0m21:45:01.275054 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`municipio` cannot be resolved. Did you mean one of the following? [`t`.`dia`, `t`.`ano`, `t`.`data`, `t`.`mes`, `n`.`sexo`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`municipio` cannot be resolved. Did you mean one of the following? [`t`.`dia`, `t`.`ano`, `t`.`data`, `t`.`mes`, `n`.`sexo`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m21:45:01.276052 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078a7-e88c-1c29-9c85-415d95780e11
[0m21:45:01.277050 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 21:45:00.013245 => 21:45:01.277050
[0m21:45:01.278019 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m21:45:01.278019 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:45:01.279015 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m21:45:01.279015 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a7-e86d-1848-a4ca-e84d86798c37
[0m21:45:01.527997 [debug] [Thread-1 (]: Runtime Error in model int_nascimento (models\intermediate\int_nascimento.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`municipio` cannot be resolved. Did you mean one of the following? [`t`.`dia`, `t`.`ano`, `t`.`data`, `t`.`mes`, `n`.`sexo`]. SQLSTATE: 42703; line 11 pos 4
[0m21:45:01.528995 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '819f7c10-6ac5-47eb-aa71-81bf81102f90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A407425990>]}
[0m21:45:01.528995 [error] [Thread-1 (]: 2 of 2 ERROR creating sql view model default.int_nascimento .................... [[31mERROR[0m in 1.52s]
[0m21:45:01.530237 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m21:45:01.532244 [debug] [MainThread]: On master: ROLLBACK
[0m21:45:01.533232 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:45:02.349769 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a7-e953-1ecc-a0e7-7b54139f14e6
[0m21:45:02.351341 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:45:02.352372 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:45:02.352372 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:45:02.353368 [debug] [MainThread]: On master: ROLLBACK
[0m21:45:02.354335 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:45:02.354335 [debug] [MainThread]: On master: Close
[0m21:45:02.355362 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a7-e953-1ecc-a0e7-7b54139f14e6
[0m21:45:02.575405 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:45:02.575405 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:45:02.576402 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:45:02.576402 [debug] [MainThread]: Connection 'model.projeto_health_insights.int_nascimento' was properly closed.
[0m21:45:02.577399 [info ] [MainThread]: 
[0m21:45:02.578567 [info ] [MainThread]: Finished running 2 view models in 0 hours 0 minutes and 8.65 seconds (8.65s).
[0m21:45:02.579567 [debug] [MainThread]: Command end result
[0m21:45:02.594534 [info ] [MainThread]: 
[0m21:45:02.596535 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:45:02.597520 [info ] [MainThread]: 
[0m21:45:02.598518 [error] [MainThread]: [33mRuntime Error in model int_nascimento (models\intermediate\int_nascimento.sql)[0m
[0m21:45:02.599526 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`municipio` cannot be resolved. Did you mean one of the following? [`t`.`dia`, `t`.`ano`, `t`.`data`, `t`.`mes`, `n`.`sexo`]. SQLSTATE: 42703; line 11 pos 4
[0m21:45:02.601510 [info ] [MainThread]: 
[0m21:45:02.602523 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m21:45:02.604529 [debug] [MainThread]: Command `dbt run` failed at 21:45:02.603533 after 10.19 seconds
[0m21:45:02.604529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A47CBE5450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A47C859810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A47C85B190>]}
[0m21:45:02.605525 [debug] [MainThread]: Flushing usage events
[0m21:46:46.724231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023393AC76D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023393AC7FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023393261E10>]}


============================== 21:46:46.729850 | c0db1cd3-aafe-4dbc-aee5-10d18a026ffa ==============================
[0m21:46:46.729850 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:46:46.731318 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:46:48.039872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c0db1cd3-aafe-4dbc-aee5-10d18a026ffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023393AC7CD0>]}
[0m21:46:48.057824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c0db1cd3-aafe-4dbc-aee5-10d18a026ffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023393AD0550>]}
[0m21:46:48.058822 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:46:48.075277 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:46:48.175986 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:46:48.176983 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\intermediate\int_nascimento.sql
[0m21:46:48.203912 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m21:46:48.282701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c0db1cd3-aafe-4dbc-aee5-10d18a026ffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002339CB0FD90>]}
[0m21:46:48.293672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c0db1cd3-aafe-4dbc-aee5-10d18a026ffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002339CCE0410>]}
[0m21:46:48.294669 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:46:48.295675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c0db1cd3-aafe-4dbc-aee5-10d18a026ffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002339A27DC50>]}
[0m21:46:48.297701 [info ] [MainThread]: 
[0m21:46:48.298671 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:46:48.300664 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:46:48.301662 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:46:48.301662 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:46:48.302687 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:46:49.140279 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a8-28fa-1619-b45a-29769dbae77f
[0m21:46:49.478320 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m21:46:49.482309 [debug] [ThreadPool]: On list_workspace: Close
[0m21:46:49.483305 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a8-28fa-1619-b45a-29769dbae77f
[0m21:46:49.710930 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:46:49.711926 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:46:49.722896 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:46:49.723894 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:46:49.723894 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:46:49.724891 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:46:50.529455 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a8-29ce-100e-bbed-4d20ecf047d5
[0m21:46:50.950197 [debug] [ThreadPool]: SQL status: OK in 1.2300000190734863 seconds
[0m21:46:50.951183 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:46:50.952181 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:46:50.953178 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:46:50.953178 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:46:50.954176 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a8-29ce-100e-bbed-4d20ecf047d5
[0m21:46:51.212266 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:46:51.218245 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:46:51.219243 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:46:51.220245 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:46:52.019516 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a8-2ab0-15ef-9667-16763044d83b
[0m21:46:52.384323 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m21:46:52.387318 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:46:52.388304 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a8-2ab0-15ef-9667-16763044d83b
[0m21:46:52.642013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c0db1cd3-aafe-4dbc-aee5-10d18a026ffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002339DDD09D0>]}
[0m21:46:52.643010 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:46:52.643010 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:46:52.644028 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:46:52.645040 [info ] [MainThread]: 
[0m21:46:52.652217 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m21:46:52.653215 [info ] [Thread-1 (]: 1 of 2 START sql view model default.int_atendimento ............................ [RUN]
[0m21:46:52.655210 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.int_atendimento'
[0m21:46:52.656207 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m21:46:52.660196 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m21:46:52.661193 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 21:46:52.656207 => 21:46:52.661193
[0m21:46:52.662191 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m21:46:52.696128 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m21:46:52.697098 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:46:52.698123 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m21:46:52.699100 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m21:46:52.700090 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:46:53.487702 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a8-2b92-125b-a7f8-34d6a6184710
[0m21:46:54.319163 [debug] [Thread-1 (]: SQL status: OK in 1.6200000047683716 seconds
[0m21:46:54.335093 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 21:46:52.662191 => 21:46:54.334096
[0m21:46:54.335093 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m21:46:54.336118 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:46:54.336118 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m21:46:54.337115 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a8-2b92-125b-a7f8-34d6a6184710
[0m21:46:54.578965 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c0db1cd3-aafe-4dbc-aee5-10d18a026ffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002339CBBBC10>]}
[0m21:46:54.581957 [info ] [Thread-1 (]: 1 of 2 OK created sql view model default.int_atendimento ....................... [[32mOK[0m in 1.92s]
[0m21:46:54.586235 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m21:46:54.588200 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m21:46:54.590195 [info ] [Thread-1 (]: 2 of 2 START sql view model default.int_nascimento ............................. [RUN]
[0m21:46:54.595207 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m21:46:54.597193 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m21:46:54.603162 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m21:46:54.604151 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 21:46:54.598168 => 21:46:54.604151
[0m21:46:54.605148 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m21:46:54.610137 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m21:46:54.611134 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:46:54.612131 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m21:46:54.613129 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    SELECT
    n.nascimento_id,
    t.data_id,
    n.CODMUNNASC AS municipio,
    n.CODUFNATU AS uf,
    n.SEXO AS sexo,
    n.PESO AS peso,
    n.IDADEMAE AS idade_mae,
    n.GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m21:46:54.613129 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:46:55.455273 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a8-2cbd-1c30-a292-ec240c121e85
[0m21:46:55.892490 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    SELECT
    n.nascimento_id,
    t.data_id,
    n.CODMUNNASC AS municipio,
    n.CODUFNATU AS uf,
    n.SEXO AS sexo,
    n.PESO AS peso,
    n.IDADEMAE AS idade_mae,
    n.GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m21:46:55.893487 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`CODMUNNASC` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`data`, `t`.`dia`, `t`.`mes`, `n`.`sexo`]. SQLSTATE: 42703; line 9 pos 4
[0m21:46:55.894485 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`CODMUNNASC` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`data`, `t`.`dia`, `t`.`mes`, `n`.`sexo`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`CODMUNNASC` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`data`, `t`.`dia`, `t`.`mes`, `n`.`sexo`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m21:46:55.895481 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078a8-2cde-1b15-aaeb-876e53d314c3
[0m21:46:55.896480 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 21:46:54.605148 => 21:46:55.896480
[0m21:46:55.897476 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m21:46:55.897476 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:46:55.898488 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m21:46:55.899471 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a8-2cbd-1c30-a292-ec240c121e85
[0m21:46:56.142705 [debug] [Thread-1 (]: Runtime Error in model int_nascimento (models\intermediate\int_nascimento.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`CODMUNNASC` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`data`, `t`.`dia`, `t`.`mes`, `n`.`sexo`]. SQLSTATE: 42703; line 9 pos 4
[0m21:46:56.144700 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c0db1cd3-aafe-4dbc-aee5-10d18a026ffa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002339DDEB010>]}
[0m21:46:56.145697 [error] [Thread-1 (]: 2 of 2 ERROR creating sql view model default.int_nascimento .................... [[31mERROR[0m in 1.55s]
[0m21:46:56.147601 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m21:46:56.149594 [debug] [MainThread]: On master: ROLLBACK
[0m21:46:56.150591 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:46:56.928628 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a8-2d9f-1512-a670-10bd3c3f6df2
[0m21:46:56.931582 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:46:56.933614 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:46:56.935575 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:46:56.936567 [debug] [MainThread]: On master: ROLLBACK
[0m21:46:56.936567 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:46:56.937577 [debug] [MainThread]: On master: Close
[0m21:46:56.937577 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a8-2d9f-1512-a670-10bd3c3f6df2
[0m21:46:57.174435 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:46:57.176430 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:46:57.177390 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:46:57.177390 [debug] [MainThread]: Connection 'model.projeto_health_insights.int_nascimento' was properly closed.
[0m21:46:57.178395 [info ] [MainThread]: 
[0m21:46:57.179417 [info ] [MainThread]: Finished running 2 view models in 0 hours 0 minutes and 8.88 seconds (8.88s).
[0m21:46:57.181379 [debug] [MainThread]: Command end result
[0m21:46:57.194344 [info ] [MainThread]: 
[0m21:46:57.195342 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:46:57.196340 [info ] [MainThread]: 
[0m21:46:57.197336 [error] [MainThread]: [33mRuntime Error in model int_nascimento (models\intermediate\int_nascimento.sql)[0m
[0m21:46:57.199332 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`CODMUNNASC` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`data`, `t`.`dia`, `t`.`mes`, `n`.`sexo`]. SQLSTATE: 42703; line 9 pos 4
[0m21:46:57.200329 [info ] [MainThread]: 
[0m21:46:57.201327 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m21:46:57.202323 [debug] [MainThread]: Command `dbt run` failed at 21:46:57.202323 after 10.50 seconds
[0m21:46:57.203351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233931BE610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002339327D590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023393261150>]}
[0m21:46:57.204319 [debug] [MainThread]: Flushing usage events
[0m21:49:46.161043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CAD30C6650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CAD2DDF090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CAD3173150>]}


============================== 21:49:46.165927 | e073e47b-87d4-4681-a135-b469ad74d157 ==============================
[0m21:49:46.165927 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:49:46.166895 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:49:47.564943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e073e47b-87d4-4681-a135-b469ad74d157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CAD36FD650>]}
[0m21:49:47.586857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e073e47b-87d4-4681-a135-b469ad74d157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CADC880850>]}
[0m21:49:47.587873 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:49:47.604808 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:49:47.730473 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:49:47.731470 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m21:49:47.758398 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m21:49:47.838214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e073e47b-87d4-4681-a135-b469ad74d157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CADC89E490>]}
[0m21:49:47.850167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e073e47b-87d4-4681-a135-b469ad74d157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CADC8EF7D0>]}
[0m21:49:47.851179 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:49:47.851179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e073e47b-87d4-4681-a135-b469ad74d157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CAD9E9D090>]}
[0m21:49:47.854173 [info ] [MainThread]: 
[0m21:49:47.855169 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:49:47.857134 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:49:47.857134 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:49:47.858135 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:49:47.858135 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:49:48.894708 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a8-941d-1c4f-b711-841e61b120c9
[0m21:49:49.496502 [debug] [ThreadPool]: SQL status: OK in 1.6399999856948853 seconds
[0m21:49:49.500198 [debug] [ThreadPool]: On list_workspace: Close
[0m21:49:49.501196 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a8-941d-1c4f-b711-841e61b120c9
[0m21:49:49.727938 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:49:49.731928 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:49:49.754856 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:49.755854 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:49:49.755854 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:49:49.756851 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:49:50.562336 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a8-951c-1e68-9228-71262f1c0a31
[0m21:49:50.957384 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m21:49:50.958381 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:49:50.959378 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:49:50.959378 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:49:50.960385 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:49:50.960385 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a8-951c-1e68-9228-71262f1c0a31
[0m21:49:51.196297 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:49:51.202283 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:49:51.203280 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:49:51.203280 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:49:52.009508 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a8-95fa-17d1-a7ec-411ca4370f5c
[0m21:49:52.359386 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m21:49:52.362377 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:49:52.363374 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a8-95fa-17d1-a7ec-411ca4370f5c
[0m21:49:52.597691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e073e47b-87d4-4681-a135-b469ad74d157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CADD949610>]}
[0m21:49:52.599684 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:52.601644 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:49:52.604635 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:49:52.607624 [info ] [MainThread]: 
[0m21:49:52.616598 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m21:49:52.618593 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m21:49:52.619590 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m21:49:52.620593 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m21:49:52.625602 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m21:49:52.626622 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 21:49:52.620593 => 21:49:52.626622
[0m21:49:52.627603 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m21:49:52.663500 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m21:49:52.665471 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:52.665471 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m21:49:52.666464 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m21:49:52.666464 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:49:54.754025 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a8-979b-1d8d-a671-07f7973bf30f
[0m21:49:55.700963 [debug] [Thread-1 (]: SQL status: OK in 3.0299999713897705 seconds
[0m21:49:55.716948 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 21:49:52.627603 => 21:49:55.716948
[0m21:49:55.717915 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m21:49:55.717915 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:49:55.718940 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m21:49:55.718940 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a8-979b-1d8d-a671-07f7973bf30f
[0m21:49:55.943178 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e073e47b-87d4-4681-a135-b469ad74d157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CADD9FF850>]}
[0m21:49:55.946169 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 3.32s]
[0m21:49:55.950308 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m21:49:55.952268 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m21:49:55.953286 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m21:49:55.955252 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m21:49:55.956249 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m21:49:55.961241 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m21:49:55.963230 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 21:49:55.956249 => 21:49:55.962233
[0m21:49:55.963230 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m21:49:55.968244 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m21:49:55.969214 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:55.969214 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m21:49:55.970239 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m21:49:55.970239 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:49:56.775890 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a8-98d1-1763-808a-3cfef03951d7
[0m21:49:57.574314 [debug] [Thread-1 (]: SQL status: OK in 1.600000023841858 seconds
[0m21:49:57.586282 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 21:49:55.964227 => 21:49:57.585285
[0m21:49:57.587276 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m21:49:57.589272 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:49:57.590268 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m21:49:57.592263 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a8-98d1-1763-808a-3cfef03951d7
[0m21:49:57.825886 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e073e47b-87d4-4681-a135-b469ad74d157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CADD96EB10>]}
[0m21:49:57.826883 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.87s]
[0m21:49:57.828191 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m21:49:57.829218 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m21:49:57.830187 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m21:49:57.831055 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m21:49:57.832083 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m21:49:57.836074 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m21:49:57.837041 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 21:49:57.833052 => 21:49:57.837041
[0m21:49:57.838060 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m21:49:57.842059 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m21:49:57.843052 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:57.844022 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m21:49:57.844022 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m21:49:57.845048 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:49:58.661735 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a8-99f1-1af7-9c17-dcef53c466e4
[0m21:49:59.466491 [debug] [Thread-1 (]: SQL status: OK in 1.6200000047683716 seconds
[0m21:49:59.471474 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 21:49:57.838060 => 21:49:59.471474
[0m21:49:59.471474 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m21:49:59.472474 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:49:59.472474 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m21:49:59.473468 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a8-99f1-1af7-9c17-dcef53c466e4
[0m21:49:59.716270 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e073e47b-87d4-4681-a135-b469ad74d157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CAD36FC610>]}
[0m21:49:59.719259 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.89s]
[0m21:49:59.719732 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m21:49:59.720760 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:49:59.721741 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m21:49:59.722727 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m21:49:59.723724 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m21:49:59.728711 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:49:59.730706 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 21:49:59.723724 => 21:49:59.729707
[0m21:49:59.730706 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m21:49:59.736717 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:49:59.738683 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:59.738683 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:49:59.739709 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(DTNASC AS DATE) AS data_nascimento,
    CAST(ROW_NUMBER() OVER() AS STRING) AS nascimento_id,
    CODMUNNASC,
    CODUFNATU,
    SEXO,
    PESO,
    IDADEMAE,
    GESTACAO
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- SC

[0m21:49:59.740679 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:50:00.531816 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a8-9b0d-1a05-88fb-a65617559e8d
[0m21:50:01.228858 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(DTNASC AS DATE) AS data_nascimento,
    CAST(ROW_NUMBER() OVER() AS STRING) AS nascimento_id,
    CODMUNNASC,
    CODUFNATU,
    SEXO,
    PESO,
    IDADEMAE,
    GESTACAO
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- SC

[0m21:50:01.230851 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.
[0m21:50:01.233876 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1037] org.apache.spark.sql.AnalysisException: Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.windowFunctionWithWindowFrameNotOrderedError(QueryCompilationErrors.scala:1178)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$$anonfun$apply$48.applyOrElse(Analyzer.scala:4685)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$$anonfun$apply$48.applyOrElse(Analyzer.scala:4683)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:189)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:248)
	at scala.collection.immutable.List.map(List.scala:251)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:248)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:189)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:160)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:308)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:307)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1360)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1357)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1496)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:307)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:304)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:279)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:277)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$.apply(Analyzer.scala:4683)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$.apply(Analyzer.scala:4681)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryCompilationErrors$.windowFunctionWithWindowFrameNotOrderedError(QueryCompilationErrors.scala:1178)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$$anonfun$apply$48.applyOrElse(Analyzer.scala:4685)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$$anonfun$apply$48.applyOrElse(Analyzer.scala:4683)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:189)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:248)
		at scala.collection.immutable.List.map(List.scala:251)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:248)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:189)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:160)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:308)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:307)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1360)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1357)
		at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1496)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:307)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:304)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:279)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:277)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$.apply(Analyzer.scala:4683)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$.apply(Analyzer.scala:4681)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 60 more

[0m21:50:01.235863 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078a8-9b2f-14b4-8ae9-d796be8193f0
[0m21:50:01.236834 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 21:49:59.731703 => 21:50:01.236834
[0m21:50:01.236834 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m21:50:01.237858 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:50:01.237858 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m21:50:01.238855 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a8-9b0d-1a05-88fb-a65617559e8d
[0m21:50:01.471190 [debug] [Thread-1 (]: Runtime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.
[0m21:50:01.472187 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e073e47b-87d4-4681-a135-b469ad74d157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CADDA92610>]}
[0m21:50:01.472187 [error] [Thread-1 (]: 4 of 5 ERROR creating sql view model default.stg_nascidos_vivos ................ [[31mERROR[0m in 1.75s]
[0m21:50:01.473544 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:50:01.474568 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m21:50:01.475566 [info ] [Thread-1 (]: 5 of 5 SKIP relation default.stg_tempo ......................................... [[33mSKIP[0m]
[0m21:50:01.476539 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m21:50:01.478533 [debug] [MainThread]: On master: ROLLBACK
[0m21:50:01.479546 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:50:02.266839 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a8-9c17-17ad-92e1-a83faa7ea6a0
[0m21:50:02.268286 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:50:02.269287 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:50:02.269287 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:50:02.270285 [debug] [MainThread]: On master: ROLLBACK
[0m21:50:02.270285 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:50:02.270285 [debug] [MainThread]: On master: Close
[0m21:50:02.271281 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a8-9c17-17ad-92e1-a83faa7ea6a0
[0m21:50:02.506838 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:50:02.508802 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:50:02.508802 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:50:02.509827 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_nascidos_vivos' was properly closed.
[0m21:50:02.510822 [info ] [MainThread]: 
[0m21:50:02.511797 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 14.66 seconds (14.66s).
[0m21:50:02.513814 [debug] [MainThread]: Command end result
[0m21:50:02.523782 [info ] [MainThread]: 
[0m21:50:02.524761 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:50:02.526752 [info ] [MainThread]: 
[0m21:50:02.527749 [error] [MainThread]: [33mRuntime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m21:50:02.528851 [error] [MainThread]:   Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.
[0m21:50:02.529879 [info ] [MainThread]: 
[0m21:50:02.531846 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m21:50:02.532843 [debug] [MainThread]: Command `dbt run` failed at 21:50:02.532843 after 16.39 seconds
[0m21:50:02.533842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CAD2D29F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CACCEDCBD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CAD2E02290>]}
[0m21:50:02.534838 [debug] [MainThread]: Flushing usage events
[0m21:51:26.966480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9832E32D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F98300F750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F980C7A710>]}


============================== 21:51:26.971011 | 6195e334-bf99-48aa-a453-e9ca2aef4f2c ==============================
[0m21:51:26.971011 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:51:26.972395 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:51:28.280339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6195e334-bf99-48aa-a453-e9ca2aef4f2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F982FD9550>]}
[0m21:51:28.298292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6195e334-bf99-48aa-a453-e9ca2aef4f2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F983900550>]}
[0m21:51:28.299288 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:51:28.315217 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:51:28.422549 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:51:28.423543 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m21:51:28.450443 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m21:51:28.528264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6195e334-bf99-48aa-a453-e9ca2aef4f2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F98CB23190>]}
[0m21:51:28.540216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6195e334-bf99-48aa-a453-e9ca2aef4f2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F98CB12950>]}
[0m21:51:28.540216 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:51:28.542198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6195e334-bf99-48aa-a453-e9ca2aef4f2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F98A096950>]}
[0m21:51:28.544220 [info ] [MainThread]: 
[0m21:51:28.545190 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:51:28.547195 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:51:28.548222 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:51:28.548222 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:51:28.549180 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:51:29.393546 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a8-d006-109b-9a8e-3584ec6885ad
[0m21:51:29.730658 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m21:51:29.734043 [debug] [ThreadPool]: On list_workspace: Close
[0m21:51:29.735024 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a8-d006-109b-9a8e-3584ec6885ad
[0m21:51:29.966557 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:51:29.970545 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:51:29.993481 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:51:29.993481 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:51:29.994479 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:51:29.995475 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:51:30.810466 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a8-d0dd-104e-9f9d-81392bec413c
[0m21:51:31.192824 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m21:51:31.194817 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:51:31.194817 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:51:31.195814 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:51:31.195814 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:51:31.196811 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a8-d0dd-104e-9f9d-81392bec413c
[0m21:51:31.438106 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:51:31.444092 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:51:31.445089 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:51:31.446087 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:51:32.274700 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a8-d1bb-1037-97f5-93c60bb4b1d1
[0m21:51:32.628061 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m21:51:32.631082 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:51:32.632081 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a8-d1bb-1037-97f5-93c60bb4b1d1
[0m21:51:32.879802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6195e334-bf99-48aa-a453-e9ca2aef4f2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F98DB21B90>]}
[0m21:51:32.879802 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:51:32.880827 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:51:32.881825 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:51:32.882794 [info ] [MainThread]: 
[0m21:51:32.889626 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m21:51:32.890625 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m21:51:32.892301 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m21:51:32.893288 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m21:51:32.898304 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m21:51:32.899273 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 21:51:32.894286 => 21:51:32.899273
[0m21:51:32.899273 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m21:51:32.932184 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m21:51:32.934182 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:51:32.934182 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m21:51:32.935176 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m21:51:32.935176 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:51:33.762480 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a8-d29e-1461-ae68-263d18f5f529
[0m21:51:34.452944 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m21:51:34.467901 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 21:51:32.900269 => 21:51:34.467901
[0m21:51:34.468899 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m21:51:34.468899 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:51:34.469905 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m21:51:34.469905 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a8-d29e-1461-ae68-263d18f5f529
[0m21:51:34.712141 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6195e334-bf99-48aa-a453-e9ca2aef4f2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F98CA77450>]}
[0m21:51:34.713140 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.82s]
[0m21:51:34.714267 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m21:51:34.715296 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m21:51:34.716264 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m21:51:34.717270 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m21:51:34.718267 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m21:51:34.721251 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m21:51:34.723245 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 21:51:34.718267 => 21:51:34.723245
[0m21:51:34.724244 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m21:51:34.729257 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m21:51:34.731224 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:51:34.731224 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m21:51:34.732221 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m21:51:34.732221 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:51:35.672489 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a8-d3b5-1e6a-84e1-0b32eb552c7e
[0m21:51:36.344229 [debug] [Thread-1 (]: SQL status: OK in 1.6100000143051147 seconds
[0m21:51:36.347222 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 21:51:34.725249 => 21:51:36.346224
[0m21:51:36.347222 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m21:51:36.348218 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:51:36.348218 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m21:51:36.349216 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a8-d3b5-1e6a-84e1-0b32eb552c7e
[0m21:51:36.580872 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6195e334-bf99-48aa-a453-e9ca2aef4f2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F98DB01F90>]}
[0m21:51:36.582865 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.86s]
[0m21:51:36.584859 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m21:51:36.584859 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m21:51:36.586093 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m21:51:36.587860 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m21:51:36.587860 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m21:51:36.591847 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m21:51:36.593811 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 21:51:36.588825 => 21:51:36.592814
[0m21:51:36.593811 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m21:51:36.598826 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m21:51:36.599811 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:51:36.599811 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m21:51:36.600821 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m21:51:36.601794 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:51:37.390116 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a8-d4c9-16de-b5d0-5d131a30bccb
[0m21:51:38.043178 [debug] [Thread-1 (]: SQL status: OK in 1.440000057220459 seconds
[0m21:51:38.046168 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 21:51:36.594810 => 21:51:38.046168
[0m21:51:38.046168 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m21:51:38.047165 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:51:38.047165 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m21:51:38.048163 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a8-d4c9-16de-b5d0-5d131a30bccb
[0m21:51:38.272785 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6195e334-bf99-48aa-a453-e9ca2aef4f2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F98DB02410>]}
[0m21:51:38.275768 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.69s]
[0m21:51:38.280791 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m21:51:38.282781 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:51:38.283743 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m21:51:38.285738 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m21:51:38.285738 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m21:51:38.291722 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:51:38.292718 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 21:51:38.286743 => 21:51:38.292718
[0m21:51:38.292718 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m21:51:38.298703 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:51:38.300703 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:51:38.302704 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:51:38.302704 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(DTNASC AS DATE) AS data_nascimento,
    CAST(ROW_NUMBER() OVER(ORDER BY DTNASC, CODMUNNASC) AS STRING) AS nascimento_id,
    CODMUNNASC,
    CODUFNATU,
    SEXO,
    PESO,
    IDADEMAE,
    GESTACAO
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- SC

[0m21:51:38.303723 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:51:39.187506 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a8-d5db-1f55-83fd-93fa9a9eccb2
[0m21:51:39.566524 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(DTNASC AS DATE) AS data_nascimento,
    CAST(ROW_NUMBER() OVER(ORDER BY DTNASC, CODMUNNASC) AS STRING) AS nascimento_id,
    CODMUNNASC,
    CODUFNATU,
    SEXO,
    PESO,
    IDADEMAE,
    GESTACAO
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- SC

[0m21:51:39.567517 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [DATATYPE_MISMATCH.CAST_WITH_FUNC_SUGGESTION] Cannot resolve "CAST(DTNASC AS DATE)" due to data type mismatch: cannot cast "BIGINT" to "DATE".
To convert values from "BIGINT" to "DATE", you can use the functions `DATE_FROM_UNIX_DATE` instead. SQLSTATE: 42K09; line 9 pos 4
[0m21:51:39.568515 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DATATYPE_MISMATCH.CAST_WITH_FUNC_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DATATYPE_MISMATCH.CAST_WITH_FUNC_SUGGESTION] Cannot resolve "CAST(DTNASC AS DATE)" due to data type mismatch: cannot cast "BIGINT" to "DATE".
To convert values from "BIGINT" to "DATE", you can use the functions `DATE_FROM_UNIX_DATE` instead. SQLSTATE: 42K09; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [DATATYPE_MISMATCH.CAST_WITH_FUNC_SUGGESTION] Cannot resolve "CAST(DTNASC AS DATE)" due to data type mismatch: cannot cast "BIGINT" to "DATE".
To convert values from "BIGINT" to "DATE", you can use the functions `DATE_FROM_UNIX_DATE` instead. SQLSTATE: 42K09; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m21:51:39.569511 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078a8-d5fc-1b75-b16d-3c74f8feb3f2
[0m21:51:39.570509 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 21:51:38.293716 => 21:51:39.569511
[0m21:51:39.570509 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m21:51:39.571506 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:51:39.571506 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m21:51:39.572504 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a8-d5db-1f55-83fd-93fa9a9eccb2
[0m21:51:39.827158 [debug] [Thread-1 (]: Runtime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  [DATATYPE_MISMATCH.CAST_WITH_FUNC_SUGGESTION] Cannot resolve "CAST(DTNASC AS DATE)" due to data type mismatch: cannot cast "BIGINT" to "DATE".
  To convert values from "BIGINT" to "DATE", you can use the functions `DATE_FROM_UNIX_DATE` instead. SQLSTATE: 42K09; line 9 pos 4
[0m21:51:39.827158 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6195e334-bf99-48aa-a453-e9ca2aef4f2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F98DD2EE10>]}
[0m21:51:39.828154 [error] [Thread-1 (]: 4 of 5 ERROR creating sql view model default.stg_nascidos_vivos ................ [[31mERROR[0m in 1.54s]
[0m21:51:39.829324 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:51:39.830352 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m21:51:39.831349 [info ] [Thread-1 (]: 5 of 5 SKIP relation default.stg_tempo ......................................... [[33mSKIP[0m]
[0m21:51:39.832319 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m21:51:39.834322 [debug] [MainThread]: On master: ROLLBACK
[0m21:51:39.835334 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:51:40.656863 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a8-d6bb-1e86-9978-031fb6510d74
[0m21:51:40.657859 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:51:40.658856 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:51:40.658856 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:51:40.659853 [debug] [MainThread]: On master: ROLLBACK
[0m21:51:40.659853 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:51:40.660852 [debug] [MainThread]: On master: Close
[0m21:51:40.660852 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a8-d6bb-1e86-9978-031fb6510d74
[0m21:51:40.894164 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:51:40.897159 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:51:40.897159 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:51:40.898152 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_nascidos_vivos' was properly closed.
[0m21:51:40.900147 [info ] [MainThread]: 
[0m21:51:40.901145 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 12.35 seconds (12.35s).
[0m21:51:40.903162 [debug] [MainThread]: Command end result
[0m21:51:40.914110 [info ] [MainThread]: 
[0m21:51:40.915110 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:51:40.916104 [info ] [MainThread]: 
[0m21:51:40.917101 [error] [MainThread]: [33mRuntime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m21:51:40.918099 [error] [MainThread]:   [DATATYPE_MISMATCH.CAST_WITH_FUNC_SUGGESTION] Cannot resolve "CAST(DTNASC AS DATE)" due to data type mismatch: cannot cast "BIGINT" to "DATE".
[0m21:51:40.919078 [error] [MainThread]:   To convert values from "BIGINT" to "DATE", you can use the functions `DATE_FROM_UNIX_DATE` instead. SQLSTATE: 42K09; line 9 pos 4
[0m21:51:40.920079 [info ] [MainThread]: 
[0m21:51:40.921076 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m21:51:40.923072 [debug] [MainThread]: Command `dbt run` failed at 21:51:40.923072 after 13.97 seconds
[0m21:51:40.924068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9835C1D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9832F0050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F98334C750>]}
[0m21:51:40.924068 [debug] [MainThread]: Flushing usage events
[0m21:52:51.398274 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180EB9ADDD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180EB999E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180EB9E1290>]}


============================== 21:52:51.403263 | 1d80c5c7-afb0-400c-bdf3-7925088005ba ==============================
[0m21:52:51.403263 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:52:51.404632 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:52:52.791562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1d80c5c7-afb0-400c-bdf3-7925088005ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180EBF30F10>]}
[0m21:52:52.810512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1d80c5c7-afb0-400c-bdf3-7925088005ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180F547D650>]}
[0m21:52:52.811488 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:52:52.828436 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:52:52.941838 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:52:52.942811 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m21:52:52.970735 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m21:52:53.050550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1d80c5c7-afb0-400c-bdf3-7925088005ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180F64E6B90>]}
[0m21:52:53.061519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1d80c5c7-afb0-400c-bdf3-7925088005ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180F54FBF90>]}
[0m21:52:53.062489 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:52:53.063515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d80c5c7-afb0-400c-bdf3-7925088005ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180F2A69B50>]}
[0m21:52:53.065509 [info ] [MainThread]: 
[0m21:52:53.066479 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:52:53.069470 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:52:53.070467 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:52:53.070467 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:52:53.071477 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:52:54.040129 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-0278-1780-821f-b9f697a57e9b
[0m21:52:54.364194 [debug] [ThreadPool]: SQL status: OK in 1.2899999618530273 seconds
[0m21:52:54.367836 [debug] [ThreadPool]: On list_workspace: Close
[0m21:52:54.367836 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-0278-1780-821f-b9f697a57e9b
[0m21:52:54.595538 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:52:54.596535 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:52:54.609502 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:52:54.610499 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:52:54.611496 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:52:54.611496 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:52:55.612661 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-0369-1b0b-b343-07ba81cf5297
[0m21:52:56.009354 [debug] [ThreadPool]: SQL status: OK in 1.399999976158142 seconds
[0m21:52:56.012347 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:52:56.013343 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:52:56.015338 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:52:56.016336 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:52:56.017333 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-0369-1b0b-b343-07ba81cf5297
[0m21:52:56.250685 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:52:56.267663 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:52:56.268631 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:52:56.268631 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:52:57.132480 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-044c-1e32-b749-70f7a62b13f5
[0m21:52:57.505719 [debug] [ThreadPool]: SQL status: OK in 1.2400000095367432 seconds
[0m21:52:57.508713 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:52:57.509708 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-044c-1e32-b749-70f7a62b13f5
[0m21:52:57.758770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d80c5c7-afb0-400c-bdf3-7925088005ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180F548C1D0>]}
[0m21:52:57.758770 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:52:57.759792 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:52:57.760792 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:52:57.761775 [info ] [MainThread]: 
[0m21:52:57.768421 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m21:52:57.769420 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m21:52:57.771162 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m21:52:57.772156 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m21:52:57.776170 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m21:52:57.777140 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 21:52:57.773151 => 21:52:57.777140
[0m21:52:57.778165 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m21:52:57.812073 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m21:52:57.814043 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:52:57.814043 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m21:52:57.815050 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m21:52:57.815050 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:52:58.690761 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-053d-1a9c-95b2-3597a9ca2629
[0m21:52:59.452728 [debug] [Thread-1 (]: SQL status: OK in 1.6399999856948853 seconds
[0m21:52:59.468717 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 21:52:57.779137 => 21:52:59.467715
[0m21:52:59.469683 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m21:52:59.470696 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:52:59.470696 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m21:52:59.471705 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-053d-1a9c-95b2-3597a9ca2629
[0m21:52:59.731008 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d80c5c7-afb0-400c-bdf3-7925088005ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180F64D27D0>]}
[0m21:52:59.732010 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.96s]
[0m21:52:59.733002 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m21:52:59.734027 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m21:52:59.734997 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m21:52:59.735994 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m21:52:59.736991 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m21:52:59.739983 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m21:52:59.740994 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 21:52:59.736991 => 21:52:59.739983
[0m21:52:59.740994 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m21:52:59.745968 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m21:52:59.747967 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:52:59.748988 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m21:52:59.748988 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m21:52:59.749984 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:53:00.989238 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-066e-1a06-8a33-6aa3083c2bfc
[0m21:53:01.653445 [debug] [Thread-1 (]: SQL status: OK in 1.899999976158142 seconds
[0m21:53:01.656437 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 21:52:59.741978 => 21:53:01.656437
[0m21:53:01.657439 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m21:53:01.657439 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:53:01.658435 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m21:53:01.659435 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-066e-1a06-8a33-6aa3083c2bfc
[0m21:53:01.904988 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d80c5c7-afb0-400c-bdf3-7925088005ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180F6503F50>]}
[0m21:53:01.905985 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 2.17s]
[0m21:53:01.907292 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m21:53:01.908319 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m21:53:01.909294 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m21:53:01.910287 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m21:53:01.911312 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m21:53:01.915277 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m21:53:01.918265 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 21:53:01.911312 => 21:53:01.917268
[0m21:53:01.918265 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m21:53:01.924279 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m21:53:01.925274 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:01.926272 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m21:53:01.926272 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m21:53:01.927270 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:53:02.829544 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-07b4-1675-9f1b-67b85eff7677
[0m21:53:03.491167 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m21:53:03.494156 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 21:53:01.919284 => 21:53:03.493159
[0m21:53:03.494156 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m21:53:03.495153 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:53:03.495153 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m21:53:03.496151 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-07b4-1675-9f1b-67b85eff7677
[0m21:53:03.723800 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d80c5c7-afb0-400c-bdf3-7925088005ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180F44A7310>]}
[0m21:53:03.725795 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.81s]
[0m21:53:03.729182 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m21:53:03.730145 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:53:03.732171 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m21:53:03.734407 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m21:53:03.735406 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m21:53:03.741389 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:53:03.742385 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 21:53:03.735406 => 21:53:03.742385
[0m21:53:03.743382 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m21:53:03.749366 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:53:03.750364 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:03.751361 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:53:03.751361 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    TO_DATE(CAST(DTNASC AS STRING), 'yyyyMMdd') AS data_nascimento,
    CAST(ROW_NUMBER() OVER(ORDER BY DTNASC, CODMUNNASC) AS STRING) AS nascimento_id,
    CODMUNNASC,
    CODUFNATU,
    SEXO,
    PESO,
    IDADEMAE,
    GESTACAO
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- SC

[0m21:53:03.752358 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:53:04.758017 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-08d8-128b-8ebe-ea0876444e91
[0m21:53:05.437227 [debug] [Thread-1 (]: SQL status: OK in 1.6799999475479126 seconds
[0m21:53:05.440182 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 21:53:03.743382 => 21:53:05.440182
[0m21:53:05.441153 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m21:53:05.441153 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:53:05.442150 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m21:53:05.443148 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-08d8-128b-8ebe-ea0876444e91
[0m21:53:05.671367 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d80c5c7-afb0-400c-bdf3-7925088005ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180F6716850>]}
[0m21:53:05.672367 [info ] [Thread-1 (]: 4 of 5 OK created sql view model default.stg_nascidos_vivos .................... [[32mOK[0m in 1.94s]
[0m21:53:05.673334 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:53:05.674331 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m21:53:05.675354 [info ] [Thread-1 (]: 5 of 5 START sql view model default.stg_tempo .................................. [RUN]
[0m21:53:05.677344 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.stg_tempo)
[0m21:53:05.678321 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m21:53:05.682311 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m21:53:05.684305 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 21:53:05.678321 => 21:53:05.683310
[0m21:53:05.684305 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m21:53:05.689320 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m21:53:05.690289 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:05.691314 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m21:53:05.692295 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m21:53:05.692295 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:53:06.504330 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-09e6-1980-9680-f8d7b233ed66
[0m21:53:07.189108 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m21:53:07.192098 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 21:53:05.685302 => 21:53:07.192098
[0m21:53:07.192098 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m21:53:07.193095 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:53:07.194093 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m21:53:07.194093 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-09e6-1980-9680-f8d7b233ed66
[0m21:53:07.474997 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d80c5c7-afb0-400c-bdf3-7925088005ba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180F6694A90>]}
[0m21:53:07.476990 [info ] [Thread-1 (]: 5 of 5 OK created sql view model default.stg_tempo ............................. [[32mOK[0m in 1.80s]
[0m21:53:07.478260 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m21:53:07.481281 [debug] [MainThread]: On master: ROLLBACK
[0m21:53:07.481281 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:53:08.298247 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a9-0af8-1fa0-9bee-50b3709808a1
[0m21:53:08.299241 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:53:08.300239 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:08.301244 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:53:08.301244 [debug] [MainThread]: On master: ROLLBACK
[0m21:53:08.302240 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:53:08.302240 [debug] [MainThread]: On master: Close
[0m21:53:08.303254 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a9-0af8-1fa0-9bee-50b3709808a1
[0m21:53:08.528710 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:53:08.530703 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:53:08.530703 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:53:08.531699 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_tempo' was properly closed.
[0m21:53:08.532700 [info ] [MainThread]: 
[0m21:53:08.533713 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 15.47 seconds (15.47s).
[0m21:53:08.535688 [debug] [MainThread]: Command end result
[0m21:53:08.546663 [info ] [MainThread]: 
[0m21:53:08.548654 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:53:08.549582 [info ] [MainThread]: 
[0m21:53:08.551580 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m21:53:08.552090 [debug] [MainThread]: Command `dbt run` succeeded at 21:53:08.552090 after 17.17 seconds
[0m21:53:08.553118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180EB9AFDD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180EC2B7F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180EBF81F90>]}
[0m21:53:08.553118 [debug] [MainThread]: Flushing usage events
[0m21:53:39.061113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8F9463210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8F9231B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8F9A97690>]}


============================== 21:53:39.065738 | 8cc89fa5-2a54-40cb-8e65-9943bdd8fdbc ==============================
[0m21:53:39.065738 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:53:39.067733 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:53:40.430676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8cc89fa5-2a54-40cb-8e65-9943bdd8fdbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8F9241610>]}
[0m21:53:40.448628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8cc89fa5-2a54-40cb-8e65-9943bdd8fdbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8F9241610>]}
[0m21:53:40.449625 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:53:40.470612 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:53:40.571338 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:53:40.571338 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:53:40.578317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8cc89fa5-2a54-40cb-8e65-9943bdd8fdbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A883D4A310>]}
[0m21:53:40.588291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8cc89fa5-2a54-40cb-8e65-9943bdd8fdbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A882CD5290>]}
[0m21:53:40.589287 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:53:40.590257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8cc89fa5-2a54-40cb-8e65-9943bdd8fdbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A882CC9D10>]}
[0m21:53:40.592252 [info ] [MainThread]: 
[0m21:53:40.593830 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:53:40.595803 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:53:40.595803 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:53:40.596795 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:53:40.596795 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:53:41.595163 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-1ed1-1399-a6f4-99b2475bfe70
[0m21:53:41.930050 [debug] [ThreadPool]: SQL status: OK in 1.3300000429153442 seconds
[0m21:53:41.934390 [debug] [ThreadPool]: On list_workspace: Close
[0m21:53:41.935387 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-1ed1-1399-a6f4-99b2475bfe70
[0m21:53:42.186179 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:53:42.190169 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:53:42.256987 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:42.256987 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:53:42.257985 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:53:42.258982 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:53:43.085838 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-1fb4-1786-a76f-128e8cc087ba
[0m21:53:43.462055 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m21:53:43.464052 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:53:43.464052 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:53:43.465049 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:53:43.466018 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:53:43.466018 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-1fb4-1786-a76f-128e8cc087ba
[0m21:53:43.698546 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:53:43.714504 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:53:43.715501 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:53:43.716497 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:53:44.540325 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-2093-1375-baed-c5a89b9eb7ab
[0m21:53:44.894993 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m21:53:44.898965 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:53:44.899727 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-2093-1375-baed-c5a89b9eb7ab
[0m21:53:45.133710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8cc89fa5-2a54-40cb-8e65-9943bdd8fdbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8F7137F90>]}
[0m21:53:45.134708 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:45.134708 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:53:45.135700 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:53:45.136681 [info ] [MainThread]: 
[0m21:53:45.143716 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m21:53:45.144716 [info ] [Thread-1 (]: 1 of 2 START sql view model default.int_atendimento ............................ [RUN]
[0m21:53:45.146710 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.int_atendimento'
[0m21:53:45.146710 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m21:53:45.151697 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m21:53:45.153692 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 21:53:45.147708 => 21:53:45.152723
[0m21:53:45.153692 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m21:53:45.188626 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m21:53:45.189624 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:45.190618 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m21:53:45.190618 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m21:53:45.191591 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:53:46.015595 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-2172-1c00-93a5-7d9e4ec133a0
[0m21:53:46.784594 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m21:53:46.798557 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 21:53:45.154719 => 21:53:46.798557
[0m21:53:46.799554 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m21:53:46.799554 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:53:46.800552 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m21:53:46.800552 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-2172-1c00-93a5-7d9e4ec133a0
[0m21:53:47.032405 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8cc89fa5-2a54-40cb-8e65-9943bdd8fdbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A883D1B4D0>]}
[0m21:53:47.034396 [info ] [Thread-1 (]: 1 of 2 OK created sql view model default.int_atendimento ....................... [[32mOK[0m in 1.89s]
[0m21:53:47.034979 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m21:53:47.036007 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m21:53:47.036978 [info ] [Thread-1 (]: 2 of 2 START sql view model default.int_nascimento ............................. [RUN]
[0m21:53:47.037987 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m21:53:47.038998 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m21:53:47.041993 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m21:53:47.043957 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 21:53:47.038998 => 21:53:47.042988
[0m21:53:47.043957 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m21:53:47.047974 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m21:53:47.049941 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:47.049941 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m21:53:47.050950 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    SELECT
    n.nascimento_id,
    t.data_id,
    n.CODMUNNASC AS municipio,
    n.CODUFNATU AS uf,
    n.SEXO AS sexo,
    n.PESO AS peso,
    n.IDADEMAE AS idade_mae,
    n.GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m21:53:47.050950 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:53:47.874394 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-228f-135e-96b5-2ddbd92417a7
[0m21:53:48.559286 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m21:53:48.561281 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 21:53:47.044982 => 21:53:48.561281
[0m21:53:48.562279 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m21:53:48.562279 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:53:48.563276 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m21:53:48.564273 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-228f-135e-96b5-2ddbd92417a7
[0m21:53:48.787982 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8cc89fa5-2a54-40cb-8e65-9943bdd8fdbc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A882C4C390>]}
[0m21:53:48.791972 [info ] [Thread-1 (]: 2 of 2 OK created sql view model default.int_nascimento ........................ [[32mOK[0m in 1.75s]
[0m21:53:48.795979 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m21:53:48.801926 [debug] [MainThread]: On master: ROLLBACK
[0m21:53:48.802924 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:53:49.600416 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a9-2397-14d3-9ccd-55bfa50c4556
[0m21:53:49.602407 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:53:49.603404 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:53:49.603404 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:53:49.604401 [debug] [MainThread]: On master: ROLLBACK
[0m21:53:49.604401 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:53:49.605399 [debug] [MainThread]: On master: Close
[0m21:53:49.606395 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a9-2397-14d3-9ccd-55bfa50c4556
[0m21:53:49.842523 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:53:49.843518 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:53:49.843518 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:53:49.844516 [debug] [MainThread]: Connection 'model.projeto_health_insights.int_nascimento' was properly closed.
[0m21:53:49.845513 [info ] [MainThread]: 
[0m21:53:49.846883 [info ] [MainThread]: Finished running 2 view models in 0 hours 0 minutes and 9.25 seconds (9.25s).
[0m21:53:49.849878 [debug] [MainThread]: Command end result
[0m21:53:49.862843 [info ] [MainThread]: 
[0m21:53:49.863841 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:53:49.864851 [info ] [MainThread]: 
[0m21:53:49.865836 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m21:53:49.867350 [debug] [MainThread]: Command `dbt run` succeeded at 21:53:49.867350 after 10.83 seconds
[0m21:53:49.868350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8F9535910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8F94FC410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A8F307BDD0>]}
[0m21:53:49.869347 [debug] [MainThread]: Flushing usage events
[0m21:54:03.681435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227A9BD68D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227A92E9810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227A9381510>]}


============================== 21:54:03.686071 | 86025046-0982-4296-892a-6955c3fa8f92 ==============================
[0m21:54:03.686071 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:54:03.687426 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:54:05.016509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '86025046-0982-4296-892a-6955c3fa8f92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227A9699450>]}
[0m21:54:05.034461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '86025046-0982-4296-892a-6955c3fa8f92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227B2D80850>]}
[0m21:54:05.035458 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:54:05.051480 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:54:05.148193 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:54:05.148193 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:54:05.155174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '86025046-0982-4296-892a-6955c3fa8f92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227B3F0ACD0>]}
[0m21:54:05.165148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '86025046-0982-4296-892a-6955c3fa8f92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227B2D67610>]}
[0m21:54:05.166145 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:54:05.166826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '86025046-0982-4296-892a-6955c3fa8f92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227A62F4250>]}
[0m21:54:05.168845 [info ] [MainThread]: 
[0m21:54:05.170840 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:54:05.172974 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:54:05.172974 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:54:05.173971 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:54:05.173971 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:54:06.769854 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-2dd3-1545-8b3d-eb8789217e49
[0m21:54:07.075105 [debug] [ThreadPool]: SQL status: OK in 1.899999976158142 seconds
[0m21:54:07.079389 [debug] [ThreadPool]: On list_workspace: Close
[0m21:54:07.079389 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-2dd3-1545-8b3d-eb8789217e49
[0m21:54:07.323581 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:54:07.327568 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:54:07.392424 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:54:07.393390 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:54:07.393390 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:54:07.394417 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:54:08.228801 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-2eaf-191b-bca4-5eaecd27ebbe
[0m21:54:08.613980 [debug] [ThreadPool]: SQL status: OK in 1.2200000286102295 seconds
[0m21:54:08.614950 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:54:08.615976 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:54:08.615976 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:54:08.616973 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:54:08.616973 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-2eaf-191b-bca4-5eaecd27ebbe
[0m21:54:08.861171 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:54:08.866160 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:54:08.867157 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:54:08.867157 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:54:09.983560 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-2fb5-177e-9b63-9b33c370eb4e
[0m21:54:10.610831 [debug] [ThreadPool]: SQL status: OK in 1.7400000095367432 seconds
[0m21:54:10.614848 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:54:10.615818 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-2fb5-177e-9b63-9b33c370eb4e
[0m21:54:10.980113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '86025046-0982-4296-892a-6955c3fa8f92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227A8688590>]}
[0m21:54:10.980113 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:54:10.981123 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:54:10.982121 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:54:10.983090 [info ] [MainThread]: 
[0m21:54:10.989881 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m21:54:10.990878 [info ] [Thread-1 (]: 1 of 5 START sql view model default.dim_doenca ................................. [RUN]
[0m21:54:10.991876 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.dim_doenca'
[0m21:54:10.992873 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m21:54:10.997887 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m21:54:10.999853 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 21:54:10.993870 => 21:54:10.998885
[0m21:54:10.999853 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m21:54:11.033791 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m21:54:11.034792 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:54:11.035759 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m21:54:11.035759 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m21:54:11.036756 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:54:11.955794 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-30ea-1539-982d-5c73aafd6a11
[0m21:54:12.641471 [debug] [Thread-1 (]: SQL status: OK in 1.6100000143051147 seconds
[0m21:54:12.655434 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 21:54:11.000855 => 21:54:12.655434
[0m21:54:12.656431 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m21:54:12.657429 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:54:12.657429 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m21:54:12.658426 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-30ea-1539-982d-5c73aafd6a11
[0m21:54:12.897600 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86025046-0982-4296-892a-6955c3fa8f92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227B3E9EA10>]}
[0m21:54:12.898570 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.dim_doenca ............................ [[32mOK[0m in 1.91s]
[0m21:54:12.900009 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m21:54:12.901036 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m21:54:12.902006 [info ] [Thread-1 (]: 2 of 5 START sql view model default.dim_localidade ............................. [RUN]
[0m21:54:12.903005 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m21:54:12.904001 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m21:54:12.908020 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m21:54:12.909019 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 21:54:12.904001 => 21:54:12.909019
[0m21:54:12.909985 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m21:54:12.915996 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m21:54:12.917963 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:54:12.917963 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m21:54:12.918990 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m21:54:12.918990 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:54:13.876217 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-31fc-104f-8f0f-bca79f6ba9bb
[0m21:54:14.562591 [debug] [Thread-1 (]: SQL status: OK in 1.6399999856948853 seconds
[0m21:54:14.564586 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 21:54:12.909985 => 21:54:14.564586
[0m21:54:14.565583 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m21:54:14.565583 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:54:14.566580 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m21:54:14.567578 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-31fc-104f-8f0f-bca79f6ba9bb
[0m21:54:14.814797 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86025046-0982-4296-892a-6955c3fa8f92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227B3EFF790>]}
[0m21:54:14.816790 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.dim_localidade ........................ [[32mOK[0m in 1.91s]
[0m21:54:14.818813 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m21:54:14.818813 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m21:54:14.819808 [info ] [Thread-1 (]: 3 of 5 START sql table model default.dim_tempo ................................. [RUN]
[0m21:54:14.820709 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m21:54:14.821738 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m21:54:14.825726 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m21:54:14.826695 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 21:54:14.822707 => 21:54:14.826695
[0m21:54:14.827693 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m21:54:14.840660 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 21:54:14.827693 => 21:54:14.840660
[0m21:54:14.846670 [debug] [Thread-1 (]: Compilation Error in model dim_tempo (models\marts\dim_tempo.sql)
  'create_table_as' is undefined
  
  > in macro materialization_table_databricks (macros\materializations\table.sql)
  > called by model dim_tempo (models\marts\dim_tempo.sql)
[0m21:54:14.847640 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86025046-0982-4296-892a-6955c3fa8f92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227B4012D10>]}
[0m21:54:14.848638 [error] [Thread-1 (]: 3 of 5 ERROR creating sql table model default.dim_tempo ........................ [[31mERROR[0m in 0.03s]
[0m21:54:14.850633 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m21:54:14.850633 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:54:14.851657 [info ] [Thread-1 (]: 4 of 5 START sql view model default.fato_atendimento_hospitalar ................ [RUN]
[0m21:54:14.853652 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m21:54:14.853652 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:54:14.857613 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:54:14.858610 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 21:54:14.854634 => 21:54:14.858610
[0m21:54:14.859621 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:54:14.863597 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:54:14.864594 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:54:14.865591 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:54:14.866589 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m21:54:14.867587 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:54:15.699903 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-331f-1d34-950c-2abc8e579f57
[0m21:54:16.461574 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m21:54:16.464564 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 21:54:14.859621 => 21:54:16.464564
[0m21:54:16.465560 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m21:54:16.465560 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:54:16.466558 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m21:54:16.467556 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-331f-1d34-950c-2abc8e579f57
[0m21:54:16.796970 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86025046-0982-4296-892a-6955c3fa8f92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227B4003F90>]}
[0m21:54:16.798959 [info ] [Thread-1 (]: 4 of 5 OK created sql view model default.fato_atendimento_hospitalar ........... [[32mOK[0m in 1.94s]
[0m21:54:16.800919 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:54:16.800919 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m21:54:16.801916 [info ] [Thread-1 (]: 5 of 5 START sql table model default.fato_nascimento ........................... [RUN]
[0m21:54:16.802858 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.fato_nascimento)
[0m21:54:16.803887 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m21:54:16.807875 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m21:54:16.809842 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 21:54:16.803887 => 21:54:16.808872
[0m21:54:16.810852 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m21:54:16.814857 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 21:54:16.810852 => 21:54:16.814857
[0m21:54:16.818819 [debug] [Thread-1 (]: Compilation Error in model fato_nascimento (models\marts\fato_nascimento.sql)
  'create_table_as' is undefined
  
  > in macro materialization_table_databricks (macros\materializations\table.sql)
  > called by model fato_nascimento (models\marts\fato_nascimento.sql)
[0m21:54:16.819844 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86025046-0982-4296-892a-6955c3fa8f92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227B400A0D0>]}
[0m21:54:16.820841 [error] [Thread-1 (]: 5 of 5 ERROR creating sql table model default.fato_nascimento .................. [[31mERROR[0m in 0.02s]
[0m21:54:16.821810 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m21:54:16.823833 [debug] [MainThread]: On master: ROLLBACK
[0m21:54:16.823833 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:54:18.354005 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a9-349e-1651-87ac-656496159ad5
[0m21:54:18.357024 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:54:18.358021 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:54:18.359986 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:54:18.361013 [debug] [MainThread]: On master: ROLLBACK
[0m21:54:18.361980 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:54:18.361980 [debug] [MainThread]: On master: Close
[0m21:54:18.362977 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a9-349e-1651-87ac-656496159ad5
[0m21:54:18.896471 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:54:18.897462 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:54:18.899426 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:54:18.899426 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m21:54:18.900423 [info ] [MainThread]: 
[0m21:54:18.901421 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 13.73 seconds (13.73s).
[0m21:54:18.903443 [debug] [MainThread]: Command end result
[0m21:54:18.914424 [info ] [MainThread]: 
[0m21:54:18.915383 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m21:54:18.916198 [info ] [MainThread]: 
[0m21:54:18.917198 [error] [MainThread]: [33mCompilation Error in model dim_tempo (models\marts\dim_tempo.sql)[0m
[0m21:54:18.918196 [error] [MainThread]:   'create_table_as' is undefined
[0m21:54:18.919193 [error] [MainThread]:   
[0m21:54:18.920191 [error] [MainThread]:   > in macro materialization_table_databricks (macros\materializations\table.sql)
[0m21:54:18.921188 [error] [MainThread]:   > called by model dim_tempo (models\marts\dim_tempo.sql)
[0m21:54:18.922185 [info ] [MainThread]: 
[0m21:54:18.923183 [error] [MainThread]: [33mCompilation Error in model fato_nascimento (models\marts\fato_nascimento.sql)[0m
[0m21:54:18.923183 [error] [MainThread]:   'create_table_as' is undefined
[0m21:54:18.924180 [error] [MainThread]:   
[0m21:54:18.925202 [error] [MainThread]:   > in macro materialization_table_databricks (macros\materializations\table.sql)
[0m21:54:18.926199 [error] [MainThread]:   > called by model fato_nascimento (models\marts\fato_nascimento.sql)
[0m21:54:18.927172 [info ] [MainThread]: 
[0m21:54:18.928193 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=2 SKIP=0 TOTAL=5
[0m21:54:18.930187 [debug] [MainThread]: Command `dbt run` failed at 21:54:18.929191 after 15.27 seconds
[0m21:54:18.930187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227A92C6A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227A9375D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227A9BD7190>]}
[0m21:54:18.931161 [debug] [MainThread]: Flushing usage events
[0m21:56:42.119202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EAB5F3C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EA8F90510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EAB3507D0>]}


============================== 21:56:42.124214 | 48c6071e-77c2-4987-b6fe-2dcf6ab47dbb ==============================
[0m21:56:42.124214 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:56:42.126203 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:56:43.466276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '48c6071e-77c2-4987-b6fe-2dcf6ab47dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EABC37390>]}
[0m21:56:43.484228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '48c6071e-77c2-4987-b6fe-2dcf6ab47dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EABC58550>]}
[0m21:56:43.485230 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:56:43.502153 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:56:43.604889 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m21:56:43.606856 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\dim_tempo.sql
[0m21:56:43.606856 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m21:56:43.639769 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m21:56:43.656723 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m21:56:43.725568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '48c6071e-77c2-4987-b6fe-2dcf6ab47dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EAA5D3490>]}
[0m21:56:43.737508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '48c6071e-77c2-4987-b6fe-2dcf6ab47dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EB4DDFDD0>]}
[0m21:56:43.738505 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:56:43.738505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '48c6071e-77c2-4987-b6fe-2dcf6ab47dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EB2057E50>]}
[0m21:56:43.740499 [info ] [MainThread]: 
[0m21:56:43.741798 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:56:43.743808 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:56:43.744821 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:56:43.745802 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:56:43.745802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:56:45.150846 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-8c2e-1a0f-a223-4ecbb0c65570
[0m21:56:45.943760 [debug] [ThreadPool]: SQL status: OK in 2.200000047683716 seconds
[0m21:56:45.947446 [debug] [ThreadPool]: On list_workspace: Close
[0m21:56:45.947446 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-8c2e-1a0f-a223-4ecbb0c65570
[0m21:56:46.495437 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:56:46.496466 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:56:46.507435 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:56:46.508405 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:56:46.508405 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:56:46.510400 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:56:47.837363 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-8dd3-180b-89ac-8d427d9679c8
[0m21:56:48.529438 [debug] [ThreadPool]: SQL status: OK in 2.0199999809265137 seconds
[0m21:56:48.530437 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:56:48.531433 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:56:48.532424 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:56:48.532424 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:56:48.533422 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-8dd3-180b-89ac-8d427d9679c8
[0m21:56:48.864318 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:56:48.871301 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:56:48.871301 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:56:48.872298 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:56:50.902189 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-8f88-18f0-8a60-57976a57de73
[0m21:56:51.435775 [debug] [ThreadPool]: SQL status: OK in 2.559999942779541 seconds
[0m21:56:51.439764 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:56:51.439764 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-8f88-18f0-8a60-57976a57de73
[0m21:56:52.075005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '48c6071e-77c2-4987-b6fe-2dcf6ab47dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EB5F7D850>]}
[0m21:56:52.076003 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:56:52.077000 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:56:52.077997 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:56:52.078578 [info ] [MainThread]: 
[0m21:56:52.085683 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m21:56:52.086682 [info ] [Thread-1 (]: 1 of 5 START sql view model default.dim_doenca ................................. [RUN]
[0m21:56:52.088299 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.dim_doenca'
[0m21:56:52.089308 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m21:56:52.094311 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m21:56:52.095281 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 21:56:52.090294 => 21:56:52.095281
[0m21:56:52.096277 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m21:56:52.129217 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m21:56:52.131184 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:56:52.133180 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m21:56:52.133180 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m21:56:52.134204 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:56:54.524273 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-91bd-1639-ad1b-18febed40521
[0m21:56:56.108746 [debug] [Thread-1 (]: SQL status: OK in 3.9700000286102295 seconds
[0m21:56:56.123700 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 21:56:52.097283 => 21:56:56.123700
[0m21:56:56.124670 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m21:56:56.124670 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:56:56.125695 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m21:56:56.125695 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-91bd-1639-ad1b-18febed40521
[0m21:56:56.397955 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48c6071e-77c2-4987-b6fe-2dcf6ab47dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EAAE69410>]}
[0m21:56:56.399948 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.dim_doenca ............................ [[32mOK[0m in 4.31s]
[0m21:56:56.401259 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m21:56:56.402288 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m21:56:56.403257 [info ] [Thread-1 (]: 2 of 5 START sql view model default.dim_localidade ............................. [RUN]
[0m21:56:56.404254 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m21:56:56.405279 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m21:56:56.408270 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m21:56:56.409271 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 21:56:56.405279 => 21:56:56.409271
[0m21:56:56.410254 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m21:56:56.415252 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m21:56:56.416252 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:56:56.417248 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m21:56:56.417248 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m21:56:56.418244 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:56:57.573208 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-939f-13d3-8754-63eab11b7486
[0m21:56:58.252081 [debug] [Thread-1 (]: SQL status: OK in 1.8300000429153442 seconds
[0m21:56:58.255070 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 21:56:56.410254 => 21:56:58.255070
[0m21:56:58.255070 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m21:56:58.256067 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:56:58.257065 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m21:56:58.257065 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-939f-13d3-8754-63eab11b7486
[0m21:56:58.506759 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48c6071e-77c2-4987-b6fe-2dcf6ab47dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EB5F6E710>]}
[0m21:56:58.509752 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.dim_localidade ........................ [[32mOK[0m in 2.10s]
[0m21:56:58.514211 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m21:56:58.516177 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m21:56:58.517238 [info ] [Thread-1 (]: 3 of 5 START sql view model default.dim_tempo .................................. [RUN]
[0m21:56:58.519163 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m21:56:58.520163 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m21:56:58.525178 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m21:56:58.526145 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 21:56:58.520163 => 21:56:58.526145
[0m21:56:58.527141 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m21:56:58.531162 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m21:56:58.533131 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:56:58.533131 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m21:56:58.534123 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

SELECT DISTINCT
    data_id,
    data,
    ano,
    mes,
    dia,
    trimestre,
    dia_semana
FROM `workspace`.`default`.`stg_tempo`

[0m21:56:58.535120 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:56:59.460729 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-94b7-14a7-b159-739b84bcc26d
[0m21:57:00.219122 [debug] [Thread-1 (]: SQL status: OK in 1.6799999475479126 seconds
[0m21:57:00.221106 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 21:56:58.527141 => 21:57:00.221106
[0m21:57:00.222101 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m21:57:00.223099 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:57:00.223099 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m21:57:00.224099 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-94b7-14a7-b159-739b84bcc26d
[0m21:57:00.532319 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48c6071e-77c2-4987-b6fe-2dcf6ab47dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EB6086710>]}
[0m21:57:00.533316 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.dim_tempo ............................. [[32mOK[0m in 2.01s]
[0m21:57:00.534161 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m21:57:00.535191 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:57:00.536050 [info ] [Thread-1 (]: 4 of 5 START sql view model default.fato_atendimento_hospitalar ................ [RUN]
[0m21:57:00.536945 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m21:57:00.537975 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:57:00.542960 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:57:00.544926 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 21:57:00.538972 => 21:57:00.543930
[0m21:57:00.545934 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:57:00.552904 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:57:00.553901 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:57:00.553901 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:57:00.554899 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m21:57:00.554899 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:57:01.552665 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-9600-1407-8f47-00fc59a2b3d6
[0m21:57:02.383940 [debug] [Thread-1 (]: SQL status: OK in 1.8300000429153442 seconds
[0m21:57:02.386960 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 21:57:00.546921 => 21:57:02.386960
[0m21:57:02.387930 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m21:57:02.387930 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:57:02.388927 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m21:57:02.388927 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-9600-1407-8f47-00fc59a2b3d6
[0m21:57:02.799599 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48c6071e-77c2-4987-b6fe-2dcf6ab47dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EB4E634D0>]}
[0m21:57:02.801590 [info ] [Thread-1 (]: 4 of 5 OK created sql view model default.fato_atendimento_hospitalar ........... [[32mOK[0m in 2.26s]
[0m21:57:02.802150 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:57:02.803180 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m21:57:02.803874 [info ] [Thread-1 (]: 5 of 5 START sql view model default.fato_nascimento ............................ [RUN]
[0m21:57:02.804673 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.fato_nascimento)
[0m21:57:02.805701 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m21:57:02.809689 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m21:57:02.810684 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 21:57:02.805701 => 21:57:02.810684
[0m21:57:02.811657 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m21:57:02.816644 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m21:57:02.818663 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:57:02.818663 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m21:57:02.819663 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    nascimento_id,
    data_id,
    CODMUNNASC AS municipio,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`int_nascimento`

[0m21:57:02.819663 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:57:04.011782 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-9776-18ac-9f01-f047db86945b
[0m21:57:04.445002 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    nascimento_id,
    data_id,
    CODMUNNASC AS municipio,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`int_nascimento`

[0m21:57:04.445002 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CODMUNNASC` cannot be resolved. Did you mean one of the following? [`data_id`, `peso`, `sexo`, `uf`, `idade_mae`]. SQLSTATE: 42703; line 11 pos 4
[0m21:57:04.445995 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CODMUNNASC` cannot be resolved. Did you mean one of the following? [`data_id`, `peso`, `sexo`, `uf`, `idade_mae`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CODMUNNASC` cannot be resolved. Did you mean one of the following? [`data_id`, `peso`, `sexo`, `uf`, `idade_mae`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m21:57:04.446992 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078a9-9798-1822-8b59-fca24b3177e3
[0m21:57:04.447990 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 21:57:02.811657 => 21:57:04.447990
[0m21:57:04.448987 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m21:57:04.448987 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:57:04.449989 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m21:57:04.449989 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-9776-18ac-9f01-f047db86945b
[0m21:57:04.695484 [debug] [Thread-1 (]: Runtime Error in model fato_nascimento (models\marts\fato_nascimento.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CODMUNNASC` cannot be resolved. Did you mean one of the following? [`data_id`, `peso`, `sexo`, `uf`, `idade_mae`]. SQLSTATE: 42703; line 11 pos 4
[0m21:57:04.696479 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48c6071e-77c2-4987-b6fe-2dcf6ab47dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EB608CD90>]}
[0m21:57:04.697476 [error] [Thread-1 (]: 5 of 5 ERROR creating sql view model default.fato_nascimento ................... [[31mERROR[0m in 1.89s]
[0m21:57:04.698445 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m21:57:04.700630 [debug] [MainThread]: On master: ROLLBACK
[0m21:57:04.700630 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:57:05.489965 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a9-9858-10ea-854d-4065e848befb
[0m21:57:05.493405 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:57:05.494401 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:57:05.496402 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:57:05.497386 [debug] [MainThread]: On master: ROLLBACK
[0m21:57:05.498366 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:57:05.499353 [debug] [MainThread]: On master: Close
[0m21:57:05.499353 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a9-9858-10ea-854d-4065e848befb
[0m21:57:05.757175 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:57:05.759169 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:57:05.760169 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:57:05.762160 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m21:57:05.766151 [info ] [MainThread]: 
[0m21:57:05.768518 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 22.02 seconds (22.02s).
[0m21:57:05.774531 [debug] [MainThread]: Command end result
[0m21:57:05.791452 [info ] [MainThread]: 
[0m21:57:05.792448 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:57:05.793586 [info ] [MainThread]: 
[0m21:57:05.794586 [error] [MainThread]: [33mRuntime Error in model fato_nascimento (models\marts\fato_nascimento.sql)[0m
[0m21:57:05.795584 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CODMUNNASC` cannot be resolved. Did you mean one of the following? [`data_id`, `peso`, `sexo`, `uf`, `idade_mae`]. SQLSTATE: 42703; line 11 pos 4
[0m21:57:05.796581 [info ] [MainThread]: 
[0m21:57:05.798577 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m21:57:05.799772 [debug] [MainThread]: Command `dbt run` failed at 21:57:05.799772 after 23.70 seconds
[0m21:57:05.800799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EAB3512D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EAB5F8C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EAB5F2910>]}
[0m21:57:05.800799 [debug] [MainThread]: Flushing usage events
[0m21:58:26.634618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232B502E650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232B49E9D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232B49EA550>]}


============================== 21:58:26.639633 | bc59f9a8-7f32-4432-b39a-4337235805b1 ==============================
[0m21:58:26.639633 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:58:26.640611 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:58:27.910853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc59f9a8-7f32-4432-b39a-4337235805b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232B5026610>]}
[0m21:58:27.928806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bc59f9a8-7f32-4432-b39a-4337235805b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232B5026610>]}
[0m21:58:27.929801 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:58:27.948731 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:58:28.052473 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:58:28.053472 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m21:58:28.080399 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m21:58:28.160188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc59f9a8-7f32-4432-b39a-4337235805b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232B425A210>]}
[0m21:58:28.172155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc59f9a8-7f32-4432-b39a-4337235805b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232BE24FE90>]}
[0m21:58:28.173123 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:58:28.173123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc59f9a8-7f32-4432-b39a-4337235805b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232BB7AE9D0>]}
[0m21:58:28.175118 [info ] [MainThread]: 
[0m21:58:28.177141 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:58:28.179106 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m21:58:28.180104 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m21:58:28.181102 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m21:58:28.182107 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:58:29.334650 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-ca52-103c-9e8c-0ec19175eac5
[0m21:58:29.633517 [debug] [ThreadPool]: SQL status: OK in 1.4500000476837158 seconds
[0m21:58:29.637503 [debug] [ThreadPool]: On list_workspace: Close
[0m21:58:29.637503 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-ca52-103c-9e8c-0ec19175eac5
[0m21:58:29.951615 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m21:58:29.955603 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m21:58:29.965572 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:29.966570 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m21:58:29.966570 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m21:58:29.967570 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:58:31.035979 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-cb33-18e0-9909-44cd3a482d98
[0m21:58:31.399355 [debug] [ThreadPool]: SQL status: OK in 1.4299999475479126 seconds
[0m21:58:31.400353 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:58:31.401351 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m21:58:31.401351 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:58:31.402348 [debug] [ThreadPool]: On create_workspace_default: Close
[0m21:58:31.402348 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-cb33-18e0-9909-44cd3a482d98
[0m21:58:31.690590 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:58:31.698566 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:58:31.698566 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:58:31.699536 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:58:32.599867 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-cc44-18a8-b1f4-b7b905bb3388
[0m21:58:32.928661 [debug] [ThreadPool]: SQL status: OK in 1.2300000190734863 seconds
[0m21:58:32.932649 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:58:32.933646 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-cc44-18a8-b1f4-b7b905bb3388
[0m21:58:33.239778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc59f9a8-7f32-4432-b39a-4337235805b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232BF2AE850>]}
[0m21:58:33.240805 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:33.240805 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:58:33.241800 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:58:33.242778 [info ] [MainThread]: 
[0m21:58:33.249778 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m21:58:33.250805 [info ] [Thread-1 (]: 1 of 5 START sql view model default.dim_doenca ................................. [RUN]
[0m21:58:33.252774 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.dim_doenca'
[0m21:58:33.253786 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m21:58:33.257787 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m21:58:33.259754 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 21:58:33.254780 => 21:58:33.259754
[0m21:58:33.260760 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m21:58:33.293691 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m21:58:33.295680 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:33.295680 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m21:58:33.296684 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m21:58:33.296684 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:58:34.114819 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-cd2b-1c9a-b3f0-e25c3b8b9737
[0m21:58:34.823673 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m21:58:34.837636 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 21:58:33.260760 => 21:58:34.837636
[0m21:58:34.838633 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m21:58:34.838633 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:58:34.839631 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m21:58:34.840627 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-cd2b-1c9a-b3f0-e25c3b8b9737
[0m21:58:35.060793 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc59f9a8-7f32-4432-b39a-4337235805b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232BF332AD0>]}
[0m21:58:35.062786 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.dim_doenca ............................ [[32mOK[0m in 1.81s]
[0m21:58:35.064141 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m21:58:35.065170 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m21:58:35.066139 [info ] [Thread-1 (]: 2 of 5 START sql view model default.dim_localidade ............................. [RUN]
[0m21:58:35.067136 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m21:58:35.068145 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m21:58:35.071153 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m21:58:35.072150 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 21:58:35.068145 => 21:58:35.072150
[0m21:58:35.073119 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m21:58:35.079158 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m21:58:35.080101 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:35.081107 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m21:58:35.081107 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m21:58:35.082096 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:58:35.890545 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-ce39-18b7-a76e-5596145d92ed
[0m21:58:36.579140 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m21:58:36.582123 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 21:58:35.073119 => 21:58:36.582123
[0m21:58:36.582123 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m21:58:36.583121 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:58:36.584090 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m21:58:36.584090 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-ce39-18b7-a76e-5596145d92ed
[0m21:58:36.994244 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc59f9a8-7f32-4432-b39a-4337235805b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232BF2F8B10>]}
[0m21:58:36.995242 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.dim_localidade ........................ [[32mOK[0m in 1.93s]
[0m21:58:36.996335 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m21:58:36.997363 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m21:58:36.998332 [info ] [Thread-1 (]: 3 of 5 START sql view model default.dim_tempo .................................. [RUN]
[0m21:58:36.999330 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m21:58:37.000347 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m21:58:37.004346 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m21:58:37.005341 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 21:58:37.000347 => 21:58:37.005341
[0m21:58:37.006338 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m21:58:37.010331 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m21:58:37.011298 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:37.012295 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m21:58:37.012295 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

SELECT DISTINCT
    data_id,
    data,
    ano,
    mes,
    dia,
    trimestre,
    dia_semana
FROM `workspace`.`default`.`stg_tempo`

[0m21:58:37.013292 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:58:38.236474 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-cf88-1784-a066-b88afcae0bae
[0m21:58:39.040493 [debug] [Thread-1 (]: SQL status: OK in 2.0299999713897705 seconds
[0m21:58:39.042482 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 21:58:37.006338 => 21:58:39.042482
[0m21:58:39.043480 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m21:58:39.044492 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:58:39.044492 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m21:58:39.045474 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-cf88-1784-a066-b88afcae0bae
[0m21:58:39.449817 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc59f9a8-7f32-4432-b39a-4337235805b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232BE2350D0>]}
[0m21:58:39.451808 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.dim_tempo ............................. [[32mOK[0m in 2.45s]
[0m21:58:39.452377 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m21:58:39.453406 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:58:39.454381 [info ] [Thread-1 (]: 4 of 5 START sql view model default.fato_atendimento_hospitalar ................ [RUN]
[0m21:58:39.455372 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m21:58:39.456398 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:58:39.460386 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:58:39.461398 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 21:58:39.456398 => 21:58:39.461398
[0m21:58:39.462353 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:58:39.469362 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:58:39.471330 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:39.471330 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:58:39.472326 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m21:58:39.473323 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:58:40.786471 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-d119-102f-adf0-422b067febd2
[0m21:58:41.767256 [debug] [Thread-1 (]: SQL status: OK in 2.2899999618530273 seconds
[0m21:58:41.770247 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 21:58:39.462353 => 21:58:41.770247
[0m21:58:41.771216 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m21:58:41.771216 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:58:41.772243 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m21:58:41.773210 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-d119-102f-adf0-422b067febd2
[0m21:58:42.101622 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc59f9a8-7f32-4432-b39a-4337235805b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232BF478C90>]}
[0m21:58:42.103615 [info ] [Thread-1 (]: 4 of 5 OK created sql view model default.fato_atendimento_hospitalar ........... [[32mOK[0m in 2.65s]
[0m21:58:42.105409 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:58:42.106405 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m21:58:42.107397 [info ] [Thread-1 (]: 5 of 5 START sql view model default.fato_nascimento ............................ [RUN]
[0m21:58:42.110362 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.fato_nascimento)
[0m21:58:42.111359 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m21:58:42.117369 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m21:58:42.119336 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 21:58:42.113355 => 21:58:42.118381
[0m21:58:42.119336 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m21:58:42.123353 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m21:58:42.125320 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:42.125320 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m21:58:42.126318 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    nascimento_id,
    data_id,
    municipio,
    uf,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas
FROM `workspace`.`default`.`int_nascimento`

[0m21:58:42.126318 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:58:43.048757 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078a9-d27e-1560-bdff-b0087eaac9d2
[0m21:58:43.962838 [debug] [Thread-1 (]: SQL status: OK in 1.840000033378601 seconds
[0m21:58:43.965854 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 21:58:42.120334 => 21:58:43.965854
[0m21:58:43.966825 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m21:58:43.966825 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m21:58:43.967829 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m21:58:43.967829 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078a9-d27e-1560-bdff-b0087eaac9d2
[0m21:58:44.242043 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bc59f9a8-7f32-4432-b39a-4337235805b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232BF3F1050>]}
[0m21:58:44.244037 [info ] [Thread-1 (]: 5 of 5 OK created sql view model default.fato_nascimento ....................... [[32mOK[0m in 2.13s]
[0m21:58:44.245181 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m21:58:44.248204 [debug] [MainThread]: On master: ROLLBACK
[0m21:58:44.248204 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:58:45.241842 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078a9-d3cc-1917-836d-e237bc6f6878
[0m21:58:45.245269 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:58:45.246271 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:45.248266 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:58:45.249225 [debug] [MainThread]: On master: ROLLBACK
[0m21:58:45.249225 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:58:45.250220 [debug] [MainThread]: On master: Close
[0m21:58:45.250220 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078a9-d3cc-1917-836d-e237bc6f6878
[0m21:58:45.525914 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:58:45.525914 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m21:58:45.526941 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:58:45.526941 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m21:58:45.527912 [info ] [MainThread]: 
[0m21:58:45.528971 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 17.35 seconds (17.35s).
[0m21:58:45.531093 [debug] [MainThread]: Command end result
[0m21:58:45.543032 [info ] [MainThread]: 
[0m21:58:45.544031 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:58:45.545049 [info ] [MainThread]: 
[0m21:58:45.546025 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m21:58:45.547023 [debug] [MainThread]: Command `dbt run` succeeded at 21:58:45.547023 after 18.93 seconds
[0m21:58:45.548049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232B4760B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232B4BEF290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AE7FCBD0>]}
[0m21:58:45.548049 [debug] [MainThread]: Flushing usage events
[0m21:59:22.254221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6C690ACD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6C6AD0B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6C9123810>]}


============================== 21:59:22.259209 | e7dd2006-9d52-4189-8aca-c3a4adf35f69 ==============================
[0m21:59:22.259209 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:59:22.260183 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:59:23.523804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e7dd2006-9d52-4189-8aca-c3a4adf35f69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6C977FF90>]}
[0m21:59:23.542753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e7dd2006-9d52-4189-8aca-c3a4adf35f69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6C977FF90>]}
[0m21:59:23.543750 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m21:59:23.563697 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m21:59:23.672409 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:59:23.672409 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:59:23.679388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e7dd2006-9d52-4189-8aca-c3a4adf35f69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6C9780D10>]}
[0m21:59:23.681383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e7dd2006-9d52-4189-8aca-c3a4adf35f69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6D2A977D0>]}
[0m21:59:23.682353 [info ] [MainThread]: Found 12 models, 0 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m21:59:23.683350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e7dd2006-9d52-4189-8aca-c3a4adf35f69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6D29136D0>]}
[0m21:59:23.685372 [info ] [MainThread]: 
[0m21:59:23.687367 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:59:23.689348 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m21:59:23.694347 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m21:59:23.695345 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m21:59:23.695345 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:59:24.767009 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-eb52-17f4-8f90-f2293301bddb
[0m21:59:25.163517 [debug] [ThreadPool]: SQL status: OK in 1.4700000286102295 seconds
[0m21:59:25.168504 [debug] [ThreadPool]: On list_workspace_default: Close
[0m21:59:25.169501 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-eb52-17f4-8f90-f2293301bddb
[0m21:59:25.450926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e7dd2006-9d52-4189-8aca-c3a4adf35f69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6C867D890>]}
[0m21:59:25.451945 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:59:25.452947 [info ] [MainThread]: 
[0m21:59:25.460183 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m21:59:25.461183 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m21:59:25.461183 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m21:59:25.466201 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m21:59:25.467168 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 21:59:25.462180 => 21:59:25.467168
[0m21:59:25.468164 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m21:59:25.468164 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 21:59:25.468164 => 21:59:25.468164
[0m21:59:25.469175 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m21:59:25.470186 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m21:59:25.471156 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m21:59:25.472153 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m21:59:25.476143 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m21:59:25.479136 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 21:59:25.472153 => 21:59:25.478137
[0m21:59:25.479136 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m21:59:25.480160 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 21:59:25.480160 => 21:59:25.480160
[0m21:59:25.481159 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m21:59:25.482158 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m21:59:25.483124 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m21:59:25.483124 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m21:59:25.543991 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m21:59:25.544995 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 21:59:25.484121 => 21:59:25.544995
[0m21:59:25.545986 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m21:59:25.546982 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 21:59:25.545986 => 21:59:25.545986
[0m21:59:25.547981 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m21:59:25.548965 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:59:25.549946 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m21:59:25.550943 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m21:59:25.554932 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m21:59:25.555929 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 21:59:25.550943 => 21:59:25.555929
[0m21:59:25.556927 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m21:59:25.556927 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 21:59:25.556927 => 21:59:25.556927
[0m21:59:25.558921 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m21:59:25.559919 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m21:59:25.560945 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m21:59:25.560945 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m21:59:25.564934 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m21:59:25.565903 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 21:59:25.561942 => 21:59:25.565903
[0m21:59:25.566929 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m21:59:25.567926 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 21:59:25.566929 => 21:59:25.566929
[0m21:59:25.568921 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m21:59:25.569893 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m21:59:25.570346 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m21:59:25.571346 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m21:59:25.574338 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m21:59:25.575336 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 21:59:25.571346 => 21:59:25.575336
[0m21:59:25.576333 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m21:59:25.577330 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 21:59:25.576333 => 21:59:25.576333
[0m21:59:25.578328 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m21:59:25.578328 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m21:59:25.579325 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m21:59:25.580352 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m21:59:25.584339 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m21:59:25.585333 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 21:59:25.580352 => 21:59:25.584339
[0m21:59:25.586320 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m21:59:25.586320 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 21:59:25.586320 => 21:59:25.586320
[0m21:59:25.587331 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m21:59:25.588331 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m21:59:25.589299 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.stg_tempo)
[0m21:59:25.590343 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m21:59:25.593316 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m21:59:25.594285 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 21:59:25.590343 => 21:59:25.594285
[0m21:59:25.595312 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m21:59:25.596307 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 21:59:25.595312 => 21:59:25.595312
[0m21:59:25.597305 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m21:59:25.597305 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:59:25.598303 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m21:59:25.599302 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:59:25.602292 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m21:59:25.603291 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 21:59:25.599302 => 21:59:25.603291
[0m21:59:25.604286 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:59:25.605256 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 21:59:25.604286 => 21:59:25.604286
[0m21:59:25.607252 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m21:59:25.607252 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m21:59:25.609268 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_tempo)
[0m21:59:25.609268 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m21:59:25.614233 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m21:59:25.616233 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 21:59:25.610243 => 21:59:25.616233
[0m21:59:25.617224 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m21:59:25.617224 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 21:59:25.617224 => 21:59:25.617224
[0m21:59:25.619246 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m21:59:25.619246 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m21:59:25.620227 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.int_nascimento)
[0m21:59:25.621214 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m21:59:25.624232 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m21:59:25.626200 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 21:59:25.621214 => 21:59:25.625230
[0m21:59:25.626200 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m21:59:25.627197 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 21:59:25.627197 => 21:59:25.627197
[0m21:59:25.628195 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m21:59:25.629592 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m21:59:25.630620 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.fato_nascimento)
[0m21:59:25.630620 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m21:59:25.634609 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m21:59:25.635578 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 21:59:25.631617 => 21:59:25.634609
[0m21:59:25.635578 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m21:59:25.636603 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 21:59:25.636603 => 21:59:25.636603
[0m21:59:25.637573 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m21:59:25.639795 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:59:25.639795 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m21:59:25.640823 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m21:59:25.642784 [debug] [MainThread]: Command end result
[0m21:59:25.663282 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m21:59:25.664288 [info ] [MainThread]: Building catalog
[0m21:59:25.668296 [debug] [ThreadPool]: Acquiring new databricks connection 'default'
[0m21:59:25.678272 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:59:25.679268 [debug] [ThreadPool]: Using databricks connection "default"
[0m21:59:25.679268 [debug] [ThreadPool]: On default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "default"} */

      select current_catalog()
  
[0m21:59:25.680237 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:59:26.762902 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-ec79-12f4-bbb9-c85d517b768d
[0m21:59:28.237485 [debug] [ThreadPool]: SQL status: OK in 2.559999942779541 seconds
[0m21:59:28.246934 [debug] [ThreadPool]: Using databricks connection "default"
[0m21:59:28.247931 [debug] [ThreadPool]: On default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "default"} */
show table extended in `workspace`.`default` like 'int_nascimento|fato_nascimento|fato_atendimento_hospitalar|int_atendimento|stg_atendimento|sinasc_2022_sc_clean|stg_doenca|stg_tempo|stg_localidade|stg_nascidos_vivos|dim_doenca|dim_tempo|dim_localidade'
  
[0m21:59:29.050156 [debug] [ThreadPool]: SQL status: OK in 0.800000011920929 seconds
[0m21:59:29.059590 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`dim_doenca`
[0m21:59:29.060587 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`dim_localidade`
[0m21:59:29.061584 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`dim_tempo`
[0m21:59:29.061584 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`fato_atendimento_hospitalar`
[0m21:59:29.062580 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`fato_nascimento`
[0m21:59:29.062580 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`int_atendimento`
[0m21:59:29.063578 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`int_nascimento`
[0m21:59:29.064575 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`sinasc_2022_sc_clean`
[0m21:59:29.065573 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_atendimento`
[0m21:59:29.065573 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_doenca`
[0m21:59:29.066570 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_localidade`
[0m21:59:29.066570 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_nascidos_vivos`
[0m21:59:29.067568 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_tempo`
[0m21:59:29.074514 [debug] [ThreadPool]: On default: ROLLBACK
[0m21:59:29.075512 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:59:29.075512 [debug] [ThreadPool]: On default: Close
[0m21:59:29.076509 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-ec79-12f4-bbb9-c85d517b768d
[0m21:59:29.323737 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly default, now raw)
[0m21:59:29.327723 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:59:29.328721 [debug] [ThreadPool]: Using databricks connection "raw"
[0m21:59:29.328721 [debug] [ThreadPool]: On raw: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "raw"} */

      select current_catalog()
  
[0m21:59:29.329718 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:59:30.270005 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078a9-ee95-186a-9244-06964713f18e
[0m21:59:30.608843 [debug] [ThreadPool]: SQL status: OK in 1.2799999713897705 seconds
[0m21:59:30.614827 [debug] [ThreadPool]: Using databricks connection "raw"
[0m21:59:30.615846 [debug] [ThreadPool]: On raw: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "raw"} */
show table extended in `workspace`.`raw` like 'doenca|atendimento|faixa_etaria_sexo|procedimento|localidade'
  
[0m21:59:31.088905 [debug] [ThreadPool]: SQL status: OK in 0.4699999988079071 seconds
[0m21:59:31.091898 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`atendimento`
[0m21:59:31.091898 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`doenca`
[0m21:59:31.092896 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`localidade`
[0m21:59:31.093923 [debug] [ThreadPool]: On raw: ROLLBACK
[0m21:59:31.093923 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:59:31.094918 [debug] [ThreadPool]: On raw: Close
[0m21:59:31.094918 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078a9-ee95-186a-9244-06964713f18e
[0m21:59:31.474078 [info ] [MainThread]: Catalog written to C:\Users\Marisa\Desktop\projeto_health_insights\target\catalog.json
[0m21:59:31.475075 [debug] [MainThread]: Command `dbt docs generate` succeeded at 21:59:31.475075 after 9.24 seconds
[0m21:59:31.476073 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m21:59:31.476073 [debug] [MainThread]: Connection 'raw' was properly closed.
[0m21:59:31.477071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6C7F47A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6D2A5E950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E6C2F0C6D0>]}
[0m21:59:31.478068 [debug] [MainThread]: Flushing usage events
[0m21:59:35.897258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADBE0B3D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADC075FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADBA32BD0>]}


============================== 21:59:35.902244 | e0c69b02-6e0b-4952-b861-8661ddf427ed ==============================
[0m21:59:35.902244 [info ] [MainThread]: Running with dbt=1.5.2
[0m21:59:35.904211 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m21:59:37.172482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e0c69b02-6e0b-4952-b861-8661ddf427ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADBD41410>]}
[0m21:59:37.190434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e0c69b02-6e0b-4952-b861-8661ddf427ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADC07A110>]}
[0m22:08:25.079917 [error] [MainThread]: Encountered an error:

[0m22:08:25.136241 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 86, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 71, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 142, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 215, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\main.py", line 299, in docs_serve
    results = task.run()
              ^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\task\serve.py", line 28, in run
    httpd.serve_forever()
  File "C:\Users\Marisa\AppData\Local\Programs\Python\Python311\Lib\socketserver.py", line 233, in serve_forever
    ready = selector.select(poll_interval)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\AppData\Local\Programs\Python\Python311\Lib\selectors.py", line 323, in select
    r, w, _ = self._select(self._readers, self._writers, [], timeout)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\AppData\Local\Programs\Python\Python311\Lib\selectors.py", line 314, in _select
    r, w, x = select.select(r, w, w, timeout)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

[0m22:08:25.139881 [debug] [MainThread]: Command `dbt docs serve` failed at 22:08:25.139881 after 529.26 seconds
[0m22:08:25.140879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADBACC610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADC076250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AADBA63B50>]}
[0m22:08:25.140879 [debug] [MainThread]: Flushing usage events
[0m22:08:52.710325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AB843B2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AB8139E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AB843B150>]}


============================== 22:08:52.715308 | 5f7373ab-ba09-4e4a-b5f6-d99e2948065f ==============================
[0m22:08:52.715308 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:08:52.716324 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:08:54.013545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5f7373ab-ba09-4e4a-b5f6-d99e2948065f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AB7DD6C90>]}
[0m22:08:54.031497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5f7373ab-ba09-4e4a-b5f6-d99e2948065f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AB7DD6C90>]}
[0m22:08:54.032495 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:08:54.052179 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:08:54.156644 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 4 files changed.
[0m22:08:54.156644 [debug] [MainThread]: Partial parsing: added file: projeto_health_insights://models\marts\schema.yaml
[0m22:08:54.157641 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m22:08:54.158618 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m22:08:54.159630 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_tempo.sql
[0m22:08:54.159630 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\intermediate\int_nascimento.sql
[0m22:08:54.186491 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m22:08:54.200484 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m22:08:54.205453 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_tempo.sql
[0m22:08:54.210429 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m22:08:54.342109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5f7373ab-ba09-4e4a-b5f6-d99e2948065f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AC2CA8850>]}
[0m22:08:54.353077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5f7373ab-ba09-4e4a-b5f6-d99e2948065f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AC2CDE950>]}
[0m22:08:54.354046 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:08:54.355047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5f7373ab-ba09-4e4a-b5f6-d99e2948065f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017ABEEA7210>]}
[0m22:08:54.357065 [info ] [MainThread]: 
[0m22:08:54.358038 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:08:54.360029 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:08:54.361054 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:08:54.361054 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m22:08:54.362024 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:08:55.493305 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-3f80-1d8f-9a09-45971be99374
[0m22:08:56.123194 [debug] [ThreadPool]: SQL status: OK in 1.7599999904632568 seconds
[0m22:08:56.127108 [debug] [ThreadPool]: On list_workspace: Close
[0m22:08:56.128105 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-3f80-1d8f-9a09-45971be99374
[0m22:08:56.387571 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m22:08:56.390563 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m22:08:56.413527 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:08:56.414524 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m22:08:56.415522 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m22:08:56.415522 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:08:57.231829 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-4092-1d43-b983-c9504fe16d62
[0m22:08:57.718484 [debug] [ThreadPool]: SQL status: OK in 1.2999999523162842 seconds
[0m22:08:57.720476 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:08:57.720476 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m22:08:57.721473 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:08:57.721473 [debug] [ThreadPool]: On create_workspace_default: Close
[0m22:08:57.722470 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-4092-1d43-b983-c9504fe16d62
[0m22:08:57.971152 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:08:57.976140 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:08:57.977150 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:08:57.977150 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:08:58.803395 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-417e-1992-b54c-bcbdadc01029
[0m22:08:59.174765 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m22:08:59.181743 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:08:59.182743 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-417e-1992-b54c-bcbdadc01029
[0m22:08:59.465557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5f7373ab-ba09-4e4a-b5f6-d99e2948065f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AC2CFE790>]}
[0m22:08:59.465557 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:08:59.466582 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:08:59.467580 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:08:59.468549 [info ] [MainThread]: 
[0m22:08:59.475666 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m22:08:59.475666 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m22:08:59.478110 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m22:08:59.479089 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m22:08:59.484091 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m22:08:59.485061 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 22:08:59.480075 => 22:08:59.485061
[0m22:08:59.486057 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m22:08:59.519995 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m22:08:59.520965 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:08:59.521962 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m22:08:59.521962 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m22:08:59.522970 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:09:00.432583 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-4273-1027-ac3c-b681e7040c17
[0m22:09:01.330415 [debug] [Thread-1 (]: SQL status: OK in 1.809999942779541 seconds
[0m22:09:01.346402 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 22:08:59.486057 => 22:09:01.346402
[0m22:09:01.347396 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m22:09:01.347396 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:09:01.348394 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m22:09:01.349364 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-4273-1027-ac3c-b681e7040c17
[0m22:09:01.672824 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f7373ab-ba09-4e4a-b5f6-d99e2948065f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AC2DF5290>]}
[0m22:09:01.673820 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 2.19s]
[0m22:09:01.676166 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m22:09:01.678162 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m22:09:01.679162 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m22:09:01.682150 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m22:09:01.683146 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m22:09:01.689157 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m22:09:01.691140 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 22:09:01.683146 => 22:09:01.690157
[0m22:09:01.691140 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m22:09:01.696141 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m22:09:01.697136 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:09:01.698115 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m22:09:01.698115 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m22:09:01.699103 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:09:02.558327 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-43be-1bb7-a0e7-39ffc0feaf5d
[0m22:09:03.264518 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m22:09:03.267510 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 22:09:01.692150 => 22:09:03.266513
[0m22:09:03.267510 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m22:09:03.268507 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:09:03.268507 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m22:09:03.269504 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-43be-1bb7-a0e7-39ffc0feaf5d
[0m22:09:03.665393 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f7373ab-ba09-4e4a-b5f6-d99e2948065f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AC2E6C490>]}
[0m22:09:03.666389 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.98s]
[0m22:09:03.668411 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m22:09:03.669379 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m22:09:03.670070 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m22:09:03.672069 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m22:09:03.673066 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m22:09:03.678084 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m22:09:03.679080 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 22:09:03.673066 => 22:09:03.679080
[0m22:09:03.680069 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m22:09:03.685037 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m22:09:03.687028 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:09:03.688025 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m22:09:03.688025 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m22:09:03.689022 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:09:05.151437 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-453a-134e-a66e-9f04bf9c6aef
[0m22:09:05.832424 [debug] [Thread-1 (]: SQL status: OK in 2.140000104904175 seconds
[0m22:09:05.835417 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 22:09:03.681068 => 22:09:05.835417
[0m22:09:05.836414 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m22:09:05.836414 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:09:05.837411 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m22:09:05.837411 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-453a-134e-a66e-9f04bf9c6aef
[0m22:09:06.140755 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f7373ab-ba09-4e4a-b5f6-d99e2948065f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AC1C12790>]}
[0m22:09:06.142746 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 2.47s]
[0m22:09:06.143208 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m22:09:06.144236 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:09:06.145205 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m22:09:06.146206 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m22:09:06.147206 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m22:09:06.151189 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:09:06.153185 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 22:09:06.147206 => 22:09:06.152186
[0m22:09:06.153185 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m22:09:06.158198 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:09:06.159199 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:09:06.160166 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:09:06.160166 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(ROW_NUMBER() OVER () AS STRING) AS nascimento_id,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,  -- se DTNASC for unix timestamp em dias
    CODMUNNATU AS municipio,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- código de SC

[0m22:09:06.161180 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:09:07.645036 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-468e-149b-916e-74fd1a23ee29
[0m22:09:08.471436 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(ROW_NUMBER() OVER () AS STRING) AS nascimento_id,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,  -- se DTNASC for unix timestamp em dias
    CODMUNNATU AS municipio,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- código de SC

[0m22:09:08.472433 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.
[0m22:09:08.474428 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [_LEGACY_ERROR_TEMP_1037] org.apache.spark.sql.AnalysisException: Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.windowFunctionWithWindowFrameNotOrderedError(QueryCompilationErrors.scala:1178)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$$anonfun$apply$48.applyOrElse(Analyzer.scala:4685)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$$anonfun$apply$48.applyOrElse(Analyzer.scala:4683)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:189)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:248)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:248)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:189)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:160)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:308)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:307)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1360)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1357)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1496)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:307)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:304)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:279)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:277)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$.apply(Analyzer.scala:4683)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$.apply(Analyzer.scala:4681)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryCompilationErrors$.windowFunctionWithWindowFrameNotOrderedError(QueryCompilationErrors.scala:1178)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$$anonfun$apply$48.applyOrElse(Analyzer.scala:4685)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$$anonfun$apply$48.applyOrElse(Analyzer.scala:4683)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:189)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:248)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:248)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:189)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:160)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:308)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:307)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1360)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1357)
		at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1496)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:307)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:304)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:279)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:277)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$.apply(Analyzer.scala:4683)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveWindowOrder$.apply(Analyzer.scala:4681)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 60 more

[0m22:09:08.476422 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078ab-46e8-1708-8196-3c2fb0013d84
[0m22:09:08.477420 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 22:09:06.154186 => 22:09:08.477420
[0m22:09:08.478417 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m22:09:08.478417 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:09:08.479420 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m22:09:08.479420 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-468e-149b-916e-74fd1a23ee29
[0m22:09:08.714833 [debug] [Thread-1 (]: Runtime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.
[0m22:09:08.715830 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f7373ab-ba09-4e4a-b5f6-d99e2948065f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AC2CC51D0>]}
[0m22:09:08.716828 [error] [Thread-1 (]: 4 of 5 ERROR creating sql view model default.stg_nascidos_vivos ................ [[31mERROR[0m in 2.57s]
[0m22:09:08.717614 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:09:08.718639 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m22:09:08.720099 [info ] [Thread-1 (]: 5 of 5 SKIP relation default.stg_tempo ......................................... [[33mSKIP[0m]
[0m22:09:08.721059 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m22:09:08.723057 [debug] [MainThread]: On master: ROLLBACK
[0m22:09:08.724054 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:09:09.596996 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ab-47f0-10be-a8f5-17d8c31285f2
[0m22:09:09.597987 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:09:09.598986 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:09:09.599982 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:09:09.599982 [debug] [MainThread]: On master: ROLLBACK
[0m22:09:09.600979 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:09:09.601976 [debug] [MainThread]: On master: Close
[0m22:09:09.601976 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ab-47f0-10be-a8f5-17d8c31285f2
[0m22:09:09.909765 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:09:09.911725 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m22:09:09.912720 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:09:09.912720 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_nascidos_vivos' was properly closed.
[0m22:09:09.913717 [info ] [MainThread]: 
[0m22:09:09.914714 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 15.56 seconds (15.56s).
[0m22:09:09.916737 [debug] [MainThread]: Command end result
[0m22:09:09.928678 [info ] [MainThread]: 
[0m22:09:09.929688 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:09:09.930675 [info ] [MainThread]: 
[0m22:09:09.931670 [error] [MainThread]: [33mRuntime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m22:09:09.932695 [error] [MainThread]:   Window function row_number() requires window to be ordered, please add ORDER BY clause. For example SELECT row_number()(value_expr) OVER (PARTITION BY window_partition ORDER BY window_ordering) from table.
[0m22:09:09.933691 [info ] [MainThread]: 
[0m22:09:09.935659 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m22:09:09.936670 [debug] [MainThread]: Command `dbt run` failed at 22:09:09.936670 after 17.24 seconds
[0m22:09:09.937653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AB84A8B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AB83F2B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017AB841C5D0>]}
[0m22:09:09.937653 [debug] [MainThread]: Flushing usage events
[0m22:10:20.982507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F5E2232D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F5DFC7F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F5DFC6290>]}


============================== 22:10:20.986496 | 8b17136e-1a89-4977-8199-1580a95d3fca ==============================
[0m22:10:20.986496 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:10:20.988463 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:10:22.248987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8b17136e-1a89-4977-8199-1580a95d3fca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F5DF86850>]}
[0m22:10:22.267937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8b17136e-1a89-4977-8199-1580a95d3fca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F5DF86850>]}
[0m22:10:22.268934 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:10:22.289428 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:10:22.400098 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:10:22.401100 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m22:10:22.427999 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m22:10:22.505791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8b17136e-1a89-4977-8199-1580a95d3fca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F68B12050>]}
[0m22:10:22.518755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8b17136e-1a89-4977-8199-1580a95d3fca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F5D796190>]}
[0m22:10:22.519754 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:10:22.519754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b17136e-1a89-4977-8199-1580a95d3fca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F5AFB4550>]}
[0m22:10:22.521748 [info ] [MainThread]: 
[0m22:10:22.523742 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:10:22.526052 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:10:22.526052 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:10:22.527049 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m22:10:22.527049 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:23.387184 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-73ed-1419-8ec1-46f38d6b0510
[0m22:10:23.688197 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m22:10:23.692217 [debug] [ThreadPool]: On list_workspace: Close
[0m22:10:23.692217 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-73ed-1419-8ec1-46f38d6b0510
[0m22:10:23.944687 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m22:10:23.947679 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m22:10:23.957649 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:10:23.957649 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m22:10:23.958658 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m22:10:23.958658 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:10:24.796719 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-74bf-16c5-b80f-760f8fb2633e
[0m22:10:25.271759 [debug] [ThreadPool]: SQL status: OK in 1.309999942779541 seconds
[0m22:10:25.272756 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:10:25.272756 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m22:10:25.273754 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:10:25.273754 [debug] [ThreadPool]: On create_workspace_default: Close
[0m22:10:25.274751 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-74bf-16c5-b80f-760f8fb2633e
[0m22:10:25.515370 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:10:25.520384 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:10:25.521354 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:10:25.522361 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:26.350240 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-75ad-187e-a242-f11befba420c
[0m22:10:26.736241 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m22:10:26.740231 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:10:26.741228 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-75ad-187e-a242-f11befba420c
[0m22:10:26.985231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b17136e-1a89-4977-8199-1580a95d3fca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F5DFAF150>]}
[0m22:10:26.986228 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:10:26.987253 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:10:26.988250 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:10:26.989235 [info ] [MainThread]: 
[0m22:10:26.996202 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m22:10:26.997199 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m22:10:26.998195 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m22:10:26.999224 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m22:10:27.002215 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m22:10:27.004179 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 22:10:26.999224 => 22:10:27.003210
[0m22:10:27.004179 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m22:10:27.044100 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m22:10:27.045098 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:10:27.046068 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m22:10:27.046068 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m22:10:27.047090 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:10:27.859433 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-7696-11c4-a167-c8b3c71d69f7
[0m22:10:28.580792 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m22:10:28.594785 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 22:10:27.005176 => 22:10:28.594785
[0m22:10:28.595780 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m22:10:28.596750 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:10:28.596750 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m22:10:28.597747 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-7696-11c4-a167-c8b3c71d69f7
[0m22:10:28.857138 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b17136e-1a89-4977-8199-1580a95d3fca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F68ABDBD0>]}
[0m22:10:28.858135 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.86s]
[0m22:10:28.859434 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m22:10:28.860462 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m22:10:28.861431 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m22:10:28.862429 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m22:10:28.863426 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m22:10:28.866445 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m22:10:28.867444 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 22:10:28.863426 => 22:10:28.867444
[0m22:10:28.868425 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m22:10:28.873399 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m22:10:28.874397 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:10:28.875395 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m22:10:28.876392 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m22:10:28.876392 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:10:29.702239 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-77b0-111a-94bd-74d29e11b9a0
[0m22:10:30.397575 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m22:10:30.399570 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 22:10:28.868425 => 22:10:30.399570
[0m22:10:30.400567 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m22:10:30.400567 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:10:30.401565 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m22:10:30.402562 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-77b0-111a-94bd-74d29e11b9a0
[0m22:10:30.640806 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b17136e-1a89-4977-8199-1580a95d3fca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F67ABF9D0>]}
[0m22:10:30.641786 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.78s]
[0m22:10:30.643795 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m22:10:30.643795 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m22:10:30.644781 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m22:10:30.645595 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m22:10:30.646625 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m22:10:30.650611 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m22:10:30.651581 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 22:10:30.647594 => 22:10:30.651581
[0m22:10:30.652577 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m22:10:30.661555 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m22:10:30.662561 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:10:30.663549 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m22:10:30.664546 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m22:10:30.664546 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:10:31.471491 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-78bd-1f49-8762-dde1fb3e5d81
[0m22:10:32.149698 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m22:10:32.152687 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 22:10:30.653587 => 22:10:32.152687
[0m22:10:32.153685 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m22:10:32.153685 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:10:32.154682 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m22:10:32.154682 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-78bd-1f49-8762-dde1fb3e5d81
[0m22:10:32.399412 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b17136e-1a89-4977-8199-1580a95d3fca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F68BAEF50>]}
[0m22:10:32.403400 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.75s]
[0m22:10:32.407621 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m22:10:32.409584 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:10:32.410582 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m22:10:32.412383 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m22:10:32.413380 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m22:10:32.418394 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:10:32.419363 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 22:10:32.413380 => 22:10:32.419363
[0m22:10:32.419363 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m22:10:32.424381 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:10:32.425376 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:10:32.425376 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:10:32.426345 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(DTRECORIGA AS STRING) AS nascimento_id,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    CODMUNNASC AS municipio,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    ROW_NUMBER() OVER (PARTITION BY CODUFNATU ORDER BY DTNASC) AS row_num
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42

[0m22:10:32.426345 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:10:33.274853 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-79d1-19b0-bc64-3fe6b1dd346d
[0m22:10:33.939304 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m22:10:33.942296 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 22:10:32.420361 => 22:10:33.942296
[0m22:10:33.943305 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m22:10:33.943305 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:10:33.944290 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m22:10:33.944290 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-79d1-19b0-bc64-3fe6b1dd346d
[0m22:10:34.173582 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b17136e-1a89-4977-8199-1580a95d3fca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F67AB1D90>]}
[0m22:10:34.176575 [info ] [Thread-1 (]: 4 of 5 OK created sql view model default.stg_nascidos_vivos .................... [[32mOK[0m in 1.76s]
[0m22:10:34.181476 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:10:34.184468 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m22:10:34.186434 [info ] [Thread-1 (]: 5 of 5 START sql view model default.stg_tempo .................................. [RUN]
[0m22:10:34.191421 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.stg_tempo)
[0m22:10:34.193414 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m22:10:34.200419 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m22:10:34.201389 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 22:10:34.194407 => 22:10:34.201389
[0m22:10:34.201389 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m22:10:34.206403 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m22:10:34.207394 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:10:34.207394 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m22:10:34.208370 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(YEAR(data_nascimento)*10000 + MONTH(data_nascimento)*100 + DAY(data_nascimento) AS INT) AS data_id,
    YEAR(data_nascimento) AS ano,
    MONTH(data_nascimento) AS mes,
    DAY(data_nascimento) AS dia,
    QUARTER(data_nascimento) AS trimestre,
    DAYOFWEEK(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m22:10:34.209368 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:10:35.127758 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-7aeb-1c2d-946d-9dcfd31ad6bf
[0m22:10:35.864846 [debug] [Thread-1 (]: SQL status: OK in 1.659999966621399 seconds
[0m22:10:35.867837 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 22:10:34.202386 => 22:10:35.867837
[0m22:10:35.868834 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m22:10:35.868834 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:10:35.869831 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m22:10:35.869831 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-7aeb-1c2d-946d-9dcfd31ad6bf
[0m22:10:36.107044 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b17136e-1a89-4977-8199-1580a95d3fca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F5DAD9710>]}
[0m22:10:36.109036 [info ] [Thread-1 (]: 5 of 5 OK created sql view model default.stg_tempo ............................. [[32mOK[0m in 1.92s]
[0m22:10:36.110261 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m22:10:36.113285 [debug] [MainThread]: On master: ROLLBACK
[0m22:10:36.113285 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:10:36.940753 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ab-7bfd-17f1-b115-2e3bb5019b76
[0m22:10:36.941340 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:10:36.942340 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:10:36.942340 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:10:36.943338 [debug] [MainThread]: On master: ROLLBACK
[0m22:10:36.943338 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:10:36.944335 [debug] [MainThread]: On master: Close
[0m22:10:36.944335 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ab-7bfd-17f1-b115-2e3bb5019b76
[0m22:10:37.171475 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:10:37.173470 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m22:10:37.174433 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:10:37.174433 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_tempo' was properly closed.
[0m22:10:37.176427 [info ] [MainThread]: 
[0m22:10:37.177425 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 14.65 seconds (14.65s).
[0m22:10:37.179449 [debug] [MainThread]: Command end result
[0m22:10:37.190403 [info ] [MainThread]: 
[0m22:10:37.191388 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:10:37.193383 [info ] [MainThread]: 
[0m22:10:37.195389 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m22:10:37.196375 [debug] [MainThread]: Command `dbt run` succeeded at 22:10:37.196375 after 16.23 seconds
[0m22:10:37.197372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F57DCBDD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F5DFC5D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F5E28C490>]}
[0m22:10:37.197372 [debug] [MainThread]: Flushing usage events
[0m22:10:55.753548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DE7641C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DE743F6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DE766E010>]}


============================== 22:10:55.758530 | 67628673-95cf-4f01-9c35-280a03608d34 ==============================
[0m22:10:55.758530 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:10:55.759507 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:10:57.052162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '67628673-95cf-4f01-9c35-280a03608d34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DE73ADD50>]}
[0m22:10:57.071114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '67628673-95cf-4f01-9c35-280a03608d34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DF1E88190>]}
[0m22:10:57.072109 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:10:57.092053 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:10:57.202754 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:10:57.202754 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:10:57.209735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '67628673-95cf-4f01-9c35-280a03608d34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DF1F82CD0>]}
[0m22:10:57.274563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '67628673-95cf-4f01-9c35-280a03608d34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DF1E280D0>]}
[0m22:10:57.275560 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:10:57.276080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67628673-95cf-4f01-9c35-280a03608d34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DE43B4490>]}
[0m22:10:57.278108 [info ] [MainThread]: 
[0m22:10:57.279075 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:10:57.282020 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:10:57.282020 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:10:57.283017 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m22:10:57.283017 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:10:58.118101 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-88a0-18da-a533-f2edeabdc39d
[0m22:10:58.449813 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m22:10:58.453523 [debug] [ThreadPool]: On list_workspace: Close
[0m22:10:58.454520 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-88a0-18da-a533-f2edeabdc39d
[0m22:10:58.682434 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m22:10:58.684428 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m22:10:58.704407 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:10:58.705405 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m22:10:58.706397 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m22:10:58.707401 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:10:59.518121 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-8974-1dde-be78-0925bc3eaec9
[0m22:10:59.899728 [debug] [ThreadPool]: SQL status: OK in 1.190000057220459 seconds
[0m22:10:59.900726 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:10:59.900726 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m22:10:59.901723 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:10:59.901723 [debug] [ThreadPool]: On create_workspace_default: Close
[0m22:10:59.902720 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-8974-1dde-be78-0925bc3eaec9
[0m22:11:00.151284 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:11:00.169233 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:11:00.170231 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:11:00.171229 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:11:01.090995 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-8a65-15a2-89a4-d77b20203b77
[0m22:11:01.485305 [debug] [ThreadPool]: SQL status: OK in 1.309999942779541 seconds
[0m22:11:01.489339 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:11:01.490336 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-8a65-15a2-89a4-d77b20203b77
[0m22:11:01.742705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67628673-95cf-4f01-9c35-280a03608d34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DE6F38510>]}
[0m22:11:01.744737 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:11:01.746698 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:11:01.749723 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:11:01.751685 [info ] [MainThread]: 
[0m22:11:01.764837 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m22:11:01.765833 [info ] [Thread-1 (]: 1 of 2 START sql view model default.int_atendimento ............................ [RUN]
[0m22:11:01.766830 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.int_atendimento'
[0m22:11:01.767855 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m22:11:01.771844 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m22:11:01.772814 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 22:11:01.767855 => 22:11:01.772814
[0m22:11:01.773811 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m22:11:01.810712 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m22:11:01.812719 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:11:01.812719 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m22:11:01.813726 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m22:11:01.813726 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:11:02.886186 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-8b6c-109a-aa63-7e8390f255e1
[0m22:11:03.652307 [debug] [Thread-1 (]: SQL status: OK in 1.840000033378601 seconds
[0m22:11:03.667266 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 22:11:01.774814 => 22:11:03.667266
[0m22:11:03.668237 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m22:11:03.668237 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:11:03.669261 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m22:11:03.669261 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-8b6c-109a-aa63-7e8390f255e1
[0m22:11:03.908630 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67628673-95cf-4f01-9c35-280a03608d34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DF1F8CE90>]}
[0m22:11:03.909619 [info ] [Thread-1 (]: 1 of 2 OK created sql view model default.int_atendimento ....................... [[32mOK[0m in 2.14s]
[0m22:11:03.910370 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m22:11:03.911400 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m22:11:03.912405 [info ] [Thread-1 (]: 2 of 2 START sql view model default.int_nascimento ............................. [RUN]
[0m22:11:03.913237 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m22:11:03.914268 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m22:11:03.918255 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m22:11:03.919224 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 22:11:03.914268 => 22:11:03.919224
[0m22:11:03.920221 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m22:11:03.928227 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m22:11:03.930194 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:11:03.931191 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m22:11:03.931191 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

SELECT
    nascimento_id,
    t.data_id,
    n.CODMUNNATU AS municipio,
    n.SEXO AS sexo,
    n.PESO AS peso,
    n.IDADEMAE AS idade_mae,
    n.GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m22:11:03.932188 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:11:04.845094 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-8ca2-1b8f-9b4c-7079e9e8c5f1
[0m22:11:05.433389 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

SELECT
    nascimento_id,
    t.data_id,
    n.CODMUNNATU AS municipio,
    n.SEXO AS sexo,
    n.PESO AS peso,
    n.IDADEMAE AS idade_mae,
    n.GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m22:11:05.434392 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`CODMUNNATU` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`data`, `t`.`dia`, `t`.`mes`, `n`.`peso`]. SQLSTATE: 42703; line 11 pos 4
[0m22:11:05.435388 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`CODMUNNATU` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`data`, `t`.`dia`, `t`.`mes`, `n`.`peso`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`CODMUNNATU` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`data`, `t`.`dia`, `t`.`mes`, `n`.`peso`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m22:11:05.436387 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078ab-8cc2-180c-b99a-14937cd64ed4
[0m22:11:05.437390 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 22:11:03.921247 => 22:11:05.437390
[0m22:11:05.438352 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m22:11:05.438352 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:11:05.439380 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m22:11:05.440344 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-8ca2-1b8f-9b4c-7079e9e8c5f1
[0m22:11:05.676470 [debug] [Thread-1 (]: Runtime Error in model int_nascimento (models\intermediate\int_nascimento.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`CODMUNNATU` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`data`, `t`.`dia`, `t`.`mes`, `n`.`peso`]. SQLSTATE: 42703; line 11 pos 4
[0m22:11:05.677466 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67628673-95cf-4f01-9c35-280a03608d34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DF1E85AD0>]}
[0m22:11:05.678465 [error] [Thread-1 (]: 2 of 2 ERROR creating sql view model default.int_nascimento .................... [[31mERROR[0m in 1.76s]
[0m22:11:05.679154 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m22:11:05.682150 [debug] [MainThread]: On master: ROLLBACK
[0m22:11:05.682150 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:11:06.481558 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ab-8d9c-1cf3-8830-7ca61f854d93
[0m22:11:06.482525 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:11:06.483551 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:11:06.484553 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:11:06.484553 [debug] [MainThread]: On master: ROLLBACK
[0m22:11:06.485548 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:11:06.486546 [debug] [MainThread]: On master: Close
[0m22:11:06.486546 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ab-8d9c-1cf3-8830-7ca61f854d93
[0m22:11:06.722756 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:11:06.723769 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m22:11:06.723769 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:11:06.724752 [debug] [MainThread]: Connection 'model.projeto_health_insights.int_nascimento' was properly closed.
[0m22:11:06.725750 [info ] [MainThread]: 
[0m22:11:06.726747 [info ] [MainThread]: Finished running 2 view models in 0 hours 0 minutes and 9.45 seconds (9.45s).
[0m22:11:06.727744 [debug] [MainThread]: Command end result
[0m22:11:06.742705 [info ] [MainThread]: 
[0m22:11:06.743723 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:11:06.745705 [info ] [MainThread]: 
[0m22:11:06.746694 [error] [MainThread]: [33mRuntime Error in model int_nascimento (models\intermediate\int_nascimento.sql)[0m
[0m22:11:06.747690 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`CODMUNNATU` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`data`, `t`.`dia`, `t`.`mes`, `n`.`peso`]. SQLSTATE: 42703; line 11 pos 4
[0m22:11:06.749686 [info ] [MainThread]: 
[0m22:11:06.750684 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m22:11:06.751696 [debug] [MainThread]: Command `dbt run` failed at 22:11:06.751696 after 11.02 seconds
[0m22:11:06.752700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DE739EA50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DE70DB690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DE7385D10>]}
[0m22:11:06.753703 [debug] [MainThread]: Flushing usage events
[0m22:12:13.030865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195C77DE8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195C77F2450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195C77B9250>]}


============================== 22:12:13.034855 | 825c9584-86cb-4040-904f-3ed9882861e1 ==============================
[0m22:12:13.034855 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:12:13.036822 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:12:14.302503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '825c9584-86cb-4040-904f-3ed9882861e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195C7CAFCD0>]}
[0m22:12:14.320455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '825c9584-86cb-4040-904f-3ed9882861e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195C7CAFCD0>]}
[0m22:12:14.321453 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:12:14.337410 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:12:14.450253 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:12:14.451251 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\intermediate\int_nascimento.sql
[0m22:12:14.477181 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m22:12:14.556000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '825c9584-86cb-4040-904f-3ed9882861e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195D234DD90>]}
[0m22:12:14.567968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '825c9584-86cb-4040-904f-3ed9882861e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195D12DA590>]}
[0m22:12:14.568965 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:12:14.569933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '825c9584-86cb-4040-904f-3ed9882861e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195D2362590>]}
[0m22:12:14.571957 [info ] [MainThread]: 
[0m22:12:14.572936 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:12:14.575939 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:12:14.575939 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:12:14.576939 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m22:12:14.576939 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:12:15.412914 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-b6b1-1a73-99f3-a5d9f452d2fb
[0m22:12:15.715107 [debug] [ThreadPool]: SQL status: OK in 1.1399999856948853 seconds
[0m22:12:15.719012 [debug] [ThreadPool]: On list_workspace: Close
[0m22:12:15.720009 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-b6b1-1a73-99f3-a5d9f452d2fb
[0m22:12:15.949401 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m22:12:15.952393 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m22:12:15.977324 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:15.977324 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m22:12:15.978322 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m22:12:15.979318 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:12:16.747084 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-b77d-19e7-adb2-f86709262b7a
[0m22:12:17.095421 [debug] [ThreadPool]: SQL status: OK in 1.1200000047683716 seconds
[0m22:12:17.097416 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:12:17.097416 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m22:12:17.098414 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:12:17.098414 [debug] [ThreadPool]: On create_workspace_default: Close
[0m22:12:17.099411 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-b77d-19e7-adb2-f86709262b7a
[0m22:12:17.335338 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:12:17.340351 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:12:17.340351 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:12:17.341347 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:12:18.137009 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-b851-14d8-b366-383a1df3cff8
[0m22:12:18.496009 [debug] [ThreadPool]: SQL status: OK in 1.149999976158142 seconds
[0m22:12:18.500002 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:12:18.500996 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-b851-14d8-b366-383a1df3cff8
[0m22:12:18.732221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '825c9584-86cb-4040-904f-3ed9882861e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195C6B781D0>]}
[0m22:12:18.732221 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:18.733244 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:12:18.734244 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:12:18.734964 [info ] [MainThread]: 
[0m22:12:18.741550 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m22:12:18.742579 [info ] [Thread-1 (]: 1 of 2 START sql view model default.int_atendimento ............................ [RUN]
[0m22:12:18.743547 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.int_atendimento'
[0m22:12:18.744572 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m22:12:18.749533 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m22:12:18.751526 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 22:12:18.745569 => 22:12:18.750528
[0m22:12:18.752522 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m22:12:18.786462 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m22:12:18.787458 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:18.788427 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m22:12:18.788427 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m22:12:18.789424 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:12:19.611528 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-b932-1782-b49b-3d7375d13bfc
[0m22:12:20.376121 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m22:12:20.391079 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 22:12:18.752522 => 22:12:20.390081
[0m22:12:20.391079 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m22:12:20.392076 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:12:20.392076 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m22:12:20.393074 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-b932-1782-b49b-3d7375d13bfc
[0m22:12:20.624818 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '825c9584-86cb-4040-904f-3ed9882861e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195D22FC690>]}
[0m22:12:20.625813 [info ] [Thread-1 (]: 1 of 2 OK created sql view model default.int_atendimento ....................... [[32mOK[0m in 1.88s]
[0m22:12:20.626594 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m22:12:20.627622 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m22:12:20.628705 [info ] [Thread-1 (]: 2 of 2 START sql view model default.int_nascimento ............................. [RUN]
[0m22:12:20.629447 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m22:12:20.630479 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m22:12:20.633466 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m22:12:20.635433 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 22:12:20.630479 => 22:12:20.634464
[0m22:12:20.635433 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m22:12:20.640447 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m22:12:20.641426 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:20.641426 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m22:12:20.642440 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    SELECT
    n.nascimento_id,
    t.data_id,
    n.municipio,
    n.sexo,
    n.peso,
    n.idade_mae,
    n.gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m22:12:20.642440 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:12:21.442795 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-ba4a-1d75-a2f5-f144f1665856
[0m22:12:22.229616 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m22:12:22.239588 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 22:12:20.636431 => 22:12:22.238592
[0m22:12:22.241582 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m22:12:22.242578 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:12:22.243576 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m22:12:22.245570 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-ba4a-1d75-a2f5-f144f1665856
[0m22:12:22.492897 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '825c9584-86cb-4040-904f-3ed9882861e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195D2395810>]}
[0m22:12:22.493895 [info ] [Thread-1 (]: 2 of 2 OK created sql view model default.int_nascimento ........................ [[32mOK[0m in 1.86s]
[0m22:12:22.495289 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m22:12:22.497286 [debug] [MainThread]: On master: ROLLBACK
[0m22:12:22.498311 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:12:23.292948 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ab-bb63-1c54-aaac-6a2d7e094bba
[0m22:12:23.294093 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:12:23.296136 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:23.296136 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:12:23.297133 [debug] [MainThread]: On master: ROLLBACK
[0m22:12:23.298133 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:12:23.298133 [debug] [MainThread]: On master: Close
[0m22:12:23.299119 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ab-bb63-1c54-aaac-6a2d7e094bba
[0m22:12:23.544048 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:12:23.546041 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m22:12:23.547037 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:12:23.548035 [debug] [MainThread]: Connection 'model.projeto_health_insights.int_nascimento' was properly closed.
[0m22:12:23.549031 [info ] [MainThread]: 
[0m22:12:23.549894 [info ] [MainThread]: Finished running 2 view models in 0 hours 0 minutes and 8.98 seconds (8.98s).
[0m22:12:23.550893 [debug] [MainThread]: Command end result
[0m22:12:23.568859 [info ] [MainThread]: 
[0m22:12:23.569843 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:12:23.570841 [info ] [MainThread]: 
[0m22:12:23.571838 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m22:12:23.573836 [debug] [MainThread]: Command `dbt run` succeeded at 22:12:23.573836 after 10.56 seconds
[0m22:12:23.574830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195C7A6B1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195C77BA5D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195C7A8B550>]}
[0m22:12:23.574830 [debug] [MainThread]: Flushing usage events
[0m22:12:36.391109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7A94E2750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7A6EB0610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7A95F9510>]}


============================== 22:12:36.396096 | 3055db34-27a8-480e-baef-750f1254ef84 ==============================
[0m22:12:36.396096 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:12:36.397065 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:12:37.666642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3055db34-27a8-480e-baef-750f1254ef84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7A9186110>]}
[0m22:12:37.685591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3055db34-27a8-480e-baef-750f1254ef84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7B2CE0850>]}
[0m22:12:37.685591 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:12:37.702829 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:12:37.815247 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:12:37.816245 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:12:37.823248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3055db34-27a8-480e-baef-750f1254ef84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7A9B542D0>]}
[0m22:12:37.888081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3055db34-27a8-480e-baef-750f1254ef84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7B2D241D0>]}
[0m22:12:37.889080 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:12:37.890048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3055db34-27a8-480e-baef-750f1254ef84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7B02EEA50>]}
[0m22:12:37.893039 [info ] [MainThread]: 
[0m22:12:37.894781 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:12:37.896739 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:12:37.897766 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:12:37.898735 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m22:12:37.899733 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:12:38.705318 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-c493-15f8-a0e1-437b1451a694
[0m22:12:38.995504 [debug] [ThreadPool]: SQL status: OK in 1.100000023841858 seconds
[0m22:12:39.001705 [debug] [ThreadPool]: On list_workspace: Close
[0m22:12:39.002702 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-c493-15f8-a0e1-437b1451a694
[0m22:12:39.240145 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m22:12:39.243137 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m22:12:39.253108 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:39.254105 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m22:12:39.254105 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m22:12:39.255894 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:12:40.088631 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-c566-1194-a1ab-8e3001d6726a
[0m22:12:40.478531 [debug] [ThreadPool]: SQL status: OK in 1.2200000286102295 seconds
[0m22:12:40.479529 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:12:40.480526 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m22:12:40.480526 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:12:40.481530 [debug] [ThreadPool]: On create_workspace_default: Close
[0m22:12:40.481530 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-c566-1194-a1ab-8e3001d6726a
[0m22:12:40.722489 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:12:40.737448 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:12:40.738445 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:12:40.739443 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:12:41.536975 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-c644-1794-9e9c-39c06900e47f
[0m22:12:41.908183 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m22:12:41.912174 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:12:41.913144 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-c644-1794-9e9c-39c06900e47f
[0m22:12:42.135753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3055db34-27a8-480e-baef-750f1254ef84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7A715EA90>]}
[0m22:12:42.136706 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:42.136706 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:12:42.137740 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:12:42.139709 [info ] [MainThread]: 
[0m22:12:42.146679 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m22:12:42.147676 [info ] [Thread-1 (]: 1 of 5 START sql view model default.dim_doenca ................................. [RUN]
[0m22:12:42.149671 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.dim_doenca'
[0m22:12:42.150669 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m22:12:42.155654 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m22:12:42.156651 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 22:12:42.150669 => 22:12:42.156651
[0m22:12:42.157682 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m22:12:42.193582 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m22:12:42.194582 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:42.195547 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m22:12:42.195547 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m22:12:42.196544 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:12:43.002014 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-c720-1e0b-a4cd-7ec836f392ab
[0m22:12:43.660453 [debug] [Thread-1 (]: SQL status: OK in 1.4600000381469727 seconds
[0m22:12:43.677435 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 22:12:42.157682 => 22:12:43.676438
[0m22:12:43.677435 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m22:12:43.678433 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:12:43.679430 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m22:12:43.679430 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-c720-1e0b-a4cd-7ec836f392ab
[0m22:12:43.911700 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3055db34-27a8-480e-baef-750f1254ef84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7B2D6A850>]}
[0m22:12:43.914693 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.dim_doenca ............................ [[32mOK[0m in 1.76s]
[0m22:12:43.915665 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m22:12:43.916693 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m22:12:43.917662 [info ] [Thread-1 (]: 2 of 5 START sql view model default.dim_localidade ............................. [RUN]
[0m22:12:43.918660 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m22:12:43.919657 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m22:12:43.922676 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m22:12:43.923646 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 22:12:43.919657 => 22:12:43.923646
[0m22:12:43.924672 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m22:12:43.931625 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m22:12:43.932622 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:43.933633 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m22:12:43.935614 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m22:12:43.936641 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:12:44.763132 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-c82e-1b68-a1d5-8e4d9173adb8
[0m22:12:45.450082 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m22:12:45.453072 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 22:12:43.924672 => 22:12:45.453072
[0m22:12:45.454069 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m22:12:45.454069 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:12:45.455066 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m22:12:45.455066 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-c82e-1b68-a1d5-8e4d9173adb8
[0m22:12:45.700108 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3055db34-27a8-480e-baef-750f1254ef84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7B2D74350>]}
[0m22:12:45.701109 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.dim_localidade ........................ [[32mOK[0m in 1.78s]
[0m22:12:45.703066 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m22:12:45.703066 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m22:12:45.704062 [info ] [Thread-1 (]: 3 of 5 START sql view model default.dim_tempo .................................. [RUN]
[0m22:12:45.705905 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m22:12:45.705905 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m22:12:45.709894 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m22:12:45.711867 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 22:12:45.706889 => 22:12:45.710891
[0m22:12:45.711867 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m22:12:45.716848 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m22:12:45.717845 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:45.718844 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m22:12:45.719841 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

SELECT DISTINCT
    data_id,
    data,
    ano,
    mes,
    dia,
    trimestre,
    dia_semana
FROM `workspace`.`default`.`stg_tempo`

[0m22:12:45.720867 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:12:46.514091 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-c93b-1a3a-a7c4-3db7368fb913
[0m22:12:47.224306 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m22:12:47.227296 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 22:12:45.712887 => 22:12:47.227296
[0m22:12:47.228293 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m22:12:47.228293 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:12:47.229291 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m22:12:47.229291 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-c93b-1a3a-a7c4-3db7368fb913
[0m22:12:47.459455 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3055db34-27a8-480e-baef-750f1254ef84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7B3F16410>]}
[0m22:12:47.462444 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.dim_tempo ............................. [[32mOK[0m in 1.75s]
[0m22:12:47.463951 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m22:12:47.463951 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m22:12:47.464955 [info ] [Thread-1 (]: 4 of 5 START sql view model default.fato_atendimento_hospitalar ................ [RUN]
[0m22:12:47.465954 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m22:12:47.466951 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m22:12:47.470941 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m22:12:47.471938 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 22:12:47.466951 => 22:12:47.471938
[0m22:12:47.471938 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m22:12:47.476925 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m22:12:47.478919 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:47.478919 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m22:12:47.479919 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m22:12:47.480915 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:12:48.314731 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-ca4c-1ff1-9de5-94494c7ea414
[0m22:12:49.077419 [debug] [Thread-1 (]: SQL status: OK in 1.600000023841858 seconds
[0m22:12:49.080412 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 22:12:47.472936 => 22:12:49.079414
[0m22:12:49.080412 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m22:12:49.081409 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:12:49.081409 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m22:12:49.082406 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-ca4c-1ff1-9de5-94494c7ea414
[0m22:12:49.315741 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3055db34-27a8-480e-baef-750f1254ef84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7B3E4D5D0>]}
[0m22:12:49.318730 [info ] [Thread-1 (]: 4 of 5 OK created sql view model default.fato_atendimento_hospitalar ........... [[32mOK[0m in 1.85s]
[0m22:12:49.319728 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m22:12:49.319728 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m22:12:49.320753 [info ] [Thread-1 (]: 5 of 5 START sql view model default.fato_nascimento ............................ [RUN]
[0m22:12:49.322720 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.fato_nascimento)
[0m22:12:49.323866 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m22:12:49.327886 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m22:12:49.329856 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 22:12:49.324894 => 22:12:49.328883
[0m22:12:49.329856 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m22:12:49.333873 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m22:12:49.335841 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:49.335841 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m22:12:49.336844 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    nascimento_id,
    data_id,
    municipio,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas
FROM `workspace`.`default`.`int_nascimento`

[0m22:12:49.336844 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:12:50.145171 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-cb65-1ee5-b850-059019425893
[0m22:12:50.940819 [debug] [Thread-1 (]: SQL status: OK in 1.600000023841858 seconds
[0m22:12:50.943808 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 22:12:49.330877 => 22:12:50.943808
[0m22:12:50.944806 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m22:12:50.944806 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:12:50.945805 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m22:12:50.945805 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-cb65-1ee5-b850-059019425893
[0m22:12:51.175239 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3055db34-27a8-480e-baef-750f1254ef84', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7B3D87090>]}
[0m22:12:51.177233 [info ] [Thread-1 (]: 5 of 5 OK created sql view model default.fato_nascimento ....................... [[32mOK[0m in 1.85s]
[0m22:12:51.178370 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m22:12:51.181364 [debug] [MainThread]: On master: ROLLBACK
[0m22:12:51.181364 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:12:51.999153 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ab-cc81-14ab-9f32-4bf01cd582dc
[0m22:12:52.000149 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:12:52.001146 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:12:52.001146 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:12:52.002145 [debug] [MainThread]: On master: ROLLBACK
[0m22:12:52.003141 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:12:52.003141 [debug] [MainThread]: On master: Close
[0m22:12:52.004137 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ab-cc81-14ab-9f32-4bf01cd582dc
[0m22:12:52.230887 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:12:52.232884 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m22:12:52.232884 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:12:52.233865 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m22:12:52.234865 [info ] [MainThread]: 
[0m22:12:52.235832 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 14.34 seconds (14.34s).
[0m22:12:52.237855 [debug] [MainThread]: Command end result
[0m22:12:52.249795 [info ] [MainThread]: 
[0m22:12:52.250793 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:12:52.251790 [info ] [MainThread]: 
[0m22:12:52.252786 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m22:12:52.254782 [debug] [MainThread]: Command `dbt run` succeeded at 22:12:52.254782 after 15.88 seconds
[0m22:12:52.255779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7A9596C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7A32EE010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D7A304BDD0>]}
[0m22:12:52.256776 [debug] [MainThread]: Flushing usage events
[0m22:13:06.875211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019AD2B0F4D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019AD2E49990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019AD282EE10>]}


============================== 22:13:06.880638 | e9b338ee-7775-460e-aa85-e1aeb033063b ==============================
[0m22:13:06.880638 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:13:06.881130 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:13:08.206283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e9b338ee-7775-460e-aa85-e1aeb033063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019AD2D92150>]}
[0m22:13:08.224235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e9b338ee-7775-460e-aa85-e1aeb033063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019AD2D92150>]}
[0m22:13:08.225232 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:13:08.244764 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:13:08.339496 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:13:08.340494 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:13:08.347475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e9b338ee-7775-460e-aa85-e1aeb033063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019ADC58F0D0>]}
[0m22:13:08.412302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e9b338ee-7775-460e-aa85-e1aeb033063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019ADC5CE750>]}
[0m22:13:08.412302 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:13:08.413593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e9b338ee-7775-460e-aa85-e1aeb033063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019AD2B0E8D0>]}
[0m22:13:08.415620 [info ] [MainThread]: 
[0m22:13:08.417587 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:13:08.420802 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:13:08.426786 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:13:08.426786 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:13:08.427784 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:13:09.336489 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ab-d6d5-1acb-8549-a6e3cdc4546c
[0m22:13:09.714927 [debug] [ThreadPool]: SQL status: OK in 1.2899999618530273 seconds
[0m22:13:09.720323 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:13:09.720323 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ab-d6d5-1acb-8549-a6e3cdc4546c
[0m22:13:09.952546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e9b338ee-7775-460e-aa85-e1aeb033063b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019ADC5CC050>]}
[0m22:13:09.952546 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:13:09.953571 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:13:09.954540 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:13:09.955538 [info ] [MainThread]: 
[0m22:13:09.961697 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m22:13:09.962693 [info ] [Thread-1 (]: 1 of 5 START test not_null_dim_tempo_data_id ................................... [RUN]
[0m22:13:09.964692 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e'
[0m22:13:09.965714 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m22:13:09.981642 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m22:13:09.983639 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 22:13:09.965714 => 22:13:09.983639
[0m22:13:09.983639 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m22:13:10.005606 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m22:13:10.006583 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:13:10.007595 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m22:13:10.008571 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m22:13:10.008571 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:13:10.791992 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-d7b3-1db3-9f03-68c605492a4e
[0m22:13:14.705185 [debug] [Thread-1 (]: SQL status: OK in 4.699999809265137 seconds
[0m22:13:14.723545 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 22:13:09.984635 => 22:13:14.723545
[0m22:13:14.724542 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m22:13:14.725540 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:13:14.725540 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m22:13:14.726537 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-d7b3-1db3-9f03-68c605492a4e
[0m22:13:14.968753 [info ] [Thread-1 (]: 1 of 5 PASS not_null_dim_tempo_data_id ......................................... [[32mPASS[0m in 5.00s]
[0m22:13:14.970713 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m22:13:14.971738 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m22:13:14.972736 [info ] [Thread-1 (]: 2 of 5 START test not_null_fato_nascimento_data_id ............................. [RUN]
[0m22:13:14.973706 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m22:13:14.974731 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m22:13:14.979717 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m22:13:14.980717 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 22:13:14.974731 => 22:13:14.980717
[0m22:13:14.981712 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m22:13:14.984705 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m22:13:14.986670 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:13:14.987669 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m22:13:14.987669 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m22:13:14.988666 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:13:15.888245 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-dabe-10f1-83a5-c6e6627a7654
[0m22:13:17.410997 [debug] [Thread-1 (]: SQL status: OK in 2.4200000762939453 seconds
[0m22:13:17.413990 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 22:13:14.981712 => 22:13:17.413990
[0m22:13:17.413990 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m22:13:17.414987 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:13:17.415988 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m22:13:17.415988 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-dabe-10f1-83a5-c6e6627a7654
[0m22:13:17.655581 [info ] [Thread-1 (]: 2 of 5 PASS not_null_fato_nascimento_data_id ................................... [[32mPASS[0m in 2.68s]
[0m22:13:17.656362 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m22:13:17.657391 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m22:13:17.658341 [info ] [Thread-1 (]: 3 of 5 START test not_null_fato_nascimento_nascimento_id ....................... [RUN]
[0m22:13:17.659341 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m22:13:17.660338 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m22:13:17.665325 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m22:13:17.667264 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 22:13:17.660338 => 22:13:17.667264
[0m22:13:17.668263 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m22:13:17.673250 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m22:13:17.675245 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:13:17.675245 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m22:13:17.676245 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m22:13:17.677254 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:13:18.466419 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-dc46-17f9-977e-3894bf1654a4
[0m22:13:19.352015 [debug] [Thread-1 (]: SQL status: OK in 1.6699999570846558 seconds
[0m22:13:19.355140 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 22:13:17.668263 => 22:13:19.354146
[0m22:13:19.355140 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m22:13:19.356137 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:13:19.356137 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m22:13:19.357134 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-dc46-17f9-977e-3894bf1654a4
[0m22:13:19.602544 [info ] [Thread-1 (]: 3 of 5 PASS not_null_fato_nascimento_nascimento_id ............................. [[32mPASS[0m in 1.94s]
[0m22:13:19.604532 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m22:13:19.605529 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m22:13:19.606526 [info ] [Thread-1 (]: 4 of 5 START test unique_dim_tempo_data_id ..................................... [RUN]
[0m22:13:19.608521 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m22:13:19.609523 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m22:13:19.619492 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m22:13:19.621487 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 22:13:19.609523 => 22:13:19.621487
[0m22:13:19.622484 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m22:13:19.626501 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m22:13:19.627470 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:13:19.627470 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m22:13:19.628495 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m22:13:19.628495 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:13:20.445935 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-dd74-1872-beb6-e7b778f3e69b
[0m22:13:21.331733 [debug] [Thread-1 (]: SQL status: OK in 1.7000000476837158 seconds
[0m22:13:21.335720 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 22:13:19.623482 => 22:13:21.334723
[0m22:13:21.335720 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m22:13:21.336714 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:13:21.336714 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m22:13:21.337715 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-dd74-1872-beb6-e7b778f3e69b
[0m22:13:21.573282 [info ] [Thread-1 (]: 4 of 5 PASS unique_dim_tempo_data_id ........................................... [[32mPASS[0m in 1.96s]
[0m22:13:21.574641 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m22:13:21.575670 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m22:13:21.576569 [info ] [Thread-1 (]: 5 of 5 START test unique_fato_nascimento_nascimento_id ......................... [RUN]
[0m22:13:21.577265 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m22:13:21.578288 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m22:13:21.584277 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m22:13:21.585262 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 22:13:21.578288 => 22:13:21.585262
[0m22:13:21.586257 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m22:13:21.589264 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m22:13:21.590253 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:13:21.591238 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m22:13:21.591238 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m22:13:21.592239 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:13:22.385561 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ab-de9c-19b6-b9a1-0e84e1b5c7fe
[0m22:13:23.400743 [debug] [Thread-1 (]: SQL status: OK in 1.809999942779541 seconds
[0m22:13:23.403735 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 22:13:21.586257 => 22:13:23.403735
[0m22:13:23.404728 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m22:13:23.404728 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:13:23.405731 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m22:13:23.405731 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ab-de9c-19b6-b9a1-0e84e1b5c7fe
[0m22:13:23.650175 [error] [Thread-1 (]: 5 of 5 FAIL 299 unique_fato_nascimento_nascimento_id ........................... [[31mFAIL 299[0m in 2.07s]
[0m22:13:23.650878 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m22:13:23.653654 [debug] [MainThread]: On master: ROLLBACK
[0m22:13:23.653654 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:13:24.440225 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ab-dfd6-1fda-bf0e-5c888d508fee
[0m22:13:24.443186 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:13:24.445219 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:13:24.447179 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:13:24.447179 [debug] [MainThread]: On master: ROLLBACK
[0m22:13:24.448198 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:13:24.448198 [debug] [MainThread]: On master: Close
[0m22:13:24.449168 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ab-dfd6-1fda-bf0e-5c888d508fee
[0m22:13:24.677980 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:13:24.679974 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:13:24.681006 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m22:13:24.683960 [info ] [MainThread]: 
[0m22:13:24.685958 [info ] [MainThread]: Finished running 5 tests in 0 hours 0 minutes and 16.27 seconds (16.27s).
[0m22:13:24.691976 [debug] [MainThread]: Command end result
[0m22:13:24.715874 [info ] [MainThread]: 
[0m22:13:24.716873 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:13:24.717870 [info ] [MainThread]: 
[0m22:13:24.718913 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m22:13:24.719896 [error] [MainThread]:   Got 299 results, configured to fail if != 0
[0m22:13:24.720861 [info ] [MainThread]: 
[0m22:13:24.721824 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m22:13:24.722825 [info ] [MainThread]: 
[0m22:13:24.723822 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m22:13:24.724819 [debug] [MainThread]: Command `dbt test` failed at 22:13:24.724819 after 17.87 seconds
[0m22:13:24.725816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019ACCB9C9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019AD2EA7090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019AD2A35F90>]}
[0m22:13:24.725816 [debug] [MainThread]: Flushing usage events
[0m22:18:45.493095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF15911890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF15920190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF15B8AE10>]}


============================== 22:18:45.498136 | eb09dc08-eec6-4088-b469-30be2babb0db ==============================
[0m22:18:45.498136 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:18:45.499128 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:18:46.921819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'eb09dc08-eec6-4088-b469-30be2babb0db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF15C4CF90>]}
[0m22:18:46.941765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'eb09dc08-eec6-4088-b469-30be2babb0db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF20373090>]}
[0m22:18:46.942769 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:18:46.961712 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:18:47.070658 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 5 files changed.
[0m22:18:47.071628 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_tempo.sql
[0m22:18:47.071628 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\intermediate\int_nascimento.sql
[0m22:18:47.072653 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m22:18:47.073623 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m22:18:47.073623 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\dim_tempo.sql
[0m22:18:47.100562 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_tempo.sql
[0m22:18:47.115538 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m22:18:47.119527 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m22:18:47.123517 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m22:18:47.128475 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m22:18:47.252145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eb09dc08-eec6-4088-b469-30be2babb0db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF20411C10>]}
[0m22:18:47.263116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eb09dc08-eec6-4088-b469-30be2babb0db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF150E6090>]}
[0m22:18:47.263116 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:18:47.264355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb09dc08-eec6-4088-b469-30be2babb0db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF1C9A9B50>]}
[0m22:18:47.266380 [info ] [MainThread]: 
[0m22:18:47.268377 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:18:47.270342 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:18:47.271339 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:18:47.271339 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m22:18:47.272336 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:18:48.320949 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ac-a0e0-1927-b3c4-741c7c786d88
[0m22:18:48.831082 [debug] [ThreadPool]: SQL status: OK in 1.559999942779541 seconds
[0m22:18:48.834929 [debug] [ThreadPool]: On list_workspace: Close
[0m22:18:48.835928 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ac-a0e0-1927-b3c4-741c7c786d88
[0m22:18:49.065810 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m22:18:49.066803 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m22:18:49.076777 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:49.076777 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m22:18:49.077774 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m22:18:49.077774 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:18:49.869300 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ac-a1ce-1084-936d-d0d7ca21c662
[0m22:18:50.273564 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m22:18:50.274562 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:18:50.275559 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m22:18:50.275559 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:18:50.276557 [debug] [ThreadPool]: On create_workspace_default: Close
[0m22:18:50.276557 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ac-a1ce-1084-936d-d0d7ca21c662
[0m22:18:50.511322 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:18:50.516311 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:18:50.517309 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:18:50.517309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:18:51.308724 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ac-a2a8-193c-88a4-2fbc1c17219a
[0m22:18:51.650949 [debug] [ThreadPool]: SQL status: OK in 1.1299999952316284 seconds
[0m22:18:51.654938 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:18:51.654938 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ac-a2a8-193c-88a4-2fbc1c17219a
[0m22:18:51.901540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb09dc08-eec6-4088-b469-30be2babb0db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF1C9A9B50>]}
[0m22:18:51.901540 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:51.902545 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:18:51.903513 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:18:51.904512 [info ] [MainThread]: 
[0m22:18:51.911686 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m22:18:51.912692 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m22:18:51.914180 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m22:18:51.915177 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m22:18:51.919166 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m22:18:51.921161 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 22:18:51.915177 => 22:18:51.920163
[0m22:18:51.921161 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m22:18:51.958092 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m22:18:51.959060 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:51.960057 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m22:18:51.960057 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m22:18:51.961060 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:18:52.741724 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ac-a384-1d61-bcfe-5932f6b60e21
[0m22:18:53.445381 [debug] [Thread-1 (]: SQL status: OK in 1.4800000190734863 seconds
[0m22:18:53.460334 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 22:18:51.922159 => 22:18:53.459337
[0m22:18:53.460334 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m22:18:53.461332 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:18:53.461332 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m22:18:53.462329 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ac-a384-1d61-bcfe-5932f6b60e21
[0m22:18:53.695027 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eb09dc08-eec6-4088-b469-30be2babb0db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF203DE750>]}
[0m22:18:53.697022 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.78s]
[0m22:18:53.700531 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m22:18:53.701531 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m22:18:53.703523 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m22:18:53.706515 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m22:18:53.708480 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m22:18:53.719485 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m22:18:53.721445 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 22:18:53.709477 => 22:18:53.721445
[0m22:18:53.723437 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m22:18:53.731446 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m22:18:53.733411 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:53.733411 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m22:18:53.734409 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m22:18:53.735436 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:18:54.553960 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ac-a496-194b-a8ea-caa5f75cf98e
[0m22:18:55.203050 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m22:18:55.207037 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 22:18:53.724435 => 22:18:55.206045
[0m22:18:55.207037 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m22:18:55.208035 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:18:55.209033 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m22:18:55.209033 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ac-a496-194b-a8ea-caa5f75cf98e
[0m22:18:55.447458 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eb09dc08-eec6-4088-b469-30be2babb0db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF203DE750>]}
[0m22:18:55.448453 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.74s]
[0m22:18:55.450612 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m22:18:55.451578 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m22:18:55.452605 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m22:18:55.454203 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m22:18:55.455198 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m22:18:55.460185 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m22:18:55.462152 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 22:18:55.456196 => 22:18:55.462152
[0m22:18:55.463177 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m22:18:55.471126 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m22:18:55.473120 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:55.473120 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m22:18:55.474143 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m22:18:55.474143 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:18:56.271423 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ac-a59f-11a2-b999-2a91ff470f94
[0m22:18:57.021539 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m22:18:57.024531 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 22:18:55.464175 => 22:18:57.023533
[0m22:18:57.024531 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m22:18:57.025528 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:18:57.025528 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m22:18:57.026526 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ac-a59f-11a2-b999-2a91ff470f94
[0m22:18:57.321494 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eb09dc08-eec6-4088-b469-30be2babb0db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF2053F490>]}
[0m22:18:57.322495 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.87s]
[0m22:18:57.324456 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m22:18:57.325453 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:18:57.326451 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m22:18:57.328446 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m22:18:57.329443 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m22:18:57.335426 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:18:57.337420 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 22:18:57.330440 => 22:18:57.336423
[0m22:18:57.337420 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m22:18:57.342407 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:18:57.343404 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:57.344402 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:18:57.345419 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH base AS (
    SELECT
        *,
        ROW_NUMBER() OVER (ORDER BY DTNASC, SEXO, PESO) AS nascimento_id
    FROM `workspace`.`default`.`sinasc_2022_sc_clean`
    WHERE CODUFNATU = 42  -- SC (substitua se necessário)
)

SELECT
    CAST(nascimento_id AS STRING) AS nascimento_id,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,  -- converte BIGINT para DATE
    CODMUNNATU AS municipio,
    SEXO,
    PESO,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas
FROM base

[0m22:18:57.346410 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:18:58.264731 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ac-a6ce-17d8-80b7-a040013a445d
[0m22:18:59.002706 [debug] [Thread-1 (]: SQL status: OK in 1.659999966621399 seconds
[0m22:18:59.005695 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 22:18:57.338417 => 22:18:59.005695
[0m22:18:59.006693 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m22:18:59.006693 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:18:59.007690 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m22:18:59.008687 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ac-a6ce-17d8-80b7-a040013a445d
[0m22:18:59.251542 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eb09dc08-eec6-4088-b469-30be2babb0db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF2043CF90>]}
[0m22:18:59.253533 [info ] [Thread-1 (]: 4 of 5 OK created sql view model default.stg_nascidos_vivos .................... [[32mOK[0m in 1.92s]
[0m22:18:59.254482 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:18:59.255511 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m22:18:59.256507 [info ] [Thread-1 (]: 5 of 5 START sql view model default.stg_tempo .................................. [RUN]
[0m22:18:59.258503 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.stg_tempo)
[0m22:18:59.258503 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m22:18:59.263483 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m22:18:59.265455 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 22:18:59.259495 => 22:18:59.264460
[0m22:18:59.266453 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m22:18:59.271469 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m22:18:59.272437 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:59.273440 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m22:18:59.273440 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    DATE_FROM_UNIX_DATE(DTNASC) AS data,
    CAST(FORMAT(DATE_FROM_UNIX_DATE(DTNASC), 'yyyyMMdd') AS INT) AS data_id,
    YEAR(DATE_FROM_UNIX_DATE(DTNASC)) AS ano,
    MONTH(DATE_FROM_UNIX_DATE(DTNASC)) AS mes,
    DAY(DATE_FROM_UNIX_DATE(DTNASC)) AS dia,
    QUARTER(DATE_FROM_UNIX_DATE(DTNASC)) AS trimestre,
    DAYOFWEEK(DATE_FROM_UNIX_DATE(DTNASC)) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m22:18:59.274460 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:19:00.125588 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ac-a7eb-1151-a3cb-0ef3dc269c8c
[0m22:19:00.847625 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    DATE_FROM_UNIX_DATE(DTNASC) AS data,
    CAST(FORMAT(DATE_FROM_UNIX_DATE(DTNASC), 'yyyyMMdd') AS INT) AS data_id,
    YEAR(DATE_FROM_UNIX_DATE(DTNASC)) AS ano,
    MONTH(DATE_FROM_UNIX_DATE(DTNASC)) AS mes,
    DAY(DATE_FROM_UNIX_DATE(DTNASC)) AS dia,
    QUARTER(DATE_FROM_UNIX_DATE(DTNASC)) AS trimestre,
    DAYOFWEEK(DATE_FROM_UNIX_DATE(DTNASC)) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m22:19:00.848618 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_ROUTINE] Cannot resolve routine `FORMAT` on search path [`system`.`builtin`, `system`.`session`, `workspace`.`default`].
Verify the spelling of `FORMAT`, check that the routine exists, and confirm you have `USE` privilege on the catalog and schema, and EXECUTE on the routine. SQLSTATE: 42883; line 10 pos 9
[0m22:19:00.850612 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_ROUTINE] org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve routine `FORMAT` on search path [`system`.`builtin`, `system`.`session`, `workspace`.`default`].
Verify the spelling of `FORMAT`, check that the routine exists, and confirm you have `USE` privilege on the catalog and schema, and EXECUTE on the routine. SQLSTATE: 42883; line 10 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve routine `FORMAT` on search path [`system`.`builtin`, `system`.`session`, `workspace`.`default`].
Verify the spelling of `FORMAT`, check that the routine exists, and confirm you have `USE` privilege on the catalog and schema, and EXECUTE on the routine. SQLSTATE: 42883; line 10 pos 9
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedRoutineError(QueryCompilationErrors.scala:1239)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2705)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2682)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:189)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:248)
	at scala.collection.immutable.List.map(List.scala:251)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:248)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:189)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:160)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:308)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:307)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.plans.logical.Distinct.mapChildren(basicLogicalOperators.scala:2479)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1360)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1357)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1496)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:307)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:304)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:279)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:277)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2682)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2676)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)
	at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)
	at scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:672)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:698)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:845)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:845)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedRoutineError(QueryCompilationErrors.scala:1239)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2705)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$19.applyOrElse(Analyzer.scala:2682)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:189)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:248)
		at scala.collection.immutable.List.map(List.scala:251)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:248)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:189)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:160)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:308)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:307)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.plans.logical.Distinct.mapChildren(basicLogicalOperators.scala:2479)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1360)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1357)
		at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1496)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:307)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:304)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:279)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:277)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2682)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2676)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)
		at com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)
		at scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:500)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:493)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:493)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:427)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:480)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:480)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:698)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
		at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
		at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
		at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
		at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:698)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1339)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:691)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:688)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:688)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 60 more

[0m22:19:00.853604 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078ac-a80c-10a9-9097-17082203a829
[0m22:19:00.853604 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 22:18:59.266453 => 22:19:00.853604
[0m22:19:00.854602 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m22:19:00.855599 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:19:00.855599 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m22:19:00.856596 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ac-a7eb-1151-a3cb-0ef3dc269c8c
[0m22:19:01.126996 [debug] [Thread-1 (]: Runtime Error in model stg_tempo (models\staging\stg_tempo.sql)
  [UNRESOLVED_ROUTINE] Cannot resolve routine `FORMAT` on search path [`system`.`builtin`, `system`.`session`, `workspace`.`default`].
  Verify the spelling of `FORMAT`, check that the routine exists, and confirm you have `USE` privilege on the catalog and schema, and EXECUTE on the routine. SQLSTATE: 42883; line 10 pos 9
[0m22:19:01.127993 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eb09dc08-eec6-4088-b469-30be2babb0db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF2040AD90>]}
[0m22:19:01.128991 [error] [Thread-1 (]: 5 of 5 ERROR creating sql view model default.stg_tempo ......................... [[31mERROR[0m in 1.87s]
[0m22:19:01.129695 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m22:19:01.131693 [debug] [MainThread]: On master: ROLLBACK
[0m22:19:01.132689 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:19:02.082087 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ac-a913-1b5e-b02e-7cf9b2e6ccf1
[0m22:19:02.085048 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:19:02.087049 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:19:02.089041 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:19:02.089041 [debug] [MainThread]: On master: ROLLBACK
[0m22:19:02.090061 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:19:02.091030 [debug] [MainThread]: On master: Close
[0m22:19:02.091030 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ac-a913-1b5e-b02e-7cf9b2e6ccf1
[0m22:19:02.329819 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:19:02.331813 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m22:19:02.332808 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:19:02.332808 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_tempo' was properly closed.
[0m22:19:02.334803 [info ] [MainThread]: 
[0m22:19:02.335801 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 15.07 seconds (15.07s).
[0m22:19:02.336797 [debug] [MainThread]: Command end result
[0m22:19:02.352756 [info ] [MainThread]: 
[0m22:19:02.354188 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:19:02.355187 [info ] [MainThread]: 
[0m22:19:02.356185 [error] [MainThread]: [33mRuntime Error in model stg_tempo (models\staging\stg_tempo.sql)[0m
[0m22:19:02.357182 [error] [MainThread]:   [UNRESOLVED_ROUTINE] Cannot resolve routine `FORMAT` on search path [`system`.`builtin`, `system`.`session`, `workspace`.`default`].
[0m22:19:02.358180 [error] [MainThread]:   Verify the spelling of `FORMAT`, check that the routine exists, and confirm you have `USE` privilege on the catalog and schema, and EXECUTE on the routine. SQLSTATE: 42883; line 10 pos 9
[0m22:19:02.359177 [info ] [MainThread]: 
[0m22:19:02.360174 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m22:19:02.361172 [debug] [MainThread]: Command `dbt run` failed at 22:19:02.361172 after 16.89 seconds
[0m22:19:02.362169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF161F7F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF158FF750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF15BDFCD0>]}
[0m22:19:02.362169 [debug] [MainThread]: Flushing usage events
[0m22:21:36.903266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CDD6CE650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CDD167090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CDD051D50>]}


============================== 22:21:36.907255 | a02ab4d4-b31a-42d7-ac04-a0b5b9929ccc ==============================
[0m22:21:36.907255 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:21:36.908420 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:21:38.175035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a02ab4d4-b31a-42d7-ac04-a0b5b9929ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CDCE4C610>]}
[0m22:21:38.192987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a02ab4d4-b31a-42d7-ac04-a0b5b9929ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CDCE4C610>]}
[0m22:21:38.193986 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:21:38.209914 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:21:38.319620 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m22:21:38.319620 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m22:21:38.320618 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m22:21:38.321615 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_tempo.sql
[0m22:21:38.321615 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\intermediate\int_nascimento.sql
[0m22:21:38.348369 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m22:21:38.363360 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m22:21:38.368327 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_tempo.sql
[0m22:21:38.372468 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m22:21:38.484199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a02ab4d4-b31a-42d7-ac04-a0b5b9929ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE7A062D0>]}
[0m22:21:38.495170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a02ab4d4-b31a-42d7-ac04-a0b5b9929ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE78D8CD0>]}
[0m22:21:38.496144 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:21:38.497150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a02ab4d4-b31a-42d7-ac04-a0b5b9929ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CDCFCDFD0>]}
[0m22:21:38.499160 [info ] [MainThread]: 
[0m22:21:38.500130 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:21:38.502124 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:21:38.503149 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:21:38.504118 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m22:21:38.504118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:21:39.365610 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ad-06d3-18b4-a278-508c2abf13c2
[0m22:21:39.673271 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m22:21:39.678256 [debug] [ThreadPool]: On list_workspace: Close
[0m22:21:39.678256 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ad-06d3-18b4-a278-508c2abf13c2
[0m22:21:39.924974 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m22:21:39.928966 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m22:21:39.937936 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:21:39.938934 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m22:21:39.939931 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m22:21:39.939931 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:21:40.742014 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ad-07a6-19e1-8a09-04e4dc4134aa
[0m22:21:41.145556 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m22:21:41.146553 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:21:41.147551 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m22:21:41.148548 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:21:41.148548 [debug] [ThreadPool]: On create_workspace_default: Close
[0m22:21:41.149545 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ad-07a6-19e1-8a09-04e4dc4134aa
[0m22:21:41.391202 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:21:41.396191 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:21:41.397188 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:21:41.397188 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:21:42.193673 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ad-0884-1b9e-8f30-3c272d33d068
[0m22:21:42.583343 [debug] [ThreadPool]: SQL status: OK in 1.190000057220459 seconds
[0m22:21:42.587307 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:21:42.587307 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ad-0884-1b9e-8f30-3c272d33d068
[0m22:21:42.863068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a02ab4d4-b31a-42d7-ac04-a0b5b9929ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE79EEF10>]}
[0m22:21:42.864065 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:21:42.864065 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:21:42.865034 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:21:42.866031 [info ] [MainThread]: 
[0m22:21:42.873185 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m22:21:42.874214 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m22:21:42.875183 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m22:21:42.876180 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m22:21:42.880198 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m22:21:42.881193 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 22:21:42.877178 => 22:21:42.881193
[0m22:21:42.882163 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m22:21:42.916103 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m22:21:42.918067 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:21:42.919065 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m22:21:42.920062 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m22:21:42.921060 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:21:43.727848 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-096f-1563-bc15-666146cac26a
[0m22:21:44.481198 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m22:21:44.496186 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 22:21:42.883164 => 22:21:44.496186
[0m22:21:44.497181 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m22:21:44.498151 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:21:44.498151 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m22:21:44.499148 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-096f-1563-bc15-666146cac26a
[0m22:21:44.740555 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a02ab4d4-b31a-42d7-ac04-a0b5b9929ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE7964C50>]}
[0m22:21:44.742548 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.87s]
[0m22:21:44.742995 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m22:21:44.744023 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m22:21:44.744997 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m22:21:44.745990 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m22:21:44.747017 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m22:21:44.752003 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m22:21:44.753968 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 22:21:44.747017 => 22:21:44.752999
[0m22:21:44.753968 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m22:21:44.757988 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m22:21:44.759961 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:21:44.759961 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m22:21:44.760952 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m22:21:44.761947 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:21:45.608151 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-0a8c-1700-b9eb-7e00cb97e604
[0m22:21:46.302467 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m22:21:46.305459 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 22:21:44.754993 => 22:21:46.305459
[0m22:21:46.305459 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m22:21:46.306457 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:21:46.306457 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m22:21:46.307454 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-0a8c-1700-b9eb-7e00cb97e604
[0m22:21:46.548520 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a02ab4d4-b31a-42d7-ac04-a0b5b9929ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE794D310>]}
[0m22:21:46.550511 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.80s]
[0m22:21:46.551535 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m22:21:46.552566 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m22:21:46.553532 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m22:21:46.554529 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m22:21:46.555543 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m22:21:46.558546 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m22:21:46.559567 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 22:21:46.555543 => 22:21:46.559567
[0m22:21:46.560520 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m22:21:46.565502 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m22:21:46.567494 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:21:46.567494 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m22:21:46.568520 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m22:21:46.568520 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:21:47.367719 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-0b99-1d91-a40d-e7810b900625
[0m22:21:48.035846 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m22:21:48.037841 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 22:21:46.560520 => 22:21:48.037841
[0m22:21:48.038838 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m22:21:48.038838 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:21:48.039836 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m22:21:48.040833 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-0b99-1d91-a40d-e7810b900625
[0m22:21:48.281874 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a02ab4d4-b31a-42d7-ac04-a0b5b9929ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE790A250>]}
[0m22:21:48.284868 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.73s]
[0m22:21:48.285806 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m22:21:48.286834 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:21:48.287803 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m22:21:48.288801 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m22:21:48.289805 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m22:21:48.293818 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:21:48.294785 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 22:21:48.289805 => 22:21:48.294785
[0m22:21:48.295809 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m22:21:48.299801 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:21:48.300769 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:21:48.301766 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:21:48.301766 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(CODNASC AS STRING) AS nascimento_id,
    CAST(DATE_FROM_UNIX_DATE(DTNASC) AS DATE) AS data_nascimento,
    CODMUNNASC AS cod_municipio,
    CAST(SEXO AS STRING) AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- SC

[0m22:21:48.302791 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:21:49.098035 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-0ca2-1358-866b-81fa8a8cdbef
[0m22:21:49.427043 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(CODNASC AS STRING) AS nascimento_id,
    CAST(DATE_FROM_UNIX_DATE(DTNASC) AS DATE) AS data_nascimento,
    CODMUNNASC AS cod_municipio,
    CAST(SEXO AS STRING) AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- SC

[0m22:21:49.428035 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CODNASC` cannot be resolved. Did you mean one of the following? [`DTNASC`, `LOCNASC`, `CODMUNNASC`, `HORANASC`, `CODANOMAL`]. SQLSTATE: 42703; line 9 pos 9
[0m22:21:49.429032 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CODNASC` cannot be resolved. Did you mean one of the following? [`DTNASC`, `LOCNASC`, `CODMUNNASC`, `HORANASC`, `CODANOMAL`]. SQLSTATE: 42703; line 9 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CODNASC` cannot be resolved. Did you mean one of the following? [`DTNASC`, `LOCNASC`, `CODMUNNASC`, `HORANASC`, `CODANOMAL`]. SQLSTATE: 42703; line 9 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m22:21:49.430030 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078ad-0cc2-15cc-9fe0-bada8bc0cce0
[0m22:21:49.430030 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 22:21:48.295809 => 22:21:49.430030
[0m22:21:49.431027 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m22:21:49.431027 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:21:49.432024 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m22:21:49.433022 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-0ca2-1358-866b-81fa8a8cdbef
[0m22:21:49.677195 [debug] [Thread-1 (]: Runtime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CODNASC` cannot be resolved. Did you mean one of the following? [`DTNASC`, `LOCNASC`, `CODMUNNASC`, `HORANASC`, `CODANOMAL`]. SQLSTATE: 42703; line 9 pos 9
[0m22:21:49.678193 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a02ab4d4-b31a-42d7-ac04-a0b5b9929ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE7965B10>]}
[0m22:21:49.679191 [error] [Thread-1 (]: 4 of 5 ERROR creating sql view model default.stg_nascidos_vivos ................ [[31mERROR[0m in 1.39s]
[0m22:21:49.680645 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:21:49.681672 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m22:21:49.682669 [info ] [Thread-1 (]: 5 of 5 SKIP relation default.stg_tempo ......................................... [[33mSKIP[0m]
[0m22:21:49.684638 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m22:21:49.686660 [debug] [MainThread]: On master: ROLLBACK
[0m22:21:49.686660 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:21:50.585593 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ad-0d84-1cb4-8e14-3ed6690975de
[0m22:21:50.587770 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:21:50.588797 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:21:50.589794 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:21:50.590792 [debug] [MainThread]: On master: ROLLBACK
[0m22:21:50.590792 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:21:50.591790 [debug] [MainThread]: On master: Close
[0m22:21:50.592786 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ad-0d84-1cb4-8e14-3ed6690975de
[0m22:21:50.845778 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:21:50.847741 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m22:21:50.848736 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:21:50.848736 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_nascidos_vivos' was properly closed.
[0m22:21:50.849744 [info ] [MainThread]: 
[0m22:21:50.850761 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 12.35 seconds (12.35s).
[0m22:21:50.852754 [debug] [MainThread]: Command end result
[0m22:21:50.863735 [info ] [MainThread]: 
[0m22:21:50.864701 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:21:50.865701 [info ] [MainThread]: 
[0m22:21:50.866689 [error] [MainThread]: [33mRuntime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m22:21:50.867708 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `CODNASC` cannot be resolved. Did you mean one of the following? [`DTNASC`, `LOCNASC`, `CODMUNNASC`, `HORANASC`, `CODANOMAL`]. SQLSTATE: 42703; line 9 pos 9
[0m22:21:50.869682 [info ] [MainThread]: 
[0m22:21:50.870763 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m22:21:50.872762 [debug] [MainThread]: Command `dbt run` failed at 22:21:50.872762 after 13.99 seconds
[0m22:21:50.872762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CDCDDDD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CDD6B3590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CDD6B3FD0>]}
[0m22:21:50.873776 [debug] [MainThread]: Flushing usage events
[0m22:25:36.327181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F2B711890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F2919AA90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F2B9B7110>]}


============================== 22:25:36.331165 | 0bc4d3c8-e212-4913-8a72-6e3038e60858 ==============================
[0m22:25:36.331165 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:25:36.333138 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:25:37.589072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0bc4d3c8-e212-4913-8a72-6e3038e60858', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F2C0104D0>]}
[0m22:25:37.609990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0bc4d3c8-e212-4913-8a72-6e3038e60858', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F351BD950>]}
[0m22:25:37.611015 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:25:37.631957 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:25:37.738655 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:25:37.739650 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m22:25:37.769570 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m22:25:37.850356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0bc4d3c8-e212-4913-8a72-6e3038e60858', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F36347E50>]}
[0m22:25:37.863322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0bc4d3c8-e212-4913-8a72-6e3038e60858', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F3520F310>]}
[0m22:25:37.864292 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:25:37.865286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0bc4d3c8-e212-4913-8a72-6e3038e60858', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F36224F50>]}
[0m22:25:37.867310 [info ] [MainThread]: 
[0m22:25:37.868279 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:25:37.870879 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:25:37.870879 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:25:37.871875 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m22:25:37.871875 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:25:38.761991 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ad-9583-18f6-afb9-7efdc51db2ed
[0m22:25:39.416525 [debug] [ThreadPool]: SQL status: OK in 1.5399999618530273 seconds
[0m22:25:39.420565 [debug] [ThreadPool]: On list_workspace: Close
[0m22:25:39.421561 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ad-9583-18f6-afb9-7efdc51db2ed
[0m22:25:39.651035 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m22:25:39.653062 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m22:25:39.669017 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:39.670015 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m22:25:39.670015 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m22:25:39.671010 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:40.455820 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ad-9687-1ccf-b801-3c13d61c5d6a
[0m22:25:40.842943 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m22:25:40.843935 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:25:40.844906 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m22:25:40.844906 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:25:40.845931 [debug] [ThreadPool]: On create_workspace_default: Close
[0m22:25:40.845931 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ad-9687-1ccf-b801-3c13d61c5d6a
[0m22:25:41.071711 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:25:41.085697 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:25:41.086695 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:25:41.086695 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:25:41.876937 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ad-9761-198b-9564-8b6b2750e79d
[0m22:25:42.258436 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m22:25:42.262426 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:25:42.263423 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ad-9761-198b-9564-8b6b2750e79d
[0m22:25:42.499377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0bc4d3c8-e212-4913-8a72-6e3038e60858', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F2B283F50>]}
[0m22:25:42.500346 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:42.500346 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:25:42.501372 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:25:42.502341 [info ] [MainThread]: 
[0m22:25:42.508532 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m22:25:42.510557 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m22:25:42.511527 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m22:25:42.512525 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m22:25:42.516513 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m22:25:42.517509 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 22:25:42.513522 => 22:25:42.517509
[0m22:25:42.518508 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m22:25:42.552447 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m22:25:42.554412 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:42.555435 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m22:25:42.556407 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m22:25:42.557404 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:25:43.390788 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-9848-10fb-9e24-2132bace41d7
[0m22:25:44.180660 [debug] [Thread-1 (]: SQL status: OK in 1.6200000047683716 seconds
[0m22:25:44.195619 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 22:25:42.519505 => 22:25:44.195619
[0m22:25:44.196625 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m22:25:44.196625 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:25:44.197613 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m22:25:44.197613 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-9848-10fb-9e24-2132bace41d7
[0m22:25:44.427123 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bc4d3c8-e212-4913-8a72-6e3038e60858', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F36318310>]}
[0m22:25:44.428130 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.91s]
[0m22:25:44.429528 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m22:25:44.430556 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m22:25:44.431553 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m22:25:44.432522 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m22:25:44.432522 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m22:25:44.436540 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m22:25:44.438507 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 22:25:44.433520 => 22:25:44.437509
[0m22:25:44.438507 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m22:25:44.444490 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m22:25:44.445493 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:44.446484 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m22:25:44.446484 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m22:25:44.447482 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:25:45.267551 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-9966-1d07-ac7f-e5c8cd981612
[0m22:25:46.008520 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m22:25:46.011513 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 22:25:44.439532 => 22:25:46.010515
[0m22:25:46.011513 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m22:25:46.012523 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:25:46.012523 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m22:25:46.013507 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-9966-1d07-ac7f-e5c8cd981612
[0m22:25:46.249759 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bc4d3c8-e212-4913-8a72-6e3038e60858', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F3521A2D0>]}
[0m22:25:46.252744 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.82s]
[0m22:25:46.253231 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m22:25:46.254260 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m22:25:46.255229 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m22:25:46.256234 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m22:25:46.257236 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m22:25:46.261243 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m22:25:46.262250 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 22:25:46.257236 => 22:25:46.262250
[0m22:25:46.263208 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m22:25:46.269221 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m22:25:46.271185 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:46.272184 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m22:25:46.272184 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m22:25:46.273181 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:25:47.042494 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-9a76-136f-825f-931fca892f83
[0m22:25:47.810952 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m22:25:47.813936 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 22:25:46.263208 => 22:25:47.812939
[0m22:25:47.813936 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m22:25:47.814933 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:25:47.814933 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m22:25:47.815931 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-9a76-136f-825f-931fca892f83
[0m22:25:48.054680 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bc4d3c8-e212-4913-8a72-6e3038e60858', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F3504FED0>]}
[0m22:25:48.057669 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.80s]
[0m22:25:48.058183 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m22:25:48.059214 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:25:48.060194 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m22:25:48.061193 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m22:25:48.062203 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m22:25:48.066192 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:25:48.067161 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 22:25:48.062203 => 22:25:48.067161
[0m22:25:48.068187 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m22:25:48.072176 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:25:48.073146 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:48.074142 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:25:48.074142 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CONCAT(CAST(CODMUNNASC AS STRING), '_', CAST(DTNASC AS STRING), '_', CAST(ROW_NUMBER() OVER (PARTITION BY CODMUNNASC, DTNASC ORDER BY HORANASC) AS STRING)) AS nascimento_id,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    CODMUNNASC AS cod_municipio,
    CAST(SEXO AS STRING) AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- SC

[0m22:25:48.075140 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:25:48.980846 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-9b9b-1ad2-9555-dcdf9d3223e1
[0m22:25:49.637308 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m22:25:49.639302 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 22:25:48.068187 => 22:25:49.639302
[0m22:25:49.640300 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m22:25:49.640300 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:25:49.641297 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m22:25:49.642267 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-9b9b-1ad2-9555-dcdf9d3223e1
[0m22:25:49.879825 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bc4d3c8-e212-4913-8a72-6e3038e60858', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F36244810>]}
[0m22:25:49.882814 [info ] [Thread-1 (]: 4 of 5 OK created sql view model default.stg_nascidos_vivos .................... [[32mOK[0m in 1.82s]
[0m22:25:49.883814 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:25:49.884814 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m22:25:49.885805 [info ] [Thread-1 (]: 5 of 5 START sql view model default.stg_tempo .................................. [RUN]
[0m22:25:49.886471 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.stg_tempo)
[0m22:25:49.887499 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m22:25:49.891490 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m22:25:49.893454 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 22:25:49.887499 => 22:25:49.892485
[0m22:25:49.893454 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m22:25:49.898469 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m22:25:49.899439 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:49.900437 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m22:25:49.900437 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

WITH base AS (
    SELECT DISTINCT
        DTNASC AS data_bigint,
        CAST(DATE_FROM_UNIX_DATE(DTNASC) AS DATE) AS data
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    data,
    CAST(FORMAT_STRING('%d%02d%02d', YEAR(data), MONTH(data), DAY(data)) AS INT) AS data_id,
    YEAR(data) AS ano,
    MONTH(data) AS mes,
    DAY(data) AS dia,
    QUARTER(data) AS trimestre,
    DAYOFWEEK(data) AS dia_semana
FROM base

[0m22:25:49.901435 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:25:50.719367 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-9ca5-119c-a78b-20c2114dc9ef
[0m22:25:51.126129 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

WITH base AS (
    SELECT DISTINCT
        DTNASC AS data_bigint,
        CAST(DATE_FROM_UNIX_DATE(DTNASC) AS DATE) AS data
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    data,
    CAST(FORMAT_STRING('%d%02d%02d', YEAR(data), MONTH(data), DAY(data)) AS INT) AS data_id,
    YEAR(data) AS ano,
    MONTH(data) AS mes,
    DAY(data) AS dia,
    QUARTER(data) AS trimestre,
    DAYOFWEEK(data) AS dia_semana
FROM base

[0m22:25:51.127122 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `idade_mae`, `cod_municipio`, `nascimento_id`]. SQLSTATE: 42703; line 10 pos 8
[0m22:25:51.127122 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `idade_mae`, `cod_municipio`, `nascimento_id`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `idade_mae`, `cod_municipio`, `nascimento_id`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m22:25:51.128120 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078ad-9cc6-1e9b-8426-90c1ef5fd2ff
[0m22:25:51.129117 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 22:25:49.894451 => 22:25:51.129117
[0m22:25:51.130114 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m22:25:51.130114 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:25:51.131112 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m22:25:51.131112 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-9ca5-119c-a78b-20c2114dc9ef
[0m22:25:51.367462 [debug] [Thread-1 (]: Runtime Error in model stg_tempo (models\staging\stg_tempo.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `idade_mae`, `cod_municipio`, `nascimento_id`]. SQLSTATE: 42703; line 10 pos 8
[0m22:25:51.368459 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0bc4d3c8-e212-4913-8a72-6e3038e60858', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F36459E90>]}
[0m22:25:51.369457 [error] [Thread-1 (]: 5 of 5 ERROR creating sql view model default.stg_tempo ......................... [[31mERROR[0m in 1.48s]
[0m22:25:51.370200 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m22:25:51.372207 [debug] [MainThread]: On master: ROLLBACK
[0m22:25:51.373194 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:25:52.191228 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ad-9d86-1edc-add3-8723d9527d82
[0m22:25:52.193214 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:25:52.193214 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:52.194218 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:25:52.194218 [debug] [MainThread]: On master: ROLLBACK
[0m22:25:52.195239 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:25:52.196207 [debug] [MainThread]: On master: Close
[0m22:25:52.196207 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ad-9d86-1edc-add3-8723d9527d82
[0m22:25:52.424284 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:25:52.426251 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m22:25:52.426251 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:25:52.427271 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_tempo' was properly closed.
[0m22:25:52.428267 [info ] [MainThread]: 
[0m22:25:52.429237 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 14.56 seconds (14.56s).
[0m22:25:52.431259 [debug] [MainThread]: Command end result
[0m22:25:52.443200 [info ] [MainThread]: 
[0m22:25:52.444545 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:25:52.445545 [info ] [MainThread]: 
[0m22:25:52.446542 [error] [MainThread]: [33mRuntime Error in model stg_tempo (models\staging\stg_tempo.sql)[0m
[0m22:25:52.447539 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `idade_mae`, `cod_municipio`, `nascimento_id`]. SQLSTATE: 42703; line 10 pos 8
[0m22:25:52.449534 [info ] [MainThread]: 
[0m22:25:52.450532 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m22:25:52.451529 [debug] [MainThread]: Command `dbt run` failed at 22:25:52.451529 after 16.14 seconds
[0m22:25:52.452555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F2BBE8AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F255C8A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F2B98A610>]}
[0m22:25:52.452555 [debug] [MainThread]: Flushing usage events
[0m22:28:02.292874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D451B6050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D45585F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D454932D0>]}


============================== 22:28:02.297789 | 8b8bb8d7-a846-456e-af23-7cc0f161e7f6 ==============================
[0m22:28:02.297789 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:28:02.298759 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:28:03.549437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8b8bb8d7-a846-456e-af23-7cc0f161e7f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D4552C910>]}
[0m22:28:03.567385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8b8bb8d7-a846-456e-af23-7cc0f161e7f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D4552C910>]}
[0m22:28:03.567385 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:28:03.583344 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:28:03.699034 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:28:03.700031 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m22:28:03.727952 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m22:28:03.808741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8b8bb8d7-a846-456e-af23-7cc0f161e7f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D4FDFA790>]}
[0m22:28:03.820709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8b8bb8d7-a846-456e-af23-7cc0f161e7f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D4ECEC190>]}
[0m22:28:03.821706 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:28:03.822675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b8bb8d7-a846-456e-af23-7cc0f161e7f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D4C25E150>]}
[0m22:28:03.824698 [info ] [MainThread]: 
[0m22:28:03.825695 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:28:03.827662 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:28:03.828673 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:28:03.828673 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m22:28:03.829657 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:28:04.679568 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ad-ec7c-1cf9-bd48-0c20b87c129d
[0m22:28:05.017623 [debug] [ThreadPool]: SQL status: OK in 1.190000057220459 seconds
[0m22:28:05.025654 [debug] [ThreadPool]: On list_workspace: Close
[0m22:28:05.026652 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ad-ec7c-1cf9-bd48-0c20b87c129d
[0m22:28:05.280914 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m22:28:05.284902 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m22:28:05.294872 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:28:05.294872 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m22:28:05.295870 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m22:28:05.295870 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:28:06.089403 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ad-ed54-183c-8b32-63b4e2c0dd23
[0m22:28:06.488772 [debug] [ThreadPool]: SQL status: OK in 1.190000057220459 seconds
[0m22:28:06.492762 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:28:06.493759 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m22:28:06.495252 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:28:06.496254 [debug] [ThreadPool]: On create_workspace_default: Close
[0m22:28:06.498250 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ad-ed54-183c-8b32-63b4e2c0dd23
[0m22:28:06.736627 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:28:06.744602 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:28:06.745599 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:28:06.745599 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:28:07.565711 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ad-ee35-1194-bfbc-9f74c68b0e4f
[0m22:28:07.887196 [debug] [ThreadPool]: SQL status: OK in 1.1399999856948853 seconds
[0m22:28:07.891185 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:28:07.892173 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ad-ee35-1194-bfbc-9f74c68b0e4f
[0m22:28:08.133834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b8bb8d7-a846-456e-af23-7cc0f161e7f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D4ECCA610>]}
[0m22:28:08.135795 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:28:08.137820 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:28:08.140812 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:28:08.142772 [info ] [MainThread]: 
[0m22:28:08.157010 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m22:28:08.158005 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m22:28:08.162030 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m22:28:08.162993 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m22:28:08.171002 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m22:28:08.172997 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 22:28:08.164990 => 22:28:08.172997
[0m22:28:08.174960 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m22:28:08.212886 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m22:28:08.213886 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:28:08.213886 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m22:28:08.214880 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m22:28:08.214880 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:28:08.997347 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-ef11-18c4-8c46-24ee1c19d2a6
[0m22:28:09.759674 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m22:28:09.773632 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 22:28:08.175957 => 22:28:09.773632
[0m22:28:09.774629 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m22:28:09.774629 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:28:09.775627 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m22:28:09.776624 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-ef11-18c4-8c46-24ee1c19d2a6
[0m22:28:09.991741 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b8bb8d7-a846-456e-af23-7cc0f161e7f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D4ECA7F50>]}
[0m22:28:09.993734 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.83s]
[0m22:28:09.995292 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m22:28:09.996260 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m22:28:09.997257 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m22:28:09.999285 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m22:28:10.000279 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m22:28:10.006232 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m22:28:10.008226 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 22:28:10.001245 => 22:28:10.008226
[0m22:28:10.009223 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m22:28:10.015207 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m22:28:10.016205 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:28:10.016205 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m22:28:10.017202 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m22:28:10.017202 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:28:10.888172 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-f032-1916-ab44-39908de728c3
[0m22:28:11.613985 [debug] [Thread-1 (]: SQL status: OK in 1.600000023841858 seconds
[0m22:28:11.617969 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 22:28:10.010222 => 22:28:11.617969
[0m22:28:11.618966 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m22:28:11.618966 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:28:11.619963 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m22:28:11.619963 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-f032-1916-ab44-39908de728c3
[0m22:28:11.845754 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b8bb8d7-a846-456e-af23-7cc0f161e7f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D4EC9D4D0>]}
[0m22:28:11.848713 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.85s]
[0m22:28:11.853732 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m22:28:11.855726 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m22:28:11.856748 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m22:28:11.858483 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m22:28:11.859493 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m22:28:11.864498 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m22:28:11.865492 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 22:28:11.859493 => 22:28:11.865492
[0m22:28:11.866460 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m22:28:11.872475 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m22:28:11.873442 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:28:11.874440 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m22:28:11.874440 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m22:28:11.875437 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:28:12.683289 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-f144-15b9-9e6a-7b38ed1cb08c
[0m22:28:13.481511 [debug] [Thread-1 (]: SQL status: OK in 1.6100000143051147 seconds
[0m22:28:13.484504 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 22:28:11.866460 => 22:28:13.484504
[0m22:28:13.484504 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m22:28:13.485501 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:28:13.485501 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m22:28:13.486498 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-f144-15b9-9e6a-7b38ed1cb08c
[0m22:28:13.714971 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b8bb8d7-a846-456e-af23-7cc0f161e7f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D4FF33DD0>]}
[0m22:28:13.717964 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.86s]
[0m22:28:13.722987 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m22:28:13.724975 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:28:13.726971 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m22:28:13.730931 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m22:28:13.733959 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m22:28:13.741925 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:28:13.742894 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 22:28:13.734955 => 22:28:13.742894
[0m22:28:13.743892 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m22:28:13.748879 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:28:13.749876 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:28:13.750902 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:28:13.750902 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(id AS STRING) AS nascimento_id,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,  -- converter BIGINT para DATE
    LOCNASC AS cod_municipio,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- SC, caso queira filtrar pelo UF de nascimento

[0m22:28:13.751900 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:28:14.547729 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ad-f260-12bb-a21f-77a2f8a5cf85
[0m22:28:14.898246 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(id AS STRING) AS nascimento_id,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,  -- converter BIGINT para DATE
    LOCNASC AS cod_municipio,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas
FROM `workspace`.`default`.`sinasc_2022_sc_clean`
WHERE CODUFNATU = 42  -- SC, caso queira filtrar pelo UF de nascimento

[0m22:28:14.899238 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 9 pos 9
[0m22:28:14.899238 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 9 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 9 pos 9
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m22:28:14.900236 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078ad-f284-1ef7-bb6f-ba534d9038d8
[0m22:28:14.901233 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 22:28:13.743892 => 22:28:14.901233
[0m22:28:14.902232 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m22:28:14.902232 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:28:14.903227 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m22:28:14.903227 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ad-f260-12bb-a21f-77a2f8a5cf85
[0m22:28:15.151696 [debug] [Thread-1 (]: Runtime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 9 pos 9
[0m22:28:15.151696 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b8bb8d7-a846-456e-af23-7cc0f161e7f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D4FD4F8D0>]}
[0m22:28:15.152694 [error] [Thread-1 (]: 4 of 5 ERROR creating sql view model default.stg_nascidos_vivos ................ [[31mERROR[0m in 1.42s]
[0m22:28:15.153816 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:28:15.154840 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m22:28:15.155838 [info ] [Thread-1 (]: 5 of 5 SKIP relation default.stg_tempo ......................................... [[33mSKIP[0m]
[0m22:28:15.156816 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m22:28:15.158805 [debug] [MainThread]: On master: ROLLBACK
[0m22:28:15.159812 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:28:15.987353 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ad-f33a-1f0e-9c66-4492248ebfac
[0m22:28:15.988773 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:28:15.989801 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:28:15.989801 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:28:15.990797 [debug] [MainThread]: On master: ROLLBACK
[0m22:28:15.990797 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:28:15.991795 [debug] [MainThread]: On master: Close
[0m22:28:15.991795 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ad-f33a-1f0e-9c66-4492248ebfac
[0m22:28:16.223056 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:28:16.225032 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m22:28:16.225032 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:28:16.226042 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_nascidos_vivos' was properly closed.
[0m22:28:16.227041 [info ] [MainThread]: 
[0m22:28:16.228019 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 12.40 seconds (12.40s).
[0m22:28:16.230032 [debug] [MainThread]: Command end result
[0m22:28:16.239999 [info ] [MainThread]: 
[0m22:28:16.240975 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:28:16.241971 [info ] [MainThread]: 
[0m22:28:16.243967 [error] [MainThread]: [33mRuntime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m22:28:16.244965 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 9 pos 9
[0m22:28:16.245965 [info ] [MainThread]: 
[0m22:28:16.246958 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m22:28:16.248954 [debug] [MainThread]: Command `dbt run` failed at 22:28:16.248954 after 13.97 seconds
[0m22:28:16.248954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D451F6F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D45262AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019D4527E2D0>]}
[0m22:28:16.249950 [debug] [MainThread]: Flushing usage events
[0m22:29:28.276283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931C7B32D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019319F8ACD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931C87DBD0>]}


============================== 22:29:28.280275 | 525e824a-392c-4aa7-b7de-6a5eb7c0226c ==============================
[0m22:29:28.280275 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:29:28.281251 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:29:29.545284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '525e824a-392c-4aa7-b7de-6a5eb7c0226c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931C7B2BD0>]}
[0m22:29:29.563236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '525e824a-392c-4aa7-b7de-6a5eb7c0226c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931C7B2BD0>]}
[0m22:29:29.564204 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:29:29.583181 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:29:29.696848 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:29:29.697845 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m22:29:29.725793 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m22:29:29.808550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '525e824a-392c-4aa7-b7de-6a5eb7c0226c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193270B3BD0>]}
[0m22:29:29.827499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '525e824a-392c-4aa7-b7de-6a5eb7c0226c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931A26CE50>]}
[0m22:29:29.828497 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:29:29.830491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '525e824a-392c-4aa7-b7de-6a5eb7c0226c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019323351990>]}
[0m22:29:29.833505 [info ] [MainThread]: 
[0m22:29:29.834481 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:29:29.837479 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:29:29.838471 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:29:29.838471 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m22:29:29.839469 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:29:30.726489 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ae-1fc7-1e56-90af-4f28fc0e6cd1
[0m22:29:31.045134 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m22:29:31.049975 [debug] [ThreadPool]: On list_workspace: Close
[0m22:29:31.050972 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ae-1fc7-1e56-90af-4f28fc0e6cd1
[0m22:29:31.307526 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m22:29:31.310519 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m22:29:31.320489 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:29:31.321497 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m22:29:31.321497 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m22:29:31.322486 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:29:32.127697 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ae-209d-16c4-b5e6-906b082363c1
[0m22:29:32.502720 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m22:29:32.503718 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:29:32.504729 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m22:29:32.504729 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:29:32.505740 [debug] [ThreadPool]: On create_workspace_default: Close
[0m22:29:32.505740 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ae-209d-16c4-b5e6-906b082363c1
[0m22:29:32.740627 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:29:32.756582 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:29:32.757578 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:29:32.758575 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:29:33.562186 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ae-2178-1b2e-b43d-65ed72d3e922
[0m22:29:33.929858 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m22:29:33.932849 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:29:33.933849 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ae-2178-1b2e-b43d-65ed72d3e922
[0m22:29:34.162407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '525e824a-392c-4aa7-b7de-6a5eb7c0226c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019325E4FF90>]}
[0m22:29:34.163424 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:29:34.163424 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:29:34.164393 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:29:34.165390 [info ] [MainThread]: 
[0m22:29:34.172211 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m22:29:34.173211 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m22:29:34.174950 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m22:29:34.175948 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m22:29:34.179966 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m22:29:34.181932 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 22:29:34.175948 => 22:29:34.180966
[0m22:29:34.182930 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m22:29:34.215842 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m22:29:34.217849 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:29:34.218834 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m22:29:34.218834 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m22:29:34.219831 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:29:35.018372 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ae-2257-1493-a5f3-364a6c6c0ef3
[0m22:29:35.719143 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m22:29:35.734096 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 22:29:34.183927 => 22:29:35.734096
[0m22:29:35.735094 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m22:29:35.735094 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:29:35.736095 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m22:29:35.737089 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ae-2257-1493-a5f3-364a6c6c0ef3
[0m22:29:35.964289 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '525e824a-392c-4aa7-b7de-6a5eb7c0226c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019327019750>]}
[0m22:29:35.966286 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.79s]
[0m22:29:35.968276 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m22:29:35.968276 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m22:29:35.969302 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m22:29:35.971287 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m22:29:35.971287 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m22:29:35.974279 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m22:29:35.976245 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 22:29:35.972260 => 22:29:35.975277
[0m22:29:35.976245 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m22:29:35.981260 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m22:29:35.982242 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:29:35.982242 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m22:29:35.983255 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m22:29:35.984224 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:29:36.818815 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ae-2368-1b2c-af2c-b82fc85a22ab
[0m22:29:37.492038 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m22:29:37.495028 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 22:29:35.977242 => 22:29:37.495028
[0m22:29:37.496025 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m22:29:37.496025 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:29:37.497022 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m22:29:37.497022 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ae-2368-1b2c-af2c-b82fc85a22ab
[0m22:29:37.735842 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '525e824a-392c-4aa7-b7de-6a5eb7c0226c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001932701B790>]}
[0m22:29:37.738836 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.77s]
[0m22:29:37.741879 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m22:29:37.743839 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m22:29:37.745861 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m22:29:37.747832 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m22:29:37.748825 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m22:29:37.753812 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m22:29:37.755807 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 22:29:37.748825 => 22:29:37.754809
[0m22:29:37.755807 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m22:29:37.763784 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m22:29:37.764782 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:29:37.765779 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m22:29:37.765779 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m22:29:37.766777 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:29:38.579019 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ae-2477-143f-a7c5-bbe7936f620d
[0m22:29:39.281535 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m22:29:39.284525 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 22:29:37.756803 => 22:29:39.284525
[0m22:29:39.285522 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m22:29:39.285522 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:29:39.286520 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m22:29:39.286520 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ae-2477-143f-a7c5-bbe7936f620d
[0m22:29:39.521099 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '525e824a-392c-4aa7-b7de-6a5eb7c0226c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931B792E50>]}
[0m22:29:39.523094 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.77s]
[0m22:29:39.524411 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m22:29:39.525439 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:29:39.526407 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m22:29:39.527420 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m22:29:39.528411 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m22:29:39.532420 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:29:39.533390 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 22:29:39.528411 => 22:29:39.533390
[0m22:29:39.534387 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m22:29:39.538405 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:29:39.539375 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:29:39.540371 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:29:39.540371 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH base AS (
    SELECT
        ROW_NUMBER() OVER (ORDER BY DTNASC, LOCNASC, PESO) AS nascimento_id,  -- cria ID único
        DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
        LOCNASC AS cod_municipio,
        SEXO AS sexo,
        PESO AS peso,
        IDADEMAE AS idade_mae,
        GESTACAO AS gestacao_semanas
    FROM `workspace`.`default`.`sinasc_2022_sc_clean`
    WHERE CODUFNATU = 42  -- SC
)

SELECT * 
FROM base

[0m22:29:39.541397 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:29:40.363337 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ae-2587-1204-8c0d-4d39e17e21a6
[0m22:29:41.052078 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m22:29:41.055068 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 22:29:39.534387 => 22:29:41.055068
[0m22:29:41.056065 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m22:29:41.057062 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:29:41.057062 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m22:29:41.058060 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ae-2587-1204-8c0d-4d39e17e21a6
[0m22:29:41.284645 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '525e824a-392c-4aa7-b7de-6a5eb7c0226c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019327056810>]}
[0m22:29:41.286640 [info ] [Thread-1 (]: 4 of 5 OK created sql view model default.stg_nascidos_vivos .................... [[32mOK[0m in 1.76s]
[0m22:29:41.288631 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:29:41.289583 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m22:29:41.289583 [info ] [Thread-1 (]: 5 of 5 START sql view model default.stg_tempo .................................. [RUN]
[0m22:29:41.291609 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.stg_tempo)
[0m22:29:41.291609 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m22:29:41.296595 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m22:29:41.297578 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 22:29:41.292608 => 22:29:41.296595
[0m22:29:41.297578 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m22:29:41.302579 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m22:29:41.303549 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:29:41.304546 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m22:29:41.304546 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

WITH base AS (
    SELECT DISTINCT
        DTNASC AS data_bigint,
        CAST(DATE_FROM_UNIX_DATE(DTNASC) AS DATE) AS data
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    data,
    CAST(FORMAT_STRING('%d%02d%02d', YEAR(data), MONTH(data), DAY(data)) AS INT) AS data_id,
    YEAR(data) AS ano,
    MONTH(data) AS mes,
    DAY(data) AS dia,
    QUARTER(data) AS trimestre,
    DAYOFWEEK(data) AS dia_semana
FROM base

[0m22:29:41.305543 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:29:42.140160 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ae-2696-14cb-94f4-821d62d70409
[0m22:29:42.501716 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

WITH base AS (
    SELECT DISTINCT
        DTNASC AS data_bigint,
        CAST(DATE_FROM_UNIX_DATE(DTNASC) AS DATE) AS data
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    data,
    CAST(FORMAT_STRING('%d%02d%02d', YEAR(data), MONTH(data), DAY(data)) AS INT) AS data_id,
    YEAR(data) AS ano,
    MONTH(data) AS mes,
    DAY(data) AS dia,
    QUARTER(data) AS trimestre,
    DAYOFWEEK(data) AS dia_semana
FROM base

[0m22:29:42.503712 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `idade_mae`, `cod_municipio`, `nascimento_id`]. SQLSTATE: 42703; line 10 pos 8
[0m22:29:42.507699 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `idade_mae`, `cod_municipio`, `nascimento_id`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `idade_mae`, `cod_municipio`, `nascimento_id`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m22:29:42.510690 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078ae-26b5-1be6-823c-98e7b1654f70
[0m22:29:42.513682 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 22:29:41.298561 => 22:29:42.512685
[0m22:29:42.514678 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m22:29:42.515677 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:29:42.517671 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m22:29:42.518667 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ae-2696-14cb-94f4-821d62d70409
[0m22:29:42.753579 [debug] [Thread-1 (]: Runtime Error in model stg_tempo (models\staging\stg_tempo.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `idade_mae`, `cod_municipio`, `nascimento_id`]. SQLSTATE: 42703; line 10 pos 8
[0m22:29:42.754577 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '525e824a-392c-4aa7-b7de-6a5eb7c0226c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019327072790>]}
[0m22:29:42.755573 [error] [Thread-1 (]: 5 of 5 ERROR creating sql view model default.stg_tempo ......................... [[31mERROR[0m in 1.46s]
[0m22:29:42.756497 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m22:29:42.758494 [debug] [MainThread]: On master: ROLLBACK
[0m22:29:42.759516 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:29:43.554198 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ae-276d-1921-8071-2e225b7d1427
[0m22:29:43.557692 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:29:43.559656 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:29:43.561653 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:29:43.563674 [debug] [MainThread]: On master: ROLLBACK
[0m22:29:43.564671 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:29:43.565664 [debug] [MainThread]: On master: Close
[0m22:29:43.565664 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ae-276d-1921-8071-2e225b7d1427
[0m22:29:43.806448 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:29:43.808439 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m22:29:43.809494 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:29:43.810409 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_tempo' was properly closed.
[0m22:29:43.811398 [info ] [MainThread]: 
[0m22:29:43.812073 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 13.98 seconds (13.98s).
[0m22:29:43.814098 [debug] [MainThread]: Command end result
[0m22:29:43.825063 [info ] [MainThread]: 
[0m22:29:43.827037 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:29:43.828033 [info ] [MainThread]: 
[0m22:29:43.829048 [error] [MainThread]: [33mRuntime Error in model stg_tempo (models\staging\stg_tempo.sql)[0m
[0m22:29:43.831033 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `idade_mae`, `cod_municipio`, `nascimento_id`]. SQLSTATE: 42703; line 10 pos 8
[0m22:29:43.832036 [info ] [MainThread]: 
[0m22:29:43.833020 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m22:29:43.834024 [debug] [MainThread]: Command `dbt run` failed at 22:29:43.834024 after 15.58 seconds
[0m22:29:43.835014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931C5943D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931C7CD5D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001931C7CE690>]}
[0m22:29:43.835014 [debug] [MainThread]: Flushing usage events
[0m22:31:28.626595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026463E8E290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026464153FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000264641E32D0>]}


============================== 22:31:28.631582 | f8ee5f63-34b8-471c-9b64-e2b41d149cc1 ==============================
[0m22:31:28.631582 [info ] [MainThread]: Running with dbt=1.5.2
[0m22:31:28.632558 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m22:31:29.947600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f8ee5f63-34b8-471c-9b64-e2b41d149cc1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026463E8F490>]}
[0m22:31:29.966550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f8ee5f63-34b8-471c-9b64-e2b41d149cc1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002646D94D950>]}
[0m22:31:29.967526 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m22:31:29.983483 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m22:31:30.098978 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:31:30.098978 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m22:31:30.126909 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m22:31:30.206697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f8ee5f63-34b8-471c-9b64-e2b41d149cc1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002646EA71450>]}
[0m22:31:30.219661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f8ee5f63-34b8-471c-9b64-e2b41d149cc1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026463686310>]}
[0m22:31:30.219661 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m22:31:30.220661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8ee5f63-34b8-471c-9b64-e2b41d149cc1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002646ABE6310>]}
[0m22:31:30.222625 [info ] [MainThread]: 
[0m22:31:30.224642 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:31:30.226614 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m22:31:30.227612 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m22:31:30.228609 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m22:31:30.228609 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:31:31.088440 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ae-6784-18c6-8597-baba7f5c15ed
[0m22:31:31.662968 [debug] [ThreadPool]: SQL status: OK in 1.4299999475479126 seconds
[0m22:31:31.669510 [debug] [ThreadPool]: On list_workspace: Close
[0m22:31:31.670507 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ae-6784-18c6-8597-baba7f5c15ed
[0m22:31:31.916951 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m22:31:31.917947 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m22:31:31.927948 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:31:31.928948 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m22:31:31.928948 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m22:31:31.929943 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:31:32.708483 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ae-687b-1ac0-b451-a63901f027b8
[0m22:31:33.084915 [debug] [ThreadPool]: SQL status: OK in 1.149999976158142 seconds
[0m22:31:33.085911 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m22:31:33.085911 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m22:31:33.086880 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:31:33.086880 [debug] [ThreadPool]: On create_workspace_default: Close
[0m22:31:33.087878 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ae-687b-1ac0-b451-a63901f027b8
[0m22:31:33.327520 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m22:31:33.341480 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m22:31:33.342477 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m22:31:33.343475 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:31:34.144820 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f078ae-6957-1890-8b3e-c8803b4566d4
[0m22:31:34.517153 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m22:31:34.520142 [debug] [ThreadPool]: On list_workspace_default: Close
[0m22:31:34.521139 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f078ae-6957-1890-8b3e-c8803b4566d4
[0m22:31:34.769576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8ee5f63-34b8-471c-9b64-e2b41d149cc1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002646D7DFF10>]}
[0m22:31:34.770545 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:31:34.771570 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:31:34.772567 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:31:34.773539 [info ] [MainThread]: 
[0m22:31:34.780545 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m22:31:34.780545 [info ] [Thread-1 (]: 1 of 5 START sql view model default.stg_atendimento ............................ [RUN]
[0m22:31:34.782513 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m22:31:34.783510 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m22:31:34.787527 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m22:31:34.789493 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 22:31:34.783510 => 22:31:34.788528
[0m22:31:34.789493 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m22:31:34.823404 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m22:31:34.825399 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:31:34.826396 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m22:31:34.827393 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m22:31:34.828390 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:31:35.660979 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ae-6a3e-1410-84d6-16270bafdb78
[0m22:31:36.361086 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m22:31:36.376046 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 22:31:34.790491 => 22:31:36.376046
[0m22:31:36.377043 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m22:31:36.377043 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:31:36.378050 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m22:31:36.378050 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ae-6a3e-1410-84d6-16270bafdb78
[0m22:31:36.615523 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8ee5f63-34b8-471c-9b64-e2b41d149cc1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002646EABC2D0>]}
[0m22:31:36.618483 [info ] [Thread-1 (]: 1 of 5 OK created sql view model default.stg_atendimento ....................... [[32mOK[0m in 1.83s]
[0m22:31:36.623503 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m22:31:36.625462 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m22:31:36.627482 [info ] [Thread-1 (]: 2 of 5 START sql view model default.stg_doenca ................................. [RUN]
[0m22:31:36.630480 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m22:31:36.631453 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m22:31:36.637456 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m22:31:36.639421 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 22:31:36.632440 => 22:31:36.639421
[0m22:31:36.640447 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m22:31:36.647399 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m22:31:36.648426 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:31:36.648426 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m22:31:36.649422 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m22:31:36.649422 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:31:37.465992 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ae-6b52-1dac-a78a-b0927238bbe1
[0m22:31:38.106477 [debug] [Thread-1 (]: SQL status: OK in 1.4600000381469727 seconds
[0m22:31:38.109441 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 22:31:36.640447 => 22:31:38.108443
[0m22:31:38.109441 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m22:31:38.110439 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:31:38.110439 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m22:31:38.111438 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ae-6b52-1dac-a78a-b0927238bbe1
[0m22:31:38.352706 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8ee5f63-34b8-471c-9b64-e2b41d149cc1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002646E9DB450>]}
[0m22:31:38.354699 [info ] [Thread-1 (]: 2 of 5 OK created sql view model default.stg_doenca ............................ [[32mOK[0m in 1.72s]
[0m22:31:38.356693 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m22:31:38.357694 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m22:31:38.358871 [info ] [Thread-1 (]: 3 of 5 START sql view model default.stg_localidade ............................. [RUN]
[0m22:31:38.361871 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m22:31:38.362867 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m22:31:38.370844 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m22:31:38.373836 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 22:31:38.363895 => 22:31:38.372837
[0m22:31:38.374833 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m22:31:38.386831 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m22:31:38.388794 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:31:38.389794 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m22:31:38.389794 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m22:31:38.390818 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:31:39.207535 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ae-6c5b-1aeb-90c1-24d6107d8038
[0m22:31:39.887930 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m22:31:39.890922 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 22:31:38.375830 => 22:31:39.889925
[0m22:31:39.890922 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m22:31:39.891919 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:31:39.891919 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m22:31:39.892917 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ae-6c5b-1aeb-90c1-24d6107d8038
[0m22:31:40.138211 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8ee5f63-34b8-471c-9b64-e2b41d149cc1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002646EBF3A10>]}
[0m22:31:40.141204 [info ] [Thread-1 (]: 3 of 5 OK created sql view model default.stg_localidade ........................ [[32mOK[0m in 1.78s]
[0m22:31:40.145193 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m22:31:40.148184 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:31:40.150180 [info ] [Thread-1 (]: 4 of 5 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m22:31:40.154172 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m22:31:40.157167 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m22:31:40.167134 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:31:40.169095 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 22:31:40.158127 => 22:31:40.169095
[0m22:31:40.170123 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m22:31:40.176105 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:31:40.177102 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:31:40.178101 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m22:31:40.178101 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH raw_nascidos AS (
    SELECT
        ROW_NUMBER() OVER (PARTITION BY CODMUNNASC ORDER BY DTNASC) AS nascimento_id,
        TIMESTAMP_SECONDS(DTNASC / 1000) AS data_nascimento,
        CODMUNNASC AS cod_municipio,
        SEXO AS sexo,
        PESO AS peso,
        IDADEMAE AS idade_mae,
        GESTACAO AS gestacao_semanas,
        PARTO
    FROM `workspace`.`default`.`sinasc_2022_sc_clean`
    WHERE CODUFNATU = 42  -- SC
)

SELECT *
FROM raw_nascidos

[0m22:31:40.179096 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:31:40.986694 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ae-6d6c-14b4-a776-16b718e01b6a
[0m22:31:41.702019 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m22:31:41.710996 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 22:31:40.171120 => 22:31:41.709999
[0m22:31:41.712991 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m22:31:41.714986 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:31:41.715981 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m22:31:41.717976 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ae-6d6c-14b4-a776-16b718e01b6a
[0m22:31:41.966274 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8ee5f63-34b8-471c-9b64-e2b41d149cc1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002646EBF0A50>]}
[0m22:31:41.968243 [info ] [Thread-1 (]: 4 of 5 OK created sql view model default.stg_nascidos_vivos .................... [[32mOK[0m in 1.81s]
[0m22:31:41.969235 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m22:31:41.971230 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m22:31:41.971794 [info ] [Thread-1 (]: 5 of 5 START sql view model default.stg_tempo .................................. [RUN]
[0m22:31:41.972549 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.stg_tempo)
[0m22:31:41.973580 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m22:31:41.977565 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m22:31:41.979532 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 22:31:41.973580 => 22:31:41.978566
[0m22:31:41.979532 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m22:31:41.984548 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m22:31:41.986515 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m22:31:41.986515 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m22:31:41.987512 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

WITH base AS (
    SELECT DISTINCT
        DTNASC AS data_bigint,
        CAST(DATE_FROM_UNIX_DATE(DTNASC) AS DATE) AS data
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    data,
    CAST(FORMAT_STRING('%d%02d%02d', YEAR(data), MONTH(data), DAY(data)) AS INT) AS data_id,
    YEAR(data) AS ano,
    MONTH(data) AS mes,
    DAY(data) AS dia,
    QUARTER(data) AS trimestre,
    DAYOFWEEK(data) AS dia_semana
FROM base

[0m22:31:41.987512 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:31:42.785748 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f078ae-6e7e-12b4-a3c1-a5c8fab2bac6
[0m22:31:43.169069 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

WITH base AS (
    SELECT DISTINCT
        DTNASC AS data_bigint,
        CAST(DATE_FROM_UNIX_DATE(DTNASC) AS DATE) AS data
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    data,
    CAST(FORMAT_STRING('%d%02d%02d', YEAR(data), MONTH(data), DAY(data)) AS INT) AS data_id,
    YEAR(data) AS ano,
    MONTH(data) AS mes,
    DAY(data) AS dia,
    QUARTER(data) AS trimestre,
    DAYOFWEEK(data) AS dia_semana
FROM base

[0m22:31:43.171071 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `PARTO`, `idade_mae`, `cod_municipio`]. SQLSTATE: 42703; line 10 pos 8
[0m22:31:43.175053 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `PARTO`, `idade_mae`, `cod_municipio`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `PARTO`, `idade_mae`, `cod_municipio`]. SQLSTATE: 42703; line 10 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m22:31:43.178043 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f078ae-6e9f-1e90-95c6-70301d0c65a8
[0m22:31:43.180038 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 22:31:41.980529 => 22:31:43.180038
[0m22:31:43.182038 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m22:31:43.184033 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m22:31:43.186028 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m22:31:43.187024 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f078ae-6e7e-12b4-a3c1-a5c8fab2bac6
[0m22:31:43.435738 [debug] [Thread-1 (]: Runtime Error in model stg_tempo (models\staging\stg_tempo.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `PARTO`, `idade_mae`, `cod_municipio`]. SQLSTATE: 42703; line 10 pos 8
[0m22:31:43.436735 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8ee5f63-34b8-471c-9b64-e2b41d149cc1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002646369F010>]}
[0m22:31:43.437737 [error] [Thread-1 (]: 5 of 5 ERROR creating sql view model default.stg_tempo ......................... [[31mERROR[0m in 1.46s]
[0m22:31:43.438701 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m22:31:43.440702 [debug] [MainThread]: On master: ROLLBACK
[0m22:31:43.441722 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:31:44.256907 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f078ae-6f5e-1544-b527-06706a6bf9e4
[0m22:31:44.258873 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:31:44.259898 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:31:44.260898 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:31:44.261893 [debug] [MainThread]: On master: ROLLBACK
[0m22:31:44.262895 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:31:44.263859 [debug] [MainThread]: On master: Close
[0m22:31:44.263859 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f078ae-6f5e-1544-b527-06706a6bf9e4
[0m22:31:44.503157 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:31:44.505125 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m22:31:44.505125 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m22:31:44.506146 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_tempo' was properly closed.
[0m22:31:44.507141 [info ] [MainThread]: 
[0m22:31:44.507868 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 14.28 seconds (14.28s).
[0m22:31:44.509895 [debug] [MainThread]: Command end result
[0m22:31:44.521833 [info ] [MainThread]: 
[0m22:31:44.522830 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:31:44.523827 [info ] [MainThread]: 
[0m22:31:44.524826 [error] [MainThread]: [33mRuntime Error in model stg_tempo (models\staging\stg_tempo.sql)[0m
[0m22:31:44.525851 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `PARTO`, `idade_mae`, `cod_municipio`]. SQLSTATE: 42703; line 10 pos 8
[0m22:31:44.526873 [info ] [MainThread]: 
[0m22:31:44.527928 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m22:31:44.528927 [debug] [MainThread]: Command `dbt run` failed at 22:31:44.528927 after 15.92 seconds
[0m22:31:44.529949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002646415A250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000264641D0B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002646479AA10>]}
[0m22:31:44.529949 [debug] [MainThread]: Flushing usage events
[0m08:42:05.958870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B5B79A850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B5B568650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B5BAA9510>]}


============================== 08:42:05.974471 | 854f544a-2ce5-48dd-993d-7f47f773ea69 ==============================
[0m08:42:05.974471 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:42:05.975481 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:42:07.324776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B5B568C10>]}
[0m08:42:07.340396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B64EC8210>]}
[0m08:42:07.340396 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m08:42:07.377870 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:42:08.691197 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m08:42:08.691197 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m08:42:08.691197 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_tempo.sql
[0m08:42:08.691197 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\intermediate\int_nascimento.sql
[0m08:42:08.691197 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m08:42:08.734522 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m08:42:08.752476 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_tempo.sql
[0m08:42:08.756495 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m08:42:08.761241 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m08:42:08.876675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6502ED10>]}
[0m08:42:08.892298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6507F8D0>]}
[0m08:42:08.892298 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m08:42:08.892298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6255F090>]}
[0m08:42:08.892298 [info ] [MainThread]: 
[0m08:42:08.892298 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:42:08.892298 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:42:08.892298 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:42:08.892298 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m08:42:08.892298 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:42:09.969752 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07903-b4fe-1f7a-95be-68e1a3a556d4
[0m08:42:10.187892 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y\x03\xb4\xfe\x1fz\x95\xbeh\xe1\xa3\xa5V\xd4'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.2181403636932373/900.0')])
[0m08:42:17.642847 [debug] [ThreadPool]: SQL status: OK in 8.75 seconds
[0m08:42:17.686027 [debug] [ThreadPool]: On list_workspace: Close
[0m08:42:17.686027 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07903-b4fe-1f7a-95be-68e1a3a556d4
[0m08:42:17.935491 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m08:42:17.937396 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m08:42:17.953018 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:17.953018 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m08:42:17.953018 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m08:42:17.953018 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:42:18.769758 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07903-ba56-10d2-b51a-7ac7cb47f8a2
[0m08:42:20.525404 [debug] [ThreadPool]: SQL status: OK in 2.569999933242798 seconds
[0m08:42:20.525404 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m08:42:20.525404 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m08:42:20.525404 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m08:42:20.525404 [debug] [ThreadPool]: On create_workspace_default: Close
[0m08:42:20.525404 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07903-ba56-10d2-b51a-7ac7cb47f8a2
[0m08:42:20.765328 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m08:42:20.765328 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m08:42:20.765328 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m08:42:20.765328 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:42:21.577749 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07903-bbff-1b3e-9a51-e92e9bf8802c
[0m08:42:22.157345 [debug] [ThreadPool]: SQL status: OK in 1.3899999856948853 seconds
[0m08:42:22.157345 [debug] [ThreadPool]: On list_workspace_default: Close
[0m08:42:22.157345 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07903-bbff-1b3e-9a51-e92e9bf8802c
[0m08:42:22.401934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B650B3E90>]}
[0m08:42:22.401934 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:22.401934 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:42:22.408620 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:42:22.409497 [info ] [MainThread]: 
[0m08:42:22.419055 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m08:42:22.419055 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m08:42:22.419055 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m08:42:22.425957 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m08:42:22.428900 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m08:42:22.428900 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 08:42:22.426954 => 08:42:22.428900
[0m08:42:22.428900 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m08:42:22.460925 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m08:42:22.460925 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:22.460925 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m08:42:22.460925 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m08:42:22.460925 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m08:42:23.271205 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07903-bd05-1d8c-96c5-6abf37d1102c
[0m08:42:26.210006 [debug] [Thread-1 (]: SQL status: OK in 3.75 seconds
[0m08:42:26.225418 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 08:42:22.428900 => 08:42:26.225418
[0m08:42:26.225418 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m08:42:26.225418 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:42:26.225418 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m08:42:26.225418 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07903-bd05-1d8c-96c5-6abf37d1102c
[0m08:42:26.469053 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B64F577D0>]}
[0m08:42:26.469053 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 4.05s]
[0m08:42:26.469053 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m08:42:26.469053 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m08:42:26.469053 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m08:42:26.469053 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m08:42:26.469053 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m08:42:26.486754 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m08:42:26.487313 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 08:42:26.469053 => 08:42:26.487313
[0m08:42:26.487313 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m08:42:26.487313 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m08:42:26.487313 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:26.496179 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m08:42:26.496179 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m08:42:26.497052 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:42:27.291712 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07903-bf68-1914-8888-47c7a0bcd1ab
[0m08:42:28.389303 [debug] [Thread-1 (]: SQL status: OK in 1.8899999856948853 seconds
[0m08:42:28.389303 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 08:42:26.487313 => 08:42:28.389303
[0m08:42:28.389303 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m08:42:28.389303 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:42:28.389303 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m08:42:28.404743 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07903-bf68-1914-8888-47c7a0bcd1ab
[0m08:42:28.623503 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B66207A90>]}
[0m08:42:28.623503 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 2.15s]
[0m08:42:28.623503 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m08:42:28.623503 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m08:42:28.623503 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m08:42:28.639487 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m08:42:28.640274 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m08:42:28.640886 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m08:42:28.640886 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 08:42:28.640886 => 08:42:28.640886
[0m08:42:28.640886 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m08:42:28.649804 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m08:42:28.649804 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:28.649804 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m08:42:28.649804 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m08:42:28.649804 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:42:29.456512 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07903-c0b5-1e71-8896-d16c65b4edcf
[0m08:42:30.524560 [debug] [Thread-1 (]: SQL status: OK in 1.8700000047683716 seconds
[0m08:42:30.524560 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 08:42:28.640886 => 08:42:30.524560
[0m08:42:30.524560 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m08:42:30.524560 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:42:30.524560 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m08:42:30.540181 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07903-c0b5-1e71-8896-d16c65b4edcf
[0m08:42:30.774272 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B64FFF510>]}
[0m08:42:30.774272 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 2.15s]
[0m08:42:30.774272 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m08:42:30.774272 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m08:42:30.774272 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m08:42:30.789685 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m08:42:30.789685 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m08:42:30.789685 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m08:42:30.789685 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 08:42:30.789685 => 08:42:30.789685
[0m08:42:30.789685 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m08:42:30.789685 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m08:42:30.789685 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:30.789685 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m08:42:30.805307 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(id AS STRING) AS nascimento_id,
    CAST(data_nascimento AS DATE) AS data_nascimento,
    municipio,
    uf,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas
FROM default.sinasc_2022_sc_clean
WHERE uf = 'SC'

[0m08:42:30.806368 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:42:31.606959 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07903-c1f9-17b8-8f1e-85ed446a3a7e
[0m08:42:33.417115 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

SELECT
    CAST(id AS STRING) AS nascimento_id,
    CAST(data_nascimento AS DATE) AS data_nascimento,
    municipio,
    uf,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas
FROM default.sinasc_2022_sc_clean
WHERE uf = 'SC'

[0m08:42:33.417115 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 18 pos 6
[0m08:42:33.417115 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 18 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m08:42:33.417115 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07903-c21e-120d-bfee-622ef20fcc15
[0m08:42:33.417115 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 08:42:30.789685 => 08:42:33.417115
[0m08:42:33.417115 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m08:42:33.417115 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:42:33.417115 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m08:42:33.417115 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07903-c1f9-17b8-8f1e-85ed446a3a7e
[0m08:42:33.834263 [debug] [Thread-1 (]: Runtime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 18 pos 6
[0m08:42:33.835257 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B64FFD4D0>]}
[0m08:42:33.836256 [error] [Thread-1 (]: 4 of 12 ERROR creating sql view model default.stg_nascidos_vivos ............... [[31mERROR[0m in 3.05s]
[0m08:42:33.837265 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m08:42:33.837265 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m08:42:33.837265 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m08:42:33.837265 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m08:42:33.837265 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m08:42:33.837265 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m08:42:33.837265 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 08:42:33.837265 => 08:42:33.837265
[0m08:42:33.837265 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m08:42:33.837265 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m08:42:33.854908 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:33.855454 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m08:42:33.855454 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m08:42:33.855454 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:42:34.661655 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07903-c3ce-16ac-aab9-c111e3796e86
[0m08:42:35.449517 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m08:42:35.449517 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 08:42:33.837265 => 08:42:35.449517
[0m08:42:35.464922 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m08:42:35.464922 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:42:35.464922 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m08:42:35.464922 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07903-c3ce-16ac-aab9-c111e3796e86
[0m08:42:35.688634 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B65072C90>]}
[0m08:42:35.688634 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.84s]
[0m08:42:35.688634 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m08:42:35.688634 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m08:42:35.688634 [info ] [Thread-1 (]: 6 of 12 START sql view model default.dim_localidade ............................ [RUN]
[0m08:42:35.688634 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m08:42:35.688634 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m08:42:35.688634 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m08:42:35.688634 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 08:42:35.688634 => 08:42:35.688634
[0m08:42:35.688634 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m08:42:35.705697 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m08:42:35.706727 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:35.707721 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m08:42:35.707721 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m08:42:35.708719 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:42:36.473399 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07903-c4e3-1d33-8a74-695477909d4c
[0m08:42:37.165174 [debug] [Thread-1 (]: SQL status: OK in 1.4600000381469727 seconds
[0m08:42:37.165174 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 08:42:35.688634 => 08:42:37.165174
[0m08:42:37.165174 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m08:42:37.165174 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:42:37.165174 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m08:42:37.165174 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07903-c4e3-1d33-8a74-695477909d4c
[0m08:42:37.399799 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B6611AF10>]}
[0m08:42:37.415200 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.dim_localidade ....................... [[32mOK[0m in 1.71s]
[0m08:42:37.415200 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m08:42:37.415200 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m08:42:37.415200 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m08:42:37.425662 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m08:42:37.426381 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m08:42:37.427645 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m08:42:37.427645 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 08:42:37.426381 => 08:42:37.427645
[0m08:42:37.427645 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m08:42:37.442322 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m08:42:37.442322 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:37.442322 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m08:42:37.442322 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m08:42:37.442322 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:42:38.231038 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07903-c5ee-1909-a5cd-e42970aadb1a
[0m08:42:39.031515 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m08:42:39.046955 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 08:42:37.427645 => 08:42:39.046955
[0m08:42:39.046955 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m08:42:39.046955 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:42:39.046955 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m08:42:39.046955 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07903-c5ee-1909-a5cd-e42970aadb1a
[0m08:42:39.283601 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B662FA6D0>]}
[0m08:42:39.283601 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.87s]
[0m08:42:39.283601 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m08:42:39.283601 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m08:42:39.283601 [info ] [Thread-1 (]: 8 of 12 SKIP relation default.stg_tempo ........................................ [[33mSKIP[0m]
[0m08:42:39.283601 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m08:42:39.283601 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m08:42:39.283601 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m08:42:39.294939 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m08:42:39.294939 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m08:42:39.298481 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m08:42:39.298481 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 08:42:39.295967 => 08:42:39.298481
[0m08:42:39.298481 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m08:42:39.309726 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m08:42:39.310747 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:39.310747 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m08:42:39.310747 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m08:42:39.310747 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:42:40.114136 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07903-c70d-1e8d-af31-9dec6fd376d4
[0m08:42:40.906639 [debug] [Thread-1 (]: SQL status: OK in 1.600000023841858 seconds
[0m08:42:40.922057 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 08:42:39.298481 => 08:42:40.922057
[0m08:42:40.922057 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m08:42:40.922057 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:42:40.922057 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m08:42:40.922057 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07903-c70d-1e8d-af31-9dec6fd376d4
[0m08:42:41.167730 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '854f544a-2ce5-48dd-993d-7f47f773ea69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B662F8A90>]}
[0m08:42:41.167730 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.88s]
[0m08:42:41.167730 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m08:42:41.167730 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m08:42:41.167730 [info ] [Thread-1 (]: 10 of 12 SKIP relation default.dim_tempo ....................................... [[33mSKIP[0m]
[0m08:42:41.167730 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m08:42:41.167730 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m08:42:41.167730 [info ] [Thread-1 (]: 11 of 12 SKIP relation default.int_nascimento .................................. [[33mSKIP[0m]
[0m08:42:41.167730 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m08:42:41.180538 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m08:42:41.181601 [info ] [Thread-1 (]: 12 of 12 SKIP relation default.fato_nascimento ................................. [[33mSKIP[0m]
[0m08:42:41.182552 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m08:42:41.182552 [debug] [MainThread]: On master: ROLLBACK
[0m08:42:41.182552 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:42:41.954468 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07903-c827-1a68-b79e-099c2c9698f0
[0m08:42:41.954468 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:42:41.954468 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:42:41.954468 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:42:41.954468 [debug] [MainThread]: On master: ROLLBACK
[0m08:42:41.954468 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:42:41.954468 [debug] [MainThread]: On master: Close
[0m08:42:41.954468 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07903-c827-1a68-b79e-099c2c9698f0
[0m08:42:42.187753 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:42:42.187753 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m08:42:42.187753 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m08:42:42.187753 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_atendimento_hospitalar' was properly closed.
[0m08:42:42.187753 [info ] [MainThread]: 
[0m08:42:42.187753 [info ] [MainThread]: Finished running 10 view models, 2 table models in 0 hours 0 minutes and 33.30 seconds (33.30s).
[0m08:42:42.203375 [debug] [MainThread]: Command end result
[0m08:42:42.220577 [info ] [MainThread]: 
[0m08:42:42.221427 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m08:42:42.221427 [info ] [MainThread]: 
[0m08:42:42.221427 [error] [MainThread]: [33mRuntime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m08:42:42.221427 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`PESO`, `SEXO`, `PARTO`, `APGAR1`, `APGAR5`]. SQLSTATE: 42703; line 18 pos 6
[0m08:42:42.221427 [info ] [MainThread]: 
[0m08:42:42.221427 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=4 TOTAL=12
[0m08:42:42.229523 [debug] [MainThread]: Command `dbt run` failed at 08:42:42.229523 after 36.31 seconds
[0m08:42:42.230533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B5B9B2850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B5B20B5D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020B5B889690>]}
[0m08:42:42.230533 [debug] [MainThread]: Flushing usage events
[0m08:57:02.257669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A915F750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A9A77F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A937E0D0>]}


============================== 08:57:02.267428 | 38e83ff5-44b8-4811-81bf-7c503a636e4f ==============================
[0m08:57:02.267428 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:57:02.269427 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:57:04.376590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '38e83ff5-44b8-4811-81bf-7c503a636e4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A9148BD0>]}
[0m08:57:04.394541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '38e83ff5-44b8-4811-81bf-7c503a636e4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A9148BD0>]}
[0m08:57:04.395510 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m08:57:04.415244 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:57:04.560919 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m08:57:04.561912 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m08:57:04.589902 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m08:57:04.669717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '38e83ff5-44b8-4811-81bf-7c503a636e4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6B3DB7010>]}
[0m08:57:04.681685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '38e83ff5-44b8-4811-81bf-7c503a636e4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A8471310>]}
[0m08:57:04.682655 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m08:57:04.683652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '38e83ff5-44b8-4811-81bf-7c503a636e4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6B022CE10>]}
[0m08:57:04.685677 [info ] [MainThread]: 
[0m08:57:04.686672 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:57:04.688917 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:57:04.688917 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:57:04.689914 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m08:57:04.689914 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:57:05.784248 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07905-caf8-139f-9c50-93b51e6680b9
[0m08:57:05.979432 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y\x05\xca\xf8\x13\x9f\x9cP\x93\xb5\x1ef\x80\xb9'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.19419169425964355/900.0')])
[0m08:57:13.281084 [debug] [ThreadPool]: SQL status: OK in 8.59000015258789 seconds
[0m08:57:13.301424 [debug] [ThreadPool]: On list_workspace: Close
[0m08:57:13.301424 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07905-caf8-139f-9c50-93b51e6680b9
[0m08:57:13.548911 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m08:57:13.551937 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m08:57:13.563896 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m08:57:13.564918 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m08:57:13.564918 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m08:57:13.565891 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:57:14.384820 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07905-d023-1e8c-a6ad-cb2a7fa12fd1
[0m08:57:16.400423 [debug] [ThreadPool]: SQL status: OK in 2.8299999237060547 seconds
[0m08:57:16.401425 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m08:57:16.402421 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m08:57:16.402421 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m08:57:16.403419 [debug] [ThreadPool]: On create_workspace_default: Close
[0m08:57:16.403419 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07905-d023-1e8c-a6ad-cb2a7fa12fd1
[0m08:57:16.650496 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m08:57:16.666456 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m08:57:16.667452 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m08:57:16.668450 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:57:17.465822 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07905-d1fe-161f-ad42-098c5ae1a411
[0m08:57:18.014629 [debug] [ThreadPool]: SQL status: OK in 1.350000023841858 seconds
[0m08:57:18.022421 [debug] [ThreadPool]: On list_workspace_default: Close
[0m08:57:18.023419 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07905-d1fe-161f-ad42-098c5ae1a411
[0m08:57:18.272774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '38e83ff5-44b8-4811-81bf-7c503a636e4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A9AB3E50>]}
[0m08:57:18.273744 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:57:18.273744 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:57:18.274741 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:57:18.275664 [info ] [MainThread]: 
[0m08:57:18.286638 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m08:57:18.287636 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stg_nascidos_vivos ......................... [RUN]
[0m08:57:18.288635 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_nascidos_vivos'
[0m08:57:18.289660 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m08:57:18.292649 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m08:57:18.294617 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 08:57:18.289660 => 08:57:18.293646
[0m08:57:18.294617 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m08:57:18.341490 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m08:57:18.343486 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:57:18.344483 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m08:57:18.344483 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m08:57:18.345480 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m08:57:19.236119 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-d303-1967-891d-40e2edddac02
[0m08:57:21.265559 [debug] [Thread-1 (]: SQL status: OK in 2.9200000762939453 seconds
[0m08:57:21.280516 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 08:57:18.295647 => 08:57:21.280516
[0m08:57:21.281514 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m08:57:21.281514 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:57:21.282511 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m08:57:21.282511 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-d303-1967-891d-40e2edddac02
[0m08:57:21.517076 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '38e83ff5-44b8-4811-81bf-7c503a636e4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6B2C89390>]}
[0m08:57:21.519069 [info ] [Thread-1 (]: 1 of 1 OK created sql view model default.stg_nascidos_vivos .................... [[32mOK[0m in 3.23s]
[0m08:57:21.521064 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m08:57:21.523058 [debug] [MainThread]: On master: ROLLBACK
[0m08:57:21.524056 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:57:22.359962 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07905-d4e4-1108-ab16-283ffde353ea
[0m08:57:22.360862 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:57:22.361897 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:57:22.361897 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:57:22.362890 [debug] [MainThread]: On master: ROLLBACK
[0m08:57:22.363857 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:57:22.363857 [debug] [MainThread]: On master: Close
[0m08:57:22.364885 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07905-d4e4-1108-ab16-283ffde353ea
[0m08:57:22.602206 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:57:22.604207 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m08:57:22.605202 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m08:57:22.605202 [debug] [MainThread]: Connection 'model.projeto_health_insights.stg_nascidos_vivos' was properly closed.
[0m08:57:22.608184 [info ] [MainThread]: 
[0m08:57:22.609244 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 17.92 seconds (17.92s).
[0m08:57:22.611271 [debug] [MainThread]: Command end result
[0m08:57:22.623237 [info ] [MainThread]: 
[0m08:57:22.624235 [info ] [MainThread]: [32mCompleted successfully[0m
[0m08:57:22.626202 [info ] [MainThread]: 
[0m08:57:22.627199 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m08:57:22.629193 [debug] [MainThread]: Command `dbt run` succeeded at 08:57:22.628264 after 20.39 seconds
[0m08:57:22.629193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A937E0D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A8FEA850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A2F7BB90>]}
[0m08:57:22.630191 [debug] [MainThread]: Flushing usage events
[0m08:58:07.858587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7945A8E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7947BA290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79483EC90>]}


============================== 08:58:07.863225 | a03190f5-fb2b-4015-b11f-86b940237686 ==============================
[0m08:58:07.863225 [info ] [MainThread]: Running with dbt=1.5.2
[0m08:58:07.864193 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m08:58:09.211306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7945E2390>]}
[0m08:58:09.232250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7945E2390>]}
[0m08:58:09.233248 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m08:58:09.251634 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m08:58:09.368322 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m08:58:09.368322 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m08:58:09.375303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79F133810>]}
[0m08:58:09.440131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79E064390>]}
[0m08:58:09.441141 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m08:58:09.441860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79B664A50>]}
[0m08:58:09.443885 [info ] [MainThread]: 
[0m08:58:09.445852 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m08:58:09.447774 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m08:58:09.448774 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m08:58:09.449771 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m08:58:09.449771 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:58:10.260003 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07905-f175-1a90-bc5c-8f1188db1e91
[0m08:58:10.608006 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m08:58:10.615581 [debug] [ThreadPool]: On list_workspace: Close
[0m08:58:10.616577 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07905-f175-1a90-bc5c-8f1188db1e91
[0m08:58:10.855334 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m08:58:10.857329 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m08:58:10.867299 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:10.868297 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m08:58:10.868297 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m08:58:10.869294 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m08:58:11.693979 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07905-f24a-182d-9d6c-fed2f0063aaf
[0m08:58:12.090821 [debug] [ThreadPool]: SQL status: OK in 1.2200000286102295 seconds
[0m08:58:12.092813 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m08:58:12.092813 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m08:58:12.093811 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m08:58:12.093811 [debug] [ThreadPool]: On create_workspace_default: Close
[0m08:58:12.094808 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07905-f24a-182d-9d6c-fed2f0063aaf
[0m08:58:12.337999 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m08:58:12.342988 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m08:58:12.343985 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m08:58:12.344983 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m08:58:13.131349 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07905-f32b-1fd1-b4ea-48631db0e47e
[0m08:58:13.486911 [debug] [ThreadPool]: SQL status: OK in 1.1399999856948853 seconds
[0m08:58:13.490872 [debug] [ThreadPool]: On list_workspace_default: Close
[0m08:58:13.490872 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07905-f32b-1fd1-b4ea-48631db0e47e
[0m08:58:13.734924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A793DBD390>]}
[0m08:58:13.735920 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:13.736926 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:58:13.737914 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m08:58:13.738797 [info ] [MainThread]: 
[0m08:58:13.747699 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m08:58:13.748698 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m08:58:13.750168 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m08:58:13.751140 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m08:58:13.754158 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m08:58:13.755154 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 08:58:13.751140 => 08:58:13.755154
[0m08:58:13.756152 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m08:58:13.797042 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m08:58:13.799009 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:13.799009 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m08:58:13.800021 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m08:58:13.801004 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m08:58:14.600355 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-f40b-1f12-8544-6faac5df0c52
[0m08:58:15.306777 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m08:58:15.321734 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 08:58:13.756152 => 08:58:15.321734
[0m08:58:15.321734 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m08:58:15.322731 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:58:15.323729 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m08:58:15.323729 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-f40b-1f12-8544-6faac5df0c52
[0m08:58:15.563238 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79F1CBA90>]}
[0m08:58:15.566261 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.81s]
[0m08:58:15.571221 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m08:58:15.573248 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m08:58:15.574218 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m08:58:15.575203 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m08:58:15.576230 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m08:58:15.581216 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m08:58:15.582184 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 08:58:15.577207 => 08:58:15.582184
[0m08:58:15.583209 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m08:58:15.590190 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m08:58:15.591189 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:15.592158 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m08:58:15.592158 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m08:58:15.593155 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:58:16.423110 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-f521-15a9-ba60-b48f20ae1555
[0m08:58:17.112638 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m08:58:17.115627 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 08:58:15.583209 => 08:58:17.115627
[0m08:58:17.115627 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m08:58:17.116624 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:58:17.117622 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m08:58:17.117622 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-f521-15a9-ba60-b48f20ae1555
[0m08:58:17.360009 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79F26D450>]}
[0m08:58:17.363003 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.78s]
[0m08:58:17.368027 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m08:58:17.370020 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m08:58:17.370999 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m08:58:17.373084 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m08:58:17.373084 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m08:58:17.378098 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m08:58:17.380065 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 08:58:17.374081 => 08:58:17.379095
[0m08:58:17.381064 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m08:58:17.385081 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m08:58:17.387049 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:17.387049 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m08:58:17.388065 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m08:58:17.388065 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:58:18.199098 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-f630-1ccf-8727-5151a5cc6494
[0m08:58:18.903807 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m08:58:18.906786 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 08:58:17.381064 => 08:58:18.905787
[0m08:58:18.906786 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m08:58:18.907763 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:58:18.907763 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m08:58:18.908760 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-f630-1ccf-8727-5151a5cc6494
[0m08:58:19.141442 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79F2EBF90>]}
[0m08:58:19.143436 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.77s]
[0m08:58:19.144807 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m08:58:19.145834 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m08:58:19.146807 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m08:58:19.148550 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m08:58:19.148550 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m08:58:19.152539 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m08:58:19.154534 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 08:58:19.149547 => 08:58:19.153536
[0m08:58:19.155532 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m08:58:19.160546 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m08:58:19.161516 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:19.162513 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m08:58:19.162513 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m08:58:19.163538 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:58:19.964729 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-f73d-1c08-b416-a499af46b728
[0m08:58:20.659484 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m08:58:20.669458 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 08:58:19.155532 => 08:58:20.668461
[0m08:58:20.671450 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m08:58:20.673445 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:58:20.674443 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m08:58:20.676437 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-f73d-1c08-b416-a499af46b728
[0m08:58:20.910995 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79F1DC5D0>]}
[0m08:58:20.912986 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.76s]
[0m08:58:20.913984 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m08:58:20.915012 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m08:58:20.915980 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m08:58:20.917850 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m08:58:20.917850 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m08:58:20.920842 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m08:58:20.922809 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 08:58:20.918820 => 08:58:20.921862
[0m08:58:20.922809 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m08:58:20.927823 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m08:58:20.928803 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:20.928803 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m08:58:20.929813 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m08:58:20.930788 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:58:21.762123 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-f850-12ec-95e1-d072569c7bad
[0m08:58:22.496077 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m08:58:22.498071 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 08:58:20.923839 => 08:58:22.498071
[0m08:58:22.499068 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m08:58:22.499068 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:58:22.500066 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m08:58:22.501063 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-f850-12ec-95e1-d072569c7bad
[0m08:58:22.739100 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79F26DD10>]}
[0m08:58:22.742062 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.82s]
[0m08:58:22.743052 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m08:58:22.744062 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m08:58:22.745043 [info ] [Thread-1 (]: 6 of 12 START sql view model default.dim_localidade ............................ [RUN]
[0m08:58:22.746054 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m08:58:22.747037 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m08:58:22.752026 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m08:58:22.755016 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 08:58:22.747037 => 08:58:22.754032
[0m08:58:22.755016 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m08:58:22.760002 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m08:58:22.761000 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:22.761997 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m08:58:22.761997 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m08:58:22.762994 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:58:23.561342 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-f962-1327-b839-a3dc5baf4d22
[0m08:58:24.283592 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m08:58:24.286581 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 08:58:22.756013 => 08:58:24.286581
[0m08:58:24.286581 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m08:58:24.287579 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:58:24.287579 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m08:58:24.288576 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-f962-1327-b839-a3dc5baf4d22
[0m08:58:24.519850 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79F1DC5D0>]}
[0m08:58:24.522843 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.dim_localidade ....................... [[32mOK[0m in 1.77s]
[0m08:58:24.527722 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m08:58:24.529725 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m08:58:24.530725 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m08:58:24.532675 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m08:58:24.533672 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m08:58:24.539659 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m08:58:24.540682 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 08:58:24.534671 => 08:58:24.540682
[0m08:58:24.541679 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m08:58:24.545668 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m08:58:24.546679 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:24.547639 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m08:58:24.547639 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m08:58:24.548664 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:58:25.357249 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-fa72-1239-a93d-0637c68f0ccb
[0m08:58:26.180986 [debug] [Thread-1 (]: SQL status: OK in 1.6299999952316284 seconds
[0m08:58:26.183984 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 08:58:24.541679 => 08:58:26.182987
[0m08:58:26.183984 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m08:58:26.184983 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:58:26.184983 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m08:58:26.185979 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-fa72-1239-a93d-0637c68f0ccb
[0m08:58:26.416215 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79F310C50>]}
[0m08:58:26.419209 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.88s]
[0m08:58:26.424194 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m08:58:26.426190 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m08:58:26.429181 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m08:58:26.433136 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.stg_tempo)
[0m08:58:26.433136 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m08:58:26.438122 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m08:58:26.439119 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 08:58:26.434134 => 08:58:26.439119
[0m08:58:26.440117 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m08:58:26.445103 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m08:58:26.447098 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:26.448095 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m08:58:26.448095 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m08:58:26.449092 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:58:27.272459 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-fb98-1e1a-9683-04f707af150f
[0m08:58:28.029230 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m08:58:28.032220 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 08:58:26.441117 => 08:58:28.032220
[0m08:58:28.032220 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m08:58:28.033217 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:58:28.034214 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m08:58:28.034214 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-fb98-1e1a-9683-04f707af150f
[0m08:58:28.267979 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79F332FD0>]}
[0m08:58:28.270967 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.84s]
[0m08:58:28.271589 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m08:58:28.272617 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m08:58:28.273586 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m08:58:28.274416 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m08:58:28.275444 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m08:58:28.279432 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m08:58:28.281400 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 08:58:28.275444 => 08:58:28.280403
[0m08:58:28.282404 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m08:58:28.287414 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m08:58:28.288382 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:28.288382 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m08:58:28.289380 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m08:58:28.290379 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:58:29.103692 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-fcb0-1cbe-bdcd-e6b9100c34d6
[0m08:58:29.878046 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m08:58:29.882035 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 08:58:28.283424 => 08:58:29.882035
[0m08:58:29.883032 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m08:58:29.883032 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:58:29.884029 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m08:58:29.884029 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-fcb0-1cbe-bdcd-e6b9100c34d6
[0m08:58:30.140169 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79DF61850>]}
[0m08:58:30.143151 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.87s]
[0m08:58:30.144121 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m08:58:30.144121 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m08:58:30.145118 [info ] [Thread-1 (]: 10 of 12 START sql table model default.dim_tempo ............................... [RUN]
[0m08:58:30.146116 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_tempo)
[0m08:58:30.147141 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m08:58:30.151130 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m08:58:30.153149 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 08:58:30.147141 => 08:58:30.153149
[0m08:58:30.154122 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m08:58:30.172046 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:30.173044 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m08:58:30.173044 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */

      describe extended `workspace`.`default`.`dim_tempo`
  
[0m08:58:30.174044 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:58:30.968354 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-fdcd-1f6f-a616-ef9191d6246e
[0m08:58:31.555777 [debug] [Thread-1 (]: SQL status: OK in 1.3799999952316284 seconds
[0m08:58:31.559183 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 08:58:30.155094 => 08:58:31.559183
[0m08:58:31.560180 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m08:58:31.560180 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:58:31.561181 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m08:58:31.562175 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-fdcd-1f6f-a616-ef9191d6246e
[0m08:58:31.835328 [debug] [Thread-1 (]: Runtime Error in model dim_tempo (models\marts\dim_tempo.sql)
  dbt could not find a macro with the name "drop_relation" in any package
[0m08:58:31.835328 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79F339010>]}
[0m08:58:31.836325 [error] [Thread-1 (]: 10 of 12 ERROR creating sql table model default.dim_tempo ...................... [[31mERROR[0m in 1.69s]
[0m08:58:31.837321 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m08:58:31.838344 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m08:58:31.839317 [info ] [Thread-1 (]: 11 of 12 START sql view model default.int_nascimento ........................... [RUN]
[0m08:58:31.839878 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.int_nascimento)
[0m08:58:31.840909 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m08:58:31.844895 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m08:58:31.846862 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 08:58:31.840909 => 08:58:31.845892
[0m08:58:31.846862 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m08:58:31.850878 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m08:58:31.852846 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:31.852846 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m08:58:31.853855 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

SELECT
    n.nascimento_id,
    t.data_id,
    n.municipio,
    n.uf,
    n.sexo,
    n.peso,
    n.idade_mae,
    n.gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m08:58:31.854840 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m08:58:32.662551 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07905-fece-1cd0-ba0b-924968bd83fe
[0m08:58:33.330891 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

SELECT
    n.nascimento_id,
    t.data_id,
    n.municipio,
    n.uf,
    n.sexo,
    n.peso,
    n.idade_mae,
    n.gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m08:58:33.331885 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`municipio` cannot be resolved. Did you mean one of the following? [`n`.`cod_municipio`, `t`.`dia`, `n`.`peso`, `n`.`APGAR1`, `n`.`APGAR5`]. SQLSTATE: 42703; line 11 pos 4
[0m08:58:33.332882 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`municipio` cannot be resolved. Did you mean one of the following? [`n`.`cod_municipio`, `t`.`dia`, `n`.`peso`, `n`.`APGAR1`, `n`.`APGAR5`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`municipio` cannot be resolved. Did you mean one of the following? [`n`.`cod_municipio`, `t`.`dia`, `n`.`peso`, `n`.`APGAR1`, `n`.`APGAR5`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m08:58:33.333883 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07905-fef0-158d-a790-8a1c03da9634
[0m08:58:33.334877 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 08:58:31.847887 => 08:58:33.333883
[0m08:58:33.334877 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m08:58:33.335874 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m08:58:33.335874 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m08:58:33.336871 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07905-fece-1cd0-ba0b-924968bd83fe
[0m08:58:33.592673 [debug] [Thread-1 (]: Runtime Error in model int_nascimento (models\intermediate\int_nascimento.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`municipio` cannot be resolved. Did you mean one of the following? [`n`.`cod_municipio`, `t`.`dia`, `n`.`peso`, `n`.`APGAR1`, `n`.`APGAR5`]. SQLSTATE: 42703; line 11 pos 4
[0m08:58:33.593669 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a03190f5-fb2b-4015-b11f-86b940237686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79E059A10>]}
[0m08:58:33.595665 [error] [Thread-1 (]: 11 of 12 ERROR creating sql view model default.int_nascimento .................. [[31mERROR[0m in 1.75s]
[0m08:58:33.597683 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m08:58:33.599677 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m08:58:33.600701 [info ] [Thread-1 (]: 12 of 12 SKIP relation default.fato_nascimento ................................. [[33mSKIP[0m]
[0m08:58:33.602669 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m08:58:33.606658 [debug] [MainThread]: On master: ROLLBACK
[0m08:58:33.607684 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:58:34.395479 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07905-ffd8-1ac8-a684-2b110925786d
[0m08:58:34.397474 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:58:34.397474 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m08:58:34.398471 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m08:58:34.399469 [debug] [MainThread]: On master: ROLLBACK
[0m08:58:34.399469 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m08:58:34.400466 [debug] [MainThread]: On master: Close
[0m08:58:34.400466 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07905-ffd8-1ac8-a684-2b110925786d
[0m08:58:34.631758 [debug] [MainThread]: Connection 'master' was properly closed.
[0m08:58:34.633760 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m08:58:34.633760 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m08:58:34.634745 [debug] [MainThread]: Connection 'model.projeto_health_insights.int_nascimento' was properly closed.
[0m08:58:34.635743 [info ] [MainThread]: 
[0m08:58:34.636733 [info ] [MainThread]: Finished running 10 view models, 2 table models in 0 hours 0 minutes and 25.19 seconds (25.19s).
[0m08:58:34.639730 [debug] [MainThread]: Command end result
[0m08:58:34.651700 [info ] [MainThread]: 
[0m08:58:34.652668 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m08:58:34.653666 [info ] [MainThread]: 
[0m08:58:34.655662 [error] [MainThread]: [33mRuntime Error in model dim_tempo (models\marts\dim_tempo.sql)[0m
[0m08:58:34.656659 [error] [MainThread]:   dbt could not find a macro with the name "drop_relation" in any package
[0m08:58:34.657655 [info ] [MainThread]: 
[0m08:58:34.658600 [error] [MainThread]: [33mRuntime Error in model int_nascimento (models\intermediate\int_nascimento.sql)[0m
[0m08:58:34.659623 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`municipio` cannot be resolved. Did you mean one of the following? [`n`.`cod_municipio`, `t`.`dia`, `n`.`peso`, `n`.`APGAR1`, `n`.`APGAR5`]. SQLSTATE: 42703; line 11 pos 4
[0m08:58:34.660598 [info ] [MainThread]: 
[0m08:58:34.662598 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=2 SKIP=1 TOTAL=12
[0m08:58:34.663590 [debug] [MainThread]: Command `dbt run` failed at 08:58:34.663590 after 26.82 seconds
[0m08:58:34.663590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79459E310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7945AAD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A79485F750>]}
[0m08:58:34.664615 [debug] [MainThread]: Flushing usage events
[0m09:00:16.916893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002699FC6A250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002699FA4DF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002699FAF81D0>]}


============================== 09:00:16.921903 | 2cde7370-055d-4cd8-bead-f5b109e15cd5 ==============================
[0m09:00:16.921903 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:00:16.922876 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:00:18.229183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A9578BD0>]}
[0m09:00:18.247132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A9578F50>]}
[0m09:00:18.248130 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m09:00:18.267052 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:00:18.366761 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:00:18.367750 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\intermediate\int_nascimento.sql
[0m09:00:18.394665 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m09:00:18.472485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269AA604150>]}
[0m09:00:18.485422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A958FE10>]}
[0m09:00:18.486422 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m09:00:18.487121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A68B1E10>]}
[0m09:00:18.489119 [info ] [MainThread]: 
[0m09:00:18.490374 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:00:18.493368 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m09:00:18.494365 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m09:00:18.494365 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m09:00:18.495362 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:00:19.300976 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07906-3e5d-1b81-9697-e00b8d342a1a
[0m09:00:19.628892 [debug] [ThreadPool]: SQL status: OK in 1.1299999952316284 seconds
[0m09:00:19.632585 [debug] [ThreadPool]: On list_workspace: Close
[0m09:00:19.632585 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07906-3e5d-1b81-9697-e00b8d342a1a
[0m09:00:19.851292 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m09:00:19.852289 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m09:00:19.862292 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:19.863264 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m09:00:19.863264 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m09:00:19.864271 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:00:20.679532 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07906-3f2f-14da-a737-e570bc739b55
[0m09:00:21.193894 [debug] [ThreadPool]: SQL status: OK in 1.3300000429153442 seconds
[0m09:00:21.195910 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m09:00:21.195910 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m09:00:21.196916 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m09:00:21.196916 [debug] [ThreadPool]: On create_workspace_default: Close
[0m09:00:21.197906 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07906-3f2f-14da-a737-e570bc739b55
[0m09:00:21.444759 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:00:21.449748 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:00:21.450745 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m09:00:21.450745 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:00:22.222415 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07906-401c-12cb-b096-77e249240c3e
[0m09:00:22.634380 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m09:00:22.638400 [debug] [ThreadPool]: On list_workspace_default: Close
[0m09:00:22.639367 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07906-401c-12cb-b096-77e249240c3e
[0m09:00:22.874313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002699F26D210>]}
[0m09:00:22.875340 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:22.876337 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:00:22.878335 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:00:22.880297 [info ] [MainThread]: 
[0m09:00:22.894183 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m09:00:22.896148 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m09:00:22.900134 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m09:00:22.901130 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m09:00:22.909112 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m09:00:22.913101 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 09:00:22.902129 => 09:00:22.911135
[0m09:00:22.915095 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m09:00:22.954986 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m09:00:22.956993 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:22.956993 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m09:00:22.957978 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m09:00:22.957978 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:00:23.750470 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-4104-1b19-aae9-7da879bf9d3b
[0m09:00:25.839463 [debug] [Thread-1 (]: SQL status: OK in 2.880000114440918 seconds
[0m09:00:25.854423 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 09:00:22.916090 => 09:00:25.854423
[0m09:00:25.855421 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m09:00:25.856418 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:00:25.856418 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m09:00:25.857415 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-4104-1b19-aae9-7da879bf9d3b
[0m09:00:26.094675 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269A9563D10>]}
[0m09:00:26.095673 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 3.20s]
[0m09:00:26.096826 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m09:00:26.097854 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m09:00:26.098859 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m09:00:26.099859 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m09:00:26.100885 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m09:00:26.103876 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m09:00:26.104846 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 09:00:26.100885 => 09:00:26.104846
[0m09:00:26.105843 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m09:00:26.109860 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m09:00:26.111826 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:26.111826 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m09:00:26.112824 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m09:00:26.112824 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:00:26.918075 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-42e8-171c-a8b2-656e9a47977e
[0m09:00:27.644894 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m09:00:27.646889 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 09:00:26.105843 => 09:00:27.646889
[0m09:00:27.647886 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m09:00:27.648883 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:00:27.648883 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m09:00:27.649880 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-42e8-171c-a8b2-656e9a47977e
[0m09:00:27.874736 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269AA5BC350>]}
[0m09:00:27.876731 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.77s]
[0m09:00:27.880209 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m09:00:27.882205 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m09:00:27.883200 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m09:00:27.885588 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m09:00:27.886573 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m09:00:27.890590 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m09:00:27.892557 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 09:00:27.886573 => 09:00:27.891587
[0m09:00:27.892557 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m09:00:27.899565 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m09:00:27.900535 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:27.901533 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m09:00:27.902531 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m09:00:27.903541 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:00:28.691824 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-43f7-1507-9b8b-20f1c3481d48
[0m09:00:29.405796 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m09:00:29.408785 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 09:00:27.893582 => 09:00:29.408785
[0m09:00:29.409784 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m09:00:29.409784 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:00:29.410780 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m09:00:29.410780 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-43f7-1507-9b8b-20f1c3481d48
[0m09:00:29.652055 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002699FAF8090>]}
[0m09:00:29.655049 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.77s]
[0m09:00:29.659994 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m09:00:29.661989 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:00:29.662970 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m09:00:29.664929 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m09:00:29.665937 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m09:00:29.671906 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:00:29.672903 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 09:00:29.665937 => 09:00:29.672903
[0m09:00:29.673901 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m09:00:29.677890 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:00:29.678887 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:29.679885 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:00:29.679885 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m09:00:29.680882 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:00:30.499853 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-450b-1755-8626-be14cab1d3a7
[0m09:00:31.256444 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m09:00:31.259432 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 09:00:29.673901 => 09:00:31.258432
[0m09:00:31.259432 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m09:00:31.260427 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:00:31.260427 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m09:00:31.261424 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-450b-1755-8626-be14cab1d3a7
[0m09:00:31.496609 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269AA7C9F10>]}
[0m09:00:31.498602 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.83s]
[0m09:00:31.499702 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:00:31.500729 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m09:00:31.501701 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m09:00:31.502571 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m09:00:31.503598 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m09:00:31.507592 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m09:00:31.508557 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 09:00:31.504568 => 09:00:31.508557
[0m09:00:31.509577 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m09:00:31.514542 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m09:00:31.516536 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:31.517534 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m09:00:31.517534 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m09:00:31.518558 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:00:32.316602 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-4621-1366-91b9-5d6d5d10d1de
[0m09:00:33.030638 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m09:00:33.033637 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 09:00:31.510552 => 09:00:33.033637
[0m09:00:33.034630 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m09:00:33.035602 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:00:33.035602 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m09:00:33.036598 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-4621-1366-91b9-5d6d5d10d1de
[0m09:00:33.276907 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269AA6B0D50>]}
[0m09:00:33.277902 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.77s]
[0m09:00:33.280067 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m09:00:33.281067 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m09:00:33.282062 [info ] [Thread-1 (]: 6 of 12 START sql view model default.dim_localidade ............................ [RUN]
[0m09:00:33.284059 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m09:00:33.284059 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m09:00:33.292005 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m09:00:33.293014 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 09:00:33.285059 => 09:00:33.293014
[0m09:00:33.293999 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m09:00:33.298016 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m09:00:33.298998 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:33.299983 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m09:00:33.299983 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m09:00:33.301009 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:00:34.097280 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-4730-1b41-99b2-dc7fae1615b4
[0m09:00:34.771422 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:00:34.774413 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 09:00:33.293999 => 09:00:34.774413
[0m09:00:34.774413 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m09:00:34.775410 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:00:34.776408 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m09:00:34.777405 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-4730-1b41-99b2-dc7fae1615b4
[0m09:00:35.011240 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269AA64E590>]}
[0m09:00:35.013240 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.dim_localidade ....................... [[32mOK[0m in 1.73s]
[0m09:00:35.014600 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m09:00:35.015630 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m09:00:35.016525 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m09:00:35.017525 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m09:00:35.018553 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m09:00:35.022540 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m09:00:35.024505 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 09:00:35.018553 => 09:00:35.023520
[0m09:00:35.024505 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m09:00:35.029494 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m09:00:35.031487 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:35.031487 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m09:00:35.032485 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m09:00:35.032485 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:00:35.864419 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-483d-14be-9182-c26d8fcaccc9
[0m09:00:36.643892 [debug] [Thread-1 (]: SQL status: OK in 1.6100000143051147 seconds
[0m09:00:36.646883 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 09:00:35.025503 => 09:00:36.646883
[0m09:00:36.647859 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m09:00:36.647859 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:00:36.648885 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m09:00:36.649879 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-483d-14be-9182-c26d8fcaccc9
[0m09:00:36.888482 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269AA605790>]}
[0m09:00:36.889476 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.87s]
[0m09:00:36.891442 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m09:00:36.891442 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m09:00:36.892782 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m09:00:36.894886 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.stg_tempo)
[0m09:00:36.894886 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m09:00:36.900852 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m09:00:36.902835 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 09:00:36.895883 => 09:00:36.901838
[0m09:00:36.902835 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m09:00:36.908819 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m09:00:36.909817 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:36.910814 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m09:00:36.911813 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m09:00:36.912837 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:00:37.727846 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-495a-15ad-9853-d10581727caa
[0m09:00:38.601921 [debug] [Thread-1 (]: SQL status: OK in 1.690000057220459 seconds
[0m09:00:38.604913 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 09:00:36.903833 => 09:00:38.604913
[0m09:00:38.605910 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m09:00:38.605910 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:00:38.606908 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m09:00:38.607905 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-495a-15ad-9853-d10581727caa
[0m09:00:38.824576 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269AA813F10>]}
[0m09:00:38.825571 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.93s]
[0m09:00:38.826683 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m09:00:38.827713 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:00:38.829679 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m09:00:38.830676 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m09:00:38.831703 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:00:38.837686 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:00:38.839653 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 09:00:38.832700 => 09:00:38.838655
[0m09:00:38.840650 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:00:38.846635 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:00:38.848627 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:38.848627 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:00:38.849624 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m09:00:38.849624 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:00:39.661029 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-4a82-1cca-80d6-e621080f7bb8
[0m09:00:40.477455 [debug] [Thread-1 (]: SQL status: OK in 1.6299999952316284 seconds
[0m09:00:40.479477 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 09:00:38.841646 => 09:00:40.479477
[0m09:00:40.480468 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m09:00:40.480468 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:00:40.481466 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m09:00:40.482435 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-4a82-1cca-80d6-e621080f7bb8
[0m09:00:40.710721 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269AA808CD0>]}
[0m09:00:40.712711 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.88s]
[0m09:00:40.713718 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:00:40.714735 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m09:00:40.715732 [info ] [Thread-1 (]: 10 of 12 START sql table model default.dim_tempo ............................... [RUN]
[0m09:00:40.716262 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_tempo)
[0m09:00:40.717290 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m09:00:40.722249 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m09:00:40.723275 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 09:00:40.717290 => 09:00:40.723275
[0m09:00:40.724244 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m09:00:40.739203 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:40.740200 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m09:00:40.740200 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */

      describe extended `workspace`.`default`.`dim_tempo`
  
[0m09:00:40.741198 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:00:41.564235 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-4ba3-1aa6-a472-bc6e31068dd3
[0m09:00:42.053183 [debug] [Thread-1 (]: SQL status: OK in 1.309999942779541 seconds
[0m09:00:42.056635 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 09:00:40.724244 => 09:00:42.056635
[0m09:00:42.057635 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m09:00:42.057635 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:00:42.058630 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m09:00:42.059626 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-4ba3-1aa6-a472-bc6e31068dd3
[0m09:00:42.306399 [debug] [Thread-1 (]: Runtime Error in model dim_tempo (models\marts\dim_tempo.sql)
  dbt could not find a macro with the name "drop_relation" in any package
[0m09:00:42.307396 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002699F5722D0>]}
[0m09:00:42.308394 [error] [Thread-1 (]: 10 of 12 ERROR creating sql table model default.dim_tempo ...................... [[31mERROR[0m in 1.59s]
[0m09:00:42.308942 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m09:00:42.309943 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m09:00:42.310940 [info ] [Thread-1 (]: 11 of 12 START sql view model default.int_nascimento ........................... [RUN]
[0m09:00:42.312677 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.int_nascimento)
[0m09:00:42.313674 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m09:00:42.317690 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m09:00:42.318659 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 09:00:42.313674 => 09:00:42.318659
[0m09:00:42.319685 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m09:00:42.325672 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m09:00:42.326667 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:42.327637 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m09:00:42.327637 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

SELECT
    n.nascimento_id,
    t.data_id,
    n.cod_municipio,
    n.uf,
    n.sexo,
    n.peso,
    n.idade_mae,
    n.gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m09:00:42.328662 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:00:43.136462 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-4c94-1a47-b138-f71f8f117dfa
[0m09:00:43.584058 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

SELECT
    n.nascimento_id,
    t.data_id,
    n.cod_municipio,
    n.uf,
    n.sexo,
    n.peso,
    n.idade_mae,
    n.gestacao_semanas
FROM `workspace`.`default`.`stg_nascidos_vivos` n
LEFT JOIN `workspace`.`default`.`stg_tempo` t
    ON n.data_nascimento = t.data

[0m09:00:43.586056 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`uf` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`dia`, `t`.`mes`, `n`.`peso`, `n`.`sexo`]. SQLSTATE: 42703; line 12 pos 4
[0m09:00:43.587046 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`uf` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`dia`, `t`.`mes`, `n`.`peso`, `n`.`sexo`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`uf` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`dia`, `t`.`mes`, `n`.`peso`, `n`.`sexo`]. SQLSTATE: 42703; line 12 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m09:00:43.587046 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07906-4cb4-1441-af61-e76c62b2e105
[0m09:00:43.588044 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 09:00:42.319685 => 09:00:43.588044
[0m09:00:43.589041 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m09:00:43.589041 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:00:43.590039 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m09:00:43.590039 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-4c94-1a47-b138-f71f8f117dfa
[0m09:00:43.840194 [debug] [Thread-1 (]: Runtime Error in model int_nascimento (models\intermediate\int_nascimento.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`uf` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`dia`, `t`.`mes`, `n`.`peso`, `n`.`sexo`]. SQLSTATE: 42703; line 12 pos 4
[0m09:00:43.841192 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2cde7370-055d-4cd8-bead-f5b109e15cd5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269AA5AFF50>]}
[0m09:00:43.842190 [error] [Thread-1 (]: 11 of 12 ERROR creating sql view model default.int_nascimento .................. [[31mERROR[0m in 1.53s]
[0m09:00:43.844185 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m09:00:43.845182 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m09:00:43.846211 [info ] [Thread-1 (]: 12 of 12 SKIP relation default.fato_nascimento ................................. [[33mSKIP[0m]
[0m09:00:43.848203 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m09:00:43.850199 [debug] [MainThread]: On master: ROLLBACK
[0m09:00:43.851195 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:00:44.638763 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07906-4d78-1b22-b019-e6e157d85aa0
[0m09:00:44.642229 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:00:44.644232 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:00:44.645231 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:00:44.646216 [debug] [MainThread]: On master: ROLLBACK
[0m09:00:44.646216 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:00:44.647183 [debug] [MainThread]: On master: Close
[0m09:00:44.647183 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07906-4d78-1b22-b019-e6e157d85aa0
[0m09:00:44.884307 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:00:44.885304 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m09:00:44.886307 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:00:44.888281 [debug] [MainThread]: Connection 'model.projeto_health_insights.int_nascimento' was properly closed.
[0m09:00:44.891294 [info ] [MainThread]: 
[0m09:00:44.894250 [info ] [MainThread]: Finished running 10 view models, 2 table models in 0 hours 0 minutes and 26.40 seconds (26.40s).
[0m09:00:44.902254 [debug] [MainThread]: Command end result
[0m09:00:44.914196 [info ] [MainThread]: 
[0m09:00:44.915193 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m09:00:44.917187 [info ] [MainThread]: 
[0m09:00:44.918184 [error] [MainThread]: [33mRuntime Error in model dim_tempo (models\marts\dim_tempo.sql)[0m
[0m09:00:44.920180 [error] [MainThread]:   dbt could not find a macro with the name "drop_relation" in any package
[0m09:00:44.921463 [info ] [MainThread]: 
[0m09:00:44.922472 [error] [MainThread]: [33mRuntime Error in model int_nascimento (models\intermediate\int_nascimento.sql)[0m
[0m09:00:44.923460 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `n`.`uf` cannot be resolved. Did you mean one of the following? [`t`.`ano`, `t`.`dia`, `t`.`mes`, `n`.`peso`, `n`.`sexo`]. SQLSTATE: 42703; line 12 pos 4
[0m09:00:44.924461 [info ] [MainThread]: 
[0m09:00:44.925464 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=2 SKIP=1 TOTAL=12
[0m09:00:44.926480 [debug] [MainThread]: Command `dbt run` failed at 09:00:44.926480 after 28.03 seconds
[0m09:00:44.927450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002699FAEF3D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002699FA4F410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002699D8C0B50>]}
[0m09:00:44.928447 [debug] [MainThread]: Flushing usage events
[0m09:02:38.792940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79163D550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7918EE0D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79163ED90>]}


============================== 09:02:38.797954 | ecd2c00f-68ee-40c8-8dc4-0012bfc1f083 ==============================
[0m09:02:38.797954 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:02:38.798925 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:02:40.201437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79162E390>]}
[0m09:02:40.220389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79162E390>]}
[0m09:02:40.221385 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m09:02:40.239339 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:02:40.358600 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:02:40.359599 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\intermediate\int_nascimento.sql
[0m09:02:40.388549 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m09:02:40.474292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E791169010>]}
[0m09:02:40.488254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79B159350>]}
[0m09:02:40.488254 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m09:02:40.489251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E78E644910>]}
[0m09:02:40.492271 [info ] [MainThread]: 
[0m09:02:40.494032 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:02:40.496559 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m09:02:40.496559 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m09:02:40.497556 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m09:02:40.497556 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:02:41.363327 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07906-930a-10cf-adcf-5359205a4d32
[0m09:02:41.869395 [debug] [ThreadPool]: SQL status: OK in 1.3700000047683716 seconds
[0m09:02:41.872752 [debug] [ThreadPool]: On list_workspace: Close
[0m09:02:41.873749 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07906-930a-10cf-adcf-5359205a4d32
[0m09:02:42.100108 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m09:02:42.104096 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m09:02:42.114066 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:02:42.114066 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m09:02:42.115063 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m09:02:42.115063 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:02:42.885038 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07906-93f3-1970-9d02-cd847c252f31
[0m09:02:43.292508 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m09:02:43.294503 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m09:02:43.294503 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m09:02:43.295531 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m09:02:43.296504 [debug] [ThreadPool]: On create_workspace_default: Close
[0m09:02:43.296504 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07906-93f3-1970-9d02-cd847c252f31
[0m09:02:43.536798 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:02:43.541787 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:02:43.542784 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m09:02:43.543800 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:02:44.347340 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07906-94d1-13e7-a66f-a8fdff916d62
[0m09:02:44.753754 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m09:02:44.757744 [debug] [ThreadPool]: On list_workspace_default: Close
[0m09:02:44.758769 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07906-94d1-13e7-a66f-a8fdff916d62
[0m09:02:44.995059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79B160650>]}
[0m09:02:44.998051 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:02:44.999047 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:02:44.999870 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:02:45.000895 [info ] [MainThread]: 
[0m09:02:45.007213 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m09:02:45.009208 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m09:02:45.011202 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m09:02:45.012201 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m09:02:45.016217 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m09:02:45.017214 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 09:02:45.013211 => 09:02:45.017214
[0m09:02:45.018212 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m09:02:45.052121 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m09:02:45.053090 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:02:45.054119 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m09:02:45.055085 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m09:02:45.055085 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:02:45.860500 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-95b9-1c2e-9439-aff4197e129c
[0m09:02:46.603955 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m09:02:46.619910 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 09:02:45.019189 => 09:02:46.619910
[0m09:02:46.620880 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m09:02:46.620880 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:02:46.621903 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m09:02:46.621903 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-95b9-1c2e-9439-aff4197e129c
[0m09:02:46.863396 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79B0DCF50>]}
[0m09:02:46.864394 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.85s]
[0m09:02:46.865391 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m09:02:46.866388 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m09:02:46.867385 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m09:02:46.869380 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m09:02:46.869380 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m09:02:46.873369 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m09:02:46.874367 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 09:02:46.870377 => 09:02:46.874367
[0m09:02:46.875364 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m09:02:46.879353 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m09:02:46.880350 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:02:46.881348 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m09:02:46.881348 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m09:02:46.882345 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:02:47.692253 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-96d1-184f-9390-d6a3d987da8b
[0m09:02:48.474294 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m09:02:48.477283 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 09:02:46.875364 => 09:02:48.477283
[0m09:02:48.477283 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m09:02:48.478280 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:02:48.479278 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m09:02:48.479278 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-96d1-184f-9390-d6a3d987da8b
[0m09:02:48.720447 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E791F49310>]}
[0m09:02:48.721458 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.85s]
[0m09:02:48.722438 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m09:02:48.723457 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m09:02:48.724492 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m09:02:48.725492 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m09:02:48.726490 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m09:02:48.730510 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m09:02:48.731477 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 09:02:48.727487 => 09:02:48.731477
[0m09:02:48.732495 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m09:02:48.739482 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m09:02:48.740452 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:02:48.740452 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m09:02:48.741450 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m09:02:48.742447 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:02:49.552920 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-97ed-1110-bb27-bb8d51feedae
[0m09:02:50.287736 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m09:02:50.292722 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 09:02:48.733471 => 09:02:50.292722
[0m09:02:50.294713 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m09:02:50.295710 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:02:50.296681 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m09:02:50.297678 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-97ed-1110-bb27-bb8d51feedae
[0m09:02:50.540814 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E791661150>]}
[0m09:02:50.542804 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.81s]
[0m09:02:50.543404 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m09:02:50.544430 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:02:50.545400 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m09:02:50.547397 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m09:02:50.548396 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m09:02:50.552384 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:02:50.554377 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 09:02:50.548396 => 09:02:50.553408
[0m09:02:50.554377 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m09:02:50.559391 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:02:50.560361 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:02:50.561386 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:02:50.562356 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m09:02:50.562356 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:02:51.428662 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-9909-15dc-a2f3-6254b40125b2
[0m09:02:52.126585 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m09:02:52.128579 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 09:02:50.555375 => 09:02:52.128579
[0m09:02:52.129577 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m09:02:52.130574 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:02:52.130574 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m09:02:52.131571 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-9909-15dc-a2f3-6254b40125b2
[0m09:02:52.391668 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79C3383D0>]}
[0m09:02:52.393661 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.84s]
[0m09:02:52.395042 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:02:52.396071 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m09:02:52.397043 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m09:02:52.399042 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m09:02:52.399042 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m09:02:52.404001 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m09:02:52.405997 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 09:02:52.399042 => 09:02:52.405010
[0m09:02:52.405997 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m09:02:52.411010 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m09:02:52.412006 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:02:52.412977 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m09:02:52.412977 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m09:02:52.413974 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:02:53.303559 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-9a29-180a-9386-a5405f3d8f73
[0m09:02:53.964502 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m09:02:53.967494 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 09:02:52.407020 => 09:02:53.966497
[0m09:02:53.967494 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m09:02:53.968492 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:02:53.968492 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m09:02:53.969489 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-9a29-180a-9386-a5405f3d8f73
[0m09:02:54.210266 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79C3383D0>]}
[0m09:02:54.212263 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.81s]
[0m09:02:54.214280 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m09:02:54.214280 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m09:02:54.215297 [info ] [Thread-1 (]: 6 of 12 START sql view model default.dim_localidade ............................ [RUN]
[0m09:02:54.216177 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m09:02:54.217206 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m09:02:54.221194 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m09:02:54.222164 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 09:02:54.217206 => 09:02:54.222164
[0m09:02:54.223190 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m09:02:54.227178 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m09:02:54.228148 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:02:54.229145 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m09:02:54.229145 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m09:02:54.230160 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:02:55.048236 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-9b34-174f-9a3d-b4172c638391
[0m09:02:55.701422 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:02:55.710397 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 09:02:54.223190 => 09:02:55.710397
[0m09:02:55.712391 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m09:02:55.714385 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:02:55.715381 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m09:02:55.716379 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-9b34-174f-9a3d-b4172c638391
[0m09:02:56.011743 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79C275C90>]}
[0m09:02:56.014732 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.dim_localidade ....................... [[32mOK[0m in 1.80s]
[0m09:02:56.015240 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m09:02:56.016268 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m09:02:56.017248 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m09:02:56.017971 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m09:02:56.018999 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m09:02:56.022987 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m09:02:56.024959 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 09:02:56.018999 => 09:02:56.023984
[0m09:02:56.024959 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m09:02:56.029968 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m09:02:56.030961 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:02:56.030961 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m09:02:56.031936 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m09:02:56.032933 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:02:56.837641 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-9c44-1112-85de-1b6b69583b96
[0m09:02:57.572390 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m09:02:57.575380 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 09:02:56.025977 => 09:02:57.575380
[0m09:02:57.575380 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m09:02:57.576377 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:02:57.577374 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m09:02:57.577374 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-9c44-1112-85de-1b6b69583b96
[0m09:02:57.823447 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79C3383D0>]}
[0m09:02:57.824443 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.81s]
[0m09:02:57.824980 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m09:02:57.826007 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m09:02:57.826990 [info ] [Thread-1 (]: 8 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m09:02:57.827981 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m09:02:57.828999 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m09:02:57.832991 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m09:02:57.833958 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 09:02:57.828999 => 09:02:57.833958
[0m09:02:57.834990 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m09:02:57.839969 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m09:02:57.841939 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:02:57.841939 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m09:02:57.842938 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    WITH base AS (
    SELECT
        row_number() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
        DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
        CODMUNNATU AS cod_municipio,
        CODUFNATU AS uf,
        SEXO AS sexo,
        PESO AS peso,
        IDADEMAE AS idade_mae
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT *
FROM base

[0m09:02:57.842938 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:02:58.652941 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-9d59-1e6c-81e8-48db9267178d
[0m09:02:59.043609 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    WITH base AS (
    SELECT
        row_number() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
        DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
        CODMUNNATU AS cod_municipio,
        CODUFNATU AS uf,
        SEXO AS sexo,
        PESO AS peso,
        IDADEMAE AS idade_mae
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT *
FROM base

[0m09:02:59.044602 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 8 pos 36
[0m09:02:59.045599 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 8 pos 36
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 8 pos 36
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m09:02:59.046611 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07906-9d7b-1ae4-b24f-de5714c6d41d
[0m09:02:59.047594 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 09:02:57.834990 => 09:02:59.046611
[0m09:02:59.047594 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m09:02:59.048591 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:02:59.048591 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m09:02:59.049597 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-9d59-1e6c-81e8-48db9267178d
[0m09:02:59.291228 [debug] [Thread-1 (]: Runtime Error in model int_nascimento (models\intermediate\int_nascimento.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 8 pos 36
[0m09:02:59.292231 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79C3D5990>]}
[0m09:02:59.293222 [error] [Thread-1 (]: 8 of 12 ERROR creating sql view model default.int_nascimento ................... [[31mERROR[0m in 1.46s]
[0m09:02:59.293706 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m09:02:59.294733 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m09:02:59.295702 [info ] [Thread-1 (]: 9 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m09:02:59.296406 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m09:02:59.297433 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m09:02:59.301422 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m09:02:59.303389 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 09:02:59.298403 => 09:02:59.302419
[0m09:02:59.304387 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m09:02:59.309401 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m09:02:59.310371 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:02:59.311369 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m09:02:59.312366 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m09:02:59.312366 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:03:00.157981 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-9e3e-19bd-bd45-67193fd9599e
[0m09:03:00.955344 [debug] [Thread-1 (]: SQL status: OK in 1.6399999856948853 seconds
[0m09:03:00.958333 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 09:02:59.304387 => 09:03:00.958333
[0m09:03:00.958333 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m09:03:00.959331 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:03:00.959331 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m09:03:00.960328 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-9e3e-19bd-bd45-67193fd9599e
[0m09:03:01.187836 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79C3DE750>]}
[0m09:03:01.188833 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.89s]
[0m09:03:01.189358 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m09:03:01.190388 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:03:01.191298 [info ] [Thread-1 (]: 10 of 12 START sql view model default.fato_atendimento_hospitalar .............. [RUN]
[0m09:03:01.192310 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m09:03:01.193324 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:03:01.198282 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:03:01.199280 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 09:03:01.193324 => 09:03:01.199280
[0m09:03:01.200277 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:03:01.204265 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:03:01.205262 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:03:01.206259 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:03:01.206259 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m09:03:01.207257 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:03:01.997638 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-9f58-1ca4-9f5a-9f22bbc5cf93
[0m09:03:02.777109 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m09:03:02.781095 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 09:03:01.200277 => 09:03:02.781095
[0m09:03:02.782092 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m09:03:02.783090 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:03:02.784087 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m09:03:02.785084 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-9f58-1ca4-9f5a-9f22bbc5cf93
[0m09:03:03.021663 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79C3D5C50>]}
[0m09:03:03.023655 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.fato_atendimento_hospitalar ......... [[32mOK[0m in 1.83s]
[0m09:03:03.024502 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:03:03.025531 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m09:03:03.026506 [info ] [Thread-1 (]: 11 of 12 SKIP relation default.fato_nascimento ................................. [[33mSKIP[0m]
[0m09:03:03.027203 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m09:03:03.028241 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m09:03:03.028241 [info ] [Thread-1 (]: 12 of 12 START sql table model default.dim_tempo ............................... [RUN]
[0m09:03:03.030197 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_tempo)
[0m09:03:03.031226 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m09:03:03.035211 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m09:03:03.037201 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 09:03:03.031226 => 09:03:03.036209
[0m09:03:03.037201 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m09:03:03.054159 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:03:03.055140 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m09:03:03.055140 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */

      describe extended `workspace`.`default`.`dim_tempo`
  
[0m09:03:03.056128 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:03:03.881246 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07906-a076-1601-89c4-3ef25593dc80
[0m09:03:04.337076 [debug] [Thread-1 (]: SQL status: OK in 1.2799999713897705 seconds
[0m09:03:04.340195 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 09:03:03.038176 => 09:03:04.340195
[0m09:03:04.341195 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m09:03:04.341195 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:03:04.342190 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m09:03:04.342190 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07906-a076-1601-89c4-3ef25593dc80
[0m09:03:04.577945 [debug] [Thread-1 (]: Runtime Error in model dim_tempo (models\marts\dim_tempo.sql)
  dbt could not find a macro with the name "drop_relation" in any package
[0m09:03:04.578942 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecd2c00f-68ee-40c8-8dc4-0012bfc1f083', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E79B102090>]}
[0m09:03:04.579940 [error] [Thread-1 (]: 12 of 12 ERROR creating sql table model default.dim_tempo ...................... [[31mERROR[0m in 1.55s]
[0m09:03:04.580511 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m09:03:04.582509 [debug] [MainThread]: On master: ROLLBACK
[0m09:03:04.583535 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:03:05.383149 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07906-a15c-1e71-96da-1eeab58a6089
[0m09:03:05.383919 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:03:05.384949 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:03:05.385947 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:03:05.386917 [debug] [MainThread]: On master: ROLLBACK
[0m09:03:05.386917 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:03:05.387941 [debug] [MainThread]: On master: Close
[0m09:03:05.388909 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07906-a15c-1e71-96da-1eeab58a6089
[0m09:03:05.618517 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:03:05.620482 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m09:03:05.621502 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:03:05.621502 [debug] [MainThread]: Connection 'model.projeto_health_insights.dim_tempo' was properly closed.
[0m09:03:05.622500 [info ] [MainThread]: 
[0m09:03:05.623839 [info ] [MainThread]: Finished running 10 view models, 2 table models in 0 hours 0 minutes and 25.13 seconds (25.13s).
[0m09:03:05.626863 [debug] [MainThread]: Command end result
[0m09:03:05.639799 [info ] [MainThread]: 
[0m09:03:05.640796 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m09:03:05.641821 [info ] [MainThread]: 
[0m09:03:05.642791 [error] [MainThread]: [33mRuntime Error in model int_nascimento (models\intermediate\int_nascimento.sql)[0m
[0m09:03:05.643816 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `DTNASC` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 8 pos 36
[0m09:03:05.644821 [info ] [MainThread]: 
[0m09:03:05.645783 [error] [MainThread]: [33mRuntime Error in model dim_tempo (models\marts\dim_tempo.sql)[0m
[0m09:03:05.646780 [error] [MainThread]:   dbt could not find a macro with the name "drop_relation" in any package
[0m09:03:05.647777 [info ] [MainThread]: 
[0m09:03:05.648788 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=2 SKIP=1 TOTAL=12
[0m09:03:05.649772 [debug] [MainThread]: Command `dbt run` failed at 09:03:05.649772 after 26.88 seconds
[0m09:03:05.650769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E78EC17090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E78D32F7D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7918F8590>]}
[0m09:03:05.651768 [debug] [MainThread]: Flushing usage events
[0m09:05:55.307496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7ABF1610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7B02FAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7AB6E710>]}


============================== 09:05:55.312540 | 9a2b6692-4313-4a30-910c-840633b547e9 ==============================
[0m09:05:55.312540 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:05:55.313946 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:05:56.595677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7B45FE50>]}
[0m09:05:56.611298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7B459DD0>]}
[0m09:05:56.611298 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m09:05:56.626941 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:05:56.746268 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:05:56.746268 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\intermediate\int_nascimento.sql
[0m09:05:56.777482 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m09:05:56.860650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB057DE290>]}
[0m09:05:56.876272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB046E7450>]}
[0m09:05:56.876272 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m09:05:56.876272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7FC0C510>]}
[0m09:05:56.876272 [info ] [MainThread]: 
[0m09:05:56.876272 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:05:56.876272 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m09:05:56.876272 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m09:05:56.876272 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m09:05:56.876272 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:05:57.809720 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-0821-17f4-abc5-867b84ff661d
[0m09:05:58.170386 [debug] [ThreadPool]: SQL status: OK in 1.2899999618530273 seconds
[0m09:05:58.170386 [debug] [ThreadPool]: On list_workspace: Close
[0m09:05:58.170386 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-0821-17f4-abc5-867b84ff661d
[0m09:05:58.485489 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m09:05:58.485489 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m09:05:58.501844 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:05:58.501844 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m09:05:58.501844 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m09:05:58.501844 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:05:59.284787 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-0903-14c4-af4d-9e72ba68401d
[0m09:05:59.677281 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m09:05:59.678279 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m09:05:59.678279 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m09:05:59.679276 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m09:05:59.679276 [debug] [ThreadPool]: On create_workspace_default: Close
[0m09:05:59.680274 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-0903-14c4-af4d-9e72ba68401d
[0m09:05:59.912080 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:05:59.927502 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:05:59.927502 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m09:05:59.927502 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:06:00.696061 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-09da-18a9-9d9d-ab0456a94137
[0m09:06:01.168382 [debug] [ThreadPool]: SQL status: OK in 1.2400000095367432 seconds
[0m09:06:01.168382 [debug] [ThreadPool]: On list_workspace_default: Close
[0m09:06:01.168382 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-09da-18a9-9d9d-ab0456a94137
[0m09:06:01.418476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7A36D390>]}
[0m09:06:01.418476 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:01.418476 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:06:01.418476 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:06:01.418476 [info ] [MainThread]: 
[0m09:06:01.418476 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m09:06:01.418476 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m09:06:01.432223 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m09:06:01.433284 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m09:06:01.435986 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m09:06:01.435986 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 09:06:01.433489 => 09:06:01.435986
[0m09:06:01.435986 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m09:06:01.469944 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m09:06:01.469944 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:01.469944 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m09:06:01.469944 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m09:06:01.469944 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:06:02.361620 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-0ad8-15fe-9824-4f3e5960838e
[0m09:06:03.101920 [debug] [Thread-1 (]: SQL status: OK in 1.6299999952316284 seconds
[0m09:06:03.117547 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 09:06:01.435986 => 09:06:03.117547
[0m09:06:03.117547 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m09:06:03.117547 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:03.117547 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m09:06:03.117547 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-0ad8-15fe-9824-4f3e5960838e
[0m09:06:03.334612 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB046B37D0>]}
[0m09:06:03.350031 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.92s]
[0m09:06:03.350031 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m09:06:03.350031 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m09:06:03.350031 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m09:06:03.350031 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m09:06:03.350031 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m09:06:03.350031 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m09:06:03.350031 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 09:06:03.350031 => 09:06:03.350031
[0m09:06:03.350031 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m09:06:03.365675 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m09:06:03.367499 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:03.367678 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m09:06:03.367678 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m09:06:03.367678 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:06:04.133701 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-0be8-1ec3-9e65-f03df5e30aac
[0m09:06:04.855627 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m09:06:04.858648 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 09:06:03.350031 => 09:06:04.857655
[0m09:06:04.858648 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m09:06:04.859649 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:04.860627 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m09:06:04.860627 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-0be8-1ec3-9e65-f03df5e30aac
[0m09:06:05.096682 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB046B1690>]}
[0m09:06:05.096682 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.75s]
[0m09:06:05.096682 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m09:06:05.096682 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m09:06:05.096682 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m09:06:05.096682 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m09:06:05.096682 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m09:06:05.096682 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m09:06:05.096682 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 09:06:05.096682 => 09:06:05.096682
[0m09:06:05.096682 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m09:06:05.114534 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m09:06:05.114534 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:05.114534 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m09:06:05.114534 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m09:06:05.114534 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:06:05.901736 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-0cf5-1fbd-8422-2ada98b70765
[0m09:06:06.583540 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:06:06.583540 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 09:06:05.096682 => 09:06:06.583540
[0m09:06:06.583540 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m09:06:06.583540 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:06.583540 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m09:06:06.583540 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-0cf5-1fbd-8422-2ada98b70765
[0m09:06:06.822288 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7AB5DD10>]}
[0m09:06:06.823284 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.73s]
[0m09:06:06.824219 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m09:06:06.825326 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:06:06.825326 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m09:06:06.825326 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m09:06:06.825326 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m09:06:06.825326 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:06:06.825326 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 09:06:06.825326 => 09:06:06.825326
[0m09:06:06.825326 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m09:06:06.825326 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:06:06.825326 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:06.825326 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:06:06.825326 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m09:06:06.825326 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:06:07.667332 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-0e02-1e68-95df-5428d4075bfa
[0m09:06:08.331130 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m09:06:08.346572 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 09:06:06.825326 => 09:06:08.346572
[0m09:06:08.346572 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m09:06:08.346572 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:08.346572 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m09:06:08.346572 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-0e02-1e68-95df-5428d4075bfa
[0m09:06:08.590831 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB058B10D0>]}
[0m09:06:08.591827 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.77s]
[0m09:06:08.593751 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:06:08.593829 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m09:06:08.593829 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m09:06:08.593829 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m09:06:08.593829 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m09:06:08.593829 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m09:06:08.593829 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 09:06:08.593829 => 09:06:08.593829
[0m09:06:08.593829 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m09:06:08.593829 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m09:06:08.610374 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:08.611300 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m09:06:08.612267 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m09:06:08.612267 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:06:09.392496 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-0f0a-1999-ad75-88febd2c2dfb
[0m09:06:10.041340 [debug] [Thread-1 (]: SQL status: OK in 1.4299999475479126 seconds
[0m09:06:10.051627 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 09:06:08.593829 => 09:06:10.050865
[0m09:06:10.051627 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m09:06:10.052626 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:10.052626 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m09:06:10.053651 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-0f0a-1999-ad75-88febd2c2dfb
[0m09:06:10.276192 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB057B3B10>]}
[0m09:06:10.276192 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.68s]
[0m09:06:10.276192 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m09:06:10.291599 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m09:06:10.291599 [info ] [Thread-1 (]: 6 of 12 START sql view model default.dim_localidade ............................ [RUN]
[0m09:06:10.295908 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m09:06:10.296683 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m09:06:10.297650 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m09:06:10.297650 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 09:06:10.297414 => 09:06:10.297650
[0m09:06:10.297650 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m09:06:10.297650 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m09:06:10.297650 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:10.297650 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m09:06:10.297650 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m09:06:10.297650 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:06:11.108877 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-1010-158d-8029-10ace616b031
[0m09:06:11.829650 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m09:06:11.845301 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 09:06:10.297650 => 09:06:11.845301
[0m09:06:11.845301 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m09:06:11.845301 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:11.845301 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m09:06:11.845301 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-1010-158d-8029-10ace616b031
[0m09:06:12.071635 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB046735D0>]}
[0m09:06:12.087190 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.dim_localidade ....................... [[32mOK[0m in 1.78s]
[0m09:06:12.087190 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m09:06:12.087190 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m09:06:12.087190 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m09:06:12.087190 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m09:06:12.098038 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m09:06:12.098773 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m09:06:12.098773 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 09:06:12.098598 => 09:06:12.098773
[0m09:06:12.098773 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m09:06:12.098773 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m09:06:12.098773 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:12.098773 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m09:06:12.098773 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m09:06:12.098773 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:06:12.925809 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-1125-1ac5-a750-adeca87841fa
[0m09:06:13.677233 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m09:06:13.677233 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 09:06:12.098773 => 09:06:13.677233
[0m09:06:13.677233 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m09:06:13.677233 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:13.677233 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m09:06:13.677233 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-1125-1ac5-a750-adeca87841fa
[0m09:06:13.914601 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB046E7B90>]}
[0m09:06:13.914601 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.83s]
[0m09:06:13.914601 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m09:06:13.914601 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m09:06:13.914601 [info ] [Thread-1 (]: 8 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m09:06:13.930649 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m09:06:13.931357 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m09:06:13.935376 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m09:06:13.937343 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 09:06:13.931357 => 09:06:13.936375
[0m09:06:13.937343 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m09:06:13.943354 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m09:06:13.944335 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:13.945350 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m09:06:13.945350 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH nv AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM nv

[0m09:06:13.946346 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:06:14.712751 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-1237-1f88-83e2-0c6df677040a
[0m09:06:15.418566 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:06:15.418566 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 09:06:13.938368 => 09:06:15.418566
[0m09:06:15.418566 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m09:06:15.418566 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:15.434184 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m09:06:15.434184 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-1237-1f88-83e2-0c6df677040a
[0m09:06:15.674032 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB05963590>]}
[0m09:06:15.674032 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 1.76s]
[0m09:06:15.674032 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m09:06:15.674032 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m09:06:15.674032 [info ] [Thread-1 (]: 9 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m09:06:15.674032 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m09:06:15.674032 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m09:06:15.692059 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m09:06:15.692059 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 09:06:15.674032 => 09:06:15.692059
[0m09:06:15.692059 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m09:06:15.692059 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m09:06:15.692059 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:15.700132 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m09:06:15.701515 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m09:06:15.701515 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:06:16.504217 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-1348-1a76-b587-27509fdd5ec4
[0m09:06:17.194818 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m09:06:17.194818 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 09:06:15.692059 => 09:06:17.194818
[0m09:06:17.194818 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m09:06:17.194818 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:17.210232 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m09:06:17.210232 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-1348-1a76-b587-27509fdd5ec4
[0m09:06:17.442431 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB05936C90>]}
[0m09:06:17.442431 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.77s]
[0m09:06:17.442431 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m09:06:17.442431 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:06:17.442431 [info ] [Thread-1 (]: 10 of 12 START sql view model default.fato_atendimento_hospitalar .............. [RUN]
[0m09:06:17.442431 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m09:06:17.442431 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:06:17.457975 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:06:17.459795 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 09:06:17.442431 => 09:06:17.459795
[0m09:06:17.460801 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:06:17.460801 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:06:17.460801 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:17.460801 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:06:17.460801 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m09:06:17.468638 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:06:18.277307 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-1456-109c-a2a3-71910b536e7f
[0m09:06:19.054938 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m09:06:19.054938 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 09:06:17.460801 => 09:06:19.054938
[0m09:06:19.054938 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m09:06:19.054938 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:19.054938 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m09:06:19.054938 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-1456-109c-a2a3-71910b536e7f
[0m09:06:19.297894 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB05936F10>]}
[0m09:06:19.297894 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.fato_atendimento_hospitalar ......... [[32mOK[0m in 1.86s]
[0m09:06:19.297894 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:06:19.297894 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m09:06:19.297894 [info ] [Thread-1 (]: 11 of 12 START sql table model default.fato_nascimento ......................... [RUN]
[0m09:06:19.297894 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.fato_nascimento)
[0m09:06:19.297894 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m09:06:19.297894 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m09:06:19.297894 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 09:06:19.297894 => 09:06:19.297894
[0m09:06:19.297894 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m09:06:19.321693 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:19.321693 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m09:06:19.321693 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */

      describe extended `workspace`.`default`.`fato_nascimento`
  
[0m09:06:19.321693 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:06:20.102873 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-156d-14a5-b9c8-f32880e3965a
[0m09:06:20.567927 [debug] [Thread-1 (]: SQL status: OK in 1.25 seconds
[0m09:06:20.567927 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 09:06:19.297894 => 09:06:20.567927
[0m09:06:20.567927 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m09:06:20.567927 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:20.567927 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m09:06:20.567927 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-156d-14a5-b9c8-f32880e3965a
[0m09:06:20.822381 [debug] [Thread-1 (]: Runtime Error in model fato_nascimento (models\marts\fato_nascimento.sql)
  dbt could not find a macro with the name "drop_relation" in any package
[0m09:06:20.823406 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB059160D0>]}
[0m09:06:20.824404 [error] [Thread-1 (]: 11 of 12 ERROR creating sql table model default.fato_nascimento ................ [[31mERROR[0m in 1.53s]
[0m09:06:20.825388 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m09:06:20.826371 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m09:06:20.826371 [info ] [Thread-1 (]: 12 of 12 START sql table model default.dim_tempo ............................... [RUN]
[0m09:06:20.828396 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_nascimento, now model.projeto_health_insights.dim_tempo)
[0m09:06:20.828396 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m09:06:20.833379 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m09:06:20.834356 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 09:06:20.829391 => 09:06:20.834356
[0m09:06:20.834356 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m09:06:20.839146 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:20.839146 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m09:06:20.839146 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */

      describe extended `workspace`.`default`.`dim_tempo`
  
[0m09:06:20.839146 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:06:21.665782 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-165a-11a4-8b8f-43c46b43d89c
[0m09:06:22.090254 [debug] [Thread-1 (]: SQL status: OK in 1.25 seconds
[0m09:06:22.090254 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 09:06:20.835375 => 09:06:22.090254
[0m09:06:22.090254 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m09:06:22.090254 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:06:22.090254 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m09:06:22.090254 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-165a-11a4-8b8f-43c46b43d89c
[0m09:06:22.349497 [debug] [Thread-1 (]: Runtime Error in model dim_tempo (models\marts\dim_tempo.sql)
  dbt could not find a macro with the name "drop_relation" in any package
[0m09:06:22.349497 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a2b6692-4313-4a30-910c-840633b547e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB0590EE50>]}
[0m09:06:22.349497 [error] [Thread-1 (]: 12 of 12 ERROR creating sql table model default.dim_tempo ...................... [[31mERROR[0m in 1.52s]
[0m09:06:22.349497 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m09:06:22.358304 [debug] [MainThread]: On master: ROLLBACK
[0m09:06:22.358304 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:06:23.157181 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07907-173f-10c0-9cbc-7e9f7de1aaa3
[0m09:06:23.157181 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:06:23.157181 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:06:23.157181 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:06:23.157181 [debug] [MainThread]: On master: ROLLBACK
[0m09:06:23.157181 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:06:23.157181 [debug] [MainThread]: On master: Close
[0m09:06:23.157181 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07907-173f-10c0-9cbc-7e9f7de1aaa3
[0m09:06:23.399905 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:06:23.399905 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m09:06:23.399905 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:06:23.399905 [debug] [MainThread]: Connection 'model.projeto_health_insights.dim_tempo' was properly closed.
[0m09:06:23.399905 [info ] [MainThread]: 
[0m09:06:23.399905 [info ] [MainThread]: Finished running 10 view models, 2 table models in 0 hours 0 minutes and 26.52 seconds (26.52s).
[0m09:06:23.399905 [debug] [MainThread]: Command end result
[0m09:06:23.416074 [info ] [MainThread]: 
[0m09:06:23.424943 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m09:06:23.426078 [info ] [MainThread]: 
[0m09:06:23.426078 [error] [MainThread]: [33mRuntime Error in model fato_nascimento (models\marts\fato_nascimento.sql)[0m
[0m09:06:23.426078 [error] [MainThread]:   dbt could not find a macro with the name "drop_relation" in any package
[0m09:06:23.426078 [info ] [MainThread]: 
[0m09:06:23.426078 [error] [MainThread]: [33mRuntime Error in model dim_tempo (models\marts\dim_tempo.sql)[0m
[0m09:06:23.426078 [error] [MainThread]:   dbt could not find a macro with the name "drop_relation" in any package
[0m09:06:23.426078 [info ] [MainThread]: 
[0m09:06:23.426078 [info ] [MainThread]: Done. PASS=10 WARN=0 ERROR=2 SKIP=0 TOTAL=12
[0m09:06:23.426078 [debug] [MainThread]: Command `dbt run` failed at 09:06:23.426078 after 28.15 seconds
[0m09:06:23.426078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7ABF0D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7AB84250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB7B447E90>]}
[0m09:06:23.426078 [debug] [MainThread]: Flushing usage events
[0m09:07:58.524027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E438BB2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E435DA1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E435FFFD0>]}


============================== 09:07:58.524027 | a5815d8d-3348-4edf-b47c-2879931c9b49 ==============================
[0m09:07:58.524027 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:07:58.524027 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:07:59.858658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E438B91D0>]}
[0m09:07:59.874286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E438B91D0>]}
[0m09:07:59.874286 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m09:07:59.894733 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:07:59.992717 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m09:07:59.992717 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\dim_tempo.sql
[0m09:07:59.992717 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m09:08:00.023960 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m09:08:00.039581 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m09:08:00.171428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4E22E5D0>]}
[0m09:08:00.171428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4E2A7710>]}
[0m09:08:00.187050 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m09:08:00.187050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E40604750>]}
[0m09:08:00.187050 [info ] [MainThread]: 
[0m09:08:00.187050 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:08:00.187050 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m09:08:00.187050 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m09:08:00.187050 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m09:08:00.187050 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:08:01.139379 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-51a5-13b6-9da6-78e5a5b0d7a3
[0m09:08:01.694537 [debug] [ThreadPool]: SQL status: OK in 1.5099999904632568 seconds
[0m09:08:01.694537 [debug] [ThreadPool]: On list_workspace: Close
[0m09:08:01.694537 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-51a5-13b6-9da6-78e5a5b0d7a3
[0m09:08:01.985233 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m09:08:01.985233 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m09:08:02.000624 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:02.000624 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m09:08:02.000624 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m09:08:02.000624 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:08:02.799167 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-52a1-1628-b2be-52a515f57a22
[0m09:08:03.206102 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m09:08:03.206102 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m09:08:03.206102 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m09:08:03.206102 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m09:08:03.206102 [debug] [ThreadPool]: On create_workspace_default: Close
[0m09:08:03.206102 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-52a1-1628-b2be-52a515f57a22
[0m09:08:03.436443 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:08:03.452063 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:08:03.452063 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m09:08:03.452063 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:08:04.222572 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-537c-1949-b6ca-57eb4dcbcd7f
[0m09:08:04.550169 [debug] [ThreadPool]: SQL status: OK in 1.100000023841858 seconds
[0m09:08:04.550169 [debug] [ThreadPool]: On list_workspace_default: Close
[0m09:08:04.550169 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-537c-1949-b6ca-57eb4dcbcd7f
[0m09:08:04.790005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4E140810>]}
[0m09:08:04.790005 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:04.790005 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:08:04.790005 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:08:04.790005 [info ] [MainThread]: 
[0m09:08:04.790005 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m09:08:04.790005 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m09:08:04.806398 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m09:08:04.807349 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m09:08:04.807407 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m09:08:04.807407 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 09:08:04.807407 => 09:08:04.807407
[0m09:08:04.807407 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m09:08:04.833549 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m09:08:04.833549 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:04.848398 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m09:08:04.849693 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m09:08:04.849693 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:08:05.649221 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-5456-12e8-a591-fa04c6abfab6
[0m09:08:06.400315 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m09:08:06.432823 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 09:08:04.807407 => 09:08:06.432823
[0m09:08:06.434069 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m09:08:06.434069 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:06.434069 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m09:08:06.434069 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-5456-12e8-a591-fa04c6abfab6
[0m09:08:06.659438 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4D0B8490>]}
[0m09:08:06.659438 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.85s]
[0m09:08:06.659438 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m09:08:06.659438 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m09:08:06.659438 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m09:08:06.659438 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m09:08:06.675024 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m09:08:06.675024 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m09:08:06.675024 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 09:08:06.675024 => 09:08:06.675024
[0m09:08:06.675024 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m09:08:06.675024 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m09:08:06.675024 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:06.675024 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m09:08:06.675024 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m09:08:06.690647 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:08:07.475019 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-556b-1ca8-ba1f-7002c7ddb350
[0m09:08:08.145713 [debug] [Thread-1 (]: SQL status: OK in 1.4600000381469727 seconds
[0m09:08:08.145713 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 09:08:06.675024 => 09:08:08.145713
[0m09:08:08.145713 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m09:08:08.145713 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:08.145713 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m09:08:08.145713 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-556b-1ca8-ba1f-7002c7ddb350
[0m09:08:08.376084 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E428B5850>]}
[0m09:08:08.376084 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.72s]
[0m09:08:08.376084 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m09:08:08.376084 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m09:08:08.376084 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m09:08:08.391564 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m09:08:08.391564 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m09:08:08.391564 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m09:08:08.391564 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 09:08:08.391564 => 09:08:08.391564
[0m09:08:08.391564 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m09:08:08.391564 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m09:08:08.391564 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:08.391564 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m09:08:08.391564 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m09:08:08.391564 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:08:09.198128 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-5672-1487-9b25-eca48e0e378e
[0m09:08:09.853003 [debug] [Thread-1 (]: SQL status: OK in 1.4600000381469727 seconds
[0m09:08:09.853003 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 09:08:08.391564 => 09:08:09.853003
[0m09:08:09.853003 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m09:08:09.853003 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:09.853003 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m09:08:09.853003 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-5672-1487-9b25-eca48e0e378e
[0m09:08:10.095600 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4E113090>]}
[0m09:08:10.095600 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.70s]
[0m09:08:10.095600 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m09:08:10.095600 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:08:10.095600 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m09:08:10.110258 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m09:08:10.111101 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m09:08:10.111338 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:08:10.111338 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 09:08:10.111338 => 09:08:10.111338
[0m09:08:10.111338 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m09:08:10.121962 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:08:10.121962 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:10.121962 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:08:10.121962 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m09:08:10.121962 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:08:10.901047 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-5776-18ae-a500-1b344fa32c76
[0m09:08:11.550404 [debug] [Thread-1 (]: SQL status: OK in 1.4299999475479126 seconds
[0m09:08:11.550404 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 09:08:10.111338 => 09:08:11.550404
[0m09:08:11.550404 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m09:08:11.550404 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:11.550404 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m09:08:11.550404 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-5776-18ae-a500-1b344fa32c76
[0m09:08:11.787253 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4D0C1810>]}
[0m09:08:11.787253 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.69s]
[0m09:08:11.787253 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:08:11.787253 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m09:08:11.787253 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m09:08:11.787253 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m09:08:11.787253 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m09:08:11.802847 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m09:08:11.804654 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 09:08:11.787253 => 09:08:11.803706
[0m09:08:11.805544 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m09:08:11.807018 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m09:08:11.807018 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:11.807018 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m09:08:11.807018 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m09:08:11.807018 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:08:12.601054 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-587a-1217-a4f5-6bff6f8047fa
[0m09:08:13.277570 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:08:13.277570 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 09:08:11.806548 => 09:08:13.277570
[0m09:08:13.277570 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m09:08:13.293192 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:13.293192 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m09:08:13.293192 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-587a-1217-a4f5-6bff6f8047fa
[0m09:08:13.532021 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4E1A1B90>]}
[0m09:08:13.532021 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.74s]
[0m09:08:13.532021 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m09:08:13.532021 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m09:08:13.532021 [info ] [Thread-1 (]: 6 of 12 START sql view model default.dim_localidade ............................ [RUN]
[0m09:08:13.532021 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m09:08:13.532021 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m09:08:13.532021 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m09:08:13.547642 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 09:08:13.532021 => 09:08:13.547642
[0m09:08:13.547642 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m09:08:13.550273 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m09:08:13.550273 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:13.550273 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m09:08:13.550273 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m09:08:13.550273 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:08:14.328601 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-5982-15c7-ba1b-954d8b5b37bf
[0m09:08:15.019631 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:08:15.019631 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 09:08:13.548871 => 09:08:15.019631
[0m09:08:15.019631 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m09:08:15.019631 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:15.019631 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m09:08:15.019631 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-5982-15c7-ba1b-954d8b5b37bf
[0m09:08:15.250331 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4E31A050>]}
[0m09:08:15.250331 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.dim_localidade ....................... [[32mOK[0m in 1.72s]
[0m09:08:15.250331 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m09:08:15.250331 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m09:08:15.250331 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m09:08:15.250331 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m09:08:15.250331 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m09:08:15.250331 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m09:08:15.265918 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 09:08:15.250331 => 09:08:15.265918
[0m09:08:15.266775 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m09:08:15.267905 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m09:08:15.267905 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:15.267905 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m09:08:15.267905 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m09:08:15.267905 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:08:16.087571 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-5a8e-1743-9962-cbbe2906f35c
[0m09:08:16.811441 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m09:08:16.811441 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 09:08:15.267905 => 09:08:16.811441
[0m09:08:16.811441 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m09:08:16.811441 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:16.811441 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m09:08:16.811441 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-5a8e-1743-9962-cbbe2906f35c
[0m09:08:17.044947 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4E372090>]}
[0m09:08:17.044947 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.79s]
[0m09:08:17.060566 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m09:08:17.060566 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m09:08:17.060566 [info ] [Thread-1 (]: 8 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m09:08:17.060566 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m09:08:17.060566 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m09:08:17.060566 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m09:08:17.060566 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 09:08:17.060566 => 09:08:17.060566
[0m09:08:17.060566 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m09:08:17.060566 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m09:08:17.060566 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:17.076187 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m09:08:17.076187 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH nv AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM nv

[0m09:08:17.077193 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:08:17.849289 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-5b9b-1108-96d7-2d4b67956cdc
[0m09:08:18.506097 [debug] [Thread-1 (]: SQL status: OK in 1.4299999475479126 seconds
[0m09:08:18.521472 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 09:08:17.060566 => 09:08:18.521472
[0m09:08:18.521472 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m09:08:18.521472 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:18.521472 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m09:08:18.521472 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-5b9b-1108-96d7-2d4b67956cdc
[0m09:08:18.787106 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4E31A0D0>]}
[0m09:08:18.787106 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 1.73s]
[0m09:08:18.802727 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m09:08:18.802727 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m09:08:18.802727 [info ] [Thread-1 (]: 9 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m09:08:18.802727 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m09:08:18.802727 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m09:08:18.802727 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m09:08:18.802727 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 09:08:18.802727 => 09:08:18.802727
[0m09:08:18.802727 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m09:08:18.817093 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m09:08:18.818091 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:18.819061 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m09:08:18.820069 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m09:08:18.821068 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:08:19.592716 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-5ca5-1272-ac37-04d7666d94e7
[0m09:08:20.294007 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:08:20.294007 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 09:08:18.813077 => 09:08:20.294007
[0m09:08:20.294007 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m09:08:20.294007 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:20.294007 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m09:08:20.294007 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-5ca5-1272-ac37-04d7666d94e7
[0m09:08:20.522168 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4D10BF10>]}
[0m09:08:20.522168 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.72s]
[0m09:08:20.522168 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m09:08:20.522168 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:08:20.537597 [info ] [Thread-1 (]: 10 of 12 START sql view model default.fato_atendimento_hospitalar .............. [RUN]
[0m09:08:20.537597 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m09:08:20.537597 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:08:20.537597 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:08:20.537597 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 09:08:20.537597 => 09:08:20.537597
[0m09:08:20.537597 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:08:20.537597 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:08:20.537597 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:20.537597 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:08:20.537597 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m09:08:20.537597 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:08:21.361010 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-5db2-1fe6-a1bf-440cd4af2fd3
[0m09:08:22.080560 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m09:08:22.095986 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 09:08:20.537597 => 09:08:22.095986
[0m09:08:22.095986 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m09:08:22.095986 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:22.095986 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m09:08:22.095986 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-5db2-1fe6-a1bf-440cd4af2fd3
[0m09:08:22.334666 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4E22CAD0>]}
[0m09:08:22.334666 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.fato_atendimento_hospitalar ......... [[32mOK[0m in 1.80s]
[0m09:08:22.336636 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:08:22.336636 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m09:08:22.337973 [info ] [Thread-1 (]: 11 of 12 START sql view model default.fato_nascimento .......................... [RUN]
[0m09:08:22.338712 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.fato_nascimento)
[0m09:08:22.339740 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m09:08:22.343732 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m09:08:22.345696 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 09:08:22.339740 => 09:08:22.344726
[0m09:08:22.345696 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m09:08:22.350205 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m09:08:22.350205 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:22.350205 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m09:08:22.350205 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    nascimento_id,
    data_id,
    municipio,
    uf,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas
FROM `workspace`.`default`.`int_nascimento`

[0m09:08:22.350205 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:08:23.135316 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-5ec1-1eaa-954e-bd555284622a
[0m09:08:23.496307 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    nascimento_id,
    data_id,
    municipio,
    uf,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas
FROM `workspace`.`default`.`int_nascimento`

[0m09:08:23.511705 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_id` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 10 pos 4
[0m09:08:23.511705 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_id` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 10 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_id` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 10 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m09:08:23.511705 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07907-5ee2-1973-8557-31fb877eb5da
[0m09:08:23.511705 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 09:08:22.346715 => 09:08:23.511705
[0m09:08:23.511705 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m09:08:23.511705 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:23.511705 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m09:08:23.511705 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-5ec1-1eaa-954e-bd555284622a
[0m09:08:23.757606 [debug] [Thread-1 (]: Runtime Error in model fato_nascimento (models\marts\fato_nascimento.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_id` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 10 pos 4
[0m09:08:23.757606 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4E239950>]}
[0m09:08:23.757606 [error] [Thread-1 (]: 11 of 12 ERROR creating sql view model default.fato_nascimento ................. [[31mERROR[0m in 1.42s]
[0m09:08:23.757606 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m09:08:23.757606 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m09:08:23.757606 [info ] [Thread-1 (]: 12 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m09:08:23.773223 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_nascimento, now model.projeto_health_insights.dim_tempo)
[0m09:08:23.773223 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m09:08:23.773223 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m09:08:23.773223 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 09:08:23.773223 => 09:08:23.773223
[0m09:08:23.773223 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m09:08:23.788872 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m09:08:23.790977 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:23.790977 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m09:08:23.790977 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

SELECT DISTINCT
    data_id,
    data,
    ano,
    mes,
    dia,
    trimestre,
    dia_semana
FROM `workspace`.`default`.`stg_tempo`

[0m09:08:23.790977 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:08:24.593559 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-5fa0-1525-98b7-cf01d9cd3b04
[0m09:08:25.334972 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m09:08:25.334972 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 09:08:23.773223 => 09:08:25.334972
[0m09:08:25.334972 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m09:08:25.334972 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:08:25.334972 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m09:08:25.334972 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-5fa0-1525-98b7-cf01d9cd3b04
[0m09:08:25.567457 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a5815d8d-3348-4edf-b47c-2879931c9b49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4E142690>]}
[0m09:08:25.567457 [info ] [Thread-1 (]: 12 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 1.79s]
[0m09:08:25.567457 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m09:08:25.582899 [debug] [MainThread]: On master: ROLLBACK
[0m09:08:25.582899 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:08:26.379246 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07907-60b0-107d-8efb-07d89087a873
[0m09:08:26.379246 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:08:26.379246 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:08:26.379246 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:08:26.379246 [debug] [MainThread]: On master: ROLLBACK
[0m09:08:26.379246 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:08:26.379246 [debug] [MainThread]: On master: Close
[0m09:08:26.379246 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07907-60b0-107d-8efb-07d89087a873
[0m09:08:26.604090 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:08:26.619525 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m09:08:26.619525 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:08:26.619525 [debug] [MainThread]: Connection 'model.projeto_health_insights.dim_tempo' was properly closed.
[0m09:08:26.619525 [info ] [MainThread]: 
[0m09:08:26.619525 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 26.43 seconds (26.43s).
[0m09:08:26.619525 [debug] [MainThread]: Command end result
[0m09:08:26.638000 [info ] [MainThread]: 
[0m09:08:26.638899 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:08:26.638899 [info ] [MainThread]: 
[0m09:08:26.638899 [error] [MainThread]: [33mRuntime Error in model fato_nascimento (models\marts\fato_nascimento.sql)[0m
[0m09:08:26.638899 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_id` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 10 pos 4
[0m09:08:26.638899 [info ] [MainThread]: 
[0m09:08:26.638899 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 TOTAL=12
[0m09:08:26.638899 [debug] [MainThread]: Command `dbt run` failed at 09:08:26.638899 after 28.14 seconds
[0m09:08:26.638899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E438B8C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E4394C410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E43EE3F10>]}
[0m09:08:26.638899 [debug] [MainThread]: Flushing usage events
[0m09:09:58.646316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCA1FD24D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCA2066310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCA21FA3D0>]}


============================== 09:09:58.658420 | d5356a64-81ea-4d59-adff-7077bc9aa22b ==============================
[0m09:09:58.658420 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:09:58.658420 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:09:59.945363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCA2606DD0>]}
[0m09:09:59.976603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCA2606DD0>]}
[0m09:09:59.976603 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m09:09:59.993991 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:10:00.102935 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:10:00.102935 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m09:10:00.118590 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m09:10:00.243527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCAC825950>]}
[0m09:10:00.259149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCAC830210>]}
[0m09:10:00.259149 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m09:10:00.259149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCA8D9ED10>]}
[0m09:10:00.259149 [info ] [MainThread]: 
[0m09:10:00.259149 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:10:00.275640 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m09:10:00.276320 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m09:10:00.277341 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m09:10:00.277341 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:10:01.154231 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-992d-179d-b4ed-51be70746e0b
[0m09:10:01.473065 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m09:10:01.473065 [debug] [ThreadPool]: On list_workspace: Close
[0m09:10:01.473065 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-992d-179d-b4ed-51be70746e0b
[0m09:10:01.719678 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m09:10:01.719678 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m09:10:01.735207 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:01.735207 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m09:10:01.735207 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m09:10:01.735207 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:10:02.503656 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-99fd-128c-9bd0-8dd6cbd61ee5
[0m09:10:02.870635 [debug] [ThreadPool]: SQL status: OK in 1.1399999856948853 seconds
[0m09:10:02.870635 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m09:10:02.870635 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m09:10:02.870635 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m09:10:02.886185 [debug] [ThreadPool]: On create_workspace_default: Close
[0m09:10:02.886185 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-99fd-128c-9bd0-8dd6cbd61ee5
[0m09:10:03.119586 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:10:03.119586 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:10:03.119586 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m09:10:03.119586 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:10:03.918944 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-9ad3-1034-85f0-6eb4710bb4a3
[0m09:10:04.274358 [debug] [ThreadPool]: SQL status: OK in 1.149999976158142 seconds
[0m09:10:04.274358 [debug] [ThreadPool]: On list_workspace_default: Close
[0m09:10:04.274358 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-9ad3-1034-85f0-6eb4710bb4a3
[0m09:10:04.554729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCAC8C01D0>]}
[0m09:10:04.554729 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:04.554729 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:10:04.554729 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:10:04.554729 [info ] [MainThread]: 
[0m09:10:04.554729 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m09:10:04.570354 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m09:10:04.572372 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m09:10:04.572372 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m09:10:04.575505 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m09:10:04.575505 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 09:10:04.574322 => 09:10:04.575505
[0m09:10:04.575505 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m09:10:04.608976 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m09:10:04.608976 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:04.608976 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m09:10:04.608976 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m09:10:04.608976 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:10:05.408777 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-9bb7-18d9-8450-74ec9a3b592b
[0m09:10:06.129310 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m09:10:06.129310 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 09:10:04.575505 => 09:10:06.129310
[0m09:10:06.144919 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m09:10:06.144919 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:06.144919 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m09:10:06.144919 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-9bb7-18d9-8450-74ec9a3b592b
[0m09:10:06.383470 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCAC929050>]}
[0m09:10:06.383470 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.81s]
[0m09:10:06.383470 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m09:10:06.383470 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m09:10:06.383470 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m09:10:06.383470 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m09:10:06.383470 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m09:10:06.399086 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m09:10:06.400032 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 09:10:06.383470 => 09:10:06.400032
[0m09:10:06.401058 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m09:10:06.401058 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m09:10:06.401058 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:06.401058 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m09:10:06.401058 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m09:10:06.401058 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:10:07.212845 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-9cca-1aa5-9aa1-7407c1955536
[0m09:10:07.860569 [debug] [Thread-1 (]: SQL status: OK in 1.4600000381469727 seconds
[0m09:10:07.860569 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 09:10:06.401058 => 09:10:07.860569
[0m09:10:07.860569 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m09:10:07.860569 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:07.860569 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m09:10:07.860569 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-9cca-1aa5-9aa1-7407c1955536
[0m09:10:08.107282 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCA10BA450>]}
[0m09:10:08.107282 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.72s]
[0m09:10:08.107282 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m09:10:08.107282 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m09:10:08.107282 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m09:10:08.107282 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m09:10:08.107282 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m09:10:08.107282 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m09:10:08.107282 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 09:10:08.107282 => 09:10:08.107282
[0m09:10:08.107282 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m09:10:08.125012 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m09:10:08.128000 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:08.128000 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m09:10:08.128986 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m09:10:08.128986 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:10:08.908388 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-9dce-1381-b196-f17953d8088e
[0m09:10:09.567606 [debug] [Thread-1 (]: SQL status: OK in 1.440000057220459 seconds
[0m09:10:09.567606 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 09:10:08.107282 => 09:10:09.567606
[0m09:10:09.567606 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m09:10:09.567606 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:09.567606 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m09:10:09.567606 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-9dce-1381-b196-f17953d8088e
[0m09:10:09.814673 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCACA54790>]}
[0m09:10:09.815641 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.71s]
[0m09:10:09.816637 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m09:10:09.817635 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:10:09.818633 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m09:10:09.819629 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m09:10:09.820658 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m09:10:09.825645 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:10:09.827608 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 09:10:09.820658 => 09:10:09.826612
[0m09:10:09.827608 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m09:10:09.832012 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:10:09.832012 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:09.832012 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:10:09.832012 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m09:10:09.832012 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:10:10.619440 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-9ed2-1451-99d3-8b8389f4ce05
[0m09:10:11.278548 [debug] [Thread-1 (]: SQL status: OK in 1.4500000476837158 seconds
[0m09:10:11.278548 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 09:10:09.828638 => 09:10:11.278548
[0m09:10:11.278548 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m09:10:11.278548 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:11.278548 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m09:10:11.278548 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-9ed2-1451-99d3-8b8389f4ce05
[0m09:10:11.506913 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCAB7D1710>]}
[0m09:10:11.506913 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.69s]
[0m09:10:11.522585 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:10:11.522585 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m09:10:11.522585 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m09:10:11.529999 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m09:10:11.532964 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m09:10:11.532964 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m09:10:11.548992 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 09:10:11.532964 => 09:10:11.548384
[0m09:10:11.548992 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m09:10:11.548992 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m09:10:11.548992 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:11.548992 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m09:10:11.548992 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m09:10:11.548992 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:10:12.345631 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-9fdb-1250-bbd8-682bb310acf9
[0m09:10:13.012494 [debug] [Thread-1 (]: SQL status: OK in 1.4600000381469727 seconds
[0m09:10:13.012494 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 09:10:11.548992 => 09:10:13.012494
[0m09:10:13.012494 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m09:10:13.012494 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:13.012494 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m09:10:13.012494 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-9fdb-1250-bbd8-682bb310acf9
[0m09:10:13.259592 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCAB831A90>]}
[0m09:10:13.259592 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.74s]
[0m09:10:13.259592 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m09:10:13.259592 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m09:10:13.259592 [info ] [Thread-1 (]: 6 of 12 START sql view model default.dim_localidade ............................ [RUN]
[0m09:10:13.259592 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m09:10:13.259592 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m09:10:13.259592 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m09:10:13.275220 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 09:10:13.259592 => 09:10:13.275220
[0m09:10:13.276633 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m09:10:13.284890 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m09:10:13.284890 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:13.284890 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m09:10:13.284890 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m09:10:13.284890 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:10:14.057658 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-a0df-1b87-ac46-4f7b56028d6d
[0m09:10:14.734500 [debug] [Thread-1 (]: SQL status: OK in 1.4500000476837158 seconds
[0m09:10:14.734500 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 09:10:13.277605 => 09:10:14.734500
[0m09:10:14.734500 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m09:10:14.734500 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:14.734500 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m09:10:14.734500 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-a0df-1b87-ac46-4f7b56028d6d
[0m09:10:14.951469 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCACA54790>]}
[0m09:10:14.951469 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.dim_localidade ....................... [[32mOK[0m in 1.69s]
[0m09:10:14.951469 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m09:10:14.966887 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m09:10:14.966887 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m09:10:14.973404 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m09:10:14.976011 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m09:10:14.987888 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m09:10:14.987888 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 09:10:14.976011 => 09:10:14.987888
[0m09:10:14.987888 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m09:10:15.003911 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m09:10:15.003911 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:15.003911 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m09:10:15.003911 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m09:10:15.003911 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:10:15.805848 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-a1e8-1a9c-b002-acbd34e24981
[0m09:10:16.520693 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m09:10:16.520693 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 09:10:14.987888 => 09:10:16.520693
[0m09:10:16.536316 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m09:10:16.536316 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:16.536316 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m09:10:16.536316 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-a1e8-1a9c-b002-acbd34e24981
[0m09:10:16.777718 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCAC954C10>]}
[0m09:10:16.777718 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.80s]
[0m09:10:16.777718 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m09:10:16.777718 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m09:10:16.777718 [info ] [Thread-1 (]: 8 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m09:10:16.777718 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m09:10:16.777718 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m09:10:16.777718 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m09:10:16.777718 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 09:10:16.777718 => 09:10:16.777718
[0m09:10:16.793356 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m09:10:16.795590 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m09:10:16.795590 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:16.795590 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m09:10:16.795590 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH nv AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM nv

[0m09:10:16.795590 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:10:17.682413 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-a308-10b7-968f-8bd375ff4e85
[0m09:10:18.399057 [debug] [Thread-1 (]: SQL status: OK in 1.600000023841858 seconds
[0m09:10:18.414481 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 09:10:16.794256 => 09:10:18.414481
[0m09:10:18.414481 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m09:10:18.414481 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:18.414481 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m09:10:18.414481 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-a308-10b7-968f-8bd375ff4e85
[0m09:10:18.639584 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCAC857590>]}
[0m09:10:18.639584 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 1.86s]
[0m09:10:18.639584 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m09:10:18.639584 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m09:10:18.639584 [info ] [Thread-1 (]: 9 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m09:10:18.655230 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m09:10:18.655230 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m09:10:18.655230 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m09:10:18.655230 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 09:10:18.655230 => 09:10:18.655230
[0m09:10:18.655230 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m09:10:18.655230 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m09:10:18.655230 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:18.655230 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m09:10:18.655230 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m09:10:18.655230 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:10:19.470328 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-a418-1b03-a8bc-02a96b6240c4
[0m09:10:20.165216 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m09:10:20.165216 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 09:10:18.655230 => 09:10:20.165216
[0m09:10:20.165216 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m09:10:20.165216 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:20.165216 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m09:10:20.165216 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-a418-1b03-a8bc-02a96b6240c4
[0m09:10:20.397487 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCAB826B50>]}
[0m09:10:20.397487 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.76s]
[0m09:10:20.397487 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m09:10:20.397487 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:10:20.397487 [info ] [Thread-1 (]: 10 of 12 START sql view model default.fato_atendimento_hospitalar .............. [RUN]
[0m09:10:20.397487 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m09:10:20.397487 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:10:20.413801 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:10:20.414828 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 09:10:20.397487 => 09:10:20.414828
[0m09:10:20.414828 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:10:20.414828 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:10:20.414828 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:20.414828 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:10:20.414828 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m09:10:20.414828 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:10:21.203106 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-a521-1392-ac04-89ed85092724
[0m09:10:21.920945 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m09:10:21.920945 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 09:10:20.414828 => 09:10:21.920945
[0m09:10:21.936359 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m09:10:21.936359 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:21.936359 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m09:10:21.936359 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-a521-1392-ac04-89ed85092724
[0m09:10:22.151086 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCACA39ED0>]}
[0m09:10:22.151086 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.fato_atendimento_hospitalar ......... [[32mOK[0m in 1.75s]
[0m09:10:22.166547 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:10:22.166547 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m09:10:22.166547 [info ] [Thread-1 (]: 11 of 12 START sql view model default.fato_nascimento .......................... [RUN]
[0m09:10:22.172785 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.fato_nascimento)
[0m09:10:22.173819 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m09:10:22.177071 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m09:10:22.177071 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 09:10:22.174053 => 09:10:22.177071
[0m09:10:22.177071 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m09:10:22.177071 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m09:10:22.177071 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:22.177071 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m09:10:22.177071 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    nascimento_id,
    data_nascimento,
    municipio,
    uf,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas
FROM `workspace`.`default`.`int_nascimento`

[0m09:10:22.177071 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:10:22.986537 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-a631-1765-838f-6d677043f55c
[0m09:10:23.358347 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    nascimento_id,
    data_nascimento,
    municipio,
    uf,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas
FROM `workspace`.`default`.`int_nascimento`

[0m09:10:23.358347 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `municipio` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `APGAR1`, `APGAR5`, `peso`, `sexo`]. SQLSTATE: 42703; line 11 pos 4
[0m09:10:23.358347 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `municipio` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `APGAR1`, `APGAR5`, `peso`, `sexo`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `municipio` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `APGAR1`, `APGAR5`, `peso`, `sexo`]. SQLSTATE: 42703; line 11 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m09:10:23.373967 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07907-a652-1376-97be-2290ab11017b
[0m09:10:23.373967 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 09:10:22.177071 => 09:10:23.373967
[0m09:10:23.373967 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m09:10:23.373967 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:23.373967 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m09:10:23.373967 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-a631-1765-838f-6d677043f55c
[0m09:10:23.618330 [debug] [Thread-1 (]: Runtime Error in model fato_nascimento (models\marts\fato_nascimento.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `municipio` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `APGAR1`, `APGAR5`, `peso`, `sexo`]. SQLSTATE: 42703; line 11 pos 4
[0m09:10:23.618330 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCA1D2C910>]}
[0m09:10:23.618330 [error] [Thread-1 (]: 11 of 12 ERROR creating sql view model default.fato_nascimento ................. [[31mERROR[0m in 1.45s]
[0m09:10:23.633756 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m09:10:23.633756 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m09:10:23.633756 [info ] [Thread-1 (]: 12 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m09:10:23.633756 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_nascimento, now model.projeto_health_insights.dim_tempo)
[0m09:10:23.633756 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m09:10:23.633756 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m09:10:23.633756 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 09:10:23.633756 => 09:10:23.633756
[0m09:10:23.633756 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m09:10:23.650299 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m09:10:23.651256 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:23.651256 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m09:10:23.651256 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

SELECT DISTINCT
    data_id,
    data,
    ano,
    mes,
    dia,
    trimestre,
    dia_semana
FROM `workspace`.`default`.`stg_tempo`

[0m09:10:23.651256 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:10:24.452501 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-a710-1678-a6d2-284aab0b8ef5
[0m09:10:25.231719 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m09:10:25.231719 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 09:10:23.633756 => 09:10:25.231719
[0m09:10:25.231719 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m09:10:25.231719 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:10:25.231719 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m09:10:25.231719 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-a710-1678-a6d2-284aab0b8ef5
[0m09:10:25.455249 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd5356a64-81ea-4d59-adff-7077bc9aa22b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCACAAA790>]}
[0m09:10:25.455249 [info ] [Thread-1 (]: 12 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 1.82s]
[0m09:10:25.455249 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m09:10:25.455249 [debug] [MainThread]: On master: ROLLBACK
[0m09:10:25.455249 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:10:26.235997 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07907-a821-1be3-b92a-cbeec9d2a570
[0m09:10:26.235997 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:10:26.235997 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:10:26.235997 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:10:26.235997 [debug] [MainThread]: On master: ROLLBACK
[0m09:10:26.235997 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:10:26.235997 [debug] [MainThread]: On master: Close
[0m09:10:26.235997 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07907-a821-1be3-b92a-cbeec9d2a570
[0m09:10:26.489295 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:10:26.489295 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m09:10:26.489295 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:10:26.489295 [debug] [MainThread]: Connection 'model.projeto_health_insights.dim_tempo' was properly closed.
[0m09:10:26.489295 [info ] [MainThread]: 
[0m09:10:26.489295 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 26.23 seconds (26.23s).
[0m09:10:26.489295 [debug] [MainThread]: Command end result
[0m09:10:26.514048 [info ] [MainThread]: 
[0m09:10:26.515061 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:10:26.515061 [info ] [MainThread]: 
[0m09:10:26.515061 [error] [MainThread]: [33mRuntime Error in model fato_nascimento (models\marts\fato_nascimento.sql)[0m
[0m09:10:26.515061 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `municipio` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `APGAR1`, `APGAR5`, `peso`, `sexo`]. SQLSTATE: 42703; line 11 pos 4
[0m09:10:26.515061 [info ] [MainThread]: 
[0m09:10:26.515061 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 TOTAL=12
[0m09:10:26.515061 [debug] [MainThread]: Command `dbt run` failed at 09:10:26.515061 after 27.88 seconds
[0m09:10:26.515061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCA1CFA010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCA1CFA610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FCA1D1D250>]}
[0m09:10:26.515061 [debug] [MainThread]: Flushing usage events
[0m09:11:59.019355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002157A2A2F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021579FA7C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002157A269510>]}


============================== 09:11:59.034976 | bd850153-fa1a-45e3-9f24-9d219b4ea555 ==============================
[0m09:11:59.034976 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:11:59.034976 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:12:00.363909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002157A815790>]}
[0m09:12:00.382101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021503AB0910>]}
[0m09:12:00.382101 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m09:12:00.411714 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:12:00.513818 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:12:00.513818 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m09:12:00.553314 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m09:12:00.673637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021503AA6010>]}
[0m09:12:00.689258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021503ADD990>]}
[0m09:12:00.689258 [info ] [MainThread]: Found 12 models, 5 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m09:12:00.689258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002157EFCD090>]}
[0m09:12:00.689258 [info ] [MainThread]: 
[0m09:12:00.689258 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:12:00.689258 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m09:12:00.689258 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m09:12:00.689258 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m09:12:00.689258 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:12:01.548741 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-e0ed-1f5b-a976-bc41cded47e6
[0m09:12:01.853193 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m09:12:01.857081 [debug] [ThreadPool]: On list_workspace: Close
[0m09:12:01.857081 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-e0ed-1f5b-a976-bc41cded47e6
[0m09:12:02.090388 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m09:12:02.090388 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m09:12:02.105831 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:02.105831 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m09:12:02.105831 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m09:12:02.105831 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:12:02.882635 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-e1bb-1999-a0a8-49ad6b275847
[0m09:12:03.269659 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m09:12:03.269659 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m09:12:03.269659 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m09:12:03.269659 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m09:12:03.269659 [debug] [ThreadPool]: On create_workspace_default: Close
[0m09:12:03.269659 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-e1bb-1999-a0a8-49ad6b275847
[0m09:12:03.507778 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:12:03.518284 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:12:03.519282 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m09:12:03.520249 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:12:04.334324 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07907-e296-1339-9cba-8287e9d90853
[0m09:12:04.740731 [debug] [ThreadPool]: SQL status: OK in 1.2200000286102295 seconds
[0m09:12:04.740731 [debug] [ThreadPool]: On list_workspace_default: Close
[0m09:12:04.756142 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07907-e296-1339-9cba-8287e9d90853
[0m09:12:04.991572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021503A6B9D0>]}
[0m09:12:04.991572 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:04.991572 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:12:04.991572 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:12:04.991572 [info ] [MainThread]: 
[0m09:12:05.008651 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m09:12:05.009015 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m09:12:05.009015 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m09:12:05.009015 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m09:12:05.009015 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m09:12:05.009015 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 09:12:05.009015 => 09:12:05.009015
[0m09:12:05.009015 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m09:12:05.036382 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m09:12:05.051101 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:05.052112 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m09:12:05.053012 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m09:12:05.053012 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:12:05.835615 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-e37f-1b3e-ace4-6c8b4d722d4d
[0m09:12:06.553961 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m09:12:06.569820 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 09:12:05.009015 => 09:12:06.569820
[0m09:12:06.569820 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m09:12:06.569820 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:06.569820 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m09:12:06.569820 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-e37f-1b3e-ace4-6c8b4d722d4d
[0m09:12:06.822748 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021503AB3A10>]}
[0m09:12:06.822748 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.81s]
[0m09:12:06.822748 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m09:12:06.822748 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m09:12:06.822748 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m09:12:06.822748 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m09:12:06.822748 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m09:12:06.822748 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m09:12:06.822748 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 09:12:06.822748 => 09:12:06.822748
[0m09:12:06.822748 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m09:12:06.840527 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m09:12:06.840527 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:06.840527 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m09:12:06.840527 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m09:12:06.840527 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:12:07.638447 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-e492-1932-8af5-89c23e3b7319
[0m09:12:08.423032 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m09:12:08.423032 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 09:12:06.838362 => 09:12:08.423032
[0m09:12:08.423032 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m09:12:08.438084 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:08.438526 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m09:12:08.438526 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-e492-1932-8af5-89c23e3b7319
[0m09:12:08.667063 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021504D04FD0>]}
[0m09:12:08.667063 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.84s]
[0m09:12:08.667063 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m09:12:08.667063 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m09:12:08.667063 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m09:12:08.667063 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m09:12:08.667063 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m09:12:08.684567 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m09:12:08.684567 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 09:12:08.667063 => 09:12:08.684567
[0m09:12:08.684567 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m09:12:08.689299 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m09:12:08.689299 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:08.689299 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m09:12:08.689299 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m09:12:08.689299 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:12:09.489593 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-e5ac-1b49-87f8-739ea92421bc
[0m09:12:10.157727 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:12:10.157727 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 09:12:08.684567 => 09:12:10.157727
[0m09:12:10.157727 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m09:12:10.157727 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:10.157727 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m09:12:10.157727 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-e5ac-1b49-87f8-739ea92421bc
[0m09:12:10.411426 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021504BC9710>]}
[0m09:12:10.413419 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.74s]
[0m09:12:10.414420 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m09:12:10.415438 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:12:10.416408 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m09:12:10.418168 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m09:12:10.418168 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m09:12:10.422157 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:12:10.423558 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 09:12:10.419165 => 09:12:10.423558
[0m09:12:10.423558 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m09:12:10.423558 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:12:10.423558 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:10.423558 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:12:10.423558 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m09:12:10.423558 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:12:11.224738 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-e6b4-194c-896b-f3c61188769d
[0m09:12:11.892272 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:12:11.909118 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 09:12:10.423558 => 09:12:11.909118
[0m09:12:11.909118 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m09:12:11.909118 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:11.909118 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m09:12:11.909118 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-e6b4-194c-896b-f3c61188769d
[0m09:12:12.150651 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021503AA75D0>]}
[0m09:12:12.150651 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.73s]
[0m09:12:12.150651 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:12:12.150651 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m09:12:12.150651 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m09:12:12.150651 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m09:12:12.150651 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m09:12:12.168154 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m09:12:12.168154 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 09:12:12.150651 => 09:12:12.168154
[0m09:12:12.168154 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m09:12:12.175251 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m09:12:12.176432 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:12.176432 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m09:12:12.176432 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m09:12:12.176432 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:12:12.977345 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-e7c0-1fdb-96a7-1c0e2dd9cf0f
[0m09:12:13.628126 [debug] [Thread-1 (]: SQL status: OK in 1.4500000476837158 seconds
[0m09:12:13.644885 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 09:12:12.168154 => 09:12:13.644885
[0m09:12:13.644885 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m09:12:13.644885 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:13.644885 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m09:12:13.644885 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-e7c0-1fdb-96a7-1c0e2dd9cf0f
[0m09:12:13.876506 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021504CF2990>]}
[0m09:12:13.876506 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.73s]
[0m09:12:13.876506 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m09:12:13.876506 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m09:12:13.876506 [info ] [Thread-1 (]: 6 of 12 START sql view model default.dim_localidade ............................ [RUN]
[0m09:12:13.892651 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m09:12:13.894990 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m09:12:13.894990 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m09:12:13.894990 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 09:12:13.894990 => 09:12:13.894990
[0m09:12:13.894990 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m09:12:13.894990 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m09:12:13.894990 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:13.894990 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m09:12:13.894990 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    -- Dimensão Localidade
select
    id as localidade_id,
    paciente,
    data_atendimento,
    procedimento as descricao_localidade
from `workspace`.`default`.`stg_localidade`

[0m09:12:13.894990 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:12:14.711600 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-e8c9-18e6-814a-4a5b99459149
[0m09:12:15.346509 [debug] [Thread-1 (]: SQL status: OK in 1.4500000476837158 seconds
[0m09:12:15.362420 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 09:12:13.894990 => 09:12:15.362420
[0m09:12:15.362420 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m09:12:15.362420 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:15.362420 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m09:12:15.362420 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-e8c9-18e6-814a-4a5b99459149
[0m09:12:15.603990 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021503AA75D0>]}
[0m09:12:15.603990 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.dim_localidade ....................... [[32mOK[0m in 1.73s]
[0m09:12:15.603990 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m09:12:15.603990 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m09:12:15.603990 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m09:12:15.603990 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m09:12:15.603990 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m09:12:15.603990 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m09:12:15.619611 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 09:12:15.603990 => 09:12:15.603990
[0m09:12:15.620437 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m09:12:15.621549 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m09:12:15.621549 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:15.621549 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m09:12:15.621549 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m09:12:15.628937 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:12:16.403994 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-e9cb-1933-b9c5-f364ffecd1ec
[0m09:12:17.115078 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m09:12:17.115078 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 09:12:15.621125 => 09:12:17.115078
[0m09:12:17.115078 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m09:12:17.115078 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:17.115078 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m09:12:17.115078 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-e9cb-1933-b9c5-f364ffecd1ec
[0m09:12:17.360399 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021579A1BF10>]}
[0m09:12:17.360399 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.76s]
[0m09:12:17.360399 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m09:12:17.360399 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m09:12:17.375818 [info ] [Thread-1 (]: 8 of 12 START sql view model default.fato_nascimento ........................... [RUN]
[0m09:12:17.378186 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_nascimento)
[0m09:12:17.378186 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m09:12:17.381813 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m09:12:17.381813 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 09:12:17.378186 => 09:12:17.381813
[0m09:12:17.381813 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m09:12:17.381813 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m09:12:17.381813 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:17.381813 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m09:12:17.381813 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m09:12:17.381813 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:12:18.182098 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-eadb-10b2-9756-916b750d4b57
[0m09:12:18.851680 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:12:18.853673 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 09:12:17.381813 => 09:12:18.853673
[0m09:12:18.854670 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m09:12:18.855639 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:18.855639 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m09:12:18.856635 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-eadb-10b2-9756-916b750d4b57
[0m09:12:19.074569 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021503AB8410>]}
[0m09:12:19.089996 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.fato_nascimento ...................... [[32mOK[0m in 1.70s]
[0m09:12:19.089996 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m09:12:19.089996 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m09:12:19.089996 [info ] [Thread-1 (]: 9 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m09:12:19.098769 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_nascimento, now model.projeto_health_insights.int_nascimento)
[0m09:12:19.099774 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m09:12:19.102785 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m09:12:19.104753 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 09:12:19.099774 => 09:12:19.103784
[0m09:12:19.104753 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m09:12:19.109767 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m09:12:19.110783 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:19.111734 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m09:12:19.111734 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH nv AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM nv

[0m09:12:19.112733 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:12:19.883877 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-ebde-1fdb-8a14-74dc4071ce8c
[0m09:12:20.585395 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:12:20.585395 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 09:12:19.105750 => 09:12:20.585395
[0m09:12:20.585395 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m09:12:20.585395 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:20.585395 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m09:12:20.585395 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-ebde-1fdb-8a14-74dc4071ce8c
[0m09:12:20.818718 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002157A819C10>]}
[0m09:12:20.834469 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 1.73s]
[0m09:12:20.835598 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m09:12:20.836627 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m09:12:20.837626 [info ] [Thread-1 (]: 10 of 12 START sql view model default.stg_tempo ................................ [RUN]
[0m09:12:20.838594 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m09:12:20.838594 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m09:12:20.843607 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m09:12:20.844584 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 09:12:20.839591 => 09:12:20.843607
[0m09:12:20.844584 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m09:12:20.848594 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m09:12:20.850565 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:20.851580 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m09:12:20.852558 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m09:12:20.852558 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:12:21.603142 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-ece4-16c6-9382-97a6b956fd0f
[0m09:12:22.270617 [debug] [Thread-1 (]: SQL status: OK in 1.4199999570846558 seconds
[0m09:12:22.287119 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 09:12:20.845606 => 09:12:22.287119
[0m09:12:22.287119 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m09:12:22.287119 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:22.287119 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m09:12:22.287119 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-ece4-16c6-9382-97a6b956fd0f
[0m09:12:22.512548 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021504CEFB90>]}
[0m09:12:22.512548 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.stg_tempo ........................... [[32mOK[0m in 1.67s]
[0m09:12:22.527962 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m09:12:22.527962 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:12:22.527962 [info ] [Thread-1 (]: 11 of 12 START sql view model default.fato_atendimento_hospitalar .............. [RUN]
[0m09:12:22.527962 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m09:12:22.527962 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:12:22.527962 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:12:22.527962 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 09:12:22.527962 => 09:12:22.527962
[0m09:12:22.527962 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:12:22.527962 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:12:22.527962 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:22.527962 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:12:22.543583 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m09:12:22.544482 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:12:23.337324 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-eded-1a22-a041-d9dcd5813e5f
[0m09:12:24.089113 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m09:12:24.104952 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 09:12:22.527962 => 09:12:24.104952
[0m09:12:24.104952 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m09:12:24.104952 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:24.104952 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m09:12:24.104952 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-eded-1a22-a041-d9dcd5813e5f
[0m09:12:24.331084 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021503AB8650>]}
[0m09:12:24.331084 [info ] [Thread-1 (]: 11 of 12 OK created sql view model default.fato_atendimento_hospitalar ......... [[32mOK[0m in 1.80s]
[0m09:12:24.331084 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:12:24.331084 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m09:12:24.331084 [info ] [Thread-1 (]: 12 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m09:12:24.331084 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_tempo)
[0m09:12:24.331084 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m09:12:24.356364 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m09:12:24.356364 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 09:12:24.331084 => 09:12:24.356364
[0m09:12:24.356364 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m09:12:24.356364 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m09:12:24.356364 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:24.356364 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m09:12:24.356364 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

SELECT DISTINCT
    data_id,
    data,
    ano,
    mes,
    dia,
    trimestre,
    dia_semana
FROM `workspace`.`default`.`stg_tempo`

[0m09:12:24.356364 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:12:25.139604 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07907-ef02-11c9-bd41-d5d68d757514
[0m09:12:25.823502 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m09:12:25.823502 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 09:12:24.356364 => 09:12:25.823502
[0m09:12:25.823502 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m09:12:25.823502 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:12:25.823502 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m09:12:25.823502 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07907-ef02-11c9-bd41-d5d68d757514
[0m09:12:26.049618 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd850153-fa1a-45e3-9f24-9d219b4ea555', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002157A819C10>]}
[0m09:12:26.065038 [info ] [Thread-1 (]: 12 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 1.72s]
[0m09:12:26.065038 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m09:12:26.065038 [debug] [MainThread]: On master: ROLLBACK
[0m09:12:26.065038 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:12:26.857984 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07907-f007-1805-a223-55fde1566e4f
[0m09:12:26.857984 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:12:26.857984 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:12:26.857984 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:12:26.857984 [debug] [MainThread]: On master: ROLLBACK
[0m09:12:26.857984 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:12:26.857984 [debug] [MainThread]: On master: Close
[0m09:12:26.857984 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07907-f007-1805-a223-55fde1566e4f
[0m09:12:27.115931 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:12:27.115931 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m09:12:27.115931 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:12:27.115931 [debug] [MainThread]: Connection 'model.projeto_health_insights.dim_tempo' was properly closed.
[0m09:12:27.115931 [info ] [MainThread]: 
[0m09:12:27.115931 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 26.43 seconds (26.43s).
[0m09:12:27.115931 [debug] [MainThread]: Command end result
[0m09:12:27.131521 [info ] [MainThread]: 
[0m09:12:27.131521 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:12:27.144849 [info ] [MainThread]: 
[0m09:12:27.145227 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 TOTAL=12
[0m09:12:27.145227 [debug] [MainThread]: Command `dbt run` succeeded at 09:12:27.145227 after 28.13 seconds
[0m09:12:27.145227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021579FA4CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021573F3EE10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002157A2C7450>]}
[0m09:12:27.145227 [debug] [MainThread]: Flushing usage events
[0m09:43:23.691682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0C3FD6F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0C0F20210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0C3A99510>]}


============================== 09:43:23.696550 | 26bc22d9-4c80-4bc7-8225-03ddda234869 ==============================
[0m09:43:23.696550 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:43:23.697548 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:43:27.238960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '26bc22d9-4c80-4bc7-8225-03ddda234869', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0C37B09D0>]}
[0m09:43:27.256912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '26bc22d9-4c80-4bc7-8225-03ddda234869', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0C37B09D0>]}
[0m09:43:27.257910 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m09:43:27.275921 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:43:27.731604 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:43:27.732602 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\schema.yaml
[0m09:43:27.759530 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m09:43:27.836092 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m09:43:27.938847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '26bc22d9-4c80-4bc7-8225-03ddda234869', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0CD1E8410>]}
[0m09:43:27.951815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '26bc22d9-4c80-4bc7-8225-03ddda234869', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0CE3C1FD0>]}
[0m09:43:27.952824 [info ] [MainThread]: Found 12 models, 11 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m09:43:27.953511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '26bc22d9-4c80-4bc7-8225-03ddda234869', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0CA76ED90>]}
[0m09:43:27.955536 [info ] [MainThread]: 
[0m09:43:27.957531 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:43:27.960315 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:43:27.965332 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:43:27.966300 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m09:43:27.966300 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:43:29.168727 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0790c-45e7-1d20-a297-d3a1e404255c
[0m09:43:29.390576 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetTables'), ('session-id', b'\x01\xf0y\x0cE\xe7\x1d \xa2\x97\xd3\xa1\xe4\x04%\\'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.21886157989501953/900.0')])
[0m09:43:37.187737 [debug] [ThreadPool]: SQL status: OK in 9.220000267028809 seconds
[0m09:43:37.193751 [debug] [ThreadPool]: On list_workspace_default: Close
[0m09:43:37.194748 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0790c-45e7-1d20-a297-d3a1e404255c
[0m09:43:37.434919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '26bc22d9-4c80-4bc7-8225-03ddda234869', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0CD1C4610>]}
[0m09:43:37.434919 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:43:37.435922 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:43:37.436892 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:43:37.437654 [info ] [MainThread]: 
[0m09:43:37.445530 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m09:43:37.445530 [info ] [Thread-1 (]: 1 of 11 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m09:43:37.447869 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e'
[0m09:43:37.448860 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m09:43:37.463847 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m09:43:37.467430 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 09:43:37.449872 => 09:43:37.466429
[0m09:43:37.467430 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m09:43:37.488372 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m09:43:37.491730 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:43:37.492730 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m09:43:37.494766 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m09:43:37.494766 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:43:38.293815 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-4b7a-105a-844e-cd87c6c68e86
[0m09:43:46.861439 [debug] [Thread-1 (]: SQL status: OK in 9.369999885559082 seconds
[0m09:43:47.149864 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 09:43:37.468426 => 09:43:47.148865
[0m09:43:47.151855 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m09:43:47.151855 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:43:47.152852 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m09:43:47.153849 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-4b7a-105a-844e-cd87c6c68e86
[0m09:43:47.389834 [info ] [Thread-1 (]: 1 of 11 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 9.94s]
[0m09:43:47.390862 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m09:43:47.391892 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m09:43:47.392879 [info ] [Thread-1 (]: 2 of 11 START test not_null_fato_nascimento_data_id ............................ [RUN]
[0m09:43:47.393858 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m09:43:47.394884 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m09:43:47.401864 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m09:43:47.402842 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 09:43:47.394884 => 09:43:47.402842
[0m09:43:47.403831 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m09:43:47.406823 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m09:43:47.408818 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:43:47.409863 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m09:43:47.409863 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m09:43:47.410841 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:43:48.326263 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-5170-15ad-8ba3-b27cf44f297f
[0m09:43:48.999486 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m09:43:49.001446 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_id` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 15 pos 6
[0m09:43:49.005437 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_id` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_id` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m09:43:49.008425 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0790c-5196-16c8-8841-510a2886082b
[0m09:43:49.011419 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 09:43:47.403831 => 09:43:49.010421
[0m09:43:49.012416 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m09:43:49.013410 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:43:49.013410 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m09:43:49.014408 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-5170-15ad-8ba3-b27cf44f297f
[0m09:43:49.404591 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_nascimento_data_id (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_id` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 15 pos 6
[0m09:43:49.405588 [error] [Thread-1 (]: 2 of 11 ERROR not_null_fato_nascimento_data_id ................................. [[31mERROR[0m in 2.01s]
[0m09:43:49.406586 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m09:43:49.407612 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:43:49.408581 [info ] [Thread-1 (]: 3 of 11 START test not_null_fato_nascimento_nascimento_id ...................... [RUN]
[0m09:43:49.409578 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m09:43:49.410575 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:43:49.415562 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m09:43:49.416616 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 09:43:49.410575 => 09:43:49.416616
[0m09:43:49.417616 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:43:49.421606 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m09:43:49.424300 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:43:49.425299 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m09:43:49.425299 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m09:43:49.426297 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:43:50.212214 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-5294-159f-a339-5f18ceb2e4a0
[0m09:43:50.985775 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m09:43:50.988788 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 09:43:49.417616 => 09:43:50.988788
[0m09:43:50.989785 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m09:43:50.990795 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:43:50.991780 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m09:43:50.992788 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-5294-159f-a339-5f18ceb2e4a0
[0m09:43:51.237249 [info ] [Thread-1 (]: 3 of 11 PASS not_null_fato_nascimento_nascimento_id ............................ [[32mPASS[0m in 1.83s]
[0m09:43:51.239858 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:43:51.239858 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc
[0m09:43:51.240827 [info ] [Thread-1 (]: 4 of 11 START test not_null_int_nascimento_nascimento_id ....................... [RUN]
[0m09:43:51.241835 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc)
[0m09:43:51.242851 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc
[0m09:43:51.247836 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc"
[0m09:43:51.249804 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc (compile): 09:43:51.242851 => 09:43:51.249804
[0m09:43:51.249804 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc
[0m09:43:51.253794 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc"
[0m09:43:51.254791 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:43:51.255787 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc"
[0m09:43:51.256800 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`int_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m09:43:51.257796 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:43:52.056048 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-53ad-14c9-beb9-17bc90c558f0
[0m09:43:52.651258 [debug] [Thread-1 (]: SQL status: OK in 1.3899999856948853 seconds
[0m09:43:52.659205 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc (execute): 09:43:51.250829 => 09:43:52.657241
[0m09:43:52.661199 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc: ROLLBACK
[0m09:43:52.662196 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:43:52.664192 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc: Close
[0m09:43:52.665188 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-53ad-14c9-beb9-17bc90c558f0
[0m09:43:52.892492 [info ] [Thread-1 (]: 4 of 11 PASS not_null_int_nascimento_nascimento_id ............................. [[32mPASS[0m in 1.65s]
[0m09:43:52.893292 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc
[0m09:43:52.894321 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6
[0m09:43:52.895317 [info ] [Thread-1 (]: 5 of 11 START test not_null_stg_nascidos_vivos_cod_municipio ................... [RUN]
[0m09:43:52.896287 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_int_nascimento_nascimento_id.576f2a29bc, now test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6)
[0m09:43:52.897331 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6
[0m09:43:52.902298 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6"
[0m09:43:52.904272 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6 (compile): 09:43:52.897331 => 09:43:52.904272
[0m09:43:52.905263 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6
[0m09:43:52.910279 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6"
[0m09:43:52.911253 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:43:52.912251 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6"
[0m09:43:52.912251 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`stg_nascidos_vivos`
where cod_municipio is null



      
    ) dbt_internal_test
[0m09:43:52.913269 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:43:53.734985 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-54ac-15dc-ade2-5a457015fad9
[0m09:43:54.479231 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m09:43:54.483057 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6 (execute): 09:43:52.905263 => 09:43:54.483057
[0m09:43:54.484054 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6: ROLLBACK
[0m09:43:54.485051 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:43:54.485051 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6: Close
[0m09:43:54.486060 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-54ac-15dc-ade2-5a457015fad9
[0m09:43:54.707942 [info ] [Thread-1 (]: 5 of 11 PASS not_null_stg_nascidos_vivos_cod_municipio ......................... [[32mPASS[0m in 1.81s]
[0m09:43:54.711931 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6
[0m09:43:54.713927 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:43:54.714912 [info ] [Thread-1 (]: 6 of 11 START test not_null_stg_nascidos_vivos_data_nascimento ................. [RUN]
[0m09:43:54.716881 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_stg_nascidos_vivos_cod_municipio.61a18dd6e6, now test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510)
[0m09:43:54.716881 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:43:54.723859 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510"
[0m09:43:54.726729 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510 (compile): 09:43:54.717900 => 09:43:54.725729
[0m09:43:54.727726 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:43:54.731743 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510"
[0m09:43:54.733147 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:43:54.734148 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510"
[0m09:43:54.734148 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_nascimento
from `workspace`.`default`.`stg_nascidos_vivos`
where data_nascimento is null



      
    ) dbt_internal_test
[0m09:43:54.735144 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:43:55.624317 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-55c8-1a39-a550-ed2f38b38288
[0m09:43:56.242320 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m09:43:56.250299 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510 (execute): 09:43:54.728735 => 09:43:56.250299
[0m09:43:56.251286 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510: ROLLBACK
[0m09:43:56.252255 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:43:56.252255 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510: Close
[0m09:43:56.253281 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-55c8-1a39-a550-ed2f38b38288
[0m09:43:56.491649 [info ] [Thread-1 (]: 6 of 11 PASS not_null_stg_nascidos_vivos_data_nascimento ....................... [[32mPASS[0m in 1.78s]
[0m09:43:56.493623 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:43:56.493623 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:43:56.494622 [info ] [Thread-1 (]: 7 of 11 START test not_null_stg_nascidos_vivos_nascimento_id ................... [RUN]
[0m09:43:56.496979 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510, now test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138)
[0m09:43:56.496979 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:43:56.505955 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138"
[0m09:43:56.509315 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138 (compile): 09:43:56.497976 => 09:43:56.508313
[0m09:43:56.509315 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:43:56.514300 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138"
[0m09:43:56.515326 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:43:56.516294 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138"
[0m09:43:56.516294 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`stg_nascidos_vivos`
where nascimento_id is null



      
    ) dbt_internal_test
[0m09:43:56.517321 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:43:57.301621 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-56cc-1aab-ba54-dd7c5d20d990
[0m09:43:57.681053 [debug] [Thread-1 (]: SQL status: OK in 1.159999966621399 seconds
[0m09:43:57.684072 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138 (execute): 09:43:56.510310 => 09:43:57.684072
[0m09:43:57.684072 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138: ROLLBACK
[0m09:43:57.685070 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:43:57.686066 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138: Close
[0m09:43:57.686066 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-56cc-1aab-ba54-dd7c5d20d990
[0m09:43:57.925122 [info ] [Thread-1 (]: 7 of 11 PASS not_null_stg_nascidos_vivos_nascimento_id ......................... [[32mPASS[0m in 1.43s]
[0m09:43:57.929110 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:43:57.931105 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m09:43:57.934065 [info ] [Thread-1 (]: 8 of 11 START test unique_dim_tempo_data_id .................................... [RUN]
[0m09:43:57.938051 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m09:43:57.940044 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m09:43:57.956026 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m09:43:57.957802 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 09:43:57.941071 => 09:43:57.957802
[0m09:43:57.958804 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m09:43:57.962791 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m09:43:57.964787 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:43:57.965787 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m09:43:57.965787 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m09:43:57.966816 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:43:58.830407 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-57b6-11e2-869e-37c88afa342b
[0m09:43:59.805785 [debug] [Thread-1 (]: SQL status: OK in 1.840000033378601 seconds
[0m09:43:59.812796 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 09:43:57.959800 => 09:43:59.812796
[0m09:43:59.814792 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m09:43:59.815758 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:43:59.816756 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m09:43:59.818782 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-57b6-11e2-869e-37c88afa342b
[0m09:44:00.035785 [info ] [Thread-1 (]: 8 of 11 PASS unique_dim_tempo_data_id .......................................... [[32mPASS[0m in 2.10s]
[0m09:44:00.036765 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m09:44:00.037793 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:44:00.038769 [info ] [Thread-1 (]: 9 of 11 START test unique_fato_nascimento_nascimento_id ........................ [RUN]
[0m09:44:00.039760 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m09:44:00.040785 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:44:00.046742 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m09:44:00.048737 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 09:44:00.040785 => 09:44:00.048737
[0m09:44:00.049733 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:44:00.052754 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m09:44:00.053750 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:44:00.054719 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m09:44:00.054719 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m09:44:00.055717 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:44:00.861162 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-58eb-1ec0-a77d-ebb778d082cf
[0m09:44:01.744557 [debug] [Thread-1 (]: SQL status: OK in 1.690000057220459 seconds
[0m09:44:01.748548 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 09:44:00.049733 => 09:44:01.748548
[0m09:44:01.750541 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m09:44:01.751507 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:44:01.752503 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m09:44:01.753530 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-58eb-1ec0-a77d-ebb778d082cf
[0m09:44:02.024418 [info ] [Thread-1 (]: 9 of 11 PASS unique_fato_nascimento_nascimento_id .............................. [[32mPASS[0m in 1.98s]
[0m09:44:02.025832 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:44:02.027829 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf
[0m09:44:02.027829 [info ] [Thread-1 (]: 10 of 11 START test unique_int_nascimento_nascimento_id ........................ [RUN]
[0m09:44:02.029852 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e, now test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf)
[0m09:44:02.029852 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf
[0m09:44:02.036806 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf"
[0m09:44:02.038802 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf (compile): 09:44:02.030849 => 09:44:02.038802
[0m09:44:02.039841 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf
[0m09:44:02.042816 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf"
[0m09:44:02.043814 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:44:02.044783 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf"
[0m09:44:02.044783 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`int_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m09:44:02.045812 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:44:02.849292 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-5a1b-1e4d-ba1e-17b44bdead04
[0m09:44:03.277831 [debug] [Thread-1 (]: SQL status: OK in 1.2300000190734863 seconds
[0m09:44:03.280829 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf (execute): 09:44:02.039841 => 09:44:03.280829
[0m09:44:03.281798 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf: ROLLBACK
[0m09:44:03.281798 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:44:03.282803 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf: Close
[0m09:44:03.282803 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-5a1b-1e4d-ba1e-17b44bdead04
[0m09:44:03.528782 [info ] [Thread-1 (]: 10 of 11 PASS unique_int_nascimento_nascimento_id .............................. [[32mPASS[0m in 1.50s]
[0m09:44:03.530006 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf
[0m09:44:03.531034 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:44:03.532005 [info ] [Thread-1 (]: 11 of 11 START test unique_stg_nascidos_vivos_nascimento_id .................... [RUN]
[0m09:44:03.533008 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_int_nascimento_nascimento_id.d8f00a1acf, now test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c)
[0m09:44:03.533998 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:44:03.542005 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c"
[0m09:44:03.543987 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c (compile): 09:44:03.533998 => 09:44:03.543001
[0m09:44:03.543987 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:44:03.546991 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c"
[0m09:44:03.548961 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:44:03.548961 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c"
[0m09:44:03.549984 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`stg_nascidos_vivos`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m09:44:03.550953 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:44:04.353709 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-5b01-1c77-8831-7df2b42efa56
[0m09:44:04.733470 [debug] [Thread-1 (]: SQL status: OK in 1.1799999475479126 seconds
[0m09:44:04.735962 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c (execute): 09:44:03.544999 => 09:44:04.735962
[0m09:44:04.736959 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c: ROLLBACK
[0m09:44:04.736959 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:44:04.737957 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c: Close
[0m09:44:04.738955 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-5b01-1c77-8831-7df2b42efa56
[0m09:44:04.976678 [info ] [Thread-1 (]: 11 of 11 PASS unique_stg_nascidos_vivos_nascimento_id .......................... [[32mPASS[0m in 1.44s]
[0m09:44:04.978634 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:44:04.980658 [debug] [MainThread]: On master: ROLLBACK
[0m09:44:04.980658 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:44:05.788149 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0790c-5bd8-1bbe-968f-1097b7b9cd67
[0m09:44:05.790145 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:44:05.791141 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:44:05.791141 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:44:05.792161 [debug] [MainThread]: On master: ROLLBACK
[0m09:44:05.792161 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:44:05.793138 [debug] [MainThread]: On master: Close
[0m09:44:05.793138 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0790c-5bd8-1bbe-968f-1097b7b9cd67
[0m09:44:06.041101 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:44:06.043094 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:44:06.044051 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c' was properly closed.
[0m09:44:06.045077 [info ] [MainThread]: 
[0m09:44:06.045688 [info ] [MainThread]: Finished running 11 tests in 0 hours 0 minutes and 38.09 seconds (38.09s).
[0m09:44:06.048710 [debug] [MainThread]: Command end result
[0m09:44:06.060663 [info ] [MainThread]: 
[0m09:44:06.062645 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:44:06.063654 [info ] [MainThread]: 
[0m09:44:06.064645 [error] [MainThread]: [33mRuntime Error in test not_null_fato_nascimento_data_id (models\marts\schema.yaml)[0m
[0m09:44:06.065638 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_id` cannot be resolved. Did you mean one of the following? [`peso`, `sexo`, `APGAR1`, `APGAR5`, `idade_mae`]. SQLSTATE: 42703; line 15 pos 6
[0m09:44:06.065638 [info ] [MainThread]: 
[0m09:44:06.066662 [info ] [MainThread]: Done. PASS=10 WARN=0 ERROR=1 SKIP=0 TOTAL=11
[0m09:44:06.067640 [debug] [MainThread]: Command `dbt test` failed at 09:44:06.067640 after 42.40 seconds
[0m09:44:06.068656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0C3BAF150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0C36E81D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B0C36E8E10>]}
[0m09:44:06.068656 [debug] [MainThread]: Flushing usage events
[0m09:47:35.891403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259BBAAE7D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259BBA9D9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259B9BB7850>]}


============================== 09:47:35.896776 | 2e02c097-2124-4989-b85c-25ba8b959d17 ==============================
[0m09:47:35.896776 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:47:35.897589 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:47:37.246676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2e02c097-2124-4989-b85c-25ba8b959d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259BBB65410>]}
[0m09:47:37.264655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2e02c097-2124-4989-b85c-25ba8b959d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259BBB65410>]}
[0m09:47:37.265652 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m09:47:37.281614 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:47:37.399290 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:47:37.402307 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\schema.yaml
[0m09:47:37.433177 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m09:47:37.505017 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m09:47:37.510002 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m09:47:37.515984 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m09:47:37.595307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2e02c097-2124-4989-b85c-25ba8b959d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C65C6ED0>]}
[0m09:47:37.607275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2e02c097-2124-4989-b85c-25ba8b959d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C5572550>]}
[0m09:47:37.607275 [info ] [MainThread]: Found 12 models, 6 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m09:47:37.608434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2e02c097-2124-4989-b85c-25ba8b959d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259C2B3EA90>]}
[0m09:47:37.610460 [info ] [MainThread]: 
[0m09:47:37.612426 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:47:37.615431 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:47:37.622400 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:47:37.622400 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m09:47:37.623425 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:47:38.496271 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0790c-daa2-1a14-bca5-9df100204f0d
[0m09:47:38.922460 [debug] [ThreadPool]: SQL status: OK in 1.2999999523162842 seconds
[0m09:47:38.951384 [debug] [ThreadPool]: On list_workspace_default: Close
[0m09:47:38.951384 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0790c-daa2-1a14-bca5-9df100204f0d
[0m09:47:39.191401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2e02c097-2124-4989-b85c-25ba8b959d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259BB2AFF10>]}
[0m09:47:39.193430 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:47:39.195425 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:47:39.197420 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:47:39.200377 [info ] [MainThread]: 
[0m09:47:39.219292 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m09:47:39.223283 [info ] [Thread-1 (]: 1 of 6 START test not_null_fato_nascimento_data_nascimento ..................... [RUN]
[0m09:47:39.228268 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d'
[0m09:47:39.230269 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m09:47:39.286146 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m09:47:39.289507 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d (compile): 09:47:39.233257 => 09:47:39.288479
[0m09:47:39.290473 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m09:47:39.316429 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m09:47:39.317427 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:47:39.318395 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m09:47:39.319393 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_nascimento
from `workspace`.`default`.`fato_nascimento`
where data_nascimento is null



      
    ) dbt_internal_test
[0m09:47:39.319393 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:47:40.118421 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-db9a-16db-b0ba-b37a3e220a2c
[0m09:47:41.793879 [debug] [Thread-1 (]: SQL status: OK in 2.4700000286102295 seconds
[0m09:47:41.798838 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d (execute): 09:47:39.291468 => 09:47:41.797841
[0m09:47:41.798838 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: ROLLBACK
[0m09:47:41.799836 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:47:41.800833 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: Close
[0m09:47:41.800833 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-db9a-16db-b0ba-b37a3e220a2c
[0m09:47:42.046895 [info ] [Thread-1 (]: 1 of 6 PASS not_null_fato_nascimento_data_nascimento ........................... [[32mPASS[0m in 2.82s]
[0m09:47:42.049680 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m09:47:42.049680 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:47:42.050700 [info ] [Thread-1 (]: 2 of 6 START test not_null_fato_nascimento_nascimento_id ....................... [RUN]
[0m09:47:42.051669 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m09:47:42.052694 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:47:42.059675 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m09:47:42.060655 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 09:47:42.052694 => 09:47:42.060655
[0m09:47:42.061655 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:47:42.065660 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m09:47:42.067628 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:47:42.068624 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m09:47:42.069621 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m09:47:42.070632 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:47:42.868775 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-dd3d-1b56-8b33-0a9a2084c2c5
[0m09:47:43.286537 [debug] [Thread-1 (]: SQL status: OK in 1.2200000286102295 seconds
[0m09:47:43.289529 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 09:47:42.061655 => 09:47:43.289529
[0m09:47:43.290544 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m09:47:43.290544 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:47:43.291525 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m09:47:43.292522 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-dd3d-1b56-8b33-0a9a2084c2c5
[0m09:47:43.527446 [info ] [Thread-1 (]: 2 of 6 PASS not_null_fato_nascimento_nascimento_id ............................. [[32mPASS[0m in 1.48s]
[0m09:47:43.528361 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:47:43.529391 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:47:43.530337 [info ] [Thread-1 (]: 3 of 6 START test not_null_stg_nascidos_vivos_data_nascimento .................. [RUN]
[0m09:47:43.531038 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510)
[0m09:47:43.532066 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:47:43.539021 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510"
[0m09:47:43.541015 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510 (compile): 09:47:43.532066 => 09:47:43.541015
[0m09:47:43.541015 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:47:43.544036 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510"
[0m09:47:43.546016 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:47:43.546016 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510"
[0m09:47:43.546999 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_nascimento
from `workspace`.`default`.`stg_nascidos_vivos`
where data_nascimento is null



      
    ) dbt_internal_test
[0m09:47:43.546999 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:47:44.357061 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-de22-1778-92af-732bf63cd8c7
[0m09:47:44.761835 [debug] [Thread-1 (]: SQL status: OK in 1.2100000381469727 seconds
[0m09:47:44.771808 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510 (execute): 09:47:43.542043 => 09:47:44.770809
[0m09:47:44.773802 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510: ROLLBACK
[0m09:47:44.774799 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:47:44.776793 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510: Close
[0m09:47:44.777790 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-de22-1778-92af-732bf63cd8c7
[0m09:47:45.018762 [info ] [Thread-1 (]: 3 of 6 PASS not_null_stg_nascidos_vivos_data_nascimento ........................ [[32mPASS[0m in 1.49s]
[0m09:47:45.020756 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:47:45.021753 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:47:45.022751 [info ] [Thread-1 (]: 4 of 6 START test not_null_stg_nascidos_vivos_nascimento_id .................... [RUN]
[0m09:47:45.024364 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510, now test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138)
[0m09:47:45.024364 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:47:45.033339 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138"
[0m09:47:45.034339 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138 (compile): 09:47:45.025361 => 09:47:45.034339
[0m09:47:45.035334 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:47:45.041318 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138"
[0m09:47:45.042286 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:47:45.043313 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138"
[0m09:47:45.044311 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`stg_nascidos_vivos`
where nascimento_id is null



      
    ) dbt_internal_test
[0m09:47:45.045278 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:47:45.844044 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-df04-1e06-aaf8-f5d3dc3a8b49
[0m09:47:46.245410 [debug] [Thread-1 (]: SQL status: OK in 1.2000000476837158 seconds
[0m09:47:46.254346 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138 (execute): 09:47:45.036302 => 09:47:46.254346
[0m09:47:46.255367 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138: ROLLBACK
[0m09:47:46.255367 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:47:46.256349 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138: Close
[0m09:47:46.256349 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-df04-1e06-aaf8-f5d3dc3a8b49
[0m09:47:46.489861 [info ] [Thread-1 (]: 4 of 6 PASS not_null_stg_nascidos_vivos_nascimento_id .......................... [[32mPASS[0m in 1.47s]
[0m09:47:46.490858 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:47:46.491855 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:47:46.492852 [info ] [Thread-1 (]: 5 of 6 START test unique_fato_nascimento_nascimento_id ......................... [RUN]
[0m09:47:46.494848 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m09:47:46.495844 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:47:46.504821 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m09:47:46.506816 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 09:47:46.495844 => 09:47:46.505818
[0m09:47:46.506816 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:47:46.509806 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m09:47:46.511807 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:47:46.511807 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m09:47:46.512798 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m09:47:46.512798 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:47:47.323081 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-dfe5-1992-94ad-3c7a3d611a90
[0m09:47:47.772451 [debug] [Thread-1 (]: SQL status: OK in 1.2599999904632568 seconds
[0m09:47:47.776440 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 09:47:46.507841 => 09:47:47.775442
[0m09:47:47.776440 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m09:47:47.777437 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:47:47.778462 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m09:47:47.779432 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-dfe5-1992-94ad-3c7a3d611a90
[0m09:47:48.006590 [info ] [Thread-1 (]: 5 of 6 PASS unique_fato_nascimento_nascimento_id ............................... [[32mPASS[0m in 1.51s]
[0m09:47:48.010609 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:47:48.013607 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:47:48.015598 [info ] [Thread-1 (]: 6 of 6 START test unique_stg_nascidos_vivos_nascimento_id ...................... [RUN]
[0m09:47:48.019590 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e, now test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c)
[0m09:47:48.022544 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:47:48.039497 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c"
[0m09:47:48.042490 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c (compile): 09:47:48.025541 => 09:47:48.041493
[0m09:47:48.043486 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:47:48.047476 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c"
[0m09:47:48.048473 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:47:48.049471 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c"
[0m09:47:48.050467 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`stg_nascidos_vivos`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m09:47:48.051494 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:47:48.880644 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0790c-e0d5-1710-b716-df0dacc21897
[0m09:47:49.321131 [debug] [Thread-1 (]: SQL status: OK in 1.2699999809265137 seconds
[0m09:47:49.324188 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c (execute): 09:47:48.043486 => 09:47:49.323191
[0m09:47:49.324188 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c: ROLLBACK
[0m09:47:49.325164 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m09:47:49.325164 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c: Close
[0m09:47:49.326181 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0790c-e0d5-1710-b716-df0dacc21897
[0m09:47:49.552906 [info ] [Thread-1 (]: 6 of 6 PASS unique_stg_nascidos_vivos_nascimento_id ............................ [[32mPASS[0m in 1.54s]
[0m09:47:49.555305 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:47:49.557268 [debug] [MainThread]: On master: ROLLBACK
[0m09:47:49.558297 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:47:50.370702 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0790c-e1b8-172e-8405-7c50d07cd6d2
[0m09:47:50.374689 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:47:50.377642 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:47:50.379635 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:47:50.379635 [debug] [MainThread]: On master: ROLLBACK
[0m09:47:50.380659 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m09:47:50.380659 [debug] [MainThread]: On master: Close
[0m09:47:50.381652 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0790c-e1b8-172e-8405-7c50d07cd6d2
[0m09:47:50.619465 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:47:50.621462 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:47:50.622445 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c' was properly closed.
[0m09:47:50.623415 [info ] [MainThread]: 
[0m09:47:50.624322 [info ] [MainThread]: Finished running 6 tests in 0 hours 0 minutes and 13.01 seconds (13.01s).
[0m09:47:50.627344 [debug] [MainThread]: Command end result
[0m09:47:50.641308 [info ] [MainThread]: 
[0m09:47:50.642277 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:47:50.643274 [info ] [MainThread]: 
[0m09:47:50.645269 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m09:47:50.646266 [debug] [MainThread]: Command `dbt test` succeeded at 09:47:50.645269 after 14.77 seconds
[0m09:47:50.646266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259B5BEC950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259B593BB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000259B5B7FE10>]}
[0m09:47:50.647263 [debug] [MainThread]: Flushing usage events
[0m09:48:40.837678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022425EA7790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002242586F690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224255AE710>]}


============================== 09:48:40.842665 | 40030bc0-728d-4419-b2ad-11ce4085a0ef ==============================
[0m09:48:40.842665 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:48:40.843654 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:48:42.145716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '40030bc0-728d-4419-b2ad-11ce4085a0ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022425EB6950>]}
[0m09:48:42.164634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '40030bc0-728d-4419-b2ad-11ce4085a0ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022425EB6950>]}
[0m09:48:42.165660 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m09:48:42.184605 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m09:48:42.289332 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:48:42.290327 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:48:42.297311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '40030bc0-728d-4419-b2ad-11ce4085a0ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022430146150>]}
[0m09:48:42.300286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '40030bc0-728d-4419-b2ad-11ce4085a0ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224301BDC90>]}
[0m09:48:42.300286 [info ] [MainThread]: Found 12 models, 6 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m09:48:42.301304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40030bc0-728d-4419-b2ad-11ce4085a0ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002242C66E1D0>]}
[0m09:48:42.304291 [info ] [MainThread]: 
[0m09:48:42.305262 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m09:48:42.308252 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m09:48:42.313265 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m09:48:42.313265 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m09:48:42.314263 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:48:43.237943 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0790d-0139-1d52-b2dd-c5009c7911f2
[0m09:48:43.996649 [debug] [ThreadPool]: SQL status: OK in 1.6799999475479126 seconds
[0m09:48:44.004627 [debug] [ThreadPool]: On list_workspace_default: Close
[0m09:48:44.005624 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0790d-0139-1d52-b2dd-c5009c7911f2
[0m09:48:44.247194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40030bc0-728d-4419-b2ad-11ce4085a0ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002242C66E1D0>]}
[0m09:48:44.248192 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:48:44.249014 [info ] [MainThread]: 
[0m09:48:44.257006 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m09:48:44.258006 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m09:48:44.258006 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m09:48:44.318872 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m09:48:44.320838 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 09:48:44.259003 => 09:48:44.320838
[0m09:48:44.321836 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m09:48:44.321836 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 09:48:44.321836 => 09:48:44.321836
[0m09:48:44.323860 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m09:48:44.324827 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m09:48:44.325243 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m09:48:44.326243 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m09:48:44.330232 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m09:48:44.331229 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 09:48:44.326243 => 09:48:44.331229
[0m09:48:44.332227 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m09:48:44.333225 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 09:48:44.332227 => 09:48:44.332227
[0m09:48:44.334252 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m09:48:44.335247 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m09:48:44.336085 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m09:48:44.337114 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m09:48:44.341103 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m09:48:44.342072 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 09:48:44.337114 => 09:48:44.342072
[0m09:48:44.343070 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m09:48:44.343070 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 09:48:44.343070 => 09:48:44.343070
[0m09:48:44.345092 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m09:48:44.345092 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:48:44.346092 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m09:48:44.347086 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m09:48:44.350078 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m09:48:44.352045 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 09:48:44.347086 => 09:48:44.351075
[0m09:48:44.352045 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m09:48:44.353072 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 09:48:44.353072 => 09:48:44.353072
[0m09:48:44.354067 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m09:48:44.355055 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m09:48:44.356034 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m09:48:44.357034 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m09:48:44.360051 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m09:48:44.361021 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 09:48:44.357034 => 09:48:44.361021
[0m09:48:44.362049 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m09:48:44.362049 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 09:48:44.362049 => 09:48:44.362049
[0m09:48:44.364040 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m09:48:44.364040 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m09:48:44.365032 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.dim_localidade)
[0m09:48:44.366036 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m09:48:44.369027 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m09:48:44.369997 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 09:48:44.366036 => 09:48:44.369997
[0m09:48:44.371022 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m09:48:44.372019 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 09:48:44.371022 => 09:48:44.371022
[0m09:48:44.373017 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m09:48:44.373017 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m09:48:44.374014 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.int_atendimento)
[0m09:48:44.375017 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m09:48:44.378972 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m09:48:44.380979 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 09:48:44.375017 => 09:48:44.380979
[0m09:48:44.381972 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m09:48:44.381972 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 09:48:44.381972 => 09:48:44.381972
[0m09:48:44.383987 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m09:48:44.383987 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m09:48:44.384966 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_nascimento)
[0m09:48:44.385983 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m09:48:44.388974 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m09:48:44.390941 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 09:48:44.385983 => 09:48:44.389971
[0m09:48:44.390941 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m09:48:44.391976 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 09:48:44.391976 => 09:48:44.391976
[0m09:48:44.392963 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m09:48:44.393955 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m09:48:44.394603 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_nascimento, now model.projeto_health_insights.int_nascimento)
[0m09:48:44.395602 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m09:48:44.398595 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m09:48:44.399593 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 09:48:44.395602 => 09:48:44.399593
[0m09:48:44.400589 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m09:48:44.401587 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 09:48:44.400589 => 09:48:44.400589
[0m09:48:44.402584 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m09:48:44.402584 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m09:48:44.403581 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m09:48:44.404578 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m09:48:44.407571 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m09:48:44.409566 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 09:48:44.404578 => 09:48:44.408568
[0m09:48:44.409566 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m09:48:44.410562 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 09:48:44.410562 => 09:48:44.410562
[0m09:48:44.411560 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m09:48:44.412558 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:48:44.413555 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510)
[0m09:48:44.413555 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:48:44.426520 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510"
[0m09:48:44.428515 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510 (compile): 09:48:44.414552 => 09:48:44.427517
[0m09:48:44.428515 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:48:44.429512 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510 (execute): 09:48:44.429512 => 09:48:44.429512
[0m09:48:44.430509 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510
[0m09:48:44.431507 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:48:44.432504 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_stg_nascidos_vivos_data_nascimento.2cdb852510, now test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138)
[0m09:48:44.432504 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:48:44.439485 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138"
[0m09:48:44.440482 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138 (compile): 09:48:44.433501 => 09:48:44.440482
[0m09:48:44.441480 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:48:44.441480 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138 (execute): 09:48:44.441480 => 09:48:44.441480
[0m09:48:44.443475 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138
[0m09:48:44.443475 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:48:44.444471 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_stg_nascidos_vivos_nascimento_id.8a14a1e138, now test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c)
[0m09:48:44.445469 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:48:44.452451 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c"
[0m09:48:44.453448 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c (compile): 09:48:44.445469 => 09:48:44.453448
[0m09:48:44.454445 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:48:44.454445 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c (execute): 09:48:44.454445 => 09:48:44.454445
[0m09:48:44.456440 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c
[0m09:48:44.456440 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:48:44.457438 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_stg_nascidos_vivos_nascimento_id.130b6de16c, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m09:48:44.458434 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:48:44.461427 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m09:48:44.462424 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 09:48:44.458434 => 09:48:44.462424
[0m09:48:44.462424 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:48:44.463421 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 09:48:44.463421 => 09:48:44.463421
[0m09:48:44.464418 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m09:48:44.465416 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m09:48:44.466413 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d)
[0m09:48:44.466413 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m09:48:44.471400 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m09:48:44.473395 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d (compile): 09:48:44.467420 => 09:48:44.472398
[0m09:48:44.473395 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m09:48:44.474393 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d (execute): 09:48:44.474393 => 09:48:44.474393
[0m09:48:44.475390 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m09:48:44.476387 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:48:44.477384 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m09:48:44.477384 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:48:44.482371 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m09:48:44.484365 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 09:48:44.478381 => 09:48:44.483367
[0m09:48:44.484365 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:48:44.485362 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 09:48:44.485362 => 09:48:44.485362
[0m09:48:44.486360 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m09:48:44.487358 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:48:44.488354 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m09:48:44.488354 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:48:44.493341 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m09:48:44.494338 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 09:48:44.489352 => 09:48:44.494338
[0m09:48:44.495358 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:48:44.495358 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 09:48:44.495358 => 09:48:44.495358
[0m09:48:44.497359 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m09:48:44.497359 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m09:48:44.498330 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e, now model.projeto_health_insights.dim_tempo)
[0m09:48:44.499354 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m09:48:44.502345 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m09:48:44.504335 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 09:48:44.499354 => 09:48:44.503346
[0m09:48:44.504335 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m09:48:44.505311 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 09:48:44.505311 => 09:48:44.505311
[0m09:48:44.506335 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m09:48:44.508302 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:48:44.508302 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m09:48:44.509299 [debug] [MainThread]: Connection 'model.projeto_health_insights.dim_tempo' was properly closed.
[0m09:48:44.512318 [debug] [MainThread]: Command end result
[0m09:48:44.750937 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m09:48:44.750937 [info ] [MainThread]: Building catalog
[0m09:48:44.754926 [debug] [ThreadPool]: Acquiring new databricks connection 'raw'
[0m09:48:44.768916 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:48:44.769913 [debug] [ThreadPool]: Using databricks connection "raw"
[0m09:48:44.769913 [debug] [ThreadPool]: On raw: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "raw"} */

      select current_catalog()
  
[0m09:48:44.770904 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:48:45.540909 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0790d-029b-1069-a98a-981fbbb45cc0
[0m09:48:45.930590 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m09:48:45.940193 [debug] [ThreadPool]: Using databricks connection "raw"
[0m09:48:45.940193 [debug] [ThreadPool]: On raw: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "raw"} */
show table extended in `workspace`.`raw` like 'doenca|atendimento|procedimento|localidade|faixa_etaria_sexo'
  
[0m09:48:46.709985 [debug] [ThreadPool]: SQL status: OK in 0.7699999809265137 seconds
[0m09:48:46.712949 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`atendimento`
[0m09:48:46.712949 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`doenca`
[0m09:48:46.713946 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`localidade`
[0m09:48:46.716601 [debug] [ThreadPool]: On raw: ROLLBACK
[0m09:48:46.717601 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m09:48:46.717601 [debug] [ThreadPool]: On raw: Close
[0m09:48:46.718597 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0790d-029b-1069-a98a-981fbbb45cc0
[0m09:48:46.955483 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly raw, now default)
[0m09:48:46.958469 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:48:46.959453 [debug] [ThreadPool]: Using databricks connection "default"
[0m09:48:46.959453 [debug] [ThreadPool]: On default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "default"} */

      select current_catalog()
  
[0m09:48:46.960441 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:48:47.752736 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0790d-03eb-194c-aaa9-8f6dcc17b5fa
[0m09:48:48.078198 [debug] [ThreadPool]: SQL status: OK in 1.1200000047683716 seconds
[0m09:48:48.083170 [debug] [ThreadPool]: Using databricks connection "default"
[0m09:48:48.083170 [debug] [ThreadPool]: On default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "default"} */
show table extended in `workspace`.`default` like 'stg_tempo|stg_localidade|stg_doenca|stg_atendimento|int_nascimento|dim_localidade|fato_atendimento_hospitalar|dim_tempo|stg_nascidos_vivos|sinasc_2022_sc_clean|dim_doenca|int_atendimento|fato_nascimento'
  
[0m09:48:48.710673 [debug] [ThreadPool]: SQL status: OK in 0.6299999952316284 seconds
[0m09:48:48.713654 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`dim_doenca`
[0m09:48:48.714652 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`dim_localidade`
[0m09:48:48.715649 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`dim_tempo`
[0m09:48:48.716646 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`fato_atendimento_hospitalar`
[0m09:48:48.716646 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`fato_nascimento`
[0m09:48:48.717644 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`int_atendimento`
[0m09:48:48.718665 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`int_nascimento`
[0m09:48:48.719640 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`sinasc_2022_sc_clean`
[0m09:48:48.720637 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_atendimento`
[0m09:48:48.721634 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_doenca`
[0m09:48:48.722630 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_localidade`
[0m09:48:48.723628 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_nascidos_vivos`
[0m09:48:48.723628 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_tempo`
[0m09:48:48.731607 [debug] [ThreadPool]: On default: ROLLBACK
[0m09:48:48.731607 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m09:48:48.732604 [debug] [ThreadPool]: On default: Close
[0m09:48:48.732604 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0790d-03eb-194c-aaa9-8f6dcc17b5fa
[0m09:48:48.988963 [info ] [MainThread]: Catalog written to C:\Users\Marisa\Desktop\projeto_health_insights\target\catalog.json
[0m09:48:48.989961 [debug] [MainThread]: Command `dbt docs generate` succeeded at 09:48:48.989961 after 8.17 seconds
[0m09:48:48.990973 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m09:48:48.990973 [debug] [MainThread]: Connection 'default' was properly closed.
[0m09:48:48.991956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022425851E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224255BA9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002241F3BBBD0>]}
[0m09:48:48.992975 [debug] [MainThread]: Flushing usage events
[0m09:49:12.974158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187E76C8E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187E7FA7410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187E7684FD0>]}


============================== 09:49:12.979139 | 3026e0e7-c459-47f3-b4ca-d558f925935b ==============================
[0m09:49:12.979139 [info ] [MainThread]: Running with dbt=1.5.2
[0m09:49:12.980108 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m09:49:14.277044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3026e0e7-c459-47f3-b4ca-d558f925935b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187E7FB4750>]}
[0m09:49:14.294996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3026e0e7-c459-47f3-b4ca-d558f925935b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187E7FB4750>]}
[0m11:03:10.125158 [error] [MainThread]: Encountered an error:

[0m11:03:10.244843 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 86, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 71, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 142, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 215, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\main.py", line 299, in docs_serve
    results = task.run()
              ^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\task\serve.py", line 28, in run
    httpd.serve_forever()
  File "C:\Users\Marisa\AppData\Local\Programs\Python\Python311\Lib\socketserver.py", line 233, in serve_forever
    ready = selector.select(poll_interval)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\AppData\Local\Programs\Python\Python311\Lib\selectors.py", line 323, in select
    r, w, _ = self._select(self._readers, self._writers, [], timeout)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\AppData\Local\Programs\Python\Python311\Lib\selectors.py", line 314, in _select
    r, w, x = select.select(r, w, w, timeout)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

[0m11:03:10.270053 [debug] [MainThread]: Command `dbt docs serve` failed at 11:03:10.270053 after 4437.32 seconds
[0m11:03:10.275095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187E7741A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187E7FB6CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187E6819690>]}
[0m11:03:10.282252 [debug] [MainThread]: Flushing usage events
[0m11:03:23.255023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F560306F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F560307E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F55FCEE190>]}


============================== 11:03:23.265455 | 7d797882-a862-43aa-802d-12205dc69691 ==============================
[0m11:03:23.265455 [info ] [MainThread]: Running with dbt=1.5.2
[0m11:03:23.265455 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:03:24.655127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F55FF6FBD0>]}
[0m11:03:24.675144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F569408210>]}
[0m11:03:24.675144 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m11:03:24.694773 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m11:03:24.813261 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 5 files changed.
[0m11:03:24.814259 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\schema.yaml
[0m11:03:24.815255 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\dim_localidade.sql
[0m11:03:24.816264 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\intermediate\int_nascimento.sql
[0m11:03:24.816264 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m11:03:24.817278 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\dim_tempo.sql
[0m11:03:24.845176 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m11:03:24.859807 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_nascidos_vivos.sql
[0m11:03:24.865351 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m11:03:24.867368 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m11:03:24.867368 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m11:03:25.014931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F56A5EB390>]}
[0m11:03:25.025087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F56A55C850>]}
[0m11:03:25.025087 [info ] [MainThread]: Found 12 models, 11 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m11:03:25.025087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F566ACA310>]}
[0m11:03:25.025087 [info ] [MainThread]: 
[0m11:03:25.025087 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:03:25.035672 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m11:03:25.036496 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m11:03:25.037524 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m11:03:25.037524 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:03:26.281800 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07917-712f-19db-b762-d4876770c408
[0m11:03:26.481943 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y\x17q/\x19\xdb\xb7b\xd4\x87gp\xc4\x08'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.20014309883117676/900.0')])
[0m11:03:31.735560 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y\x17q/\x19\xdb\xb7b\xd4\x87gp\xc4\x08'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '2/30'), ('elapsed-seconds', '5.453760147094727/900.0')])
[0m11:03:36.955336 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y\x17q/\x19\xdb\xb7b\xd4\x87gp\xc4\x08'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '3/30'), ('elapsed-seconds', '10.673535823822021/900.0')])
[0m11:03:42.155362 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y\x17q/\x19\xdb\xb7b\xd4\x87gp\xc4\x08'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '4/30'), ('elapsed-seconds', '15.873562097549438/900.0')])
[0m11:03:47.365077 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5.0625 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y\x17q/\x19\xdb\xb7b\xd4\x87gp\xc4\x08'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5.0625), ('attempt', '5/30'), ('elapsed-seconds', '21.083276748657227/900.0')])
[0m11:03:52.695745 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 7.59375 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y\x17q/\x19\xdb\xb7b\xd4\x87gp\xc4\x08'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 7.59375), ('attempt', '6/30'), ('elapsed-seconds', '26.413944482803345/900.0')])
[0m11:04:00.565394 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 11.390625 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y\x17q/\x19\xdb\xb7b\xd4\x87gp\xc4\x08'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 11.390625), ('attempt', '7/30'), ('elapsed-seconds', '34.28359365463257/900.0')])
[0m11:04:12.175428 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 17.0859375 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y\x17q/\x19\xdb\xb7b\xd4\x87gp\xc4\x08'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 17.0859375), ('attempt', '8/30'), ('elapsed-seconds', '45.89362812042236/900.0')])
[0m11:04:31.343251 [debug] [ThreadPool]: SQL status: OK in 66.30000305175781 seconds
[0m11:04:31.346829 [debug] [ThreadPool]: On list_workspace: Close
[0m11:04:31.347828 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07917-712f-19db-b762-d4876770c408
[0m11:04:31.589945 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m11:04:31.593931 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m11:04:31.616924 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:31.616924 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m11:04:31.616924 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m11:04:31.625035 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:04:32.465183 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07917-98bd-1a49-81dc-05fc33c1720a
[0m11:04:33.995121 [debug] [ThreadPool]: SQL status: OK in 2.369999885559082 seconds
[0m11:04:34.004800 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m11:04:34.004800 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m11:04:34.004800 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m11:04:34.004800 [debug] [ThreadPool]: On create_workspace_default: Close
[0m11:04:34.004800 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07917-98bd-1a49-81dc-05fc33c1720a
[0m11:04:34.255063 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m11:04:34.264994 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m11:04:34.264994 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m11:04:34.264994 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:04:35.065484 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07917-9a4a-13dd-8a99-00eb133a7be5
[0m11:04:35.595229 [debug] [ThreadPool]: SQL status: OK in 1.3300000429153442 seconds
[0m11:04:35.595229 [debug] [ThreadPool]: On list_workspace_default: Close
[0m11:04:35.604785 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07917-9a4a-13dd-8a99-00eb133a7be5
[0m11:04:35.834756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F56A55C490>]}
[0m11:04:35.834756 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:35.834756 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:04:35.834756 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:04:35.834756 [info ] [MainThread]: 
[0m11:04:35.846413 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m11:04:35.846800 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m11:04:35.846800 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m11:04:35.846800 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m11:04:35.854655 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m11:04:35.856645 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 11:04:35.846800 => 11:04:35.856645
[0m11:04:35.856645 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m11:04:35.889902 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m11:04:35.889902 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:35.889902 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m11:04:35.889902 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m11:04:35.894959 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:04:36.745055 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07917-9b4b-1f6d-9a9a-f301bbc27e7d
[0m11:04:40.515363 [debug] [Thread-1 (]: SQL status: OK in 4.619999885559082 seconds
[0m11:04:40.534999 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 11:04:35.856645 => 11:04:40.534999
[0m11:04:40.540004 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m11:04:40.540004 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:04:40.540004 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m11:04:40.540004 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07917-9b4b-1f6d-9a9a-f301bbc27e7d
[0m11:04:40.785003 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F56A65DED0>]}
[0m11:04:40.794773 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 4.94s]
[0m11:04:40.794773 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m11:04:40.794773 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m11:04:40.805091 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m11:04:40.807011 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m11:04:40.807011 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m11:04:40.811065 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m11:04:40.815113 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 11:04:40.808008 => 11:04:40.811065
[0m11:04:40.815113 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m11:04:40.820118 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m11:04:40.820118 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:40.820118 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m11:04:40.820118 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m11:04:40.820118 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:04:41.625448 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07917-9e34-1fa4-a3e3-dab981f9e2b7
[0m11:04:42.725302 [debug] [Thread-1 (]: SQL status: OK in 1.909999966621399 seconds
[0m11:04:42.734806 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 11:04:40.815113 => 11:04:42.734806
[0m11:04:42.734806 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m11:04:42.734806 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:04:42.734806 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m11:04:42.734806 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07917-9e34-1fa4-a3e3-dab981f9e2b7
[0m11:04:42.975281 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F569534350>]}
[0m11:04:42.975281 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 2.17s]
[0m11:04:42.975281 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m11:04:42.975281 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m11:04:42.975281 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m11:04:42.984783 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m11:04:42.984783 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m11:04:42.984783 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m11:04:42.984783 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 11:04:42.984783 => 11:04:42.984783
[0m11:04:42.984783 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m11:04:42.996080 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m11:04:42.997428 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:42.997428 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m11:04:42.997428 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m11:04:42.997428 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:04:43.825209 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07917-9f7e-1d61-bb68-858d8c5d72bb
[0m11:04:44.845157 [debug] [Thread-1 (]: SQL status: OK in 1.850000023841858 seconds
[0m11:04:44.855109 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 11:04:42.984783 => 11:04:44.855109
[0m11:04:44.855109 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m11:04:44.855109 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:04:44.855109 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m11:04:44.855109 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07917-9f7e-1d61-bb68-858d8c5d72bb
[0m11:04:45.085330 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F569528C50>]}
[0m11:04:45.085330 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 2.11s]
[0m11:04:45.085330 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m11:04:45.085330 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m11:04:45.085330 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m11:04:45.094756 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m11:04:45.094756 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m11:04:45.094756 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m11:04:45.094756 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 11:04:45.094756 => 11:04:45.094756
[0m11:04:45.094756 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m11:04:45.104927 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m11:04:45.104927 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:45.104927 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m11:04:45.104927 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m11:04:45.104927 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:04:45.895313 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07917-a0c0-1f42-affc-72d085d86c7c
[0m11:04:47.034866 [debug] [Thread-1 (]: SQL status: OK in 1.9299999475479126 seconds
[0m11:04:47.034866 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 11:04:45.094756 => 11:04:47.034866
[0m11:04:47.034866 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m11:04:47.034866 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:04:47.034866 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m11:04:47.034866 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07917-a0c0-1f42-affc-72d085d86c7c
[0m11:04:47.285368 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F56A645F10>]}
[0m11:04:47.295089 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 2.20s]
[0m11:04:47.295089 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m11:04:47.295089 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m11:04:47.305109 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m11:04:47.306995 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m11:04:47.307648 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m11:04:47.308151 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m11:04:47.308151 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 11:04:47.308151 => 11:04:47.308151
[0m11:04:47.314685 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m11:04:47.318444 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m11:04:47.318444 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:47.318444 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m11:04:47.318444 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m11:04:47.324989 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:04:48.129952 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07917-a214-11ad-97fd-232ebb9d2f05
[0m11:04:48.925173 [debug] [Thread-1 (]: SQL status: OK in 1.6100000143051147 seconds
[0m11:04:48.934748 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 11:04:47.314685 => 11:04:48.934748
[0m11:04:48.934748 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m11:04:48.934748 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:04:48.934748 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m11:04:48.939754 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07917-a214-11ad-97fd-232ebb9d2f05
[0m11:04:49.155087 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F56A7BC350>]}
[0m11:04:49.155087 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.85s]
[0m11:04:49.165026 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m11:04:49.165026 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m11:04:49.165026 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m11:04:49.165026 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m11:04:49.165026 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m11:04:49.165026 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m11:04:49.165026 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 11:04:49.165026 => 11:04:49.165026
[0m11:04:49.175105 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m11:04:49.180111 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m11:04:49.180111 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:49.180111 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m11:04:49.180111 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m11:04:49.180111 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:04:50.015118 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07917-a330-1fd4-b5dd-049b833268d3
[0m11:04:50.813988 [debug] [Thread-1 (]: SQL status: OK in 1.6299999952316284 seconds
[0m11:04:50.816950 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 11:04:49.175105 => 11:04:50.816950
[0m11:04:50.817975 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m11:04:50.818981 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:04:50.818981 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m11:04:50.819973 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07917-a330-1fd4-b5dd-049b833268d3
[0m11:04:51.045161 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F56A7A8AD0>]}
[0m11:04:51.045161 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.88s]
[0m11:04:51.045161 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m11:04:51.045161 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m11:04:51.045161 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m11:04:51.045161 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m11:04:51.054861 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m11:04:51.055393 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m11:04:51.055393 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 11:04:51.055393 => 11:04:51.055393
[0m11:04:51.055393 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m11:04:51.065952 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m11:04:51.067163 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:51.067163 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m11:04:51.067163 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH base AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM base

[0m11:04:51.067163 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:04:51.856671 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07917-a44c-1f74-a7b3-21858867529c
[0m11:04:52.609774 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m11:04:52.614883 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 11:04:51.055393 => 11:04:52.614883
[0m11:04:52.614883 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m11:04:52.614883 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:04:52.614883 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m11:04:52.614883 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07917-a44c-1f74-a7b3-21858867529c
[0m11:04:52.858166 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F56A77DB50>]}
[0m11:04:52.860130 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 1.81s]
[0m11:04:52.861081 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m11:04:52.862109 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m11:04:52.863083 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m11:04:52.864090 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m11:04:52.865103 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m11:04:52.869090 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m11:04:52.871057 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 11:04:52.865103 => 11:04:52.870111
[0m11:04:52.871843 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m11:04:52.875169 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m11:04:52.875169 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:52.879690 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m11:04:52.879690 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m11:04:52.879690 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:04:53.695056 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07917-a562-189f-a83d-304ebbea798d
[0m11:04:54.485091 [debug] [Thread-1 (]: SQL status: OK in 1.6100000143051147 seconds
[0m11:04:54.494741 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 11:04:52.872545 => 11:04:54.494741
[0m11:04:54.494741 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m11:04:54.494741 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:04:54.494741 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m11:04:54.494741 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07917-a562-189f-a83d-304ebbea798d
[0m11:04:54.734767 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F569531410>]}
[0m11:04:54.739780 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.87s]
[0m11:04:54.739780 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m11:04:54.744931 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:04:54.744931 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m11:04:54.749609 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m11:04:54.750485 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:04:54.754828 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m11:04:54.754828 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 11:04:54.750789 => 11:04:54.754828
[0m11:04:54.754828 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:04:54.759046 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m11:04:54.763581 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:54.763581 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m11:04:54.765121 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m11:04:54.765121 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:04:55.545296 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07917-a681-1e04-9ae0-9b1adc9be796
[0m11:04:56.324928 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m11:04:56.329936 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 11:04:54.757843 => 11:04:56.329936
[0m11:04:56.329936 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m11:04:56.329936 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:04:56.329936 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m11:04:56.329936 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07917-a681-1e04-9ae0-9b1adc9be796
[0m11:04:56.567791 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F56A7607D0>]}
[0m11:04:56.570785 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.82s]
[0m11:04:56.575526 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:04:56.576639 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m11:04:56.576639 [info ] [Thread-1 (]: 10 of 12 START sql table model default.dim_localidade .......................... [RUN]
[0m11:04:56.576639 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m11:04:56.576639 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m11:04:56.584740 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m11:04:56.584740 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 11:04:56.576639 => 11:04:56.584740
[0m11:04:56.584740 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m11:04:56.605049 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:56.605049 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m11:04:56.605049 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */

      describe extended `workspace`.`default`.`dim_localidade`
  
[0m11:04:56.605049 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:04:57.395329 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07917-a79a-16a7-a311-bae9270129a3
[0m11:04:57.960196 [debug] [Thread-1 (]: SQL status: OK in 1.3600000143051147 seconds
[0m11:04:57.960196 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 11:04:56.584740 => 11:04:57.960196
[0m11:04:57.964810 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m11:04:57.964810 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:04:57.964810 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m11:04:57.964810 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07917-a79a-16a7-a311-bae9270129a3
[0m11:04:58.234887 [debug] [Thread-1 (]: Runtime Error in model dim_localidade (models\marts\dim_localidade.sql)
  dbt could not find a macro with the name "drop_relation" in any package
[0m11:04:58.234887 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F56A7D0410>]}
[0m11:04:58.234887 [error] [Thread-1 (]: 10 of 12 ERROR creating sql table model default.dim_localidade ................. [[31mERROR[0m in 1.66s]
[0m11:04:58.244970 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m11:04:58.244970 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m11:04:58.244970 [info ] [Thread-1 (]: 11 of 12 START sql table model default.dim_tempo ............................... [RUN]
[0m11:04:58.244970 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m11:04:58.244970 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m11:04:58.244970 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m11:04:58.244970 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 11:04:58.244970 => 11:04:58.244970
[0m11:04:58.254874 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m11:04:58.260617 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:58.260617 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m11:04:58.262124 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */

      describe extended `workspace`.`default`.`dim_tempo`
  
[0m11:04:58.263410 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:04:59.064982 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07917-a899-14f3-91a6-cf9914e10424
[0m11:04:59.524741 [debug] [Thread-1 (]: SQL status: OK in 1.2599999904632568 seconds
[0m11:04:59.524741 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 11:04:58.254874 => 11:04:59.524741
[0m11:04:59.524741 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m11:04:59.524741 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:04:59.524741 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m11:04:59.524741 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07917-a899-14f3-91a6-cf9914e10424
[0m11:04:59.759821 [debug] [Thread-1 (]: Runtime Error in model dim_tempo (models\marts\dim_tempo.sql)
  dbt could not find a macro with the name "drop_relation" in any package
[0m11:04:59.760823 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d797882-a862-43aa-802d-12205dc69691', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5694D31D0>]}
[0m11:04:59.761816 [error] [Thread-1 (]: 11 of 12 ERROR creating sql table model default.dim_tempo ...................... [[31mERROR[0m in 1.52s]
[0m11:04:59.763292 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m11:04:59.763348 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m11:04:59.764897 [info ] [Thread-1 (]: 12 of 12 SKIP relation default.fato_nascimento ................................. [[33mSKIP[0m]
[0m11:04:59.764897 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m11:04:59.764897 [debug] [MainThread]: On master: ROLLBACK
[0m11:04:59.764897 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:05:00.545098 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07917-a97c-1913-b121-5b64e11ed265
[0m11:05:00.555093 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:05:00.555093 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:05:00.555093 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:05:00.555093 [debug] [MainThread]: On master: ROLLBACK
[0m11:05:00.565060 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:05:00.565060 [debug] [MainThread]: On master: Close
[0m11:05:00.565060 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07917-a97c-1913-b121-5b64e11ed265
[0m11:05:00.799872 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:05:00.801866 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m11:05:00.802827 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m11:05:00.802827 [debug] [MainThread]: Connection 'model.projeto_health_insights.dim_tempo' was properly closed.
[0m11:05:00.803824 [info ] [MainThread]: 
[0m11:05:00.804821 [info ] [MainThread]: Finished running 9 view models, 3 table models in 0 hours 1 minutes and 35.78 seconds (95.78s).
[0m11:05:00.808833 [debug] [MainThread]: Command end result
[0m11:05:00.821808 [info ] [MainThread]: 
[0m11:05:00.823775 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m11:05:00.824777 [info ] [MainThread]: 
[0m11:05:00.825769 [error] [MainThread]: [33mRuntime Error in model dim_localidade (models\marts\dim_localidade.sql)[0m
[0m11:05:00.826767 [error] [MainThread]:   dbt could not find a macro with the name "drop_relation" in any package
[0m11:05:00.827764 [info ] [MainThread]: 
[0m11:05:00.828760 [error] [MainThread]: [33mRuntime Error in model dim_tempo (models\marts\dim_tempo.sql)[0m
[0m11:05:00.829759 [error] [MainThread]:   dbt could not find a macro with the name "drop_relation" in any package
[0m11:05:00.831008 [info ] [MainThread]: 
[0m11:05:00.832185 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=2 SKIP=1 TOTAL=12
[0m11:05:00.832185 [debug] [MainThread]: Command `dbt run` failed at 11:05:00.832185 after 97.59 seconds
[0m11:05:00.834714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F55982BB50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F55FCEEAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F55FCEE490>]}
[0m11:05:00.834714 [debug] [MainThread]: Flushing usage events
[0m11:07:37.529203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182FC9C7950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182FC109F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182FC10AE50>]}


============================== 11:07:37.534779 | e09661d5-16b5-4382-afc3-3e5371b43f9f ==============================
[0m11:07:37.534779 [info ] [MainThread]: Running with dbt=1.5.2
[0m11:07:37.534779 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:07:38.949497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182F9B7AD90>]}
[0m11:07:38.964778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182F9B7AD90>]}
[0m11:07:38.969787 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m11:07:38.987399 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m11:07:39.106662 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m11:07:39.107844 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\dim_tempo.sql
[0m11:07:39.108843 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\dim_localidade.sql
[0m11:07:39.139001 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m11:07:39.154734 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m11:07:39.274688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018285CD5990>]}
[0m11:07:39.289827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286D1D690>]}
[0m11:07:39.293859 [info ] [MainThread]: Found 12 models, 11 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m11:07:39.294502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182F88E4610>]}
[0m11:07:39.296115 [info ] [MainThread]: 
[0m11:07:39.296115 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:07:39.296115 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m11:07:39.296115 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m11:07:39.296115 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m11:07:39.296115 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:07:40.198873 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07918-08a2-1c03-a586-e98694e4a53d
[0m11:07:40.584496 [debug] [ThreadPool]: SQL status: OK in 1.2899999618530273 seconds
[0m11:07:40.584496 [debug] [ThreadPool]: On list_workspace: Close
[0m11:07:40.584496 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07918-08a2-1c03-a586-e98694e4a53d
[0m11:07:40.824619 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m11:07:40.824619 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m11:07:40.834530 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:40.834530 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m11:07:40.834530 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m11:07:40.834530 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:07:41.625203 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07918-097d-1118-b538-a6b9f6173e19
[0m11:07:42.044819 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m11:07:42.044819 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m11:07:42.044819 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m11:07:42.044819 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m11:07:42.054617 [debug] [ThreadPool]: On create_workspace_default: Close
[0m11:07:42.054617 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07918-097d-1118-b538-a6b9f6173e19
[0m11:07:42.295664 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m11:07:42.300917 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m11:07:42.300917 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m11:07:42.300917 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:07:43.094854 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07918-0a5d-1a4f-9f27-5fd4d0c21ab3
[0m11:07:43.424702 [debug] [ThreadPool]: SQL status: OK in 1.1200000047683716 seconds
[0m11:07:43.434448 [debug] [ThreadPool]: On list_workspace_default: Close
[0m11:07:43.434448 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07918-0a5d-1a4f-9f27-5fd4d0c21ab3
[0m11:07:43.664751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286D1EC50>]}
[0m11:07:43.674883 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:43.674883 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:07:43.676923 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:07:43.676923 [info ] [MainThread]: 
[0m11:07:43.684491 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m11:07:43.686465 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m11:07:43.686465 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m11:07:43.686465 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m11:07:43.686465 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m11:07:43.686465 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 11:07:43.686465 => 11:07:43.686465
[0m11:07:43.686465 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m11:07:43.724461 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m11:07:43.729466 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:43.729466 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m11:07:43.729466 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m11:07:43.729466 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:07:44.535229 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-0b36-119d-a02f-b06190eea22a
[0m11:07:45.284550 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m11:07:45.294557 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 11:07:43.694468 => 11:07:45.294557
[0m11:07:45.294557 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m11:07:45.294557 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:07:45.294557 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m11:07:45.304576 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-0b36-119d-a02f-b06190eea22a
[0m11:07:45.535017 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286DC98D0>]}
[0m11:07:45.535017 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.85s]
[0m11:07:45.544880 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m11:07:45.544880 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m11:07:45.544880 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m11:07:45.544880 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m11:07:45.544880 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m11:07:45.552489 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m11:07:45.554572 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 11:07:45.544880 => 11:07:45.552489
[0m11:07:45.554572 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m11:07:45.554572 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m11:07:45.554572 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:45.554572 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m11:07:45.554572 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m11:07:45.554572 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:07:46.364628 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-0c4f-1648-a821-ba14408378c1
[0m11:07:47.029784 [debug] [Thread-1 (]: SQL status: OK in 1.4800000190734863 seconds
[0m11:07:47.034828 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 11:07:45.554572 => 11:07:47.034828
[0m11:07:47.034828 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m11:07:47.034828 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:07:47.034828 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m11:07:47.034828 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-0c4f-1648-a821-ba14408378c1
[0m11:07:47.256013 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018285C4FBD0>]}
[0m11:07:47.256013 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.71s]
[0m11:07:47.256013 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m11:07:47.256013 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m11:07:47.256013 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m11:07:47.264590 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m11:07:47.264590 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m11:07:47.264590 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m11:07:47.264590 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 11:07:47.264590 => 11:07:47.264590
[0m11:07:47.264590 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m11:07:47.274895 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m11:07:47.274895 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:47.274895 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m11:07:47.274895 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m11:07:47.274895 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:07:48.064816 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-0d54-13c3-abf6-3d2e1a3dc1ca
[0m11:07:48.744629 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m11:07:48.749667 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 11:07:47.264590 => 11:07:48.749667
[0m11:07:48.749667 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m11:07:48.749667 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:07:48.749667 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m11:07:48.749667 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-0d54-13c3-abf6-3d2e1a3dc1ca
[0m11:07:48.985169 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286DC8F10>]}
[0m11:07:48.994888 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.72s]
[0m11:07:48.994888 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m11:07:48.994888 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m11:07:49.004683 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m11:07:49.010252 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m11:07:49.010645 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m11:07:49.014699 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m11:07:49.014699 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 11:07:49.010645 => 11:07:49.014699
[0m11:07:49.014699 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m11:07:49.014699 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m11:07:49.024865 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:49.024865 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m11:07:49.024865 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m11:07:49.024865 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:07:49.819787 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-0e60-1654-9760-5ecbfaf313ab
[0m11:07:50.544694 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m11:07:50.544694 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 11:07:49.014699 => 11:07:50.544694
[0m11:07:50.544694 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m11:07:50.544694 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:07:50.544694 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m11:07:50.544694 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-0e60-1654-9760-5ecbfaf313ab
[0m11:07:50.909757 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286F08190>]}
[0m11:07:50.914899 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.90s]
[0m11:07:50.914899 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m11:07:50.914899 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m11:07:50.924880 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m11:07:50.930810 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m11:07:50.930810 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m11:07:50.934933 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m11:07:50.939438 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 11:07:50.933381 => 11:07:50.939438
[0m11:07:50.939438 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m11:07:50.945086 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m11:07:50.945086 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:50.945086 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m11:07:50.945086 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m11:07:50.945086 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:07:51.774499 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-0f8a-11b7-97ba-25ef0c08670d
[0m11:07:52.444959 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m11:07:52.449493 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 11:07:50.939438 => 11:07:52.444959
[0m11:07:52.449493 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m11:07:52.449493 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:07:52.449493 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m11:07:52.449493 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-0f8a-11b7-97ba-25ef0c08670d
[0m11:07:52.675032 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286F48050>]}
[0m11:07:52.675032 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.75s]
[0m11:07:52.684674 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m11:07:52.684674 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m11:07:52.684674 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m11:07:52.684674 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m11:07:52.684674 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m11:07:52.684674 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m11:07:52.684674 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 11:07:52.684674 => 11:07:52.684674
[0m11:07:52.684674 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m11:07:52.697215 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m11:07:52.697215 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:52.697215 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m11:07:52.697215 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m11:07:52.697215 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:07:53.515120 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-1093-1ec8-b88a-7939356cef63
[0m11:07:54.264959 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m11:07:54.274458 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 11:07:52.694751 => 11:07:54.274458
[0m11:07:54.274458 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m11:07:54.274458 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:07:54.274458 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m11:07:54.274458 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-1093-1ec8-b88a-7939356cef63
[0m11:07:54.504644 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286F48790>]}
[0m11:07:54.504644 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.82s]
[0m11:07:54.504644 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m11:07:54.514681 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m11:07:54.514681 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m11:07:54.518663 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m11:07:54.519473 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m11:07:54.520008 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m11:07:54.524613 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 11:07:54.520008 => 11:07:54.524613
[0m11:07:54.524613 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m11:07:54.531358 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m11:07:54.531607 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:54.531607 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m11:07:54.531607 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH base AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM base

[0m11:07:54.534649 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:07:55.334669 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-11a9-1bd1-b923-d6234d10a8f3
[0m11:07:56.070657 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m11:07:56.074645 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 11:07:54.524613 => 11:07:56.074645
[0m11:07:56.076640 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m11:07:56.077638 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:07:56.077638 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m11:07:56.079632 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-11a9-1bd1-b923-d6234d10a8f3
[0m11:07:56.314675 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286F20AD0>]}
[0m11:07:56.314675 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 1.80s]
[0m11:07:56.314675 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m11:07:56.314675 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m11:07:56.314675 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m11:07:56.324564 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m11:07:56.324564 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m11:07:56.324564 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m11:07:56.329569 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 11:07:56.324564 => 11:07:56.329569
[0m11:07:56.329569 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m11:07:56.335487 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m11:07:56.336913 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:56.336913 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m11:07:56.336913 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m11:07:56.336913 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:07:57.160623 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-12c0-13ec-907b-b3fe30cfce36
[0m11:07:57.848763 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m11:07:57.851700 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 11:07:56.329569 => 11:07:57.851700
[0m11:07:57.851700 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m11:07:57.854821 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:07:57.854821 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m11:07:57.854821 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-12c0-13ec-907b-b3fe30cfce36
[0m11:07:58.208867 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286F36950>]}
[0m11:07:58.209863 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.89s]
[0m11:07:58.210861 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m11:07:58.211888 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:07:58.212890 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m11:07:58.213854 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m11:07:58.213854 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:07:58.217656 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m11:07:58.217656 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 11:07:58.214879 => 11:07:58.217656
[0m11:07:58.217656 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:07:58.224756 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m11:07:58.224756 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:58.224756 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m11:07:58.224756 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m11:07:58.224756 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:07:59.034590 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-13df-1192-a0fe-b1fe0cbe341e
[0m11:07:59.800676 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m11:07:59.803651 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 11:07:58.217656 => 11:07:59.802625
[0m11:07:59.803651 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m11:07:59.804649 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:07:59.805618 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m11:07:59.805618 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-13df-1192-a0fe-b1fe0cbe341e
[0m11:08:00.019784 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018285CC5450>]}
[0m11:08:00.019784 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.81s]
[0m11:08:00.019784 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:08:00.024828 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m11:08:00.024828 [info ] [Thread-1 (]: 10 of 12 START sql view model default.dim_localidade ........................... [RUN]
[0m11:08:00.024828 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m11:08:00.024828 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m11:08:00.024828 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m11:08:00.024828 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 11:08:00.024828 => 11:08:00.024828
[0m11:08:00.024828 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m11:08:00.038691 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m11:08:00.039690 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:00.040685 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m11:08:00.041655 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    

WITH locais AS (
    SELECT DISTINCT
        cod_municipio,
        local_nascimento
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    cod_municipio,
    local_nascimento
FROM locais

[0m11:08:00.041655 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:00.836174 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-14f1-1fb3-9216-bbf59f7eaea2
[0m11:08:01.544487 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m11:08:01.544487 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 11:08:00.034904 => 11:08:01.544487
[0m11:08:01.554584 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m11:08:01.554584 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:08:01.554584 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m11:08:01.554584 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-14f1-1fb3-9216-bbf59f7eaea2
[0m11:08:01.794845 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286F36B50>]}
[0m11:08:01.804512 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.dim_localidade ...................... [[32mOK[0m in 1.77s]
[0m11:08:01.809009 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m11:08:01.810105 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m11:08:01.810105 [info ] [Thread-1 (]: 11 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m11:08:01.817515 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m11:08:01.819628 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m11:08:01.824492 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m11:08:01.824492 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 11:08:01.821392 => 11:08:01.824492
[0m11:08:01.824492 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m11:08:01.824492 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m11:08:01.834661 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:01.834661 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m11:08:01.834661 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

WITH datas AS (
    SELECT DISTINCT
        data_nascimento AS data
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    ROW_NUMBER() OVER (ORDER BY data) AS data_id,
    data,
    EXTRACT(YEAR FROM data) AS ano,
    EXTRACT(MONTH FROM data) AS mes,
    EXTRACT(DAY FROM data) AS dia,
    DAYOFWEEK(data) AS dia_semana
FROM datas

[0m11:08:01.834661 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:02.644817 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-1605-1f63-a7b6-8e8d896283d9
[0m11:08:03.394575 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m11:08:03.404833 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 11:08:01.824492 => 11:08:03.404833
[0m11:08:03.409881 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m11:08:03.409881 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:08:03.409881 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m11:08:03.414501 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-1605-1f63-a7b6-8e8d896283d9
[0m11:08:03.624846 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286D91950>]}
[0m11:08:03.624846 [info ] [Thread-1 (]: 11 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 1.81s]
[0m11:08:03.624846 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m11:08:03.634465 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m11:08:03.634465 [info ] [Thread-1 (]: 12 of 12 START sql table model default.fato_nascimento ......................... [RUN]
[0m11:08:03.634465 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_nascimento)
[0m11:08:03.634465 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m11:08:03.634465 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m11:08:03.634465 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 11:08:03.634465 => 11:08:03.634465
[0m11:08:03.634465 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m11:08:03.657958 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:03.657958 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m11:08:03.657958 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */

      describe extended `workspace`.`default`.`fato_nascimento`
  
[0m11:08:03.662963 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:04.444454 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-1717-16fe-bfbe-1790b66857b0
[0m11:08:04.894700 [debug] [Thread-1 (]: SQL status: OK in 1.2400000095367432 seconds
[0m11:08:04.904795 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 11:08:03.634465 => 11:08:04.904795
[0m11:08:04.904795 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m11:08:04.904795 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:08:04.904795 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m11:08:04.904795 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-1717-16fe-bfbe-1790b66857b0
[0m11:08:05.154257 [debug] [Thread-1 (]: Runtime Error in model fato_nascimento (models\marts\fato_nascimento.sql)
  dbt could not find a macro with the name "drop_relation" in any package
[0m11:08:05.156222 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e09661d5-16b5-4382-afc3-3e5371b43f9f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018286CE9A90>]}
[0m11:08:05.157249 [error] [Thread-1 (]: 12 of 12 ERROR creating sql table model default.fato_nascimento ................ [[31mERROR[0m in 1.52s]
[0m11:08:05.158339 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m11:08:05.158339 [debug] [MainThread]: On master: ROLLBACK
[0m11:08:05.158339 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:08:05.954773 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07918-17ff-1366-a3fb-53488017a3f3
[0m11:08:05.954773 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:08:05.954773 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:05.954773 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:08:05.954773 [debug] [MainThread]: On master: ROLLBACK
[0m11:08:05.964405 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:08:05.964405 [debug] [MainThread]: On master: Close
[0m11:08:05.964405 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07918-17ff-1366-a3fb-53488017a3f3
[0m11:08:06.184887 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:08:06.194589 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m11:08:06.194589 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m11:08:06.194589 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m11:08:06.194589 [info ] [MainThread]: 
[0m11:08:06.194589 [info ] [MainThread]: Finished running 11 view models, 1 table model in 0 hours 0 minutes and 26.90 seconds (26.90s).
[0m11:08:06.194589 [debug] [MainThread]: Command end result
[0m11:08:06.214344 [info ] [MainThread]: 
[0m11:08:06.214344 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:08:06.214344 [info ] [MainThread]: 
[0m11:08:06.214344 [error] [MainThread]: [33mRuntime Error in model fato_nascimento (models\marts\fato_nascimento.sql)[0m
[0m11:08:06.214344 [error] [MainThread]:   dbt could not find a macro with the name "drop_relation" in any package
[0m11:08:06.214344 [info ] [MainThread]: 
[0m11:08:06.214344 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 TOTAL=12
[0m11:08:06.214344 [debug] [MainThread]: Command `dbt run` failed at 11:08:06.214344 after 28.71 seconds
[0m11:08:06.214344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182FC0EE950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182FC486E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000182FC9F5150>]}
[0m11:08:06.224662 [debug] [MainThread]: Flushing usage events
[0m11:08:41.314355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9E0E032D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9E0DB9510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9E0DDF990>]}


============================== 11:08:41.324338 | d4dae228-778e-44f4-a6cd-b4d51846e507 ==============================
[0m11:08:41.324338 [info ] [MainThread]: Running with dbt=1.5.2
[0m11:08:41.324338 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:08:42.709495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9E16DDF10>]}
[0m11:08:42.729662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9E16DDF10>]}
[0m11:08:42.729662 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m11:08:42.748737 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m11:08:42.866626 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:08:42.866626 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m11:08:42.894395 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m11:08:43.024747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EBA25750>]}
[0m11:08:43.039298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EB91C2D0>]}
[0m11:08:43.044313 [info ] [MainThread]: Found 12 models, 11 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m11:08:43.044313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9E7E8CDD0>]}
[0m11:08:43.044313 [info ] [MainThread]: 
[0m11:08:43.044313 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:08:43.044313 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m11:08:43.044313 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m11:08:43.044313 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m11:08:43.044313 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:08:44.034623 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07918-2eb0-1a9f-9841-c62245af06ff
[0m11:08:44.369941 [debug] [ThreadPool]: SQL status: OK in 1.3300000429153442 seconds
[0m11:08:44.374562 [debug] [ThreadPool]: On list_workspace: Close
[0m11:08:44.374562 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07918-2eb0-1a9f-9841-c62245af06ff
[0m11:08:44.605079 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m11:08:44.614860 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m11:08:44.624793 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:44.624793 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m11:08:44.624793 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m11:08:44.624793 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:08:45.434760 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07918-2f86-1003-8753-a8c1278572ba
[0m11:08:45.834759 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m11:08:45.839771 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m11:08:45.839771 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m11:08:45.839771 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m11:08:45.844846 [debug] [ThreadPool]: On create_workspace_default: Close
[0m11:08:45.844846 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07918-2f86-1003-8753-a8c1278572ba
[0m11:08:46.094810 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m11:08:46.099823 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m11:08:46.104381 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m11:08:46.104381 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:08:46.917954 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07918-3069-164e-b754-1667786dd9da
[0m11:08:47.294632 [debug] [ThreadPool]: SQL status: OK in 1.190000057220459 seconds
[0m11:08:47.294632 [debug] [ThreadPool]: On list_workspace_default: Close
[0m11:08:47.294632 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07918-3069-164e-b754-1667786dd9da
[0m11:08:47.546435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EBA277D0>]}
[0m11:08:47.554495 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:47.554495 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:08:47.554495 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:08:47.554495 [info ] [MainThread]: 
[0m11:08:47.554495 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m11:08:47.564832 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m11:08:47.567427 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m11:08:47.568703 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m11:08:47.572735 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m11:08:47.574789 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 11:08:47.568703 => 11:08:47.572735
[0m11:08:47.574789 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m11:08:47.609427 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m11:08:47.609427 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:47.609427 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m11:08:47.609427 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m11:08:47.609427 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:08:48.414791 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-314d-151a-a81a-106613b639cf
[0m11:08:49.094884 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m11:08:49.114405 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 11:08:47.574789 => 11:08:49.114405
[0m11:08:49.114405 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m11:08:49.114405 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:08:49.114405 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m11:08:49.114405 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-314d-151a-a81a-106613b639cf
[0m11:08:49.355024 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EA5AC5D0>]}
[0m11:08:49.355024 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.79s]
[0m11:08:49.355024 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m11:08:49.355024 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m11:08:49.355024 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m11:08:49.364778 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m11:08:49.364778 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m11:08:49.364778 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m11:08:49.364778 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 11:08:49.364778 => 11:08:49.364778
[0m11:08:49.364778 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m11:08:49.374790 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m11:08:49.376931 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:49.377923 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m11:08:49.378921 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m11:08:49.378921 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:50.164474 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-3258-16ac-aa6b-eba94ac35198
[0m11:08:50.865352 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m11:08:50.868357 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 11:08:49.364778 => 11:08:50.867360
[0m11:08:50.868357 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m11:08:50.869329 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:08:50.869329 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m11:08:50.870368 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-3258-16ac-aa6b-eba94ac35198
[0m11:08:51.104538 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EBA19DD0>]}
[0m11:08:51.104538 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.75s]
[0m11:08:51.109545 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m11:08:51.109545 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m11:08:51.109545 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m11:08:51.109545 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m11:08:51.109545 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m11:08:51.116464 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m11:08:51.116464 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 11:08:51.114598 => 11:08:51.116464
[0m11:08:51.116464 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m11:08:51.124522 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m11:08:51.124522 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:51.124522 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m11:08:51.124522 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m11:08:51.124522 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:51.924802 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-3364-1d21-91be-3f37d6ba0b03
[0m11:08:52.694759 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m11:08:52.704629 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 11:08:51.116464 => 11:08:52.704629
[0m11:08:52.704629 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m11:08:52.704629 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:08:52.704629 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m11:08:52.704629 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-3364-1d21-91be-3f37d6ba0b03
[0m11:08:52.934870 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EBA948D0>]}
[0m11:08:52.934870 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.83s]
[0m11:08:52.944570 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m11:08:52.944570 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m11:08:52.944570 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m11:08:52.944570 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m11:08:52.944570 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m11:08:52.944570 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m11:08:52.944570 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 11:08:52.944570 => 11:08:52.944570
[0m11:08:52.944570 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m11:08:52.959587 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m11:08:52.959587 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:52.959587 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m11:08:52.959587 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m11:08:52.959587 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:53.774847 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-347d-1df6-8e75-278a16c42e38
[0m11:08:54.474630 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m11:08:54.479639 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 11:08:52.954625 => 11:08:54.479639
[0m11:08:54.479639 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m11:08:54.479639 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:08:54.479639 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m11:08:54.484710 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-347d-1df6-8e75-278a16c42e38
[0m11:08:54.714465 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EBB11190>]}
[0m11:08:54.714465 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.77s]
[0m11:08:54.714465 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m11:08:54.714465 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m11:08:54.714465 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m11:08:54.724453 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m11:08:54.724453 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m11:08:54.724453 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m11:08:54.729458 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 11:08:54.724453 => 11:08:54.729458
[0m11:08:54.729458 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m11:08:54.734501 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m11:08:54.734501 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:54.734501 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m11:08:54.734501 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m11:08:54.734501 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:55.514652 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-3587-19f6-8176-c7054c7431a8
[0m11:08:56.224653 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m11:08:56.236624 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 11:08:54.729458 => 11:08:56.235628
[0m11:08:56.238595 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m11:08:56.240628 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:08:56.243606 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m11:08:56.245570 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-3587-19f6-8176-c7054c7431a8
[0m11:08:56.482049 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EBA26CD0>]}
[0m11:08:56.484021 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.77s]
[0m11:08:56.485016 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m11:08:56.486030 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m11:08:56.486999 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m11:08:56.487672 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m11:08:56.488700 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m11:08:56.492689 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m11:08:56.493658 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 11:08:56.488700 => 11:08:56.492689
[0m11:08:56.493658 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m11:08:56.503042 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m11:08:56.504012 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:56.504012 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m11:08:56.505040 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m11:08:56.505040 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:57.315541 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-369a-1c35-af0d-39fe13531c69
[0m11:08:58.070265 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m11:08:58.074319 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 11:08:56.494667 => 11:08:58.074319
[0m11:08:58.074319 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m11:08:58.074319 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:08:58.074319 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m11:08:58.074319 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-369a-1c35-af0d-39fe13531c69
[0m11:08:58.304580 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EBB89310>]}
[0m11:08:58.304580 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.82s]
[0m11:08:58.304580 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m11:08:58.314659 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m11:08:58.314659 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m11:08:58.314659 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m11:08:58.314659 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m11:08:58.314659 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m11:08:58.314659 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 11:08:58.314659 => 11:08:58.314659
[0m11:08:58.314659 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m11:08:58.325013 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m11:08:58.325013 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:58.325013 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m11:08:58.330956 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH base AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM base

[0m11:08:58.332223 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:59.144745 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-37b2-1eff-b22a-bd3e2dc20673
[0m11:08:59.884702 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m11:08:59.894717 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 11:08:58.314659 => 11:08:59.894717
[0m11:08:59.894717 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m11:08:59.894717 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:08:59.894717 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m11:08:59.894717 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-37b2-1eff-b22a-bd3e2dc20673
[0m11:09:00.124709 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EBB15E10>]}
[0m11:09:00.134403 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 1.81s]
[0m11:09:00.134403 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m11:09:00.134403 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m11:09:00.134403 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m11:09:00.134403 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m11:09:00.134403 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m11:09:00.144797 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m11:09:00.146920 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 11:09:00.134403 => 11:09:00.144797
[0m11:09:00.147895 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m11:09:00.152449 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m11:09:00.154420 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:00.154420 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m11:09:00.155414 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m11:09:00.155414 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:09:00.924720 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-38c2-1070-b053-d97fe81c521c
[0m11:09:01.684553 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m11:09:01.684553 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 11:09:00.147965 => 11:09:01.684553
[0m11:09:01.684553 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m11:09:01.684553 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:09:01.684553 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m11:09:01.684553 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-38c2-1070-b053-d97fe81c521c
[0m11:09:01.934796 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EBB4B850>]}
[0m11:09:01.944335 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.80s]
[0m11:09:01.944335 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m11:09:01.944335 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:09:01.944335 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m11:09:01.954644 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m11:09:01.954644 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:09:01.957689 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m11:09:01.957689 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 11:09:01.956526 => 11:09:01.957689
[0m11:09:01.957689 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:09:01.964748 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m11:09:01.968977 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:01.969895 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m11:09:01.969895 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m11:09:01.969895 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:09:02.734794 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-39d5-1945-ade5-d20f931a959e
[0m11:09:03.564437 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m11:09:03.564437 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 11:09:01.957689 => 11:09:03.564437
[0m11:09:03.564437 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m11:09:03.564437 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:09:03.564437 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m11:09:03.564437 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-39d5-1945-ade5-d20f931a959e
[0m11:09:03.804962 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EBA24F10>]}
[0m11:09:03.804962 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.86s]
[0m11:09:03.814597 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m11:09:03.814597 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m11:09:03.814597 [info ] [Thread-1 (]: 10 of 12 START sql view model default.dim_localidade ........................... [RUN]
[0m11:09:03.814597 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m11:09:03.814597 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m11:09:03.824771 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m11:09:03.829790 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 11:09:03.823410 => 11:09:03.824771
[0m11:09:03.829790 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m11:09:03.834338 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m11:09:03.834338 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:03.836616 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m11:09:03.837749 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    

WITH locais AS (
    SELECT DISTINCT
        cod_municipio,
        local_nascimento
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    cod_municipio,
    local_nascimento
FROM locais

[0m11:09:03.837749 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:09:04.635049 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-3af8-16f5-9f46-857c0cff4396
[0m11:09:05.384983 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m11:09:05.384983 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 11:09:03.829790 => 11:09:05.384983
[0m11:09:05.384983 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m11:09:05.384983 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:09:05.384983 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m11:09:05.384983 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-3af8-16f5-9f46-857c0cff4396
[0m11:09:05.624409 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EA8D85D0>]}
[0m11:09:05.624409 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.dim_localidade ...................... [[32mOK[0m in 1.81s]
[0m11:09:05.624409 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m11:09:05.634418 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m11:09:05.634418 [info ] [Thread-1 (]: 11 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m11:09:05.634418 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m11:09:05.634418 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m11:09:05.644728 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m11:09:05.644728 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 11:09:05.639879 => 11:09:05.644728
[0m11:09:05.644728 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m11:09:05.649733 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m11:09:05.649733 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:05.649733 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m11:09:05.649733 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

WITH datas AS (
    SELECT DISTINCT
        data_nascimento AS data
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    ROW_NUMBER() OVER (ORDER BY data) AS data_id,
    data,
    EXTRACT(YEAR FROM data) AS ano,
    EXTRACT(MONTH FROM data) AS mes,
    EXTRACT(DAY FROM data) AS dia,
    DAYOFWEEK(data) AS dia_semana
FROM datas

[0m11:09:05.649733 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:09:06.444368 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-3c0b-1d65-a8d4-962474f82ec1
[0m11:09:07.194340 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m11:09:07.194340 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 11:09:05.644728 => 11:09:07.194340
[0m11:09:07.194340 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m11:09:07.199374 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:09:07.199374 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m11:09:07.199374 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-3c0b-1d65-a8d4-962474f82ec1
[0m11:09:07.424898 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9E9A73F90>]}
[0m11:09:07.434616 [info ] [Thread-1 (]: 11 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 1.79s]
[0m11:09:07.434616 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m11:09:07.434616 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m11:09:07.444426 [info ] [Thread-1 (]: 12 of 12 START sql view model default.fato_nascimento .......................... [RUN]
[0m11:09:07.451654 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_nascimento)
[0m11:09:07.453115 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m11:09:07.458843 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m11:09:07.458843 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 11:09:07.454649 => 11:09:07.458843
[0m11:09:07.458843 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m11:09:07.464387 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m11:09:07.464387 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:07.464387 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m11:09:07.464387 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    i.nascimento_id,
    t.data_id,
    l.cod_municipio,
    i.sexo,
    i.peso,
    i.idade_mae,
    i.gestacao_semanas,
    i.tipo_parto,
    i.APGAR1,
    i.APGAR5
FROM `workspace`.`default`.`int_nascimento` i
LEFT JOIN `workspace`.`default`.`dim_tempo` t
    ON i.data_nascimento = t.data
LEFT JOIN `workspace`.`default`.`dim_localidade` l
    ON i.cod_municipio = l.cod_municipio

[0m11:09:07.469393 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:09:08.274657 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07918-3d21-1b56-8dc5-9d65fa08b756
[0m11:09:09.184355 [debug] [Thread-1 (]: SQL status: OK in 1.7100000381469727 seconds
[0m11:09:09.184355 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 11:09:07.458843 => 11:09:09.184355
[0m11:09:09.184355 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m11:09:09.184355 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:09:09.184355 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m11:09:09.184355 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07918-3d21-1b56-8dc5-9d65fa08b756
[0m11:09:09.424769 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4dae228-778e-44f4-a6cd-b4d51846e507', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EBBA7D10>]}
[0m11:09:09.424769 [info ] [Thread-1 (]: 12 of 12 OK created sql view model default.fato_nascimento ..................... [[32mOK[0m in 1.98s]
[0m11:09:09.434763 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m11:09:09.434763 [debug] [MainThread]: On master: ROLLBACK
[0m11:09:09.434763 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:09:10.249792 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07918-3e50-1729-8066-cd501d252eb9
[0m11:09:10.249792 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:09:10.249792 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:10.254914 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:09:10.254914 [debug] [MainThread]: On master: ROLLBACK
[0m11:09:10.254914 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:09:10.259456 [debug] [MainThread]: On master: Close
[0m11:09:10.259456 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07918-3e50-1729-8066-cd501d252eb9
[0m11:09:10.494847 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:09:10.494847 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m11:09:10.494847 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m11:09:10.494847 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m11:09:10.494847 [info ] [MainThread]: 
[0m11:09:10.494847 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 27.45 seconds (27.45s).
[0m11:09:10.504507 [debug] [MainThread]: Command end result
[0m11:09:10.515163 [info ] [MainThread]: 
[0m11:09:10.515163 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:09:10.515163 [info ] [MainThread]: 
[0m11:09:10.515163 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 TOTAL=12
[0m11:09:10.515163 [debug] [MainThread]: Command `dbt run` succeeded at 11:09:10.515163 after 29.22 seconds
[0m11:09:10.515163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9E0DF2450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9E1097710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9EA895290>]}
[0m11:09:10.515163 [debug] [MainThread]: Flushing usage events
[0m11:48:27.850902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022743477E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022742E5E490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022742E2FBD0>]}


============================== 11:48:27.850902 | 7814f823-2742-4b6f-9c89-0cad126ff549 ==============================
[0m11:48:27.850902 [info ] [MainThread]: Running with dbt=1.5.2
[0m11:48:27.850902 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:48:29.181097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7814f823-2742-4b6f-9c89-0cad126ff549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022743477E90>]}
[0m11:48:29.203833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7814f823-2742-4b6f-9c89-0cad126ff549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022743477E90>]}
[0m11:48:29.203833 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m11:48:29.224740 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m11:48:29.330751 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:48:29.330751 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\schema.yaml
[0m11:48:29.366975 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m11:48:29.431080 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m11:48:29.440646 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m11:48:29.591041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7814f823-2742-4b6f-9c89-0cad126ff549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002274347AD50>]}
[0m11:48:29.610761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7814f823-2742-4b6f-9c89-0cad126ff549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002274C6402D0>]}
[0m11:48:29.610761 [info ] [MainThread]: Found 12 models, 26 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m11:48:29.610761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7814f823-2742-4b6f-9c89-0cad126ff549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022742E5C510>]}
[0m11:48:29.610761 [info ] [MainThread]: 
[0m11:48:29.610761 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:48:29.620973 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m11:48:29.626007 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m11:48:29.626007 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m11:48:29.626007 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:30.751166 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0791d-bd29-1e89-8406-e2efa332cc10
[0m11:48:30.971172 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetTables'), ('session-id', b'\x01\xf0y\x1d\xbd)\x1e\x89\x84\x06\xe2\xef\xa32\xcc\x10'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.21017122268676758/900.0')])
[0m11:48:39.881090 [debug] [ThreadPool]: SQL status: OK in 10.260000228881836 seconds
[0m11:48:39.887445 [debug] [ThreadPool]: On list_workspace_default: Close
[0m11:48:39.890951 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0791d-bd29-1e89-8406-e2efa332cc10
[0m11:48:40.135983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7814f823-2742-4b6f-9c89-0cad126ff549', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002274D73EA50>]}
[0m11:48:40.135983 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:40.141041 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:48:40.141041 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:48:40.141041 [info ] [MainThread]: 
[0m11:48:40.154965 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:48:40.155937 [info ] [Thread-1 (]: 1 of 26 START test accepted_values_fato_nascimento_sexo__M__F .................. [RUN]
[0m11:48:40.157932 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3'
[0m11:48:40.158955 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:48:40.165938 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m11:48:40.166938 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (compile): 11:48:40.158955 => 11:48:40.166938
[0m11:48:40.167904 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:48:40.191665 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m11:48:40.191665 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:40.191665 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m11:48:40.195668 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'M','F'
)



      
    ) dbt_internal_test
[0m11:48:40.195668 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:48:41.091395 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-c36a-19b0-9475-5c7008594065
[0m11:48:45.381006 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'M','F'
)



      
    ) dbt_internal_test
[0m11:48:45.381006 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

[0m11:48:45.381006 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [CAST_INVALID_INPUT] org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
	at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)
	at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$2(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$2$adapted(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.buildCast(Cast.scala:683)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$1(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:1408)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:730)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:89)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:109)
	at scala.collection.immutable.Vector1.map(Vector.scala:2141)
	at scala.collection.immutable.Vector1.map(Vector.scala:386)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:760)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:109)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:109)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:109)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.$anonfun$applyOrElse$1(expressions.scala:113)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:113)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:112)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$4(TreeNode.scala:536)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1705)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:536)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:487)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.apply(expressions.scala:112)
	at org.apache.spark.sql.execution.qrc.Canonicalized$.applyRules(ResultCacheManager.scala:131)
	at org.apache.spark.sql.execution.qrc.Canonicalized$.apply(ResultCacheManager.scala:122)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$lookupCache$4(ResultCacheManager.scala:514)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$lookupCache$4$adapted(ResultCacheManager.scala:514)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:146)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:142)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.recordLatency(ResultCacheManager.scala:305)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.lookupCache(ResultCacheManager.scala:514)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.lookupHybridResult(ResultCacheManager.scala:413)
	at org.apache.spark.sql.classic.Dataset.lookupHybridResultFromCache(Dataset.scala:1813)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.lookupResult(HybridCloudStoreResultHandler.scala:69)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.$anonfun$collectResult$1(ResultCollector.scala:101)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult(ResultCollector.scala:101)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult$(ResultCollector.scala:98)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.collectResult(HybridCloudStoreResultHandler.scala:33)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.org$apache$spark$sql$hive$thriftserver$HybridCloudStoreResultHandler$$initFromDataFrame(HybridCloudStoreResultHandler.scala:85)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler$.createFromDataFrame(HybridCloudStoreResultHandler.scala:214)
	at org.apache.spark.sql.hive.thriftserver.ResultHandlerFactory.getCollectorHandler(ResultHandlerFactory.scala:473)
	at org.apache.spark.sql.hive.thriftserver.ResultHandlerFactory.createResultHandler(ResultHandlerFactory.scala:443)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$18(SparkExecuteStatementOperation.scala:959)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withSuspendedQueryHangingDetection(SparkExecuteStatementOperation.scala:182)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:956)
	... 53 more

[0m11:48:45.381006 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791d-c38e-169c-85cd-db19b9720402
[0m11:48:45.381006 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (execute): 11:48:40.167904 => 11:48:45.381006
[0m11:48:45.381006 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: ROLLBACK
[0m11:48:45.381006 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:48:45.381006 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: Close
[0m11:48:45.381006 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-c36a-19b0-9475-5c7008594065
[0m11:48:45.640930 [debug] [Thread-1 (]: Runtime Error in test accepted_values_fato_nascimento_sexo__M__F (models\marts\schema.yaml)
  [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  == SQL (line 24, position 19) ==
  where value_field not in (
                    ^^^^^^^^
      'M','F'
  ^^^^^^^^^^^
  )
  ^
  
[0m11:48:45.640930 [error] [Thread-1 (]: 1 of 26 ERROR accepted_values_fato_nascimento_sexo__M__F ....................... [[31mERROR[0m in 5.48s]
[0m11:48:45.640930 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:48:45.640930 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e
[0m11:48:45.640930 [info ] [Thread-1 (]: 2 of 26 START test not_null_dim_doenca_cod_doenca .............................. [RUN]
[0m11:48:45.640930 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3, now test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e)
[0m11:48:45.650739 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e
[0m11:48:45.660833 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e"
[0m11:48:45.662953 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e (compile): 11:48:45.650739 => 11:48:45.662953
[0m11:48:45.664106 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e
[0m11:48:45.664106 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e"
[0m11:48:45.664106 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:45.664106 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e"
[0m11:48:45.664106 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_doenca
from `workspace`.`default`.`dim_doenca`
where cod_doenca is null



      
    ) dbt_internal_test
[0m11:48:45.671175 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:48:46.480832 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-c69e-199f-b3af-42cd1681d778
[0m11:48:47.250903 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_doenca
from `workspace`.`default`.`dim_doenca`
where cod_doenca is null



      
    ) dbt_internal_test
[0m11:48:47.255911 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `descricao_doenca`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:48:47.255911 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `descricao_doenca`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `descricao_doenca`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:48:47.255911 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791d-c6c3-1547-8f88-077f690febc8
[0m11:48:47.255911 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e (execute): 11:48:45.664106 => 11:48:47.255911
[0m11:48:47.255911 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e: ROLLBACK
[0m11:48:47.255911 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:48:47.255911 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e: Close
[0m11:48:47.255911 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-c69e-199f-b3af-42cd1681d778
[0m11:48:47.502437 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_doenca_cod_doenca (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `descricao_doenca`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:48:47.502895 [error] [Thread-1 (]: 2 of 26 ERROR not_null_dim_doenca_cod_doenca ................................... [[31mERROR[0m in 1.86s]
[0m11:48:47.502895 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e
[0m11:48:47.502895 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390
[0m11:48:47.511727 [info ] [Thread-1 (]: 3 of 26 START test not_null_dim_doenca_descricao ............................... [RUN]
[0m11:48:47.513723 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_doenca_cod_doenca.c166e45e0e, now test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390)
[0m11:48:47.515661 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390
[0m11:48:47.520991 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390"
[0m11:48:47.528165 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390 (compile): 11:48:47.515661 => 11:48:47.528165
[0m11:48:47.529162 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390
[0m11:48:47.531159 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390"
[0m11:48:47.531159 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:47.531159 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390"
[0m11:48:47.531159 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select descricao
from `workspace`.`default`.`dim_doenca`
where descricao is null



      
    ) dbt_internal_test
[0m11:48:47.531159 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:48:48.381177 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-c7bf-10ca-821a-1b897d17013d
[0m11:48:48.910996 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select descricao
from `workspace`.`default`.`dim_doenca`
where descricao is null



      
    ) dbt_internal_test
[0m11:48:48.920999 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `descricao` cannot be resolved. Did you mean one of the following? [`doenca_id`, `descricao_doenca`, `paciente`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:48:48.920999 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `descricao` cannot be resolved. Did you mean one of the following? [`doenca_id`, `descricao_doenca`, `paciente`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `descricao` cannot be resolved. Did you mean one of the following? [`doenca_id`, `descricao_doenca`, `paciente`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:48:48.920999 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791d-c7e6-105b-a432-707c44a28f81
[0m11:48:48.920999 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390 (execute): 11:48:47.530160 => 11:48:48.920999
[0m11:48:48.920999 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390: ROLLBACK
[0m11:48:48.920999 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:48:48.920999 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390: Close
[0m11:48:48.920999 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-c7bf-10ca-821a-1b897d17013d
[0m11:48:49.157786 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_doenca_descricao (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `descricao` cannot be resolved. Did you mean one of the following? [`doenca_id`, `descricao_doenca`, `paciente`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:48:49.158752 [error] [Thread-1 (]: 3 of 26 ERROR not_null_dim_doenca_descricao .................................... [[31mERROR[0m in 1.65s]
[0m11:48:49.159750 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390
[0m11:48:49.160777 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:48:49.161774 [info ] [Thread-1 (]: 4 of 26 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m11:48:49.164369 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_doenca_descricao.a3bd304390, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m11:48:49.164369 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:48:49.170973 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:48:49.170973 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 11:48:49.164369 => 11:48:49.170973
[0m11:48:49.170973 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:48:49.170973 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:48:49.170973 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:49.170973 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:48:49.181007 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m11:48:49.181007 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:48:49.971141 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-c8b8-1398-867b-7c615e07bfd4
[0m11:48:52.881012 [debug] [Thread-1 (]: SQL status: OK in 3.700000047683716 seconds
[0m11:48:53.191424 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 11:48:49.170973 => 11:48:53.191424
[0m11:48:53.191424 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m11:48:53.200973 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:48:53.200973 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m11:48:53.200973 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-c8b8-1398-867b-7c615e07bfd4
[0m11:48:53.430995 [info ] [Thread-1 (]: 4 of 26 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 4.27s]
[0m11:48:53.430995 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:48:53.430995 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m11:48:53.430995 [info ] [Thread-1 (]: 5 of 26 START test not_null_dim_localidade_municipio ........................... [RUN]
[0m11:48:53.430995 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771)
[0m11:48:53.440829 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m11:48:53.445859 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m11:48:53.445859 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (compile): 11:48:53.440829 => 11:48:53.445859
[0m11:48:53.445859 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m11:48:53.450928 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m11:48:53.453487 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:53.454667 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m11:48:53.454667 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select municipio
from `workspace`.`default`.`dim_localidade`
where municipio is null



      
    ) dbt_internal_test
[0m11:48:53.454667 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:48:54.321226 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-cb50-1c35-b699-e4b0dcbf45b0
[0m11:48:54.791194 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select municipio
from `workspace`.`default`.`dim_localidade`
where municipio is null



      
    ) dbt_internal_test
[0m11:48:54.791194 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `municipio` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `local_nascimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:48:54.791194 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `municipio` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `local_nascimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `municipio` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `local_nascimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:48:54.796200 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791d-cb71-1176-88fe-6c331926ea95
[0m11:48:54.796200 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (execute): 11:48:53.445859 => 11:48:54.796200
[0m11:48:54.796200 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: ROLLBACK
[0m11:48:54.796200 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:48:54.796200 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: Close
[0m11:48:54.796200 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-cb50-1c35-b699-e4b0dcbf45b0
[0m11:48:55.031424 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_localidade_municipio (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `municipio` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `local_nascimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:48:55.031424 [error] [Thread-1 (]: 5 of 26 ERROR not_null_dim_localidade_municipio ................................ [[31mERROR[0m in 1.60s]
[0m11:48:55.040989 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m11:48:55.040989 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1
[0m11:48:55.040989 [info ] [Thread-1 (]: 6 of 26 START test not_null_dim_localidade_uf .................................. [RUN]
[0m11:48:55.040989 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771, now test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1)
[0m11:48:55.040989 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1
[0m11:48:55.040989 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1"
[0m11:48:55.050857 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1 (compile): 11:48:55.040989 => 11:48:55.050857
[0m11:48:55.051874 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1
[0m11:48:55.054590 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1"
[0m11:48:55.054590 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:55.054590 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1"
[0m11:48:55.054590 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select uf
from `workspace`.`default`.`dim_localidade`
where uf is null



      
    ) dbt_internal_test
[0m11:48:55.054590 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:48:55.852871 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-cc39-1526-9439-587021cbbcdf
[0m11:48:56.320825 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select uf
from `workspace`.`default`.`dim_localidade`
where uf is null



      
    ) dbt_internal_test
[0m11:48:56.320825 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `local_nascimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:48:56.320825 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `local_nascimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `local_nascimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:48:56.320825 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791d-cc5b-1107-97f0-f293fc331b01
[0m11:48:56.320825 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1 (execute): 11:48:55.051874 => 11:48:56.320825
[0m11:48:56.320825 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1: ROLLBACK
[0m11:48:56.320825 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:48:56.320825 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1: Close
[0m11:48:56.330821 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-cc39-1526-9439-587021cbbcdf
[0m11:48:56.561196 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_localidade_uf (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `local_nascimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:48:56.570889 [error] [Thread-1 (]: 6 of 26 ERROR not_null_dim_localidade_uf ....................................... [[31mERROR[0m in 1.53s]
[0m11:48:56.570889 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1
[0m11:48:56.570889 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m11:48:56.570889 [info ] [Thread-1 (]: 7 of 26 START test not_null_dim_tempo_ano ...................................... [RUN]
[0m11:48:56.570889 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_uf.13bb8526b1, now test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6)
[0m11:48:56.582336 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m11:48:56.590801 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m11:48:56.590801 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (compile): 11:48:56.583337 => 11:48:56.590801
[0m11:48:56.590801 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m11:48:56.590801 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m11:48:56.590801 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:48:56.601071 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m11:48:56.601071 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select ano
from `workspace`.`default`.`dim_tempo`
where ano is null



      
    ) dbt_internal_test
[0m11:48:56.601071 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:48:57.370045 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-cd20-1a59-a9fc-7cb2f1600a98
[0m11:49:00.711041 [debug] [Thread-1 (]: SQL status: OK in 4.110000133514404 seconds
[0m11:49:00.711041 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (execute): 11:48:56.590801 => 11:49:00.711041
[0m11:49:00.711041 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: ROLLBACK
[0m11:49:00.711041 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:00.711041 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: Close
[0m11:49:00.711041 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-cd20-1a59-a9fc-7cb2f1600a98
[0m11:49:00.950691 [info ] [Thread-1 (]: 7 of 26 PASS not_null_dim_tempo_ano ............................................ [[32mPASS[0m in 4.38s]
[0m11:49:00.950691 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m11:49:00.950691 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:49:00.950691 [info ] [Thread-1 (]: 8 of 26 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m11:49:00.950691 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m11:49:00.950691 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:49:00.964483 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:49:00.966496 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 11:49:00.950691 => 11:49:00.965481
[0m11:49:00.966496 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:49:00.971034 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:49:00.971034 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:00.971034 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:49:00.975677 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m11:49:00.976792 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:01.801138 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-cfc5-1986-8912-50671437816f
[0m11:49:02.490725 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m11:49:02.490725 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 11:49:00.966496 => 11:49:02.490725
[0m11:49:02.490725 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m11:49:02.490725 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:02.500805 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m11:49:02.500805 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-cfc5-1986-8912-50671437816f
[0m11:49:02.720984 [info ] [Thread-1 (]: 8 of 26 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 1.77s]
[0m11:49:02.730845 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:49:02.730845 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m11:49:02.730845 [info ] [Thread-1 (]: 9 of 26 START test not_null_dim_tempo_dia ...................................... [RUN]
[0m11:49:02.741144 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306)
[0m11:49:02.743032 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m11:49:02.751008 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m11:49:02.751008 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (compile): 11:49:02.744590 => 11:49:02.751008
[0m11:49:02.751008 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m11:49:02.751008 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m11:49:02.751008 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:02.751008 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m11:49:02.751008 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select dia
from `workspace`.`default`.`dim_tempo`
where dia is null



      
    ) dbt_internal_test
[0m11:49:02.751008 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:03.601115 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-d0d5-1269-b902-6dafac7705b4
[0m11:49:04.320877 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m11:49:04.320877 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (execute): 11:49:02.751008 => 11:49:04.320877
[0m11:49:04.320877 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: ROLLBACK
[0m11:49:04.320877 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:04.320877 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: Close
[0m11:49:04.331140 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-d0d5-1269-b902-6dafac7705b4
[0m11:49:04.551391 [info ] [Thread-1 (]: 9 of 26 PASS not_null_dim_tempo_dia ............................................ [[32mPASS[0m in 1.82s]
[0m11:49:04.551391 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m11:49:04.551391 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m11:49:04.551391 [info ] [Thread-1 (]: 10 of 26 START test not_null_dim_tempo_mes ..................................... [RUN]
[0m11:49:04.551391 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306, now test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972)
[0m11:49:04.561128 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m11:49:04.571507 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m11:49:04.574499 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (compile): 11:49:04.561128 => 11:49:04.573501
[0m11:49:04.575497 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m11:49:04.579487 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m11:49:04.580651 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:04.580651 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m11:49:04.580651 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select mes
from `workspace`.`default`.`dim_tempo`
where mes is null



      
    ) dbt_internal_test
[0m11:49:04.580651 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:05.340792 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-d1e1-1ea0-a433-c5b2a5744e3d
[0m11:49:05.991254 [debug] [Thread-1 (]: SQL status: OK in 1.409999966621399 seconds
[0m11:49:06.000695 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (execute): 11:49:04.575497 => 11:49:06.000695
[0m11:49:06.000695 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: ROLLBACK
[0m11:49:06.000695 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:06.000695 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: Close
[0m11:49:06.000695 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-d1e1-1ea0-a433-c5b2a5744e3d
[0m11:49:06.226020 [info ] [Thread-1 (]: 10 of 26 PASS not_null_dim_tempo_mes ........................................... [[32mPASS[0m in 1.67s]
[0m11:49:06.231118 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m11:49:06.231118 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:49:06.231118 [info ] [Thread-1 (]: 11 of 26 START test not_null_fato_atendimento_hospitalar_atendimento_id ........ [RUN]
[0m11:49:06.231118 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22)
[0m11:49:06.231118 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:49:06.241180 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m11:49:06.248396 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22 (compile): 11:49:06.239881 => 11:49:06.241180
[0m11:49:06.248920 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:49:06.250947 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m11:49:06.250947 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:06.250947 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m11:49:06.250947 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select atendimento_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is null



      
    ) dbt_internal_test
[0m11:49:06.250947 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:07.051041 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-d2e7-12a1-ab42-7a58d785e79f
[0m11:49:07.860831 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select atendimento_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is null



      
    ) dbt_internal_test
[0m11:49:07.860831 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:07.865879 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:49:07.871005 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791d-d307-1057-9954-4ccadf054feb
[0m11:49:07.871005 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22 (execute): 11:49:06.248920 => 11:49:07.871005
[0m11:49:07.871005 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: ROLLBACK
[0m11:49:07.871005 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:07.871005 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: Close
[0m11:49:07.871005 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-d2e7-12a1-ab42-7a58d785e79f
[0m11:49:08.110894 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:08.110894 [error] [Thread-1 (]: 11 of 26 ERROR not_null_fato_atendimento_hospitalar_atendimento_id ............. [[31mERROR[0m in 1.88s]
[0m11:49:08.118860 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:49:08.118860 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de
[0m11:49:08.120874 [info ] [Thread-1 (]: 12 of 26 START test not_null_fato_atendimento_hospitalar_cod_doenca ............ [RUN]
[0m11:49:08.120874 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de)
[0m11:49:08.120874 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de
[0m11:49:08.120874 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de"
[0m11:49:08.130722 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de (compile): 11:49:08.120874 => 11:49:08.130722
[0m11:49:08.130722 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de
[0m11:49:08.135127 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de"
[0m11:49:08.137123 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:08.137123 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de"
[0m11:49:08.138120 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_doenca
from `workspace`.`default`.`fato_atendimento_hospitalar`
where cod_doenca is null



      
    ) dbt_internal_test
[0m11:49:08.139117 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:08.940954 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-d406-171f-a5b1-98a69a04c448
[0m11:49:09.380818 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_doenca
from `workspace`.`default`.`fato_atendimento_hospitalar`
where cod_doenca is null



      
    ) dbt_internal_test
[0m11:49:09.380818 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:09.380818 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:49:09.380818 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791d-d427-1b38-ba47-886a1bd040aa
[0m11:49:09.380818 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de (execute): 11:49:08.130722 => 11:49:09.380818
[0m11:49:09.380818 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de: ROLLBACK
[0m11:49:09.380818 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:09.390763 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de: Close
[0m11:49:09.390763 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-d406-171f-a5b1-98a69a04c448
[0m11:49:09.630941 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_atendimento_hospitalar_cod_doenca (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:09.630941 [error] [Thread-1 (]: 12 of 26 ERROR not_null_fato_atendimento_hospitalar_cod_doenca ................. [[31mERROR[0m in 1.51s]
[0m11:49:09.630941 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de
[0m11:49:09.630941 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a
[0m11:49:09.630941 [info ] [Thread-1 (]: 13 of 26 START test not_null_fato_atendimento_hospitalar_cod_municipio ......... [RUN]
[0m11:49:09.630941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_doenca.02a45c83de, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a)
[0m11:49:09.630941 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a
[0m11:49:09.640850 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a"
[0m11:49:09.640850 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a (compile): 11:49:09.630941 => 11:49:09.640850
[0m11:49:09.640850 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a
[0m11:49:09.640850 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a"
[0m11:49:09.651003 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:09.651003 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a"
[0m11:49:09.651003 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_atendimento_hospitalar`
where cod_municipio is null



      
    ) dbt_internal_test
[0m11:49:09.653245 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:10.460919 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-d4ee-1020-aefb-f96135c4f4a6
[0m11:49:10.900733 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_atendimento_hospitalar`
where cod_municipio is null



      
    ) dbt_internal_test
[0m11:49:10.900733 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_municipio` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:10.900733 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_municipio` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_municipio` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:49:10.900733 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791d-d50f-12bf-84c8-9028e42224db
[0m11:49:10.910778 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a (execute): 11:49:09.640850 => 11:49:10.910778
[0m11:49:10.910778 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a: ROLLBACK
[0m11:49:10.910778 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:10.910778 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a: Close
[0m11:49:10.910778 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-d4ee-1020-aefb-f96135c4f4a6
[0m11:49:11.160933 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_atendimento_hospitalar_cod_municipio (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_municipio` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:11.160933 [error] [Thread-1 (]: 13 of 26 ERROR not_null_fato_atendimento_hospitalar_cod_municipio .............. [[31mERROR[0m in 1.53s]
[0m11:49:11.160933 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a
[0m11:49:11.160933 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:49:11.160933 [info ] [Thread-1 (]: 14 of 26 START test not_null_fato_atendimento_hospitalar_data_atendimento ...... [RUN]
[0m11:49:11.170871 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5)
[0m11:49:11.170871 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:49:11.181817 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:49:11.183813 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5 (compile): 11:49:11.171843 => 11:49:11.183813
[0m11:49:11.184808 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:49:11.187773 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:49:11.190810 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:11.190810 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:49:11.190810 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_atendimento
from `workspace`.`default`.`fato_atendimento_hospitalar`
where data_atendimento is null



      
    ) dbt_internal_test
[0m11:49:11.190810 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:11.985846 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-d5d6-1d22-9b5c-50decd5024fe
[0m11:49:12.901027 [debug] [Thread-1 (]: SQL status: OK in 1.7100000381469727 seconds
[0m11:49:12.901027 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5 (execute): 11:49:11.184808 => 11:49:12.901027
[0m11:49:12.910908 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: ROLLBACK
[0m11:49:12.910908 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:12.910908 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: Close
[0m11:49:12.910908 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-d5d6-1d22-9b5c-50decd5024fe
[0m11:49:13.141190 [info ] [Thread-1 (]: 14 of 26 PASS not_null_fato_atendimento_hospitalar_data_atendimento ............ [[32mPASS[0m in 1.97s]
[0m11:49:13.141190 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:49:13.141190 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1
[0m11:49:13.141190 [info ] [Thread-1 (]: 15 of 26 START test not_null_fato_atendimento_hospitalar_procedimento .......... [RUN]
[0m11:49:13.151113 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1)
[0m11:49:13.151113 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1
[0m11:49:13.158257 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1"
[0m11:49:13.159254 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1 (compile): 11:49:13.151113 => 11:49:13.159254
[0m11:49:13.160252 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1
[0m11:49:13.163271 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1"
[0m11:49:13.165237 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:13.165237 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1"
[0m11:49:13.166236 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select procedimento
from `workspace`.`default`.`fato_atendimento_hospitalar`
where procedimento is null



      
    ) dbt_internal_test
[0m11:49:13.167232 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:13.945968 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-d701-1995-805f-20bfdfc7d47b
[0m11:49:14.690803 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m11:49:14.690803 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1 (execute): 11:49:13.161279 => 11:49:14.690803
[0m11:49:14.690803 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1: ROLLBACK
[0m11:49:14.690803 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:14.690803 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1: Close
[0m11:49:14.690803 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-d701-1995-805f-20bfdfc7d47b
[0m11:49:14.920969 [info ] [Thread-1 (]: 15 of 26 PASS not_null_fato_atendimento_hospitalar_procedimento ................ [[32mPASS[0m in 1.77s]
[0m11:49:14.930987 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1
[0m11:49:14.930987 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:49:14.930987 [info ] [Thread-1 (]: 16 of 26 START test not_null_fato_nascimento_cod_municipio ..................... [RUN]
[0m11:49:14.930987 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m11:49:14.942268 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:49:14.950823 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:49:14.958659 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 11:49:14.943661 => 11:49:14.957099
[0m11:49:14.960694 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:49:14.960772 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:49:14.960772 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:14.960772 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:49:14.960772 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m11:49:14.960772 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:15.771071 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-d816-1ff3-aa74-b2509d315c59
[0m11:49:17.158460 [debug] [Thread-1 (]: SQL status: OK in 2.200000047683716 seconds
[0m11:49:17.161091 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 11:49:14.960772 => 11:49:17.161091
[0m11:49:17.161091 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m11:49:17.161091 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:17.161091 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m11:49:17.161091 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-d816-1ff3-aa74-b2509d315c59
[0m11:49:17.400960 [info ] [Thread-1 (]: 16 of 26 PASS not_null_fato_nascimento_cod_municipio ........................... [[32mPASS[0m in 2.47s]
[0m11:49:17.410911 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:49:17.410911 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m11:49:17.410911 [info ] [Thread-1 (]: 17 of 26 START test not_null_fato_nascimento_data_nascimento ................... [RUN]
[0m11:49:17.410911 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d)
[0m11:49:17.410911 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m11:49:17.421117 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m11:49:17.421117 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d (compile): 11:49:17.410911 => 11:49:17.421117
[0m11:49:17.421117 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m11:49:17.421117 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m11:49:17.421117 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:17.421117 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m11:49:17.421117 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_nascimento
from `workspace`.`default`.`fato_nascimento`
where data_nascimento is null



      
    ) dbt_internal_test
[0m11:49:17.421117 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:18.230997 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-d98f-181e-99b1-5dda20f143f6
[0m11:49:18.811153 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_nascimento
from `workspace`.`default`.`fato_nascimento`
where data_nascimento is null



      
    ) dbt_internal_test
[0m11:49:18.811153 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:18.811153 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:49:18.811153 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791d-d9b3-1148-b1cb-9f1f47b25c22
[0m11:49:18.811153 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d (execute): 11:49:17.421117 => 11:49:18.811153
[0m11:49:18.811153 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: ROLLBACK
[0m11:49:18.811153 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:18.811153 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: Close
[0m11:49:18.811153 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-d98f-181e-99b1-5dda20f143f6
[0m11:49:19.051366 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_nascimento_data_nascimento (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:19.051366 [error] [Thread-1 (]: 17 of 26 ERROR not_null_fato_nascimento_data_nascimento ........................ [[31mERROR[0m in 1.64s]
[0m11:49:19.060976 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m11:49:19.060976 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m11:49:19.060976 [info ] [Thread-1 (]: 18 of 26 START test not_null_fato_nascimento_idade_mae ......................... [RUN]
[0m11:49:19.060976 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d, now test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42)
[0m11:49:19.060976 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m11:49:19.070794 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m11:49:19.070794 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (compile): 11:49:19.060976 => 11:49:19.070794
[0m11:49:19.070794 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m11:49:19.070794 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m11:49:19.070794 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:19.070794 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m11:49:19.070794 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select idade_mae
from `workspace`.`default`.`fato_nascimento`
where idade_mae is null



      
    ) dbt_internal_test
[0m11:49:19.070794 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:19.870938 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-da8a-1708-a66f-0394d1792609
[0m11:49:20.801047 [debug] [Thread-1 (]: SQL status: OK in 1.7300000190734863 seconds
[0m11:49:20.810769 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (execute): 11:49:19.070794 => 11:49:20.801047
[0m11:49:20.810769 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: ROLLBACK
[0m11:49:20.810769 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:20.810769 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: Close
[0m11:49:20.810769 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-da8a-1708-a66f-0394d1792609
[0m11:49:21.041325 [info ] [Thread-1 (]: 18 of 26 PASS not_null_fato_nascimento_idade_mae ............................... [[32mPASS[0m in 1.98s]
[0m11:49:21.041325 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m11:49:21.041325 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:49:21.050913 [info ] [Thread-1 (]: 19 of 26 START test not_null_fato_nascimento_nascimento_id ..................... [RUN]
[0m11:49:21.050927 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m11:49:21.050927 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:49:21.050927 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:49:21.061073 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 11:49:21.050927 => 11:49:21.061073
[0m11:49:21.061073 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:49:21.061073 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:49:21.061073 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:21.061073 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:49:21.061073 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m11:49:21.061073 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:21.875647 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-dbbb-16df-a297-9ee10245a7a1
[0m11:49:22.621234 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m11:49:22.621234 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 11:49:21.061073 => 11:49:22.621234
[0m11:49:22.621234 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m11:49:22.630696 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:22.630696 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m11:49:22.630696 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-dbbb-16df-a297-9ee10245a7a1
[0m11:49:22.861175 [info ] [Thread-1 (]: 19 of 26 PASS not_null_fato_nascimento_nascimento_id ........................... [[32mPASS[0m in 1.80s]
[0m11:49:22.861175 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:49:22.861175 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:49:22.861175 [info ] [Thread-1 (]: 20 of 26 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m11:49:22.873485 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m11:49:22.875475 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:49:22.886863 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:49:22.886863 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 11:49:22.876760 => 11:49:22.886863
[0m11:49:22.886863 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:49:22.890907 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:49:22.890907 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:22.890907 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:49:22.895912 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m11:49:22.895912 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:23.740794 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-dcd6-13d5-9363-4f0abe9cebf8
[0m11:49:24.800849 [debug] [Thread-1 (]: SQL status: OK in 1.899999976158142 seconds
[0m11:49:24.800849 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 11:49:22.886863 => 11:49:24.800849
[0m11:49:24.800849 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m11:49:24.800849 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:24.810883 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m11:49:24.810883 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-dcd6-13d5-9363-4f0abe9cebf8
[0m11:49:25.041198 [error] [Thread-1 (]: 20 of 26 FAIL 4 not_null_fato_nascimento_peso .................................. [[31mFAIL 4[0m in 2.17s]
[0m11:49:25.041198 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:49:25.041198 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:49:25.041198 [info ] [Thread-1 (]: 21 of 26 START test not_null_fato_nascimento_sexo .............................. [RUN]
[0m11:49:25.041198 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m11:49:25.041198 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:49:25.050773 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m11:49:25.050773 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 11:49:25.041198 => 11:49:25.050773
[0m11:49:25.050773 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:49:25.060685 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m11:49:25.063785 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:25.064468 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m11:49:25.064468 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sexo
from `workspace`.`default`.`fato_nascimento`
where sexo is null



      
    ) dbt_internal_test
[0m11:49:25.066034 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:25.841382 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-de19-1348-9eaf-5ac2a65c3fda
[0m11:49:26.727917 [debug] [Thread-1 (]: SQL status: OK in 1.659999966621399 seconds
[0m11:49:26.731026 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 11:49:25.050773 => 11:49:26.731026
[0m11:49:26.731026 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: ROLLBACK
[0m11:49:26.731026 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:26.731026 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: Close
[0m11:49:26.731026 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-de19-1348-9eaf-5ac2a65c3fda
[0m11:49:26.981320 [info ] [Thread-1 (]: 21 of 26 PASS not_null_fato_nascimento_sexo .................................... [[32mPASS[0m in 1.94s]
[0m11:49:26.981320 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:49:26.990840 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959
[0m11:49:26.990840 [info ] [Thread-1 (]: 22 of 26 START test unique_dim_doenca_cod_doenca ............................... [RUN]
[0m11:49:26.990840 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959)
[0m11:49:26.990840 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959
[0m11:49:27.004295 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959"
[0m11:49:27.004295 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959 (compile): 11:49:26.990840 => 11:49:27.004295
[0m11:49:27.004295 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959
[0m11:49:27.004295 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959"
[0m11:49:27.010886 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:27.010886 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959"
[0m11:49:27.010886 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_doenca as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_doenca`
where cod_doenca is not null
group by cod_doenca
having count(*) > 1



      
    ) dbt_internal_test
[0m11:49:27.010886 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:27.819502 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-df47-1b64-b2b0-e80a8f93cf28
[0m11:49:28.550928 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_doenca as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_doenca`
where cod_doenca is not null
group by cod_doenca
having count(*) > 1



      
    ) dbt_internal_test
[0m11:49:28.550928 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `descricao_doenca`, `data_atendimento`]. SQLSTATE: 42703; line 16 pos 6
[0m11:49:28.550928 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `descricao_doenca`, `data_atendimento`]. SQLSTATE: 42703; line 16 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `descricao_doenca`, `data_atendimento`]. SQLSTATE: 42703; line 16 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:49:28.560866 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791d-df66-1f86-bc99-ae0a2ea89bde
[0m11:49:28.560866 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959 (execute): 11:49:27.004295 => 11:49:28.560866
[0m11:49:28.560866 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959: ROLLBACK
[0m11:49:28.560866 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:28.560866 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959: Close
[0m11:49:28.560866 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-df47-1b64-b2b0-e80a8f93cf28
[0m11:49:28.820833 [debug] [Thread-1 (]: Runtime Error in test unique_dim_doenca_cod_doenca (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `descricao_doenca`, `data_atendimento`]. SQLSTATE: 42703; line 16 pos 6
[0m11:49:28.821885 [error] [Thread-1 (]: 22 of 26 ERROR unique_dim_doenca_cod_doenca .................................... [[31mERROR[0m in 1.83s]
[0m11:49:28.822883 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959
[0m11:49:28.822883 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:49:28.823880 [info ] [Thread-1 (]: 23 of 26 START test unique_dim_localidade_cod_municipio ........................ [RUN]
[0m11:49:28.824905 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_doenca_cod_doenca.4f21b97959, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m11:49:28.825935 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:49:28.831925 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m11:49:28.832894 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 11:49:28.825935 => 11:49:28.832894
[0m11:49:28.833896 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:49:28.836876 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m11:49:28.839321 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:28.839758 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m11:49:28.840750 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_municipio as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_localidade`
where cod_municipio is not null
group by cod_municipio
having count(*) > 1



      
    ) dbt_internal_test
[0m11:49:28.840750 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:29.640846 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-e05d-1806-a151-517cbf76a89f
[0m11:49:30.826612 [debug] [Thread-1 (]: SQL status: OK in 1.9900000095367432 seconds
[0m11:49:30.828729 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 11:49:28.833896 => 11:49:30.828729
[0m11:49:30.828729 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: ROLLBACK
[0m11:49:30.830848 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:30.830848 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: Close
[0m11:49:30.830848 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-e05d-1806-a151-517cbf76a89f
[0m11:49:31.061114 [error] [Thread-1 (]: 23 of 26 FAIL 78 unique_dim_localidade_cod_municipio ........................... [[31mFAIL 78[0m in 2.24s]
[0m11:49:31.072803 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:49:31.074799 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:49:31.075927 [info ] [Thread-1 (]: 24 of 26 START test unique_dim_tempo_data_id ................................... [RUN]
[0m11:49:31.081600 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m11:49:31.083255 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:49:31.092102 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:49:31.092102 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 11:49:31.084291 => 11:49:31.092102
[0m11:49:31.092102 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:49:31.100709 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:49:31.100709 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:31.100709 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:49:31.100709 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:49:31.100709 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:31.921149 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-e1ba-10d6-9240-afeb09deb8cd
[0m11:49:32.651301 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m11:49:32.660853 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 11:49:31.092102 => 11:49:32.660853
[0m11:49:32.660853 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m11:49:32.660853 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:32.660853 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m11:49:32.660853 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-e1ba-10d6-9240-afeb09deb8cd
[0m11:49:32.900725 [info ] [Thread-1 (]: 24 of 26 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 1.82s]
[0m11:49:32.905749 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:49:32.910819 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:49:32.910819 [info ] [Thread-1 (]: 25 of 26 START test unique_fato_atendimento_hospitalar_atendimento_id .......... [RUN]
[0m11:49:32.910819 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe)
[0m11:49:32.919119 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:49:32.926962 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m11:49:32.927299 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe (compile): 11:49:32.920616 => 11:49:32.927299
[0m11:49:32.927299 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:49:32.930845 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m11:49:32.935862 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:32.935862 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m11:49:32.935862 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    atendimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is not null
group by atendimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:49:32.935862 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:33.761080 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-e2d0-1910-9377-fa804cadf485
[0m11:49:34.200940 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    atendimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is not null
group by atendimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:49:34.200940 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m11:49:34.200940 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:49:34.200940 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791d-e2f2-190b-9fe0-c1795215fb97
[0m11:49:34.210956 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe (execute): 11:49:32.927299 => 11:49:34.210956
[0m11:49:34.210956 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: ROLLBACK
[0m11:49:34.210956 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:34.210956 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: Close
[0m11:49:34.210956 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-e2d0-1910-9377-fa804cadf485
[0m11:49:34.440791 [debug] [Thread-1 (]: Runtime Error in test unique_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m11:49:34.445796 [error] [Thread-1 (]: 25 of 26 ERROR unique_fato_atendimento_hospitalar_atendimento_id ............... [[31mERROR[0m in 1.53s]
[0m11:49:34.445796 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:49:34.445796 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:49:34.445796 [info ] [Thread-1 (]: 26 of 26 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m11:49:34.445796 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m11:49:34.445796 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:49:34.450830 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:49:34.455834 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 11:49:34.450830 => 11:49:34.455834
[0m11:49:34.455834 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:49:34.460902 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:49:34.460902 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:34.460902 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:49:34.463185 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:49:34.464183 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:49:35.280706 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791d-e3b9-1ba0-b464-07a6b66f9e33
[0m11:49:36.365013 [debug] [Thread-1 (]: SQL status: OK in 1.899999976158142 seconds
[0m11:49:36.365013 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 11:49:34.455834 => 11:49:36.365013
[0m11:49:36.365013 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m11:49:36.365013 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:49:36.365013 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m11:49:36.365013 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791d-e3b9-1ba0-b464-07a6b66f9e33
[0m11:49:36.591098 [error] [Thread-1 (]: 26 of 26 FAIL 59968 unique_fato_nascimento_nascimento_id ....................... [[31mFAIL 59968[0m in 2.15s]
[0m11:49:36.601048 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:49:36.601048 [debug] [MainThread]: On master: ROLLBACK
[0m11:49:36.601048 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:49:37.376310 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0791d-e4f9-1ef5-98ed-caf8193b8385
[0m11:49:37.380877 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:49:37.380877 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:49:37.385887 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:49:37.385887 [debug] [MainThread]: On master: ROLLBACK
[0m11:49:37.385887 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:49:37.385887 [debug] [MainThread]: On master: Close
[0m11:49:37.385887 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0791d-e4f9-1ef5-98ed-caf8193b8385
[0m11:49:37.621197 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:49:37.621197 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m11:49:37.621197 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m11:49:37.630717 [info ] [MainThread]: 
[0m11:49:37.630717 [info ] [MainThread]: Finished running 26 tests in 0 hours 1 minutes and 8.02 seconds (68.02s).
[0m11:49:37.630717 [debug] [MainThread]: Command end result
[0m11:49:37.649562 [info ] [MainThread]: 
[0m11:49:37.651095 [info ] [MainThread]: [31mCompleted with 14 errors and 0 warnings:[0m
[0m11:49:37.651095 [info ] [MainThread]: 
[0m11:49:37.651095 [error] [MainThread]: [33mRuntime Error in test accepted_values_fato_nascimento_sexo__M__F (models\marts\schema.yaml)[0m
[0m11:49:37.651095 [error] [MainThread]:   [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
[0m11:49:37.651095 [error] [MainThread]:   == SQL (line 24, position 19) ==
[0m11:49:37.651095 [error] [MainThread]:   where value_field not in (
[0m11:49:37.651095 [error] [MainThread]:                     ^^^^^^^^
[0m11:49:37.651095 [error] [MainThread]:       'M','F'
[0m11:49:37.651095 [error] [MainThread]:   ^^^^^^^^^^^
[0m11:49:37.661151 [error] [MainThread]:   )
[0m11:49:37.661151 [error] [MainThread]:   ^
[0m11:49:37.661151 [error] [MainThread]:   
[0m11:49:37.661151 [info ] [MainThread]: 
[0m11:49:37.664276 [error] [MainThread]: [33mRuntime Error in test not_null_dim_doenca_cod_doenca (models\marts\schema.yaml)[0m
[0m11:49:37.665274 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `descricao_doenca`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:37.666271 [info ] [MainThread]: 
[0m11:49:37.668266 [error] [MainThread]: [33mRuntime Error in test not_null_dim_doenca_descricao (models\marts\schema.yaml)[0m
[0m11:49:37.669263 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `descricao` cannot be resolved. Did you mean one of the following? [`doenca_id`, `descricao_doenca`, `paciente`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:37.670261 [info ] [MainThread]: 
[0m11:49:37.671326 [error] [MainThread]: [33mRuntime Error in test not_null_dim_localidade_municipio (models\marts\schema.yaml)[0m
[0m11:49:37.671326 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `municipio` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `local_nascimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:37.671326 [info ] [MainThread]: 
[0m11:49:37.671326 [error] [MainThread]: [33mRuntime Error in test not_null_dim_localidade_uf (models\marts\schema.yaml)[0m
[0m11:49:37.671326 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `uf` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `local_nascimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:37.671326 [info ] [MainThread]: 
[0m11:49:37.671326 [error] [MainThread]: [33mRuntime Error in test not_null_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)[0m
[0m11:49:37.671326 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:37.671326 [info ] [MainThread]: 
[0m11:49:37.681015 [error] [MainThread]: [33mRuntime Error in test not_null_fato_atendimento_hospitalar_cod_doenca (models\marts\schema.yaml)[0m
[0m11:49:37.682471 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:37.683468 [info ] [MainThread]: 
[0m11:49:37.684466 [error] [MainThread]: [33mRuntime Error in test not_null_fato_atendimento_hospitalar_cod_municipio (models\marts\schema.yaml)[0m
[0m11:49:37.685987 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_municipio` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:37.685987 [info ] [MainThread]: 
[0m11:49:37.685987 [error] [MainThread]: [33mRuntime Error in test not_null_fato_nascimento_data_nascimento (models\marts\schema.yaml)[0m
[0m11:49:37.685987 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m11:49:37.691009 [info ] [MainThread]: 
[0m11:49:37.691009 [error] [MainThread]: [31mFailure in test not_null_fato_nascimento_peso (models\marts\schema.yaml)[0m
[0m11:49:37.692537 [error] [MainThread]:   Got 4 results, configured to fail if != 0
[0m11:49:37.692537 [info ] [MainThread]: 
[0m11:49:37.692537 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\not_null_fato_nascimento_peso.sql
[0m11:49:37.692537 [info ] [MainThread]: 
[0m11:49:37.692537 [error] [MainThread]: [33mRuntime Error in test unique_dim_doenca_cod_doenca (models\marts\schema.yaml)[0m
[0m11:49:37.698135 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_doenca` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `descricao_doenca`, `data_atendimento`]. SQLSTATE: 42703; line 16 pos 6
[0m11:49:37.699133 [info ] [MainThread]: 
[0m11:49:37.701127 [error] [MainThread]: [31mFailure in test unique_dim_localidade_cod_municipio (models\marts\schema.yaml)[0m
[0m11:49:37.702124 [error] [MainThread]:   Got 78 results, configured to fail if != 0
[0m11:49:37.703124 [info ] [MainThread]: 
[0m11:49:37.704039 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_dim_localidade_cod_municipio.sql
[0m11:49:37.704039 [info ] [MainThread]: 
[0m11:49:37.704039 [error] [MainThread]: [33mRuntime Error in test unique_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)[0m
[0m11:49:37.704039 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m11:49:37.704039 [info ] [MainThread]: 
[0m11:49:37.704039 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m11:49:37.711055 [error] [MainThread]:   Got 59968 results, configured to fail if != 0
[0m11:49:37.711055 [info ] [MainThread]: 
[0m11:49:37.711055 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m11:49:37.714345 [info ] [MainThread]: 
[0m11:49:37.715342 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=14 SKIP=0 TOTAL=26
[0m11:49:37.717336 [debug] [MainThread]: Command `dbt test` failed at 11:49:37.717336 after 69.88 seconds
[0m11:49:37.717336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227430F4190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022742C0EE10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000227409E0F50>]}
[0m11:49:37.718334 [debug] [MainThread]: Flushing usage events
[0m11:52:51.330563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B71B788E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B71BB4D950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B71B7AEC90>]}


============================== 11:52:51.340592 | bbe9b1c9-4027-4e62-a516-f1cfb32762a3 ==============================
[0m11:52:51.340592 [info ] [MainThread]: Running with dbt=1.5.2
[0m11:52:51.340592 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:52:52.697558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bbe9b1c9-4027-4e62-a516-f1cfb32762a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7252B8BD0>]}
[0m11:52:52.715355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bbe9b1c9-4027-4e62-a516-f1cfb32762a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B71B841B10>]}
[0m11:52:52.715355 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m11:52:52.736618 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m11:52:52.850541 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:52:52.850541 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\schema.yaml
[0m11:52:52.940491 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m11:52:52.955732 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m11:52:52.960340 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_doenca.sql
[0m11:52:52.960340 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m11:52:52.960340 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m11:52:53.080459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bbe9b1c9-4027-4e62-a516-f1cfb32762a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7263E96D0>]}
[0m11:52:53.095265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bbe9b1c9-4027-4e62-a516-f1cfb32762a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B72526A810>]}
[0m11:52:53.095265 [info ] [MainThread]: Found 12 models, 19 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m11:52:53.097848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bbe9b1c9-4027-4e62-a516-f1cfb32762a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B72284EB10>]}
[0m11:52:53.100459 [info ] [MainThread]: 
[0m11:52:53.103381 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:52:53.106448 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m11:52:53.110459 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m11:52:53.110459 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m11:52:53.110459 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:52:54.030702 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0791e-5a2d-172a-8d75-0fe90a32e12f
[0m11:52:54.445682 [debug] [ThreadPool]: SQL status: OK in 1.340000033378601 seconds
[0m11:52:54.450795 [debug] [ThreadPool]: On list_workspace_default: Close
[0m11:52:54.450795 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0791e-5a2d-172a-8d75-0fe90a32e12f
[0m11:52:54.700719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bbe9b1c9-4027-4e62-a516-f1cfb32762a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B725260090>]}
[0m11:52:54.700719 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:52:54.700719 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:52:54.700719 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:52:54.704988 [info ] [MainThread]: 
[0m11:52:54.711936 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462
[0m11:52:54.711936 [info ] [Thread-1 (]: 1 of 19 START test not_null_dim_doenca_descricao_doenca ........................ [RUN]
[0m11:52:54.714319 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462'
[0m11:52:54.714319 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462
[0m11:52:54.730485 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462"
[0m11:52:54.730485 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462 (compile): 11:52:54.715194 => 11:52:54.730485
[0m11:52:54.730485 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462
[0m11:52:54.755633 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462"
[0m11:52:54.756631 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:52:54.757650 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462"
[0m11:52:54.758635 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select descricao_doenca
from `workspace`.`default`.`dim_doenca`
where descricao_doenca is null



      
    ) dbt_internal_test
[0m11:52:54.758635 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:52:55.540843 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-5b16-11cc-b1be-ca78c0156b98
[0m11:52:57.410711 [debug] [Thread-1 (]: SQL status: OK in 2.6500000953674316 seconds
[0m11:52:57.420432 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462 (execute): 11:52:54.730485 => 11:52:57.420432
[0m11:52:57.420432 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462: ROLLBACK
[0m11:52:57.420432 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:52:57.420432 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462: Close
[0m11:52:57.420432 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-5b16-11cc-b1be-ca78c0156b98
[0m11:52:57.690661 [info ] [Thread-1 (]: 1 of 19 PASS not_null_dim_doenca_descricao_doenca .............................. [[32mPASS[0m in 2.97s]
[0m11:52:57.690661 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462
[0m11:52:57.690661 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403
[0m11:52:57.690661 [info ] [Thread-1 (]: 2 of 19 START test not_null_dim_doenca_doenca_id ............................... [RUN]
[0m11:52:57.690661 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462, now test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403)
[0m11:52:57.690661 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403
[0m11:52:57.700795 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403"
[0m11:52:57.704245 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403 (compile): 11:52:57.690661 => 11:52:57.703233
[0m11:52:57.704455 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403
[0m11:52:57.708503 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403"
[0m11:52:57.710734 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:52:57.710734 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403"
[0m11:52:57.710734 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select doenca_id
from `workspace`.`default`.`dim_doenca`
where doenca_id is null



      
    ) dbt_internal_test
[0m11:52:57.710734 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:52:58.540731 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-5cde-1bda-ab10-72801a649b2d
[0m11:52:59.105609 [debug] [Thread-1 (]: SQL status: OK in 1.3899999856948853 seconds
[0m11:52:59.110767 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403 (execute): 11:52:57.704455 => 11:52:59.110767
[0m11:52:59.110767 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403: ROLLBACK
[0m11:52:59.110767 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:52:59.110767 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403: Close
[0m11:52:59.110767 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-5cde-1bda-ab10-72801a649b2d
[0m11:52:59.350447 [info ] [Thread-1 (]: 2 of 19 PASS not_null_dim_doenca_doenca_id ..................................... [[32mPASS[0m in 1.66s]
[0m11:52:59.350447 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403
[0m11:52:59.350447 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:52:59.350447 [info ] [Thread-1 (]: 3 of 19 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m11:52:59.350447 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m11:52:59.350447 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:52:59.363889 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:52:59.363889 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 11:52:59.350447 => 11:52:59.363889
[0m11:52:59.363889 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:52:59.370449 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:52:59.370449 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:52:59.370449 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:52:59.370449 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m11:52:59.370449 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:00.180792 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-5dda-144d-bf1c-9ccd5353ba7e
[0m11:53:00.630866 [debug] [Thread-1 (]: SQL status: OK in 1.2599999904632568 seconds
[0m11:53:00.630866 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 11:52:59.363889 => 11:53:00.630866
[0m11:53:00.630866 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m11:53:00.630866 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:00.630866 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m11:53:00.630866 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-5dda-144d-bf1c-9ccd5353ba7e
[0m11:53:00.875612 [info ] [Thread-1 (]: 3 of 19 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 1.53s]
[0m11:53:00.880759 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:53:00.880759 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m11:53:00.880759 [info ] [Thread-1 (]: 4 of 19 START test not_null_dim_localidade_local_nascimento .................... [RUN]
[0m11:53:00.890354 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305)
[0m11:53:00.893019 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m11:53:00.900541 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m11:53:00.900541 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305 (compile): 11:53:00.894053 => 11:53:00.900541
[0m11:53:00.900541 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m11:53:00.900541 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m11:53:00.900541 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:00.900541 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m11:53:00.900541 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select local_nascimento
from `workspace`.`default`.`dim_localidade`
where local_nascimento is null



      
    ) dbt_internal_test
[0m11:53:00.910835 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:01.680811 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-5ebf-1d0c-b3a3-d8da867d766a
[0m11:53:02.240712 [debug] [Thread-1 (]: SQL status: OK in 1.3300000429153442 seconds
[0m11:53:02.240712 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305 (execute): 11:53:00.900541 => 11:53:02.240712
[0m11:53:02.240712 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: ROLLBACK
[0m11:53:02.240712 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:02.240712 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: Close
[0m11:53:02.240712 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-5ebf-1d0c-b3a3-d8da867d766a
[0m11:53:02.474331 [info ] [Thread-1 (]: 4 of 19 PASS not_null_dim_localidade_local_nascimento .......................... [[32mPASS[0m in 1.59s]
[0m11:53:02.475436 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m11:53:02.480570 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:53:02.480570 [info ] [Thread-1 (]: 5 of 19 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m11:53:02.480570 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m11:53:02.488218 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:53:02.500595 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:53:02.500595 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 11:53:02.490191 => 11:53:02.500595
[0m11:53:02.500595 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:53:02.500595 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:53:02.500595 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:02.500595 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:53:02.510449 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m11:53:02.510449 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:03.290591 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-5fb5-1e5a-ad7e-f5f61c092bcd
[0m11:53:03.770458 [debug] [Thread-1 (]: SQL status: OK in 1.2599999904632568 seconds
[0m11:53:03.770458 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 11:53:02.500595 => 11:53:03.770458
[0m11:53:03.770458 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m11:53:03.780543 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:03.780543 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m11:53:03.780543 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-5fb5-1e5a-ad7e-f5f61c092bcd
[0m11:53:04.010643 [info ] [Thread-1 (]: 5 of 19 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 1.53s]
[0m11:53:04.010643 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:53:04.021044 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:53:04.021044 [info ] [Thread-1 (]: 6 of 19 START test not_null_fato_atendimento_hospitalar_atendimento_id ......... [RUN]
[0m11:53:04.021044 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22)
[0m11:53:04.029623 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:53:04.040373 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m11:53:04.040373 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22 (compile): 11:53:04.030649 => 11:53:04.040373
[0m11:53:04.040373 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:53:04.048556 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m11:53:04.050628 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:04.050628 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m11:53:04.050628 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select atendimento_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is null



      
    ) dbt_internal_test
[0m11:53:04.050628 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:04.841463 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-60a0-1510-95f7-39f95f04f456
[0m11:53:05.255713 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select atendimento_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is null



      
    ) dbt_internal_test
[0m11:53:05.260804 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m11:53:05.260804 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:53:05.260804 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791e-60c1-181d-b2e4-889448386967
[0m11:53:05.260804 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22 (execute): 11:53:04.040373 => 11:53:05.260804
[0m11:53:05.260804 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: ROLLBACK
[0m11:53:05.260804 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:05.260804 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: Close
[0m11:53:05.270324 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-60a0-1510-95f7-39f95f04f456
[0m11:53:05.575339 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m11:53:05.575339 [error] [Thread-1 (]: 6 of 19 ERROR not_null_fato_atendimento_hospitalar_atendimento_id .............. [[31mERROR[0m in 1.55s]
[0m11:53:05.575339 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:53:05.575339 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:53:05.580387 [info ] [Thread-1 (]: 7 of 19 START test not_null_fato_atendimento_hospitalar_data_atendimento ....... [RUN]
[0m11:53:05.580387 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5)
[0m11:53:05.580387 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:53:05.590388 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:53:05.592400 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5 (compile): 11:53:05.580387 => 11:53:05.592400
[0m11:53:05.593690 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:53:05.593690 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:53:05.593690 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:05.593690 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:53:05.599686 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_atendimento
from `workspace`.`default`.`fato_atendimento_hospitalar`
where data_atendimento is null



      
    ) dbt_internal_test
[0m11:53:05.600807 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:06.520774 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-61a0-1d19-96ea-cb94c4ce51fc
[0m11:53:07.040996 [debug] [Thread-1 (]: SQL status: OK in 1.440000057220459 seconds
[0m11:53:07.050522 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5 (execute): 11:53:05.593690 => 11:53:07.050522
[0m11:53:07.050522 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: ROLLBACK
[0m11:53:07.050522 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:07.050522 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: Close
[0m11:53:07.050522 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-61a0-1d19-96ea-cb94c4ce51fc
[0m11:53:07.290590 [info ] [Thread-1 (]: 7 of 19 PASS not_null_fato_atendimento_hospitalar_data_atendimento ............. [[32mPASS[0m in 1.71s]
[0m11:53:07.302611 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:53:07.302611 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:53:07.302611 [info ] [Thread-1 (]: 8 of 19 START test not_null_fato_atendimento_hospitalar_doenca_id .............. [RUN]
[0m11:53:07.310666 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93)
[0m11:53:07.314591 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:53:07.320340 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m11:53:07.320340 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93 (compile): 11:53:07.315618 => 11:53:07.320340
[0m11:53:07.320340 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:53:07.330791 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m11:53:07.335400 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:07.336397 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m11:53:07.336770 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select doenca_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where doenca_id is null



      
    ) dbt_internal_test
[0m11:53:07.336770 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:08.156925 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-629a-1d76-b0f2-b7d6aefc6306
[0m11:53:08.790705 [debug] [Thread-1 (]: SQL status: OK in 1.4500000476837158 seconds
[0m11:53:08.803288 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93 (execute): 11:53:07.330791 => 11:53:08.803288
[0m11:53:08.804286 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: ROLLBACK
[0m11:53:08.805282 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:08.805282 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: Close
[0m11:53:08.806100 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-629a-1d76-b0f2-b7d6aefc6306
[0m11:53:09.030582 [info ] [Thread-1 (]: 8 of 19 PASS not_null_fato_atendimento_hospitalar_doenca_id .................... [[32mPASS[0m in 1.72s]
[0m11:53:09.040426 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:53:09.040426 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m11:53:09.040426 [info ] [Thread-1 (]: 9 of 19 START test not_null_fato_atendimento_hospitalar_localidade_id .......... [RUN]
[0m11:53:09.040426 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15)
[0m11:53:09.040426 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m11:53:09.050761 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"
[0m11:53:09.050761 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15 (compile): 11:53:09.040426 => 11:53:09.050761
[0m11:53:09.050761 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m11:53:09.064089 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"
[0m11:53:09.064325 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:09.064325 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"
[0m11:53:09.064325 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select localidade_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where localidade_id is null



      
    ) dbt_internal_test
[0m11:53:09.064325 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:09.920893 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-63a8-1d7d-9780-ee784cee62c5
[0m11:53:10.590606 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m11:53:10.609449 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15 (execute): 11:53:09.050761 => 11:53:10.607454
[0m11:53:10.610438 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15: ROLLBACK
[0m11:53:10.612433 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:10.613434 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15: Close
[0m11:53:10.614428 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-63a8-1d7d-9780-ee784cee62c5
[0m11:53:10.840747 [info ] [Thread-1 (]: 9 of 19 PASS not_null_fato_atendimento_hospitalar_localidade_id ................ [[32mPASS[0m in 1.80s]
[0m11:53:10.840747 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m11:53:10.840747 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:53:10.850445 [info ] [Thread-1 (]: 10 of 19 START test not_null_fato_nascimento_cod_municipio ..................... [RUN]
[0m11:53:10.850445 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m11:53:10.850445 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:53:10.850445 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:53:10.850445 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 11:53:10.850445 => 11:53:10.850445
[0m11:53:10.850445 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:53:10.864421 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:53:10.864421 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:10.864421 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:53:10.864421 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m11:53:10.864421 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:11.673166 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-64b5-1786-a848-7bcc65b91176
[0m11:53:12.870531 [debug] [Thread-1 (]: SQL status: OK in 2.009999990463257 seconds
[0m11:53:12.870531 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 11:53:10.860652 => 11:53:12.870531
[0m11:53:12.870531 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m11:53:12.870531 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:12.870531 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m11:53:12.870531 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-64b5-1786-a848-7bcc65b91176
[0m11:53:13.110550 [info ] [Thread-1 (]: 10 of 19 PASS not_null_fato_nascimento_cod_municipio ........................... [[32mPASS[0m in 2.26s]
[0m11:53:13.110550 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:53:13.110550 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m11:53:13.115669 [info ] [Thread-1 (]: 11 of 19 START test not_null_fato_nascimento_data_nascimento ................... [RUN]
[0m11:53:13.115669 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d)
[0m11:53:13.115669 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m11:53:13.124398 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m11:53:13.126394 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d (compile): 11:53:13.115669 => 11:53:13.125396
[0m11:53:13.126394 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m11:53:13.129384 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m11:53:13.130382 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:13.131380 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m11:53:13.131380 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_nascimento
from `workspace`.`default`.`fato_nascimento`
where data_nascimento is null



      
    ) dbt_internal_test
[0m11:53:13.132376 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:13.907468 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-6609-1f53-ad78-97225f63d51c
[0m11:53:14.561361 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_nascimento
from `workspace`.`default`.`fato_nascimento`
where data_nascimento is null



      
    ) dbt_internal_test
[0m11:53:14.563354 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m11:53:14.563354 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:53:14.564372 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791e-662a-1188-b88f-7aa9c4af4341
[0m11:53:14.565388 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d (execute): 11:53:13.127403 => 11:53:14.565388
[0m11:53:14.565388 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: ROLLBACK
[0m11:53:14.565388 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:14.565388 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: Close
[0m11:53:14.565388 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-6609-1f53-ad78-97225f63d51c
[0m11:53:14.795751 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_nascimento_data_nascimento (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m11:53:14.795751 [error] [Thread-1 (]: 11 of 19 ERROR not_null_fato_nascimento_data_nascimento ........................ [[31mERROR[0m in 1.68s]
[0m11:53:14.800841 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m11:53:14.800841 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:53:14.800841 [info ] [Thread-1 (]: 12 of 19 START test not_null_fato_nascimento_nascimento_id ..................... [RUN]
[0m11:53:14.800841 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m11:53:14.800841 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:53:14.810438 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:53:14.810438 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 11:53:14.800841 => 11:53:14.810438
[0m11:53:14.810438 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:53:14.810438 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:53:14.810438 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:14.817897 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:53:14.818521 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m11:53:14.818521 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:15.635497 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-6711-16bf-8757-f39b50a85b94
[0m11:53:16.140350 [debug] [Thread-1 (]: SQL status: OK in 1.3200000524520874 seconds
[0m11:53:16.144349 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 11:53:14.810438 => 11:53:16.144349
[0m11:53:16.145331 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m11:53:16.145331 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:16.146342 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m11:53:16.146342 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-6711-16bf-8757-f39b50a85b94
[0m11:53:16.382115 [info ] [Thread-1 (]: 12 of 19 PASS not_null_fato_nascimento_nascimento_id ........................... [[32mPASS[0m in 1.58s]
[0m11:53:16.390660 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:53:16.390660 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:53:16.390660 [info ] [Thread-1 (]: 13 of 19 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m11:53:16.390660 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m11:53:16.401444 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:53:16.412122 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:53:16.412294 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 11:53:16.403738 => 11:53:16.412294
[0m11:53:16.412294 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:53:16.412294 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:53:16.412294 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:16.412294 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:53:16.420331 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m11:53:16.420331 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:17.183763 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-67fc-14e9-b6ac-22520a9028d5
[0m11:53:17.685409 [debug] [Thread-1 (]: SQL status: OK in 1.2699999809265137 seconds
[0m11:53:17.687483 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 11:53:16.412294 => 11:53:17.687483
[0m11:53:17.687483 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m11:53:17.690583 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:17.690583 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m11:53:17.690583 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-67fc-14e9-b6ac-22520a9028d5
[0m11:53:17.920417 [error] [Thread-1 (]: 13 of 19 FAIL 4 not_null_fato_nascimento_peso .................................. [[31mFAIL 4[0m in 1.52s]
[0m11:53:17.920417 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:53:17.920417 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:53:17.920417 [info ] [Thread-1 (]: 14 of 19 START test not_null_fato_nascimento_sexo .............................. [RUN]
[0m11:53:17.930674 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m11:53:17.934813 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:53:17.946619 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m11:53:17.947452 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 11:53:17.935848 => 11:53:17.947452
[0m11:53:17.947452 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:53:17.950499 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m11:53:17.950499 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:17.955505 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m11:53:17.955505 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sexo
from `workspace`.`default`.`fato_nascimento`
where sexo is null



      
    ) dbt_internal_test
[0m11:53:17.955505 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:18.790987 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-68f2-1b2e-b6c8-229b510813fb
[0m11:53:19.291022 [debug] [Thread-1 (]: SQL status: OK in 1.340000033378601 seconds
[0m11:53:19.300381 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 11:53:17.947452 => 11:53:19.300381
[0m11:53:19.300381 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: ROLLBACK
[0m11:53:19.300381 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:19.300381 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: Close
[0m11:53:19.300381 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-68f2-1b2e-b6c8-229b510813fb
[0m11:53:19.535521 [info ] [Thread-1 (]: 14 of 19 PASS not_null_fato_nascimento_sexo .................................... [[32mPASS[0m in 1.60s]
[0m11:53:19.535521 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:53:19.535521 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a
[0m11:53:19.535521 [info ] [Thread-1 (]: 15 of 19 START test unique_dim_doenca_doenca_id ................................ [RUN]
[0m11:53:19.540616 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a)
[0m11:53:19.540616 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a
[0m11:53:19.552649 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a"
[0m11:53:19.553961 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a (compile): 11:53:19.540616 => 11:53:19.553961
[0m11:53:19.553961 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a
[0m11:53:19.553961 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a"
[0m11:53:19.553961 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:19.560505 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a"
[0m11:53:19.560505 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    doenca_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_doenca`
where doenca_id is not null
group by doenca_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:53:19.560505 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:20.380503 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-69e5-1297-809a-aa355d51ec39
[0m11:53:20.890820 [debug] [Thread-1 (]: SQL status: OK in 1.3300000429153442 seconds
[0m11:53:20.890820 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a (execute): 11:53:19.553961 => 11:53:20.890820
[0m11:53:20.890820 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a: ROLLBACK
[0m11:53:20.890820 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:20.890820 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a: Close
[0m11:53:20.890820 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-69e5-1297-809a-aa355d51ec39
[0m11:53:21.130409 [info ] [Thread-1 (]: 15 of 19 PASS unique_dim_doenca_doenca_id ...................................... [[32mPASS[0m in 1.58s]
[0m11:53:21.130496 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a
[0m11:53:21.130496 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:53:21.130496 [info ] [Thread-1 (]: 16 of 19 START test unique_dim_localidade_cod_municipio ........................ [RUN]
[0m11:53:21.130496 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m11:53:21.130496 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:53:21.130496 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m11:53:21.140735 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 11:53:21.130496 => 11:53:21.140735
[0m11:53:21.140735 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:53:21.140735 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m11:53:21.140735 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:21.140735 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m11:53:21.140735 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_municipio as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_localidade`
where cod_municipio is not null
group by cod_municipio
having count(*) > 1



      
    ) dbt_internal_test
[0m11:53:21.140735 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:21.990886 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-6adb-1c1a-9477-f3c34eb8a779
[0m11:53:22.400540 [debug] [Thread-1 (]: SQL status: OK in 1.2599999904632568 seconds
[0m11:53:22.400540 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 11:53:21.140735 => 11:53:22.400540
[0m11:53:22.400540 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: ROLLBACK
[0m11:53:22.400540 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:22.400540 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: Close
[0m11:53:22.400540 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-6adb-1c1a-9477-f3c34eb8a779
[0m11:53:22.645316 [error] [Thread-1 (]: 16 of 19 FAIL 78 unique_dim_localidade_cod_municipio ........................... [[31mFAIL 78[0m in 1.51s]
[0m11:53:22.650338 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:53:22.650338 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:53:22.650338 [info ] [Thread-1 (]: 17 of 19 START test unique_dim_tempo_data_id ................................... [RUN]
[0m11:53:22.650338 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m11:53:22.650338 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:53:22.660368 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:53:22.660368 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 11:53:22.650338 => 11:53:22.660368
[0m11:53:22.660368 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:53:22.665374 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:53:22.667403 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:22.667403 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:53:22.668400 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:53:22.669398 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:23.452713 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-6bb9-1cfd-9ba7-b86e22ed9f7c
[0m11:53:23.860670 [debug] [Thread-1 (]: SQL status: OK in 1.1799999475479126 seconds
[0m11:53:23.860670 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 11:53:22.660368 => 11:53:23.860670
[0m11:53:23.860670 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m11:53:23.860670 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:23.860670 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m11:53:23.860670 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-6bb9-1cfd-9ba7-b86e22ed9f7c
[0m11:53:24.095952 [info ] [Thread-1 (]: 17 of 19 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 1.45s]
[0m11:53:24.100533 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:53:24.100533 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:53:24.100533 [info ] [Thread-1 (]: 18 of 19 START test unique_fato_atendimento_hospitalar_atendimento_id .......... [RUN]
[0m11:53:24.100533 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe)
[0m11:53:24.100533 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:53:24.110420 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m11:53:24.112636 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe (compile): 11:53:24.100533 => 11:53:24.112636
[0m11:53:24.114132 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:53:24.114132 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m11:53:24.120208 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:24.120290 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m11:53:24.120290 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    atendimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is not null
group by atendimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:53:24.120290 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:24.900902 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-6c95-1da8-bc9a-8135f601c4a0
[0m11:53:25.320634 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    atendimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is not null
group by atendimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:53:25.320634 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m11:53:25.320634 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:53:25.320634 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791e-6cb7-1207-8125-8aa3ec4e939f
[0m11:53:25.320634 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe (execute): 11:53:24.114132 => 11:53:25.320634
[0m11:53:25.320634 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: ROLLBACK
[0m11:53:25.320634 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:25.320634 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: Close
[0m11:53:25.320634 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-6c95-1da8-bc9a-8135f601c4a0
[0m11:53:25.574818 [debug] [Thread-1 (]: Runtime Error in test unique_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m11:53:25.575936 [error] [Thread-1 (]: 18 of 19 ERROR unique_fato_atendimento_hospitalar_atendimento_id ............... [[31mERROR[0m in 1.48s]
[0m11:53:25.580573 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:53:25.580573 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:53:25.580573 [info ] [Thread-1 (]: 19 of 19 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m11:53:25.580573 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m11:53:25.580573 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:53:25.590461 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:53:25.590461 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 11:53:25.587984 => 11:53:25.590461
[0m11:53:25.590461 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:53:25.600712 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:53:25.604142 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:25.605312 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:53:25.605312 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:53:25.605312 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:53:26.400860 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-6d7b-1891-bc64-5b1fe752283c
[0m11:53:26.920665 [debug] [Thread-1 (]: SQL status: OK in 1.3200000524520874 seconds
[0m11:53:26.930709 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 11:53:25.590461 => 11:53:26.930709
[0m11:53:26.930709 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m11:53:26.930709 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:53:26.930709 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m11:53:26.930709 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-6d7b-1891-bc64-5b1fe752283c
[0m11:53:27.171052 [error] [Thread-1 (]: 19 of 19 FAIL 59968 unique_fato_nascimento_nascimento_id ....................... [[31mFAIL 59968[0m in 1.59s]
[0m11:53:27.180627 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:53:27.180627 [debug] [MainThread]: On master: ROLLBACK
[0m11:53:27.180627 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:53:28.010706 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0791e-6e71-1b49-8fb9-2106006a8e23
[0m11:53:28.010706 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:53:28.010706 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:53:28.010706 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:53:28.010706 [debug] [MainThread]: On master: ROLLBACK
[0m11:53:28.010706 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:53:28.010706 [debug] [MainThread]: On master: Close
[0m11:53:28.010706 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0791e-6e71-1b49-8fb9-2106006a8e23
[0m11:53:28.250849 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:53:28.250849 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m11:53:28.250849 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m11:53:28.260476 [info ] [MainThread]: 
[0m11:53:28.260476 [info ] [MainThread]: Finished running 19 tests in 0 hours 0 minutes and 35.16 seconds (35.16s).
[0m11:53:28.275662 [debug] [MainThread]: Command end result
[0m11:53:28.293028 [info ] [MainThread]: 
[0m11:53:28.293028 [info ] [MainThread]: [31mCompleted with 6 errors and 0 warnings:[0m
[0m11:53:28.293028 [info ] [MainThread]: 
[0m11:53:28.293028 [error] [MainThread]: [33mRuntime Error in test not_null_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)[0m
[0m11:53:28.293028 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m11:53:28.293028 [info ] [MainThread]: 
[0m11:53:28.293028 [error] [MainThread]: [33mRuntime Error in test not_null_fato_nascimento_data_nascimento (models\marts\schema.yaml)[0m
[0m11:53:28.300584 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m11:53:28.300584 [info ] [MainThread]: 
[0m11:53:28.300584 [error] [MainThread]: [31mFailure in test not_null_fato_nascimento_peso (models\marts\schema.yaml)[0m
[0m11:53:28.300584 [error] [MainThread]:   Got 4 results, configured to fail if != 0
[0m11:53:28.300584 [info ] [MainThread]: 
[0m11:53:28.300584 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\not_null_fato_nascimento_peso.sql
[0m11:53:28.306956 [info ] [MainThread]: 
[0m11:53:28.307967 [error] [MainThread]: [31mFailure in test unique_dim_localidade_cod_municipio (models\marts\schema.yaml)[0m
[0m11:53:28.309948 [error] [MainThread]:   Got 78 results, configured to fail if != 0
[0m11:53:28.310946 [info ] [MainThread]: 
[0m11:53:28.310946 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_dim_localidade_cod_municipio.sql
[0m11:53:28.310946 [info ] [MainThread]: 
[0m11:53:28.310946 [error] [MainThread]: [33mRuntime Error in test unique_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)[0m
[0m11:53:28.310946 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m11:53:28.310946 [info ] [MainThread]: 
[0m11:53:28.310946 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m11:53:28.310946 [error] [MainThread]:   Got 59968 results, configured to fail if != 0
[0m11:53:28.310946 [info ] [MainThread]: 
[0m11:53:28.310946 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m11:53:28.320383 [info ] [MainThread]: 
[0m11:53:28.320383 [info ] [MainThread]: Done. PASS=13 WARN=0 ERROR=6 SKIP=0 TOTAL=19
[0m11:53:28.323659 [debug] [MainThread]: Command `dbt test` failed at 11:53:28.320383 after 37.01 seconds
[0m11:53:28.323659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B71BB4D950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B71BAEFCD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B71BAEFA10>]}
[0m11:53:28.324684 [debug] [MainThread]: Flushing usage events
[0m11:55:39.161273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032EF932D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032C77AA50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032EFC2590>]}


============================== 11:55:39.161273 | e25a21e9-ede4-43f1-acca-1630f2f8e48b ==============================
[0m11:55:39.161273 [info ] [MainThread]: Running with dbt=1.5.2
[0m11:55:39.161273 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:55:40.490165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e25a21e9-ede4-43f1-acca-1630f2f8e48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203387E8050>]}
[0m11:55:40.510365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e25a21e9-ede4-43f1-acca-1630f2f8e48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032F5D4C90>]}
[0m11:55:40.510365 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m11:55:40.530288 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m11:55:40.653264 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:55:40.653264 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\schema.yaml
[0m11:55:40.735521 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m11:55:40.753906 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m11:55:40.756014 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m11:55:40.760051 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m11:55:40.764128 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_doenca.sql
[0m11:55:40.900411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e25a21e9-ede4-43f1-acca-1630f2f8e48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032EEF8090>]}
[0m11:55:40.910025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e25a21e9-ede4-43f1-acca-1630f2f8e48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203387E9CD0>]}
[0m11:55:40.910025 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m11:55:40.910025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e25a21e9-ede4-43f1-acca-1630f2f8e48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020335D8D410>]}
[0m11:55:40.910025 [info ] [MainThread]: 
[0m11:55:40.920403 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:55:40.920403 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m11:55:40.920403 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m11:55:40.920403 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m11:55:40.920403 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:55:41.790463 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0791e-be2d-1860-ba13-648c3808a84e
[0m11:55:42.350166 [debug] [ThreadPool]: SQL status: OK in 1.4299999475479126 seconds
[0m11:55:42.350166 [debug] [ThreadPool]: On list_workspace_default: Close
[0m11:55:42.350166 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0791e-be2d-1860-ba13-648c3808a84e
[0m11:55:42.605296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e25a21e9-ede4-43f1-acca-1630f2f8e48b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020338793A50>]}
[0m11:55:42.605296 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:42.610303 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:55:42.610353 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:55:42.610353 [info ] [MainThread]: 
[0m11:55:42.615374 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:55:42.620412 [info ] [Thread-1 (]: 1 of 21 START test accepted_values_fato_nascimento_sexo__M__F .................. [RUN]
[0m11:55:42.620412 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3'
[0m11:55:42.620412 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:55:42.630229 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m11:55:42.632636 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (compile): 11:55:42.623368 => 11:55:42.632636
[0m11:55:42.633605 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:55:42.650384 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m11:55:42.650384 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:42.650384 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m11:55:42.650384 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'M','F'
)



      
    ) dbt_internal_test
[0m11:55:42.650384 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:55:43.530314 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-bf36-1a98-9ede-66a6333173b5
[0m11:55:44.295476 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'M','F'
)



      
    ) dbt_internal_test
[0m11:55:44.300048 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

[0m11:55:44.300048 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [CAST_INVALID_INPUT] org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
	at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)
	at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$2(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$2$adapted(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.buildCast(Cast.scala:683)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$1(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:1408)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:730)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:89)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:109)
	at scala.collection.immutable.Vector1.map(Vector.scala:2141)
	at scala.collection.immutable.Vector1.map(Vector.scala:386)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:760)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:109)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:109)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:109)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.$anonfun$applyOrElse$1(expressions.scala:113)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:113)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:112)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$4(TreeNode.scala:536)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1705)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:536)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:487)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.apply(expressions.scala:112)
	at org.apache.spark.sql.execution.qrc.Canonicalized$.applyRules(ResultCacheManager.scala:131)
	at org.apache.spark.sql.execution.qrc.Canonicalized$.apply(ResultCacheManager.scala:122)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$lookupCache$4(ResultCacheManager.scala:514)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$lookupCache$4$adapted(ResultCacheManager.scala:514)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:146)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:142)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.recordLatency(ResultCacheManager.scala:305)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.lookupCache(ResultCacheManager.scala:514)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.lookupHybridResult(ResultCacheManager.scala:413)
	at org.apache.spark.sql.classic.Dataset.lookupHybridResultFromCache(Dataset.scala:1813)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.lookupResult(HybridCloudStoreResultHandler.scala:69)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.$anonfun$collectResult$1(ResultCollector.scala:101)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult(ResultCollector.scala:101)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult$(ResultCollector.scala:98)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.collectResult(HybridCloudStoreResultHandler.scala:33)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.org$apache$spark$sql$hive$thriftserver$HybridCloudStoreResultHandler$$initFromDataFrame(HybridCloudStoreResultHandler.scala:85)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler$.createFromDataFrame(HybridCloudStoreResultHandler.scala:214)
	at org.apache.spark.sql.hive.thriftserver.ResultHandlerFactory.getCollectorHandler(ResultHandlerFactory.scala:473)
	at org.apache.spark.sql.hive.thriftserver.ResultHandlerFactory.createResultHandler(ResultHandlerFactory.scala:443)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$18(SparkExecuteStatementOperation.scala:959)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withSuspendedQueryHangingDetection(SparkExecuteStatementOperation.scala:182)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:956)
	... 53 more

[0m11:55:44.310467 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791e-bf59-10e8-a128-d4b16a885773
[0m11:55:44.310467 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (execute): 11:55:42.633605 => 11:55:44.310467
[0m11:55:44.310467 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: ROLLBACK
[0m11:55:44.310467 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:55:44.310467 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: Close
[0m11:55:44.310467 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-bf36-1a98-9ede-66a6333173b5
[0m11:55:44.570577 [debug] [Thread-1 (]: Runtime Error in test accepted_values_fato_nascimento_sexo__M__F (models\marts\schema.yaml)
  [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  == SQL (line 24, position 19) ==
  where value_field not in (
                    ^^^^^^^^
      'M','F'
  ^^^^^^^^^^^
  )
  ^
  
[0m11:55:44.570577 [error] [Thread-1 (]: 1 of 21 ERROR accepted_values_fato_nascimento_sexo__M__F ....................... [[31mERROR[0m in 1.95s]
[0m11:55:44.580483 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:55:44.580483 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:55:44.580483 [info ] [Thread-1 (]: 2 of 21 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m11:55:44.580483 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m11:55:44.580483 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:55:44.590471 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:55:44.590471 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 11:55:44.580483 => 11:55:44.590471
[0m11:55:44.590471 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:55:44.590471 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:55:44.590471 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:44.600190 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:55:44.601203 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m11:55:44.601203 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:55:45.410156 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-c055-1e78-92a6-1f3cfb2fd069
[0m11:55:45.805780 [debug] [Thread-1 (]: SQL status: OK in 1.2000000476837158 seconds
[0m11:55:45.810379 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 11:55:44.590471 => 11:55:45.810379
[0m11:55:45.810379 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m11:55:45.810379 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:55:45.810379 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m11:55:45.810379 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-c055-1e78-92a6-1f3cfb2fd069
[0m11:55:46.025431 [info ] [Thread-1 (]: 2 of 21 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 1.44s]
[0m11:55:46.025431 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:55:46.030552 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m11:55:46.030552 [info ] [Thread-1 (]: 3 of 21 START test not_null_dim_localidade_local_nascimento .................... [RUN]
[0m11:55:46.030552 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305)
[0m11:55:46.030552 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m11:55:46.040489 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m11:55:46.040489 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305 (compile): 11:55:46.030552 => 11:55:46.040489
[0m11:55:46.040489 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m11:55:46.040489 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m11:55:46.046086 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:46.046086 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m11:55:46.048575 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select local_nascimento
from `workspace`.`default`.`dim_localidade`
where local_nascimento is null



      
    ) dbt_internal_test
[0m11:55:46.049650 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:55:46.840727 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-c130-1719-817e-8de4d7d1d6ab
[0m11:55:47.240332 [debug] [Thread-1 (]: SQL status: OK in 1.190000057220459 seconds
[0m11:55:47.249248 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305 (execute): 11:55:46.040489 => 11:55:47.249248
[0m11:55:47.250366 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: ROLLBACK
[0m11:55:47.250366 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:55:47.250366 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: Close
[0m11:55:47.250366 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-c130-1719-817e-8de4d7d1d6ab
[0m11:55:47.490484 [info ] [Thread-1 (]: 3 of 21 PASS not_null_dim_localidade_local_nascimento .......................... [[32mPASS[0m in 1.46s]
[0m11:55:47.495537 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m11:55:47.500187 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m11:55:47.500187 [info ] [Thread-1 (]: 4 of 21 START test not_null_dim_tempo_ano ...................................... [RUN]
[0m11:55:47.500187 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305, now test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6)
[0m11:55:47.509524 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m11:55:47.521137 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m11:55:47.521580 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (compile): 11:55:47.510141 => 11:55:47.521580
[0m11:55:47.521580 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m11:55:47.521580 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m11:55:47.521580 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:47.521580 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m11:55:47.530184 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select ano
from `workspace`.`default`.`dim_tempo`
where ano is null



      
    ) dbt_internal_test
[0m11:55:47.530184 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:55:48.310098 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-c212-10f2-8bd1-60d590b5ce99
[0m11:55:48.720447 [debug] [Thread-1 (]: SQL status: OK in 1.190000057220459 seconds
[0m11:55:48.720447 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (execute): 11:55:47.521580 => 11:55:48.720447
[0m11:55:48.720447 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: ROLLBACK
[0m11:55:48.720447 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:55:48.720447 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: Close
[0m11:55:48.720447 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-c212-10f2-8bd1-60d590b5ce99
[0m11:55:48.940349 [info ] [Thread-1 (]: 4 of 21 PASS not_null_dim_tempo_ano ............................................ [[32mPASS[0m in 1.44s]
[0m11:55:48.940349 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m11:55:48.940349 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:55:48.940349 [info ] [Thread-1 (]: 5 of 21 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m11:55:48.940349 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m11:55:48.940349 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:55:48.954318 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:55:48.956304 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 11:55:48.940349 => 11:55:48.955307
[0m11:55:48.956304 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:55:48.960321 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:55:48.961302 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:48.962328 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:55:48.963286 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m11:55:48.963286 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:55:49.750605 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-c2ee-10a0-874a-9ac1d6f0c025
[0m11:55:50.166571 [debug] [Thread-1 (]: SQL status: OK in 1.2000000476837158 seconds
[0m11:55:50.173130 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 11:55:48.957301 => 11:55:50.173130
[0m11:55:50.173130 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m11:55:50.173130 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:55:50.180258 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m11:55:50.180258 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-c2ee-10a0-874a-9ac1d6f0c025
[0m11:55:50.410146 [info ] [Thread-1 (]: 5 of 21 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 1.47s]
[0m11:55:50.410146 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:55:50.410146 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m11:55:50.410146 [info ] [Thread-1 (]: 6 of 21 START test not_null_dim_tempo_dia ...................................... [RUN]
[0m11:55:50.420493 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306)
[0m11:55:50.420493 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m11:55:50.428861 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m11:55:50.429852 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (compile): 11:55:50.422847 => 11:55:50.429852
[0m11:55:50.430833 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m11:55:50.433848 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m11:55:50.434826 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:50.434826 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m11:55:50.435840 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select dia
from `workspace`.`default`.`dim_tempo`
where dia is null



      
    ) dbt_internal_test
[0m11:55:50.436811 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:55:51.270494 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-c3d1-128e-99f1-2733e4f9b9a8
[0m11:55:51.678940 [debug] [Thread-1 (]: SQL status: OK in 1.2400000095367432 seconds
[0m11:55:51.682931 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (execute): 11:55:50.430833 => 11:55:51.681905
[0m11:55:51.682931 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: ROLLBACK
[0m11:55:51.683928 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:55:51.683928 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: Close
[0m11:55:51.684925 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-c3d1-128e-99f1-2733e4f9b9a8
[0m11:55:51.910518 [info ] [Thread-1 (]: 6 of 21 PASS not_null_dim_tempo_dia ............................................ [[32mPASS[0m in 1.49s]
[0m11:55:51.910518 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m11:55:51.920328 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m11:55:51.920328 [info ] [Thread-1 (]: 7 of 21 START test not_null_dim_tempo_mes ...................................... [RUN]
[0m11:55:51.920328 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306, now test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972)
[0m11:55:51.930260 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m11:55:51.931739 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m11:55:51.931739 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (compile): 11:55:51.931114 => 11:55:51.931739
[0m11:55:51.931739 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m11:55:51.942411 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m11:55:51.943780 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:51.943780 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m11:55:51.943780 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select mes
from `workspace`.`default`.`dim_tempo`
where mes is null



      
    ) dbt_internal_test
[0m11:55:51.943780 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:55:52.738096 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-c4b4-1cd6-8897-62b6942542fd
[0m11:55:53.138216 [debug] [Thread-1 (]: SQL status: OK in 1.190000057220459 seconds
[0m11:55:53.141176 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (execute): 11:55:51.931739 => 11:55:53.141176
[0m11:55:53.142681 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: ROLLBACK
[0m11:55:53.142681 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:55:53.142681 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: Close
[0m11:55:53.142681 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-c4b4-1cd6-8897-62b6942542fd
[0m11:55:53.380237 [info ] [Thread-1 (]: 7 of 21 PASS not_null_dim_tempo_mes ............................................ [[32mPASS[0m in 1.46s]
[0m11:55:53.380237 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m11:55:53.380237 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:55:53.390309 [info ] [Thread-1 (]: 8 of 21 START test not_null_fato_atendimento_hospitalar_atendimento_id ......... [RUN]
[0m11:55:53.390309 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22)
[0m11:55:53.390309 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:55:53.400431 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m11:55:53.400431 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22 (compile): 11:55:53.394166 => 11:55:53.400431
[0m11:55:53.400431 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:55:53.400431 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m11:55:53.400431 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:53.400431 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m11:55:53.400431 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select atendimento_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is null



      
    ) dbt_internal_test
[0m11:55:53.400431 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:55:54.240491 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-c598-17de-8d2f-f86c0bd79e21
[0m11:55:54.655147 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select atendimento_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is null



      
    ) dbt_internal_test
[0m11:55:54.655147 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m11:55:54.655147 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:55:54.655147 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791e-c5bb-16ab-ab86-5feeb664634e
[0m11:55:54.655147 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22 (execute): 11:55:53.400431 => 11:55:54.655147
[0m11:55:54.655147 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: ROLLBACK
[0m11:55:54.655147 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:55:54.660238 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: Close
[0m11:55:54.660238 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-c598-17de-8d2f-f86c0bd79e21
[0m11:55:54.910423 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m11:55:54.910423 [error] [Thread-1 (]: 8 of 21 ERROR not_null_fato_atendimento_hospitalar_atendimento_id .............. [[31mERROR[0m in 1.52s]
[0m11:55:54.910423 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m11:55:54.920530 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:55:54.920530 [info ] [Thread-1 (]: 9 of 21 START test not_null_fato_atendimento_hospitalar_data_atendimento ....... [RUN]
[0m11:55:54.920530 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5)
[0m11:55:54.926299 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:55:54.930342 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:55:54.935348 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5 (compile): 11:55:54.926299 => 11:55:54.935348
[0m11:55:54.935348 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:55:54.940393 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:55:54.940393 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:54.940393 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:55:54.940393 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_atendimento
from `workspace`.`default`.`fato_atendimento_hospitalar`
where data_atendimento is null



      
    ) dbt_internal_test
[0m11:55:54.940393 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:55:55.720152 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-c67c-116a-9743-2bd78f4f0934
[0m11:55:56.200217 [debug] [Thread-1 (]: SQL status: OK in 1.2599999904632568 seconds
[0m11:55:56.200217 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5 (execute): 11:55:54.935348 => 11:55:56.200217
[0m11:55:56.200217 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: ROLLBACK
[0m11:55:56.200217 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:55:56.200217 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: Close
[0m11:55:56.200217 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-c67c-116a-9743-2bd78f4f0934
[0m11:55:56.420535 [info ] [Thread-1 (]: 9 of 21 PASS not_null_fato_atendimento_hospitalar_data_atendimento ............. [[32mPASS[0m in 1.50s]
[0m11:55:56.430286 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:55:56.430286 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:55:56.433437 [info ] [Thread-1 (]: 10 of 21 START test not_null_fato_atendimento_hospitalar_doenca_id ............. [RUN]
[0m11:55:56.438957 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93)
[0m11:55:56.441950 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:55:56.450295 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m11:55:56.450295 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93 (compile): 11:55:56.442983 => 11:55:56.450295
[0m11:55:56.450295 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:55:56.455315 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m11:55:56.455315 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:56.455315 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m11:55:56.455315 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select doenca_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where doenca_id is null



      
    ) dbt_internal_test
[0m11:55:56.455315 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:55:57.279961 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-c767-1a30-9c29-c3d519a9c219
[0m11:55:57.700516 [debug] [Thread-1 (]: SQL status: OK in 1.25 seconds
[0m11:55:57.710413 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93 (execute): 11:55:56.450295 => 11:55:57.710413
[0m11:55:57.710413 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: ROLLBACK
[0m11:55:57.710413 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:55:57.710413 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: Close
[0m11:55:57.710413 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-c767-1a30-9c29-c3d519a9c219
[0m11:55:57.950461 [info ] [Thread-1 (]: 10 of 21 PASS not_null_fato_atendimento_hospitalar_doenca_id ................... [[32mPASS[0m in 1.52s]
[0m11:55:57.950461 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:55:57.950461 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m11:55:57.960035 [info ] [Thread-1 (]: 11 of 21 START test not_null_fato_atendimento_hospitalar_localidade_id ......... [RUN]
[0m11:55:57.964311 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15)
[0m11:55:57.966299 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m11:55:57.972283 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"
[0m11:55:57.973280 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15 (compile): 11:55:57.966299 => 11:55:57.973280
[0m11:55:57.974277 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m11:55:57.977269 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"
[0m11:55:57.979264 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:57.979264 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"
[0m11:55:57.980262 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select localidade_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where localidade_id is null



      
    ) dbt_internal_test
[0m11:55:57.981266 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:55:58.774477 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-c84d-1854-a17d-a983e37477a8
[0m11:55:59.180523 [debug] [Thread-1 (]: SQL status: OK in 1.2000000476837158 seconds
[0m11:55:59.190100 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15 (execute): 11:55:57.975274 => 11:55:59.190100
[0m11:55:59.190100 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15: ROLLBACK
[0m11:55:59.190100 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:55:59.190100 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15: Close
[0m11:55:59.195111 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-c84d-1854-a17d-a983e37477a8
[0m11:55:59.440285 [info ] [Thread-1 (]: 11 of 21 PASS not_null_fato_atendimento_hospitalar_localidade_id ............... [[32mPASS[0m in 1.48s]
[0m11:55:59.440285 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m11:55:59.440285 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:55:59.440285 [info ] [Thread-1 (]: 12 of 21 START test not_null_fato_nascimento_cod_municipio ..................... [RUN]
[0m11:55:59.440285 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m11:55:59.440285 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:55:59.450638 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:55:59.455172 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 11:55:59.440285 => 11:55:59.455172
[0m11:55:59.455172 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:55:59.460192 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:55:59.460192 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:55:59.460192 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:55:59.460192 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m11:55:59.460192 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:56:00.300502 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-c936-1106-b665-533b5883571f
[0m11:56:00.790189 [debug] [Thread-1 (]: SQL status: OK in 1.3300000429153442 seconds
[0m11:56:00.800773 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 11:55:59.457436 => 11:56:00.800483
[0m11:56:00.800773 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m11:56:00.801772 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:56:00.802769 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m11:56:00.802769 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-c936-1106-b665-533b5883571f
[0m11:56:01.040314 [info ] [Thread-1 (]: 12 of 21 PASS not_null_fato_nascimento_cod_municipio ........................... [[32mPASS[0m in 1.60s]
[0m11:56:01.040314 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:56:01.040314 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m11:56:01.050454 [info ] [Thread-1 (]: 13 of 21 START test not_null_fato_nascimento_data_id ........................... [RUN]
[0m11:56:01.050454 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m11:56:01.052838 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m11:56:01.054042 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m11:56:01.060112 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 11:56:01.053955 => 11:56:01.060112
[0m11:56:01.060112 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m11:56:01.060112 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m11:56:01.065117 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:56:01.065117 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m11:56:01.065117 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m11:56:01.067795 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:56:01.872863 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-ca26-1edb-a3ed-f6f119a1f0c7
[0m11:56:03.000583 [debug] [Thread-1 (]: SQL status: OK in 1.940000057220459 seconds
[0m11:56:03.010432 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 11:56:01.060112 => 11:56:03.010432
[0m11:56:03.010432 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m11:56:03.010432 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:56:03.010432 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m11:56:03.010432 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-ca26-1edb-a3ed-f6f119a1f0c7
[0m11:56:03.244107 [info ] [Thread-1 (]: 13 of 21 PASS not_null_fato_nascimento_data_id ................................. [[32mPASS[0m in 2.19s]
[0m11:56:03.250213 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m11:56:03.250213 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m11:56:03.255261 [info ] [Thread-1 (]: 14 of 21 START test not_null_fato_nascimento_idade_mae ......................... [RUN]
[0m11:56:03.260340 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42)
[0m11:56:03.262139 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m11:56:03.272141 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m11:56:03.272141 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (compile): 11:56:03.263567 => 11:56:03.272141
[0m11:56:03.272141 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m11:56:03.280259 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m11:56:03.280259 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:56:03.280259 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m11:56:03.286798 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select idade_mae
from `workspace`.`default`.`fato_nascimento`
where idade_mae is null



      
    ) dbt_internal_test
[0m11:56:03.288257 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:56:04.080230 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-cb77-108d-9973-b9d9db866103
[0m11:56:04.570285 [debug] [Thread-1 (]: SQL status: OK in 1.2799999713897705 seconds
[0m11:56:04.570285 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (execute): 11:56:03.272141 => 11:56:04.570285
[0m11:56:04.570285 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: ROLLBACK
[0m11:56:04.570285 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:56:04.570285 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: Close
[0m11:56:04.570285 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-cb77-108d-9973-b9d9db866103
[0m11:56:04.800265 [info ] [Thread-1 (]: 14 of 21 PASS not_null_fato_nascimento_idade_mae ............................... [[32mPASS[0m in 1.55s]
[0m11:56:04.800265 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m11:56:04.800265 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:56:04.800265 [info ] [Thread-1 (]: 15 of 21 START test not_null_fato_nascimento_nascimento_id ..................... [RUN]
[0m11:56:04.800265 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m11:56:04.800265 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:56:04.810337 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:56:04.810337 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 11:56:04.800265 => 11:56:04.810337
[0m11:56:04.810337 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:56:04.810337 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:56:04.810337 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:56:04.820262 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:56:04.820262 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m11:56:04.821724 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:56:05.590554 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-cc5e-1149-9f88-4ba70d04a988
[0m11:56:06.100487 [debug] [Thread-1 (]: SQL status: OK in 1.2799999713897705 seconds
[0m11:56:06.100487 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 11:56:04.810337 => 11:56:06.100487
[0m11:56:06.100487 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m11:56:06.100487 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:56:06.100487 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m11:56:06.100487 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-cc5e-1149-9f88-4ba70d04a988
[0m11:56:06.340508 [info ] [Thread-1 (]: 15 of 21 PASS not_null_fato_nascimento_nascimento_id ........................... [[32mPASS[0m in 1.54s]
[0m11:56:06.340508 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:56:06.350216 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:56:06.350778 [info ] [Thread-1 (]: 16 of 21 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m11:56:06.350778 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m11:56:06.350778 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:56:06.360441 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:56:06.360441 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 11:56:06.354269 => 11:56:06.360441
[0m11:56:06.360441 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:56:06.365447 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:56:06.365447 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:56:06.365447 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:56:06.365447 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m11:56:06.365447 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:56:07.152444 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-cd4c-1064-b4d0-b8a46e36a5cc
[0m11:56:07.700233 [debug] [Thread-1 (]: SQL status: OK in 1.3300000429153442 seconds
[0m11:56:07.700233 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 11:56:06.360441 => 11:56:07.700233
[0m11:56:07.700233 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m11:56:07.700233 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:56:07.700233 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m11:56:07.710218 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-cd4c-1064-b4d0-b8a46e36a5cc
[0m11:56:07.960369 [error] [Thread-1 (]: 16 of 21 FAIL 4 not_null_fato_nascimento_peso .................................. [[31mFAIL 4[0m in 1.61s]
[0m11:56:07.960369 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:56:07.960369 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:56:07.960369 [info ] [Thread-1 (]: 17 of 21 START test not_null_fato_nascimento_sexo .............................. [RUN]
[0m11:56:07.970455 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m11:56:07.970455 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:56:07.976431 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m11:56:07.980466 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 11:56:07.972832 => 11:56:07.980466
[0m11:56:07.980466 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:56:07.980466 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m11:56:07.985472 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:56:07.985472 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m11:56:07.985472 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sexo
from `workspace`.`default`.`fato_nascimento`
where sexo is null



      
    ) dbt_internal_test
[0m11:56:07.985472 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:56:08.760357 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-ce43-1315-9975-2d7f6d1cc818
[0m11:56:09.240396 [debug] [Thread-1 (]: SQL status: OK in 1.25 seconds
[0m11:56:09.240396 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 11:56:07.980466 => 11:56:09.240396
[0m11:56:09.240396 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: ROLLBACK
[0m11:56:09.250571 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:56:09.250571 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: Close
[0m11:56:09.250571 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-ce43-1315-9975-2d7f6d1cc818
[0m11:56:09.490438 [info ] [Thread-1 (]: 17 of 21 PASS not_null_fato_nascimento_sexo .................................... [[32mPASS[0m in 1.52s]
[0m11:56:09.500152 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m11:56:09.500152 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:56:09.500152 [info ] [Thread-1 (]: 18 of 21 START test unique_dim_localidade_cod_municipio ........................ [RUN]
[0m11:56:09.500152 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m11:56:09.511778 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:56:09.520236 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m11:56:09.520236 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 11:56:09.513809 => 11:56:09.520236
[0m11:56:09.526624 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:56:09.530141 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m11:56:09.530141 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:56:09.530141 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m11:56:09.530141 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_municipio as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_localidade`
where cod_municipio is not null
group by cod_municipio
having count(*) > 1



      
    ) dbt_internal_test
[0m11:56:09.530141 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:56:10.330450 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-cf2f-1cee-a2fb-525870fb6178
[0m11:56:10.720404 [debug] [Thread-1 (]: SQL status: OK in 1.190000057220459 seconds
[0m11:56:10.720404 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 11:56:09.527530 => 11:56:10.720404
[0m11:56:10.720404 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: ROLLBACK
[0m11:56:10.729580 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:56:10.730163 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: Close
[0m11:56:10.730240 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-cf2f-1cee-a2fb-525870fb6178
[0m11:56:10.960450 [error] [Thread-1 (]: 18 of 21 FAIL 78 unique_dim_localidade_cod_municipio ........................... [[31mFAIL 78[0m in 1.46s]
[0m11:56:10.970535 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m11:56:10.975044 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:56:10.975044 [info ] [Thread-1 (]: 19 of 21 START test unique_dim_tempo_data_id ................................... [RUN]
[0m11:56:10.980143 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m11:56:10.980143 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:56:10.990201 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:56:10.995272 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 11:56:10.985151 => 11:56:10.990201
[0m11:56:10.996269 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:56:10.996390 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:56:11.000442 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:56:11.000442 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:56:11.000442 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:56:11.000442 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:56:11.770418 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-d00d-1199-8d30-9fb24b9c4122
[0m11:56:12.180382 [debug] [Thread-1 (]: SQL status: OK in 1.1799999475479126 seconds
[0m11:56:12.180382 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 11:56:10.996390 => 11:56:12.180382
[0m11:56:12.180382 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m11:56:12.180382 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:56:12.180382 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m11:56:12.180382 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-d00d-1199-8d30-9fb24b9c4122
[0m11:56:12.420761 [info ] [Thread-1 (]: 19 of 21 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 1.44s]
[0m11:56:12.430333 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:56:12.433984 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:56:12.433984 [info ] [Thread-1 (]: 20 of 21 START test unique_fato_atendimento_hospitalar_atendimento_id .......... [RUN]
[0m11:56:12.433984 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe)
[0m11:56:12.433984 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:56:12.447616 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m11:56:12.447616 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe (compile): 11:56:12.437963 => 11:56:12.447616
[0m11:56:12.450153 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:56:12.450153 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m11:56:12.450153 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:56:12.455157 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m11:56:12.455157 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    atendimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is not null
group by atendimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:56:12.455157 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:56:13.420585 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-d103-145e-a638-ef7916f07215
[0m11:56:13.850546 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    atendimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is not null
group by atendimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:56:13.850546 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m11:56:13.850546 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m11:56:13.850546 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791e-d129-1a80-9b5c-4cc64ae9f206
[0m11:56:13.850546 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe (execute): 11:56:12.450153 => 11:56:13.850546
[0m11:56:13.850546 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: ROLLBACK
[0m11:56:13.850546 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:56:13.850546 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: Close
[0m11:56:13.860052 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-d103-145e-a638-ef7916f07215
[0m11:56:14.100525 [debug] [Thread-1 (]: Runtime Error in test unique_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m11:56:14.100525 [error] [Thread-1 (]: 20 of 21 ERROR unique_fato_atendimento_hospitalar_atendimento_id ............... [[31mERROR[0m in 1.67s]
[0m11:56:14.100525 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m11:56:14.100525 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:56:14.100525 [info ] [Thread-1 (]: 21 of 21 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m11:56:14.100525 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m11:56:14.100525 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:56:14.114735 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:56:14.115788 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 11:56:14.100525 => 11:56:14.115788
[0m11:56:14.115788 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:56:14.120352 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:56:14.120352 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:56:14.120352 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:56:14.120352 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:56:14.125358 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:56:14.930400 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791e-d1ee-11c6-97bd-ccd74607d734
[0m11:56:15.395316 [debug] [Thread-1 (]: SQL status: OK in 1.2699999809265137 seconds
[0m11:56:15.395316 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 11:56:14.115788 => 11:56:15.395316
[0m11:56:15.395316 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m11:56:15.400380 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:56:15.400380 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m11:56:15.400380 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791e-d1ee-11c6-97bd-ccd74607d734
[0m11:56:15.641377 [error] [Thread-1 (]: 21 of 21 FAIL 59968 unique_fato_nascimento_nascimento_id ....................... [[31mFAIL 59968[0m in 1.54s]
[0m11:56:15.641377 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:56:15.650465 [debug] [MainThread]: On master: ROLLBACK
[0m11:56:15.654616 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:56:16.510486 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0791e-d2e0-17af-9ee1-404b83d3cf27
[0m11:56:16.510486 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:56:16.510486 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:56:16.510486 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:56:16.510486 [debug] [MainThread]: On master: ROLLBACK
[0m11:56:16.510486 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:56:16.510486 [debug] [MainThread]: On master: Close
[0m11:56:16.520349 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0791e-d2e0-17af-9ee1-404b83d3cf27
[0m11:56:16.750394 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:56:16.750394 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m11:56:16.750394 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m11:56:16.750394 [info ] [MainThread]: 
[0m11:56:16.750394 [info ] [MainThread]: Finished running 21 tests in 0 hours 0 minutes and 35.83 seconds (35.83s).
[0m11:56:16.760347 [debug] [MainThread]: Command end result
[0m11:56:16.773689 [info ] [MainThread]: 
[0m11:56:16.774688 [info ] [MainThread]: [31mCompleted with 6 errors and 0 warnings:[0m
[0m11:56:16.775729 [info ] [MainThread]: 
[0m11:56:16.775729 [error] [MainThread]: [33mRuntime Error in test accepted_values_fato_nascimento_sexo__M__F (models\marts\schema.yaml)[0m
[0m11:56:16.775729 [error] [MainThread]:   [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
[0m11:56:16.775729 [error] [MainThread]:   == SQL (line 24, position 19) ==
[0m11:56:16.780280 [error] [MainThread]:   where value_field not in (
[0m11:56:16.780280 [error] [MainThread]:                     ^^^^^^^^
[0m11:56:16.780280 [error] [MainThread]:       'M','F'
[0m11:56:16.784214 [error] [MainThread]:   ^^^^^^^^^^^
[0m11:56:16.785211 [error] [MainThread]:   )
[0m11:56:16.786208 [error] [MainThread]:   ^
[0m11:56:16.787207 [error] [MainThread]:   
[0m11:56:16.788202 [info ] [MainThread]: 
[0m11:56:16.788202 [error] [MainThread]: [33mRuntime Error in test not_null_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)[0m
[0m11:56:16.789200 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m11:56:16.791132 [info ] [MainThread]: 
[0m11:56:16.791132 [error] [MainThread]: [31mFailure in test not_null_fato_nascimento_peso (models\marts\schema.yaml)[0m
[0m11:56:16.791132 [error] [MainThread]:   Got 4 results, configured to fail if != 0
[0m11:56:16.791132 [info ] [MainThread]: 
[0m11:56:16.791132 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\not_null_fato_nascimento_peso.sql
[0m11:56:16.791132 [info ] [MainThread]: 
[0m11:56:16.791132 [error] [MainThread]: [31mFailure in test unique_dim_localidade_cod_municipio (models\marts\schema.yaml)[0m
[0m11:56:16.791132 [error] [MainThread]:   Got 78 results, configured to fail if != 0
[0m11:56:16.800970 [info ] [MainThread]: 
[0m11:56:16.801967 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_dim_localidade_cod_municipio.sql
[0m11:56:16.802992 [info ] [MainThread]: 
[0m11:56:16.803961 [error] [MainThread]: [33mRuntime Error in test unique_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)[0m
[0m11:56:16.805960 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m11:56:16.806954 [info ] [MainThread]: 
[0m11:56:16.807951 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m11:56:16.808948 [error] [MainThread]:   Got 59968 results, configured to fail if != 0
[0m11:56:16.809945 [info ] [MainThread]: 
[0m11:56:16.810951 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m11:56:16.811940 [info ] [MainThread]: 
[0m11:56:16.812966 [info ] [MainThread]: Done. PASS=15 WARN=0 ERROR=6 SKIP=0 TOTAL=21
[0m11:56:16.814933 [debug] [MainThread]: Command `dbt test` failed at 11:56:16.813964 after 37.67 seconds
[0m11:56:16.814933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020328BD8790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020328BD8750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032EFC0C50>]}
[0m11:56:16.815956 [debug] [MainThread]: Flushing usage events
[0m11:59:26.239730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E063A07690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0638EF510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E063B6AD50>]}


============================== 11:59:26.239730 | 7901e282-e43c-4a12-b712-fb72bd50aeca ==============================
[0m11:59:26.239730 [info ] [MainThread]: Running with dbt=1.5.2
[0m11:59:26.239730 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:59:27.573203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7901e282-e43c-4a12-b712-fb72bd50aeca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E063C62990>]}
[0m11:59:27.590041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7901e282-e43c-4a12-b712-fb72bd50aeca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E063C62990>]}
[0m11:59:27.590041 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m11:59:27.610082 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m11:59:27.725076 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:59:27.725076 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\schema.yaml
[0m11:59:27.759818 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m11:59:27.830096 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m11:59:27.830096 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m11:59:27.839718 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m11:59:27.939802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7901e282-e43c-4a12-b712-fb72bd50aeca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E06D3E9510>]}
[0m11:59:27.960700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7901e282-e43c-4a12-b712-fb72bd50aeca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E06E5BB450>]}
[0m11:59:27.961698 [info ] [MainThread]: Found 12 models, 12 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m11:59:27.962695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7901e282-e43c-4a12-b712-fb72bd50aeca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E06A994510>]}
[0m11:59:27.965759 [info ] [MainThread]: 
[0m11:59:27.965759 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:59:27.965759 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m11:59:27.969874 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m11:59:27.969874 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m11:59:27.969874 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:59:28.837807 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0791f-4580-1a0d-9459-1559dfcebde2
[0m11:59:29.339909 [debug] [ThreadPool]: SQL status: OK in 1.3700000047683716 seconds
[0m11:59:29.339909 [debug] [ThreadPool]: On list_workspace_default: Close
[0m11:59:29.349802 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0791f-4580-1a0d-9459-1559dfcebde2
[0m11:59:29.574081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7901e282-e43c-4a12-b712-fb72bd50aeca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E06D3BE190>]}
[0m11:59:29.575181 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:29.575181 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:59:29.575181 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:59:29.575181 [info ] [MainThread]: 
[0m11:59:29.580291 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:59:29.580291 [info ] [Thread-1 (]: 1 of 12 START test accepted_values_fato_nascimento_sexo__M__F .................. [RUN]
[0m11:59:29.587811 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3'
[0m11:59:29.587811 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:59:29.593896 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m11:59:29.593896 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (compile): 11:59:29.589416 => 11:59:29.593896
[0m11:59:29.593896 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:59:29.622025 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m11:59:29.622025 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:29.622025 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m11:59:29.622025 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'M','F'
)



      
    ) dbt_internal_test
[0m11:59:29.622025 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:59:30.441865 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-4676-1384-a69e-c5ba2b365574
[0m11:59:32.660097 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'M','F'
)



      
    ) dbt_internal_test
[0m11:59:32.660097 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

[0m11:59:32.660097 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [CAST_INVALID_INPUT] org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
	at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)
	at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$2(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$2$adapted(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.buildCast(Cast.scala:683)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$1(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:1408)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:730)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:89)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:109)
	at scala.collection.immutable.Vector1.map(Vector.scala:2141)
	at scala.collection.immutable.Vector1.map(Vector.scala:386)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:760)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:109)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:109)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:109)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.$anonfun$applyOrElse$1(expressions.scala:113)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:113)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:112)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$4(TreeNode.scala:536)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1705)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:536)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:487)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.apply(expressions.scala:112)
	at org.apache.spark.sql.execution.qrc.Canonicalized$.applyRules(ResultCacheManager.scala:131)
	at org.apache.spark.sql.execution.qrc.Canonicalized$.apply(ResultCacheManager.scala:122)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$lookupCache$4(ResultCacheManager.scala:514)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$lookupCache$4$adapted(ResultCacheManager.scala:514)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:146)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:142)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.recordLatency(ResultCacheManager.scala:305)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.lookupCache(ResultCacheManager.scala:514)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.lookupHybridResult(ResultCacheManager.scala:413)
	at org.apache.spark.sql.classic.Dataset.lookupHybridResultFromCache(Dataset.scala:1813)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.lookupResult(HybridCloudStoreResultHandler.scala:69)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.$anonfun$collectResult$1(ResultCollector.scala:101)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult(ResultCollector.scala:101)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult$(ResultCollector.scala:98)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.collectResult(HybridCloudStoreResultHandler.scala:33)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.org$apache$spark$sql$hive$thriftserver$HybridCloudStoreResultHandler$$initFromDataFrame(HybridCloudStoreResultHandler.scala:85)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler$.createFromDataFrame(HybridCloudStoreResultHandler.scala:214)
	at org.apache.spark.sql.hive.thriftserver.ResultHandlerFactory.getCollectorHandler(ResultHandlerFactory.scala:473)
	at org.apache.spark.sql.hive.thriftserver.ResultHandlerFactory.createResultHandler(ResultHandlerFactory.scala:443)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$18(SparkExecuteStatementOperation.scala:959)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withSuspendedQueryHangingDetection(SparkExecuteStatementOperation.scala:182)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:956)
	... 53 more

[0m11:59:32.660097 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791f-4697-18a6-8213-e23c3b5729f4
[0m11:59:32.660097 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (execute): 11:59:29.593896 => 11:59:32.660097
[0m11:59:32.660097 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: ROLLBACK
[0m11:59:32.660097 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:32.660097 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: Close
[0m11:59:32.660097 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-4676-1384-a69e-c5ba2b365574
[0m11:59:32.920132 [debug] [Thread-1 (]: Runtime Error in test accepted_values_fato_nascimento_sexo__M__F (models\marts\schema.yaml)
  [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  == SQL (line 24, position 19) ==
  where value_field not in (
                    ^^^^^^^^
      'M','F'
  ^^^^^^^^^^^
  )
  ^
  
[0m11:59:32.920132 [error] [Thread-1 (]: 1 of 12 ERROR accepted_values_fato_nascimento_sexo__M__F ....................... [[31mERROR[0m in 3.34s]
[0m11:59:32.930048 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m11:59:32.930048 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:59:32.930048 [info ] [Thread-1 (]: 2 of 12 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m11:59:32.930048 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m11:59:32.930048 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:59:32.945101 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:59:32.945101 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 11:59:32.930048 => 11:59:32.945101
[0m11:59:32.945101 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:59:32.950152 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:59:32.952451 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:32.953806 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m11:59:32.953806 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m11:59:32.953806 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:33.780044 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-4874-1e3e-9b8e-b8358506380c
[0m11:59:34.180005 [debug] [Thread-1 (]: SQL status: OK in 1.2300000190734863 seconds
[0m11:59:34.180005 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 11:59:32.945101 => 11:59:34.180005
[0m11:59:34.180005 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m11:59:34.180005 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:34.180005 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m11:59:34.180005 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-4874-1e3e-9b8e-b8358506380c
[0m11:59:34.430301 [info ] [Thread-1 (]: 2 of 12 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 1.50s]
[0m11:59:34.430301 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m11:59:34.439916 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:59:34.439916 [info ] [Thread-1 (]: 3 of 12 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m11:59:34.439916 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m11:59:34.450102 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:59:34.452338 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:59:34.460079 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 11:59:34.451710 => 11:59:34.459473
[0m11:59:34.460079 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:59:34.460079 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:59:34.465096 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:34.465096 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m11:59:34.465096 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m11:59:34.465096 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:35.230106 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-4952-14cd-971c-1eed9525bc19
[0m11:59:35.618491 [debug] [Thread-1 (]: SQL status: OK in 1.149999976158142 seconds
[0m11:59:35.622753 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 11:59:34.460079 => 11:59:35.622753
[0m11:59:35.623781 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m11:59:35.624778 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:35.625746 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m11:59:35.625746 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-4952-14cd-971c-1eed9525bc19
[0m11:59:35.849785 [info ] [Thread-1 (]: 3 of 12 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 1.41s]
[0m11:59:35.849785 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m11:59:35.849785 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:59:35.859943 [info ] [Thread-1 (]: 4 of 12 START test not_null_fato_atendimento_hospitalar_data_atendimento ....... [RUN]
[0m11:59:35.864352 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5)
[0m11:59:35.866347 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:59:35.881005 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:59:35.881005 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5 (compile): 11:59:35.867439 => 11:59:35.881005
[0m11:59:35.881005 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:59:35.885009 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:59:35.885009 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:35.890057 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m11:59:35.890057 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_atendimento
from `workspace`.`default`.`fato_atendimento_hospitalar`
where data_atendimento is null



      
    ) dbt_internal_test
[0m11:59:35.890057 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:36.672844 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-4a2c-1134-b2b1-e80a0e0ab90a
[0m11:59:37.189877 [debug] [Thread-1 (]: SQL status: OK in 1.2999999523162842 seconds
[0m11:59:37.195467 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5 (execute): 11:59:35.885009 => 11:59:37.195467
[0m11:59:37.195467 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: ROLLBACK
[0m11:59:37.195467 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:37.195467 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: Close
[0m11:59:37.200076 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-4a2c-1134-b2b1-e80a0e0ab90a
[0m11:59:37.440537 [info ] [Thread-1 (]: 4 of 12 PASS not_null_fato_atendimento_hospitalar_data_atendimento ............. [[32mPASS[0m in 1.58s]
[0m11:59:37.444492 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m11:59:37.445612 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:59:37.449656 [info ] [Thread-1 (]: 5 of 12 START test not_null_fato_atendimento_hospitalar_doenca_id .............. [RUN]
[0m11:59:37.449758 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93)
[0m11:59:37.449758 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:59:37.464122 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m11:59:37.469737 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93 (compile): 11:59:37.449758 => 11:59:37.464122
[0m11:59:37.469737 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:59:37.469737 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m11:59:37.469737 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:37.469737 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m11:59:37.469737 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select doenca_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where doenca_id is null



      
    ) dbt_internal_test
[0m11:59:37.469737 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:38.268850 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-4b20-1998-9ffa-805dc2c6fb55
[0m11:59:38.669951 [debug] [Thread-1 (]: SQL status: OK in 1.2000000476837158 seconds
[0m11:59:38.679862 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93 (execute): 11:59:37.469737 => 11:59:38.679862
[0m11:59:38.679862 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: ROLLBACK
[0m11:59:38.679862 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:38.679862 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: Close
[0m11:59:38.679862 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-4b20-1998-9ffa-805dc2c6fb55
[0m11:59:38.916520 [info ] [Thread-1 (]: 5 of 12 PASS not_null_fato_atendimento_hospitalar_doenca_id .................... [[32mPASS[0m in 1.47s]
[0m11:59:38.920084 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m11:59:38.920084 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde
[0m11:59:38.920084 [info ] [Thread-1 (]: 6 of 12 START test not_null_fato_atendimento_hospitalar_paciente ............... [RUN]
[0m11:59:38.920084 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde)
[0m11:59:38.920084 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde
[0m11:59:38.939803 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde"
[0m11:59:38.939803 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde (compile): 11:59:38.920084 => 11:59:38.939803
[0m11:59:38.939803 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde
[0m11:59:38.949799 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde"
[0m11:59:38.949799 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:38.949799 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde"
[0m11:59:38.949799 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select paciente
from `workspace`.`default`.`fato_atendimento_hospitalar`
where paciente is null



      
    ) dbt_internal_test
[0m11:59:38.949799 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:39.736283 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-4c01-164b-b76c-0523b4380904
[0m11:59:40.350037 [debug] [Thread-1 (]: SQL status: OK in 1.399999976158142 seconds
[0m11:59:40.350037 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde (execute): 11:59:38.939803 => 11:59:40.350037
[0m11:59:40.350037 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde: ROLLBACK
[0m11:59:40.350037 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:40.350037 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde: Close
[0m11:59:40.350037 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-4c01-164b-b76c-0523b4380904
[0m11:59:40.580250 [info ] [Thread-1 (]: 6 of 12 PASS not_null_fato_atendimento_hospitalar_paciente ..................... [[32mPASS[0m in 1.66s]
[0m11:59:40.584799 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde
[0m11:59:40.589892 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:59:40.589892 [info ] [Thread-1 (]: 7 of 12 START test not_null_fato_nascimento_cod_municipio ...................... [RUN]
[0m11:59:40.589892 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m11:59:40.589892 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:59:40.600122 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:59:40.600122 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 11:59:40.589892 => 11:59:40.600122
[0m11:59:40.600122 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:59:40.605127 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:59:40.605127 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:40.605127 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m11:59:40.605127 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m11:59:40.609670 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:41.360249 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-4cf9-1c6a-9f75-9dcea5b6e644
[0m11:59:41.830013 [debug] [Thread-1 (]: SQL status: OK in 1.2200000286102295 seconds
[0m11:59:41.830013 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 11:59:40.600122 => 11:59:41.830013
[0m11:59:41.830013 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m11:59:41.830013 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:41.830013 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m11:59:41.830013 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-4cf9-1c6a-9f75-9dcea5b6e644
[0m11:59:42.059315 [info ] [Thread-1 (]: 7 of 12 PASS not_null_fato_nascimento_cod_municipio ............................ [[32mPASS[0m in 1.47s]
[0m11:59:42.063298 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m11:59:42.065264 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m11:59:42.068352 [info ] [Thread-1 (]: 8 of 12 START test not_null_fato_nascimento_data_id ............................ [RUN]
[0m11:59:42.069926 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m11:59:42.074940 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m11:59:42.085642 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m11:59:42.089645 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 11:59:42.074940 => 11:59:42.085642
[0m11:59:42.089687 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m11:59:42.089687 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m11:59:42.089687 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:42.089687 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m11:59:42.089687 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m11:59:42.089687 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:42.900867 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-4de4-1af9-8ba5-8791d837021f
[0m11:59:43.410178 [debug] [Thread-1 (]: SQL status: OK in 1.3200000524520874 seconds
[0m11:59:43.410178 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 11:59:42.089687 => 11:59:43.410178
[0m11:59:43.410178 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m11:59:43.410178 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:43.419886 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m11:59:43.419886 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-4de4-1af9-8ba5-8791d837021f
[0m11:59:43.650341 [info ] [Thread-1 (]: 8 of 12 PASS not_null_fato_nascimento_data_id .................................. [[32mPASS[0m in 1.58s]
[0m11:59:43.650341 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m11:59:43.650341 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:59:43.650341 [info ] [Thread-1 (]: 9 of 12 START test not_null_fato_nascimento_nascimento_id ...................... [RUN]
[0m11:59:43.659860 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m11:59:43.659860 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:59:43.659860 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:59:43.659860 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 11:59:43.659860 => 11:59:43.659860
[0m11:59:43.659860 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:59:43.670008 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:59:43.670008 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:43.670008 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m11:59:43.670008 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m11:59:43.670008 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:44.554021 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-4edf-1b88-8c37-10922ee28609
[0m11:59:45.040128 [debug] [Thread-1 (]: SQL status: OK in 1.3700000047683716 seconds
[0m11:59:45.050045 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 11:59:43.659860 => 11:59:45.050045
[0m11:59:45.050045 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m11:59:45.050045 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:45.050045 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m11:59:45.050045 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-4edf-1b88-8c37-10922ee28609
[0m11:59:45.290420 [info ] [Thread-1 (]: 9 of 12 PASS not_null_fato_nascimento_nascimento_id ............................ [[32mPASS[0m in 1.64s]
[0m11:59:45.292378 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m11:59:45.293400 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:59:45.294369 [info ] [Thread-1 (]: 10 of 12 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m11:59:45.295367 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m11:59:45.296364 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:59:45.301380 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:59:45.303344 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 11:59:45.296364 => 11:59:45.302375
[0m11:59:45.303621 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:59:45.303621 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:59:45.303621 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:45.303621 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m11:59:45.309665 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m11:59:45.309665 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:46.100368 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-4fcc-15fc-a537-b722c458d1cc
[0m11:59:46.582177 [debug] [Thread-1 (]: SQL status: OK in 1.2699999809265137 seconds
[0m11:59:46.590190 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 11:59:45.303621 => 11:59:46.590190
[0m11:59:46.590190 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m11:59:46.590190 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:46.590190 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m11:59:46.594694 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-4fcc-15fc-a537-b722c458d1cc
[0m11:59:46.818602 [error] [Thread-1 (]: 10 of 12 FAIL 4 not_null_fato_nascimento_peso .................................. [[31mFAIL 4[0m in 1.52s]
[0m11:59:46.819566 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m11:59:46.820563 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:59:46.820563 [info ] [Thread-1 (]: 11 of 12 START test unique_dim_tempo_data_id ................................... [RUN]
[0m11:59:46.821986 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m11:59:46.822984 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:59:46.830142 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:59:46.832220 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 11:59:46.822984 => 11:59:46.832220
[0m11:59:46.833289 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:59:46.833289 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:59:46.840235 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:46.840235 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m11:59:46.840235 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:59:46.840235 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:48.217541 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-510d-1f68-ac48-a1be4f67e9be
[0m11:59:48.635871 [debug] [Thread-1 (]: SQL status: OK in 1.7999999523162842 seconds
[0m11:59:48.645422 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 11:59:46.833289 => 11:59:48.644393
[0m11:59:48.646387 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m11:59:48.647415 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:48.649410 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m11:59:48.650378 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-510d-1f68-ac48-a1be4f67e9be
[0m11:59:48.886949 [info ] [Thread-1 (]: 11 of 12 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 2.06s]
[0m11:59:48.890064 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m11:59:48.895076 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:59:48.895076 [info ] [Thread-1 (]: 12 of 12 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m11:59:48.895076 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m11:59:48.895076 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:59:48.899962 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:59:48.904998 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 11:59:48.895076 => 11:59:48.904998
[0m11:59:48.904998 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:59:48.908563 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:59:48.910102 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:48.910102 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m11:59:48.910102 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m11:59:48.910102 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:59:49.740297 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-51f7-1c82-a1c0-15db02648ad3
[0m11:59:50.210255 [debug] [Thread-1 (]: SQL status: OK in 1.2999999523162842 seconds
[0m11:59:50.220000 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 11:59:48.904998 => 11:59:50.220000
[0m11:59:50.220000 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m11:59:50.220000 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:59:50.220000 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m11:59:50.220000 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-51f7-1c82-a1c0-15db02648ad3
[0m11:59:50.459896 [error] [Thread-1 (]: 12 of 12 FAIL 59968 unique_fato_nascimento_nascimento_id ....................... [[31mFAIL 59968[0m in 1.56s]
[0m11:59:50.459896 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m11:59:50.459896 [debug] [MainThread]: On master: ROLLBACK
[0m11:59:50.459896 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:59:51.262258 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0791f-52df-1e93-959e-7de566b2851f
[0m11:59:51.265281 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:59:51.265281 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:59:51.265281 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:59:51.269853 [debug] [MainThread]: On master: ROLLBACK
[0m11:59:51.269853 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:59:51.269853 [debug] [MainThread]: On master: Close
[0m11:59:51.269853 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0791f-52df-1e93-959e-7de566b2851f
[0m11:59:51.500382 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:59:51.510195 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m11:59:51.510195 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m11:59:51.510195 [info ] [MainThread]: 
[0m11:59:51.510195 [info ] [MainThread]: Finished running 12 tests in 0 hours 0 minutes and 23.54 seconds (23.54s).
[0m11:59:51.510195 [debug] [MainThread]: Command end result
[0m11:59:51.525118 [info ] [MainThread]: 
[0m11:59:51.530173 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m11:59:51.530173 [info ] [MainThread]: 
[0m11:59:51.531962 [error] [MainThread]: [33mRuntime Error in test accepted_values_fato_nascimento_sexo__M__F (models\marts\schema.yaml)[0m
[0m11:59:51.532960 [error] [MainThread]:   [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
[0m11:59:51.533822 [error] [MainThread]:   == SQL (line 24, position 19) ==
[0m11:59:51.534009 [error] [MainThread]:   where value_field not in (
[0m11:59:51.534009 [error] [MainThread]:                     ^^^^^^^^
[0m11:59:51.534009 [error] [MainThread]:       'M','F'
[0m11:59:51.534009 [error] [MainThread]:   ^^^^^^^^^^^
[0m11:59:51.534009 [error] [MainThread]:   )
[0m11:59:51.540044 [error] [MainThread]:   ^
[0m11:59:51.540044 [error] [MainThread]:   
[0m11:59:51.540044 [info ] [MainThread]: 
[0m11:59:51.540044 [error] [MainThread]: [31mFailure in test not_null_fato_nascimento_peso (models\marts\schema.yaml)[0m
[0m11:59:51.543535 [error] [MainThread]:   Got 4 results, configured to fail if != 0
[0m11:59:51.544533 [info ] [MainThread]: 
[0m11:59:51.545533 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\not_null_fato_nascimento_peso.sql
[0m11:59:51.546530 [info ] [MainThread]: 
[0m11:59:51.547524 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m11:59:51.548522 [error] [MainThread]:   Got 59968 results, configured to fail if != 0
[0m11:59:51.549519 [info ] [MainThread]: 
[0m11:59:51.550519 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m11:59:51.551514 [info ] [MainThread]: 
[0m11:59:51.552909 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=3 SKIP=0 TOTAL=12
[0m11:59:51.552909 [debug] [MainThread]: Command `dbt test` failed at 11:59:51.552909 after 25.33 seconds
[0m11:59:51.552909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0632FD590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E063992E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0638B6390>]}
[0m11:59:51.552909 [debug] [MainThread]: Flushing usage events
[0m12:04:17.929495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141CAA47850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141CA9AFE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141CA9306D0>]}


============================== 12:04:17.929495 | dd385777-06e4-4339-8d9e-e480816640ce ==============================
[0m12:04:17.929495 [info ] [MainThread]: Running with dbt=1.5.2
[0m12:04:17.929495 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m12:04:19.273779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dd385777-06e4-4339-8d9e-e480816640ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141CABCF150>]}
[0m12:04:19.289523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dd385777-06e4-4339-8d9e-e480816640ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141CABCF150>]}
[0m12:04:19.289523 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m12:04:19.313865 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m12:04:19.429452 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m12:04:19.429452 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\schema.yaml
[0m12:04:19.429452 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m12:04:19.459391 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m12:04:19.519521 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m12:04:19.529583 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m12:04:19.539592 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m12:04:19.708092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dd385777-06e4-4339-8d9e-e480816640ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141D4410090>]}
[0m12:04:19.720658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dd385777-06e4-4339-8d9e-e480816640ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141D562BAD0>]}
[0m12:04:19.720658 [info ] [MainThread]: Found 12 models, 34 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m12:04:19.720658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dd385777-06e4-4339-8d9e-e480816640ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141D19B7310>]}
[0m12:04:19.728771 [info ] [MainThread]: 
[0m12:04:19.729356 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:04:19.733615 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m12:04:19.739657 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m12:04:19.739657 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m12:04:19.739657 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:04:20.879495 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0791f-f390-14f0-8fc5-281d65d841d8
[0m12:04:21.465041 [debug] [ThreadPool]: SQL status: OK in 1.7300000190734863 seconds
[0m12:04:21.469651 [debug] [ThreadPool]: On list_workspace_default: Close
[0m12:04:21.469651 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0791f-f390-14f0-8fc5-281d65d841d8
[0m12:04:21.729546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dd385777-06e4-4339-8d9e-e480816640ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141D43D4A90>]}
[0m12:04:21.729546 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:21.729546 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:04:21.739420 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:04:21.741473 [info ] [MainThread]: 
[0m12:04:21.749559 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m12:04:21.749559 [info ] [Thread-1 (]: 1 of 34 START test accepted_values_fato_nascimento_sexo__M__F .................. [RUN]
[0m12:04:21.749559 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3'
[0m12:04:21.749559 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m12:04:21.762899 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m12:04:21.762899 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (compile): 12:04:21.757251 => 12:04:21.762899
[0m12:04:21.762899 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m12:04:21.795013 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m12:04:21.799677 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:21.799677 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m12:04:21.799677 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'M','F'
)



      
    ) dbt_internal_test
[0m12:04:21.799677 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:04:22.590039 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-f49a-1056-8241-0e7cdeeb18e7
[0m12:04:23.319433 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'M','F'
)



      
    ) dbt_internal_test
[0m12:04:23.319433 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

[0m12:04:23.319433 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [CAST_INVALID_INPUT] org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
	at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)
	at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$2(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$2$adapted(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.buildCast(Cast.scala:683)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$1(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:1408)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:730)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:89)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:109)
	at scala.collection.immutable.Vector1.map(Vector.scala:2141)
	at scala.collection.immutable.Vector1.map(Vector.scala:386)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:760)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:109)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:109)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:109)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.$anonfun$applyOrElse$1(expressions.scala:113)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:113)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:112)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$4(TreeNode.scala:536)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1705)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:536)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:487)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.apply(expressions.scala:112)
	at org.apache.spark.sql.execution.qrc.Canonicalized$.applyRules(ResultCacheManager.scala:131)
	at org.apache.spark.sql.execution.qrc.Canonicalized$.apply(ResultCacheManager.scala:122)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$lookupCache$4(ResultCacheManager.scala:514)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$lookupCache$4$adapted(ResultCacheManager.scala:514)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:146)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:142)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.recordLatency(ResultCacheManager.scala:305)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.lookupCache(ResultCacheManager.scala:514)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.lookupHybridResult(ResultCacheManager.scala:413)
	at org.apache.spark.sql.classic.Dataset.lookupHybridResultFromCache(Dataset.scala:1813)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.lookupResult(HybridCloudStoreResultHandler.scala:69)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.$anonfun$collectResult$1(ResultCollector.scala:101)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult(ResultCollector.scala:101)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult$(ResultCollector.scala:98)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.collectResult(HybridCloudStoreResultHandler.scala:33)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.org$apache$spark$sql$hive$thriftserver$HybridCloudStoreResultHandler$$initFromDataFrame(HybridCloudStoreResultHandler.scala:85)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler$.createFromDataFrame(HybridCloudStoreResultHandler.scala:214)
	at org.apache.spark.sql.hive.thriftserver.ResultHandlerFactory.getCollectorHandler(ResultHandlerFactory.scala:473)
	at org.apache.spark.sql.hive.thriftserver.ResultHandlerFactory.createResultHandler(ResultHandlerFactory.scala:443)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$18(SparkExecuteStatementOperation.scala:959)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withSuspendedQueryHangingDetection(SparkExecuteStatementOperation.scala:182)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:956)
	... 53 more

[0m12:04:23.319433 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791f-f4b9-1d83-ae36-52b3972ec541
[0m12:04:23.319433 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (execute): 12:04:21.769439 => 12:04:23.319433
[0m12:04:23.319433 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: ROLLBACK
[0m12:04:23.319433 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:23.329438 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: Close
[0m12:04:23.329438 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-f49a-1056-8241-0e7cdeeb18e7
[0m12:04:23.589333 [debug] [Thread-1 (]: Runtime Error in test accepted_values_fato_nascimento_sexo__M__F (models\marts\schema.yaml)
  [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  == SQL (line 24, position 19) ==
  where value_field not in (
                    ^^^^^^^^
      'M','F'
  ^^^^^^^^^^^
  )
  ^
  
[0m12:04:23.589333 [error] [Thread-1 (]: 1 of 34 ERROR accepted_values_fato_nascimento_sexo__M__F ....................... [[31mERROR[0m in 1.84s]
[0m12:04:23.589333 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m12:04:23.589333 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462
[0m12:04:23.589333 [info ] [Thread-1 (]: 2 of 34 START test not_null_dim_doenca_descricao_doenca ........................ [RUN]
[0m12:04:23.589333 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3, now test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462)
[0m12:04:23.589333 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462
[0m12:04:23.603153 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462"
[0m12:04:23.609698 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462 (compile): 12:04:23.589333 => 12:04:23.609698
[0m12:04:23.609698 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462
[0m12:04:23.613733 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462"
[0m12:04:23.615039 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:23.615039 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462"
[0m12:04:23.615039 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select descricao_doenca
from `workspace`.`default`.`dim_doenca`
where descricao_doenca is null



      
    ) dbt_internal_test
[0m12:04:23.615039 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:24.449779 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-f5b1-1ef8-b8c3-b5b030147742
[0m12:04:24.819412 [debug] [Thread-1 (]: SQL status: OK in 1.2000000476837158 seconds
[0m12:04:24.824455 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462 (execute): 12:04:23.609698 => 12:04:24.819412
[0m12:04:24.824455 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462: ROLLBACK
[0m12:04:24.824455 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:24.824455 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462: Close
[0m12:04:24.824455 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-f5b1-1ef8-b8c3-b5b030147742
[0m12:04:25.062130 [info ] [Thread-1 (]: 2 of 34 PASS not_null_dim_doenca_descricao_doenca .............................. [[32mPASS[0m in 1.47s]
[0m12:04:25.064151 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462
[0m12:04:25.064151 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403
[0m12:04:25.064151 [info ] [Thread-1 (]: 3 of 34 START test not_null_dim_doenca_doenca_id ............................... [RUN]
[0m12:04:25.064151 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_doenca_descricao_doenca.09b1fb8462, now test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403)
[0m12:04:25.064151 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403
[0m12:04:25.069762 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403"
[0m12:04:25.069762 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403 (compile): 12:04:25.064151 => 12:04:25.069762
[0m12:04:25.069762 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403
[0m12:04:25.079246 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403"
[0m12:04:25.082514 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:25.082730 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403"
[0m12:04:25.082730 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select doenca_id
from `workspace`.`default`.`dim_doenca`
where doenca_id is null



      
    ) dbt_internal_test
[0m12:04:25.082730 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:25.919637 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-f693-182e-8543-49689ba39dec
[0m12:04:26.306747 [debug] [Thread-1 (]: SQL status: OK in 1.2200000286102295 seconds
[0m12:04:26.309865 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403 (execute): 12:04:25.069762 => 12:04:26.309865
[0m12:04:26.309865 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403: ROLLBACK
[0m12:04:26.309865 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:26.309865 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403: Close
[0m12:04:26.309865 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-f693-182e-8543-49689ba39dec
[0m12:04:26.539710 [info ] [Thread-1 (]: 3 of 34 PASS not_null_dim_doenca_doenca_id ..................................... [[32mPASS[0m in 1.48s]
[0m12:04:26.549448 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403
[0m12:04:26.549448 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m12:04:26.549448 [info ] [Thread-1 (]: 4 of 34 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m12:04:26.559627 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_doenca_doenca_id.c52f717403, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m12:04:26.562598 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m12:04:26.569319 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m12:04:26.569319 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 12:04:26.563627 => 12:04:26.569319
[0m12:04:26.569319 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m12:04:26.569319 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m12:04:26.569319 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:26.579767 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m12:04:26.579767 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m12:04:26.579767 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:27.374545 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-f76f-1732-ba0f-43130d140a3c
[0m12:04:27.749795 [debug] [Thread-1 (]: SQL status: OK in 1.1699999570846558 seconds
[0m12:04:27.759726 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 12:04:26.569319 => 12:04:27.749795
[0m12:04:27.759726 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m12:04:27.759726 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:27.759726 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m12:04:27.759726 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-f76f-1732-ba0f-43130d140a3c
[0m12:04:27.999696 [info ] [Thread-1 (]: 4 of 34 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 1.45s]
[0m12:04:28.009779 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m12:04:28.009779 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m12:04:28.009779 [info ] [Thread-1 (]: 5 of 34 START test not_null_dim_localidade_local_nascimento .................... [RUN]
[0m12:04:28.009779 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305)
[0m12:04:28.019681 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m12:04:28.029483 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m12:04:28.034641 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305 (compile): 12:04:28.019681 => 12:04:28.034641
[0m12:04:28.036435 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m12:04:28.039551 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m12:04:28.039551 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:28.039551 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m12:04:28.039551 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select local_nascimento
from `workspace`.`default`.`dim_localidade`
where local_nascimento is null



      
    ) dbt_internal_test
[0m12:04:28.039551 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:29.029487 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-f86f-1b66-9803-e5ca21bfc1cc
[0m12:04:29.399691 [debug] [Thread-1 (]: SQL status: OK in 1.3600000143051147 seconds
[0m12:04:29.410861 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305 (execute): 12:04:28.036435 => 12:04:29.410861
[0m12:04:29.410861 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: ROLLBACK
[0m12:04:29.410861 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:29.410861 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: Close
[0m12:04:29.410861 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-f86f-1b66-9803-e5ca21bfc1cc
[0m12:04:29.659550 [info ] [Thread-1 (]: 5 of 34 PASS not_null_dim_localidade_local_nascimento .......................... [[32mPASS[0m in 1.65s]
[0m12:04:29.659550 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m12:04:29.659550 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m12:04:29.659550 [info ] [Thread-1 (]: 6 of 34 START test not_null_dim_tempo_ano ...................................... [RUN]
[0m12:04:29.659550 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305, now test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6)
[0m12:04:29.669635 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m12:04:29.675351 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m12:04:29.675351 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (compile): 12:04:29.670251 => 12:04:29.675351
[0m12:04:29.675351 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m12:04:29.679477 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m12:04:29.684512 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:29.686294 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m12:04:29.686294 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select ano
from `workspace`.`default`.`dim_tempo`
where ano is null



      
    ) dbt_internal_test
[0m12:04:29.687836 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:30.513437 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-f952-15b5-bedb-8df10ec8819c
[0m12:04:30.929677 [debug] [Thread-1 (]: SQL status: OK in 1.2400000095367432 seconds
[0m12:04:30.939379 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (execute): 12:04:29.679477 => 12:04:30.939379
[0m12:04:30.939379 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: ROLLBACK
[0m12:04:30.939379 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:30.939379 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: Close
[0m12:04:30.939379 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-f952-15b5-bedb-8df10ec8819c
[0m12:04:31.174353 [info ] [Thread-1 (]: 6 of 34 PASS not_null_dim_tempo_ano ............................................ [[32mPASS[0m in 1.51s]
[0m12:04:31.175495 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m12:04:31.175495 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m12:04:31.175495 [info ] [Thread-1 (]: 7 of 34 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m12:04:31.175495 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m12:04:31.179603 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m12:04:31.179603 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m12:04:31.179603 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 12:04:31.179603 => 12:04:31.179603
[0m12:04:31.179603 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m12:04:31.189460 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m12:04:31.192798 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:31.193210 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m12:04:31.193210 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m12:04:31.193210 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:31.979766 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-fa30-14de-9703-62a713c106bc
[0m12:04:32.389752 [debug] [Thread-1 (]: SQL status: OK in 1.2000000476837158 seconds
[0m12:04:32.399528 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 12:04:31.179603 => 12:04:32.399528
[0m12:04:32.399528 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m12:04:32.399528 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:32.399528 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m12:04:32.399528 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-fa30-14de-9703-62a713c106bc
[0m12:04:32.639527 [info ] [Thread-1 (]: 7 of 34 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 1.45s]
[0m12:04:32.639527 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m12:04:32.639527 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m12:04:32.639527 [info ] [Thread-1 (]: 8 of 34 START test not_null_dim_tempo_dia ...................................... [RUN]
[0m12:04:32.639527 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306)
[0m12:04:32.639527 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m12:04:32.649596 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m12:04:32.649596 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (compile): 12:04:32.639527 => 12:04:32.649596
[0m12:04:32.649596 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m12:04:32.649596 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m12:04:32.649596 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:32.649596 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m12:04:32.649596 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select dia
from `workspace`.`default`.`dim_tempo`
where dia is null



      
    ) dbt_internal_test
[0m12:04:32.649596 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:33.429465 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-fb0f-1627-9796-47e5deaf83f5
[0m12:04:33.867738 [debug] [Thread-1 (]: SQL status: OK in 1.2200000286102295 seconds
[0m12:04:33.871699 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (execute): 12:04:32.649596 => 12:04:33.870702
[0m12:04:33.871699 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: ROLLBACK
[0m12:04:33.872725 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:33.872725 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: Close
[0m12:04:33.874041 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-fb0f-1627-9796-47e5deaf83f5
[0m12:04:34.121130 [info ] [Thread-1 (]: 8 of 34 PASS not_null_dim_tempo_dia ............................................ [[32mPASS[0m in 1.48s]
[0m12:04:34.121130 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m12:04:34.121130 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m12:04:34.121130 [info ] [Thread-1 (]: 9 of 34 START test not_null_dim_tempo_mes ...................................... [RUN]
[0m12:04:34.129665 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306, now test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972)
[0m12:04:34.129665 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m12:04:34.129665 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m12:04:34.139574 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (compile): 12:04:34.129665 => 12:04:34.129665
[0m12:04:34.140995 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m12:04:34.145015 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m12:04:34.147007 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:34.147007 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m12:04:34.147998 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select mes
from `workspace`.`default`.`dim_tempo`
where mes is null



      
    ) dbt_internal_test
[0m12:04:34.147998 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:34.909660 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-fbf0-1a18-b0df-6b8d385d30c7
[0m12:04:35.329877 [debug] [Thread-1 (]: SQL status: OK in 1.1799999475479126 seconds
[0m12:04:35.329877 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (execute): 12:04:34.140995 => 12:04:35.329877
[0m12:04:35.329877 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: ROLLBACK
[0m12:04:35.329877 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:35.329877 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: Close
[0m12:04:35.329877 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-fbf0-1a18-b0df-6b8d385d30c7
[0m12:04:35.579470 [info ] [Thread-1 (]: 9 of 34 PASS not_null_dim_tempo_mes ............................................ [[32mPASS[0m in 1.45s]
[0m12:04:35.579470 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m12:04:35.579470 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m12:04:35.579470 [info ] [Thread-1 (]: 10 of 34 START test not_null_fato_atendimento_hospitalar_atendimento_id ........ [RUN]
[0m12:04:35.579470 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22)
[0m12:04:35.579470 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m12:04:35.594031 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m12:04:35.595000 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22 (compile): 12:04:35.579470 => 12:04:35.595000
[0m12:04:35.595998 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m12:04:35.599017 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m12:04:35.600015 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:35.600015 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"
[0m12:04:35.601014 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select atendimento_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is null



      
    ) dbt_internal_test
[0m12:04:35.601014 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:36.374914 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-fcd0-1ae3-a0b7-75cbd68b0f69
[0m12:04:36.779402 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select atendimento_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is null



      
    ) dbt_internal_test
[0m12:04:36.779402 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m12:04:36.779402 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m12:04:36.779402 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791f-fcf0-1cc2-a9c0-07f23418c4c3
[0m12:04:36.779402 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22 (execute): 12:04:35.595998 => 12:04:36.779402
[0m12:04:36.779402 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: ROLLBACK
[0m12:04:36.779402 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:36.779402 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22: Close
[0m12:04:36.779402 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-fcd0-1ae3-a0b7-75cbd68b0f69
[0m12:04:37.019605 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m12:04:37.019605 [error] [Thread-1 (]: 10 of 34 ERROR not_null_fato_atendimento_hospitalar_atendimento_id ............. [[31mERROR[0m in 1.44s]
[0m12:04:37.024610 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22
[0m12:04:37.024610 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a
[0m12:04:37.024610 [info ] [Thread-1 (]: 11 of 34 START test not_null_fato_atendimento_hospitalar_cod_municipio ......... [RUN]
[0m12:04:37.024610 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_atendimento_id.4743c09c22, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a)
[0m12:04:37.028164 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a
[0m12:04:37.030229 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a"
[0m12:04:37.034231 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a (compile): 12:04:37.028164 => 12:04:37.034231
[0m12:04:37.034231 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a
[0m12:04:37.039304 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a"
[0m12:04:37.040975 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:37.041241 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a"
[0m12:04:37.041241 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_atendimento_hospitalar`
where cod_municipio is null



      
    ) dbt_internal_test
[0m12:04:37.041241 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:37.845050 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-fdb0-13ec-8969-e57dd9be470f
[0m12:04:38.779246 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_atendimento_hospitalar`
where cod_municipio is null



      
    ) dbt_internal_test
[0m12:04:38.779246 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_municipio` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m12:04:38.779246 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_municipio` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_municipio` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m12:04:38.789294 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0791f-fdd1-1458-935c-c6e127787c9e
[0m12:04:38.789294 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a (execute): 12:04:37.034231 => 12:04:38.789294
[0m12:04:38.789294 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a: ROLLBACK
[0m12:04:38.789294 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:38.789294 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a: Close
[0m12:04:38.789294 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-fdb0-13ec-8969-e57dd9be470f
[0m12:04:39.029996 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_atendimento_hospitalar_cod_municipio (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_municipio` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m12:04:39.029996 [error] [Thread-1 (]: 11 of 34 ERROR not_null_fato_atendimento_hospitalar_cod_municipio .............. [[31mERROR[0m in 2.01s]
[0m12:04:39.039568 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a
[0m12:04:39.039568 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m12:04:39.039568 [info ] [Thread-1 (]: 12 of 34 START test not_null_fato_atendimento_hospitalar_data_atendimento ...... [RUN]
[0m12:04:39.039568 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_cod_municipio.566496e88a, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5)
[0m12:04:39.039568 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m12:04:39.051382 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m12:04:39.052358 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5 (compile): 12:04:39.039568 => 12:04:39.052358
[0m12:04:39.052358 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m12:04:39.052358 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m12:04:39.052358 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:39.052358 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"
[0m12:04:39.059428 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_atendimento
from `workspace`.`default`.`fato_atendimento_hospitalar`
where data_atendimento is null



      
    ) dbt_internal_test
[0m12:04:39.059428 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:39.849726 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-fee3-156f-8d9a-fc7a5b3646e1
[0m12:04:40.259423 [debug] [Thread-1 (]: SQL status: OK in 1.2000000476837158 seconds
[0m12:04:40.259423 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5 (execute): 12:04:39.052358 => 12:04:40.259423
[0m12:04:40.259423 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: ROLLBACK
[0m12:04:40.259423 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:40.259423 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5: Close
[0m12:04:40.259423 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-fee3-156f-8d9a-fc7a5b3646e1
[0m12:04:40.509708 [info ] [Thread-1 (]: 12 of 34 PASS not_null_fato_atendimento_hospitalar_data_atendimento ............ [[32mPASS[0m in 1.47s]
[0m12:04:40.509708 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5
[0m12:04:40.509708 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m12:04:40.509708 [info ] [Thread-1 (]: 13 of 34 START test not_null_fato_atendimento_hospitalar_doenca_id ............. [RUN]
[0m12:04:40.519251 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_data_atendimento.53124f98f5, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93)
[0m12:04:40.519251 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m12:04:40.519251 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m12:04:40.519251 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93 (compile): 12:04:40.519251 => 12:04:40.519251
[0m12:04:40.519251 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m12:04:40.529351 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m12:04:40.529351 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:40.529351 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"
[0m12:04:40.529351 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select doenca_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where doenca_id is null



      
    ) dbt_internal_test
[0m12:04:40.529351 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:41.324539 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0791f-ffc3-1c83-a046-f7fc120b270b
[0m12:04:41.739784 [debug] [Thread-1 (]: SQL status: OK in 1.2100000381469727 seconds
[0m12:04:41.749438 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93 (execute): 12:04:40.519251 => 12:04:41.739784
[0m12:04:41.749438 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: ROLLBACK
[0m12:04:41.749438 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:41.749438 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93: Close
[0m12:04:41.749438 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0791f-ffc3-1c83-a046-f7fc120b270b
[0m12:04:41.974637 [info ] [Thread-1 (]: 13 of 34 PASS not_null_fato_atendimento_hospitalar_doenca_id ................... [[32mPASS[0m in 1.46s]
[0m12:04:41.979714 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93
[0m12:04:41.979714 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m12:04:41.979714 [info ] [Thread-1 (]: 14 of 34 START test not_null_fato_atendimento_hospitalar_localidade_id ......... [RUN]
[0m12:04:41.990969 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_doenca_id.c34f727e93, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15)
[0m12:04:41.992994 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m12:04:42.001803 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"
[0m12:04:42.001803 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15 (compile): 12:04:41.993877 => 12:04:42.001803
[0m12:04:42.001803 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m12:04:42.009347 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"
[0m12:04:42.009347 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:42.009347 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"
[0m12:04:42.009347 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select localidade_id
from `workspace`.`default`.`fato_atendimento_hospitalar`
where localidade_id is null



      
    ) dbt_internal_test
[0m12:04:42.014352 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:42.780759 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-00a1-15b2-ac8f-839c22042117
[0m12:04:43.181677 [debug] [Thread-1 (]: SQL status: OK in 1.1699999570846558 seconds
[0m12:04:43.189839 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15 (execute): 12:04:42.001803 => 12:04:43.189839
[0m12:04:43.199582 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15: ROLLBACK
[0m12:04:43.199582 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:43.199582 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15: Close
[0m12:04:43.199582 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-00a1-15b2-ac8f-839c22042117
[0m12:04:43.429442 [info ] [Thread-1 (]: 14 of 34 PASS not_null_fato_atendimento_hospitalar_localidade_id ............... [[32mPASS[0m in 1.45s]
[0m12:04:43.429442 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15
[0m12:04:43.429442 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde
[0m12:04:43.439560 [info ] [Thread-1 (]: 15 of 34 START test not_null_fato_atendimento_hospitalar_paciente .............. [RUN]
[0m12:04:43.439618 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_localidade_id.7394084a15, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde)
[0m12:04:43.439618 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde
[0m12:04:43.443195 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde"
[0m12:04:43.449213 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde (compile): 12:04:43.442091 => 12:04:43.449213
[0m12:04:43.450673 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde
[0m12:04:43.452760 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde"
[0m12:04:43.452760 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:43.452760 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde"
[0m12:04:43.452760 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select paciente
from `workspace`.`default`.`fato_atendimento_hospitalar`
where paciente is null



      
    ) dbt_internal_test
[0m12:04:43.452760 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:44.269687 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-0182-1ad7-ad22-ddbe12bcd651
[0m12:04:44.684481 [debug] [Thread-1 (]: SQL status: OK in 1.2300000190734863 seconds
[0m12:04:44.684481 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde (execute): 12:04:43.450673 => 12:04:44.684481
[0m12:04:44.684481 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde: ROLLBACK
[0m12:04:44.684481 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:44.684481 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde: Close
[0m12:04:44.689536 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-0182-1ad7-ad22-ddbe12bcd651
[0m12:04:44.919759 [info ] [Thread-1 (]: 15 of 34 PASS not_null_fato_atendimento_hospitalar_paciente .................... [[32mPASS[0m in 1.48s]
[0m12:04:44.919759 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde
[0m12:04:44.919759 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1
[0m12:04:44.919759 [info ] [Thread-1 (]: 16 of 34 START test not_null_fato_atendimento_hospitalar_procedimento .......... [RUN]
[0m12:04:44.929440 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_paciente.a4acf53bde, now test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1)
[0m12:04:44.929440 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1
[0m12:04:44.929440 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1"
[0m12:04:44.929440 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1 (compile): 12:04:44.929440 => 12:04:44.929440
[0m12:04:44.929440 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1
[0m12:04:44.942769 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1"
[0m12:04:44.942769 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:44.942769 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1"
[0m12:04:44.942769 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select procedimento
from `workspace`.`default`.`fato_atendimento_hospitalar`
where procedimento is null



      
    ) dbt_internal_test
[0m12:04:44.942769 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:45.739531 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-0265-1222-9898-08969d88e96c
[0m12:04:46.144345 [debug] [Thread-1 (]: SQL status: OK in 1.2000000476837158 seconds
[0m12:04:46.147184 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1 (execute): 12:04:44.939399 => 12:04:46.147184
[0m12:04:46.148180 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1: ROLLBACK
[0m12:04:46.149150 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:46.149150 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1: Close
[0m12:04:46.150147 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-0265-1222-9898-08969d88e96c
[0m12:04:46.404288 [info ] [Thread-1 (]: 16 of 34 PASS not_null_fato_atendimento_hospitalar_procedimento ................ [[32mPASS[0m in 1.48s]
[0m12:04:46.409439 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1
[0m12:04:46.409439 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m12:04:46.409439 [info ] [Thread-1 (]: 17 of 34 START test not_null_fato_nascimento_APGAR1 ............................ [RUN]
[0m12:04:46.409439 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_atendimento_hospitalar_procedimento.f5cd5fa4c1, now test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074)
[0m12:04:46.420059 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m12:04:46.426068 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m12:04:46.427910 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (compile): 12:04:46.421058 => 12:04:46.427910
[0m12:04:46.428911 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m12:04:46.431902 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m12:04:46.433896 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:46.434897 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m12:04:46.434897 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR1
from `workspace`.`default`.`fato_nascimento`
where APGAR1 is null



      
    ) dbt_internal_test
[0m12:04:46.435892 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:47.239538 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-034a-157a-a97e-afc11af06206
[0m12:04:48.419385 [debug] [Thread-1 (]: SQL status: OK in 1.9800000190734863 seconds
[0m12:04:48.419385 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (execute): 12:04:46.428911 => 12:04:48.419385
[0m12:04:48.419385 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: ROLLBACK
[0m12:04:48.419385 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:48.419385 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: Close
[0m12:04:48.419385 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-034a-157a-a97e-afc11af06206
[0m12:04:48.672032 [error] [Thread-1 (]: 17 of 34 FAIL 677 not_null_fato_nascimento_APGAR1 .............................. [[31mFAIL 677[0m in 2.26s]
[0m12:04:48.673985 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m12:04:48.673985 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m12:04:48.675029 [info ] [Thread-1 (]: 18 of 34 START test not_null_fato_nascimento_APGAR5 ............................ [RUN]
[0m12:04:48.675029 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074, now test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994)
[0m12:04:48.675029 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m12:04:48.679570 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m12:04:48.679570 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (compile): 12:04:48.675029 => 12:04:48.679570
[0m12:04:48.684576 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m12:04:48.684576 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m12:04:48.684576 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:48.684576 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m12:04:48.689625 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR5
from `workspace`.`default`.`fato_nascimento`
where APGAR5 is null



      
    ) dbt_internal_test
[0m12:04:48.690276 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:49.479753 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-049f-1867-981e-b999a809146a
[0m12:04:50.464691 [debug] [Thread-1 (]: SQL status: OK in 1.7699999809265137 seconds
[0m12:04:50.469262 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (execute): 12:04:48.684576 => 12:04:50.469262
[0m12:04:50.474273 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: ROLLBACK
[0m12:04:50.474273 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:50.474273 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: Close
[0m12:04:50.474273 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-049f-1867-981e-b999a809146a
[0m12:04:50.729710 [error] [Thread-1 (]: 18 of 34 FAIL 666 not_null_fato_nascimento_APGAR5 .............................. [[31mFAIL 666[0m in 2.05s]
[0m12:04:50.729710 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m12:04:50.729710 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m12:04:50.729710 [info ] [Thread-1 (]: 19 of 34 START test not_null_fato_nascimento_cod_municipio ..................... [RUN]
[0m12:04:50.729710 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m12:04:50.729710 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m12:04:50.739682 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m12:04:50.739682 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 12:04:50.729710 => 12:04:50.739682
[0m12:04:50.739682 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m12:04:50.739682 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m12:04:50.749403 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:50.749403 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m12:04:50.749403 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m12:04:50.749403 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:51.549687 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-05da-1024-9497-118f4f87ea70
[0m12:04:52.029632 [debug] [Thread-1 (]: SQL status: OK in 1.2699999809265137 seconds
[0m12:04:52.029632 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 12:04:50.739682 => 12:04:52.029632
[0m12:04:52.039437 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m12:04:52.039437 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:52.039437 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m12:04:52.039437 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-05da-1024-9497-118f4f87ea70
[0m12:04:52.279656 [info ] [Thread-1 (]: 19 of 34 PASS not_null_fato_nascimento_cod_municipio ........................... [[32mPASS[0m in 1.55s]
[0m12:04:52.284671 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m12:04:52.284671 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m12:04:52.284671 [info ] [Thread-1 (]: 20 of 34 START test not_null_fato_nascimento_data_id ........................... [RUN]
[0m12:04:52.284671 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m12:04:52.284671 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m12:04:52.294285 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m12:04:52.294285 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 12:04:52.284671 => 12:04:52.294285
[0m12:04:52.294285 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m12:04:52.299445 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m12:04:52.301287 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:52.302365 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m12:04:52.302632 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m12:04:52.302632 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:53.089658 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-06c7-1738-8036-248bb4da708b
[0m12:04:53.584644 [debug] [Thread-1 (]: SQL status: OK in 1.2799999713897705 seconds
[0m12:04:53.584644 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 12:04:52.294285 => 12:04:53.584644
[0m12:04:53.589751 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m12:04:53.589751 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:53.589751 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m12:04:53.589751 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-06c7-1738-8036-248bb4da708b
[0m12:04:53.831394 [info ] [Thread-1 (]: 20 of 34 PASS not_null_fato_nascimento_data_id ................................. [[32mPASS[0m in 1.55s]
[0m12:04:53.832348 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m12:04:53.833376 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m12:04:53.834061 [info ] [Thread-1 (]: 21 of 34 START test not_null_fato_nascimento_data_nascimento ................... [RUN]
[0m12:04:53.834774 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d)
[0m12:04:53.835802 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m12:04:53.841791 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m12:04:53.842783 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d (compile): 12:04:53.835802 => 12:04:53.842783
[0m12:04:53.842783 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m12:04:53.847009 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m12:04:53.848138 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:53.848138 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"
[0m12:04:53.849710 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_nascimento
from `workspace`.`default`.`fato_nascimento`
where data_nascimento is null



      
    ) dbt_internal_test
[0m12:04:53.849710 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:54.659613 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-07b6-14de-8d95-bb4d22917770
[0m12:04:55.129419 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_nascimento
from `workspace`.`default`.`fato_nascimento`
where data_nascimento is null



      
    ) dbt_internal_test
[0m12:04:55.129419 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m12:04:55.129419 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m12:04:55.129419 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07920-07d6-1970-93cf-3508ef03d4cb
[0m12:04:55.129419 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d (execute): 12:04:53.843752 => 12:04:55.129419
[0m12:04:55.129419 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: ROLLBACK
[0m12:04:55.129419 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:55.129419 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d: Close
[0m12:04:55.129419 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-07b6-14de-8d95-bb4d22917770
[0m12:04:55.379494 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_nascimento_data_nascimento (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m12:04:55.379494 [error] [Thread-1 (]: 21 of 34 ERROR not_null_fato_nascimento_data_nascimento ........................ [[31mERROR[0m in 1.54s]
[0m12:04:55.379494 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d
[0m12:04:55.379494 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m12:04:55.379494 [info ] [Thread-1 (]: 22 of 34 START test not_null_fato_nascimento_gestacao_semanas .................. [RUN]
[0m12:04:55.379494 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_nascimento.266c143d4d, now test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2)
[0m12:04:55.379494 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m12:04:55.392847 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m12:04:55.393152 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (compile): 12:04:55.379494 => 12:04:55.393152
[0m12:04:55.393152 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m12:04:55.399602 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m12:04:55.399602 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:55.399602 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m12:04:55.399602 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select gestacao_semanas
from `workspace`.`default`.`fato_nascimento`
where gestacao_semanas is null



      
    ) dbt_internal_test
[0m12:04:55.399602 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:56.199662 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-08a1-19d7-a60b-4204a1859779
[0m12:04:57.110011 [debug] [Thread-1 (]: SQL status: OK in 1.7100000381469727 seconds
[0m12:04:57.110011 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (execute): 12:04:55.393152 => 12:04:57.110011
[0m12:04:57.110011 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: ROLLBACK
[0m12:04:57.110011 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:57.110011 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: Close
[0m12:04:57.110011 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-08a1-19d7-a60b-4204a1859779
[0m12:04:57.369396 [error] [Thread-1 (]: 22 of 34 FAIL 173 not_null_fato_nascimento_gestacao_semanas .................... [[31mFAIL 173[0m in 1.99s]
[0m12:04:57.369396 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m12:04:57.369396 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9
[0m12:04:57.369396 [info ] [Thread-1 (]: 23 of 34 START test not_null_fato_nascimento_hora_nascimento ................... [RUN]
[0m12:04:57.369396 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2, now test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9)
[0m12:04:57.369396 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9
[0m12:04:57.382477 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9"
[0m12:04:57.384473 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9 (compile): 12:04:57.369396 => 12:04:57.383476
[0m12:04:57.385471 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9
[0m12:04:57.391478 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9"
[0m12:04:57.392474 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:57.393449 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9"
[0m12:04:57.393449 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select hora_nascimento
from `workspace`.`default`.`fato_nascimento`
where hora_nascimento is null



      
    ) dbt_internal_test
[0m12:04:57.394446 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:04:58.654384 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-0a17-18a3-a56e-e34ad0a5229f
[0m12:04:59.119266 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select hora_nascimento
from `workspace`.`default`.`fato_nascimento`
where hora_nascimento is null



      
    ) dbt_internal_test
[0m12:04:59.119266 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `hora_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `cod_municipio`, `idade_mae`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m12:04:59.119266 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `hora_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `cod_municipio`, `idade_mae`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `hora_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `cod_municipio`, `idade_mae`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m12:04:59.119266 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07920-0a38-109f-aa4a-507b81ce1d3b
[0m12:04:59.119266 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9 (execute): 12:04:57.385471 => 12:04:59.119266
[0m12:04:59.119266 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9: ROLLBACK
[0m12:04:59.119266 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:04:59.119266 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9: Close
[0m12:04:59.119266 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-0a17-18a3-a56e-e34ad0a5229f
[0m12:04:59.379151 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_nascimento_hora_nascimento (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `hora_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `cod_municipio`, `idade_mae`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m12:04:59.379151 [error] [Thread-1 (]: 23 of 34 ERROR not_null_fato_nascimento_hora_nascimento ........................ [[31mERROR[0m in 2.01s]
[0m12:04:59.385182 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9
[0m12:04:59.389207 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m12:04:59.389207 [info ] [Thread-1 (]: 24 of 34 START test not_null_fato_nascimento_idade_mae ......................... [RUN]
[0m12:04:59.389207 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_hora_nascimento.60f2d9d0e9, now test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42)
[0m12:04:59.394215 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m12:04:59.401902 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m12:04:59.402969 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (compile): 12:04:59.394215 => 12:04:59.402969
[0m12:04:59.402969 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m12:04:59.402969 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m12:04:59.409630 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:04:59.409630 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m12:04:59.409630 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select idade_mae
from `workspace`.`default`.`fato_nascimento`
where idade_mae is null



      
    ) dbt_internal_test
[0m12:04:59.409630 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:00.189416 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-0b03-10a1-840a-8aa4ed059262
[0m12:05:00.659718 [debug] [Thread-1 (]: SQL status: OK in 1.25 seconds
[0m12:05:00.659718 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (execute): 12:04:59.402969 => 12:05:00.659718
[0m12:05:00.659718 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: ROLLBACK
[0m12:05:00.659718 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:05:00.659718 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: Close
[0m12:05:00.659718 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-0b03-10a1-840a-8aa4ed059262
[0m12:05:00.919428 [info ] [Thread-1 (]: 24 of 34 PASS not_null_fato_nascimento_idade_mae ............................... [[32mPASS[0m in 1.53s]
[0m12:05:00.919428 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m12:05:00.919428 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12
[0m12:05:00.919428 [info ] [Thread-1 (]: 25 of 34 START test not_null_fato_nascimento_local_nascimento .................. [RUN]
[0m12:05:00.929809 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42, now test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12)
[0m12:05:00.929809 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12
[0m12:05:00.944439 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12"
[0m12:05:00.944439 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12 (compile): 12:05:00.936147 => 12:05:00.944439
[0m12:05:00.944439 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12
[0m12:05:00.949482 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12"
[0m12:05:00.953003 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:00.953635 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12"
[0m12:05:00.953635 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select local_nascimento
from `workspace`.`default`.`fato_nascimento`
where local_nascimento is null



      
    ) dbt_internal_test
[0m12:05:00.953635 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:01.722365 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-0bea-18a8-8a3d-2fe889ed56a2
[0m12:05:02.196062 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select local_nascimento
from `workspace`.`default`.`fato_nascimento`
where local_nascimento is null



      
    ) dbt_internal_test
[0m12:05:02.197059 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `local_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `cod_municipio`, `data_id`, `idade_mae`, `tipo_parto`]. SQLSTATE: 42703; line 15 pos 6
[0m12:05:02.198055 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `local_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `cod_municipio`, `data_id`, `idade_mae`, `tipo_parto`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `local_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `cod_municipio`, `data_id`, `idade_mae`, `tipo_parto`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m12:05:02.199025 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07920-0c0c-16ce-8a17-03f3ea4340ce
[0m12:05:02.199025 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12 (execute): 12:05:00.944439 => 12:05:02.199025
[0m12:05:02.200022 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12: ROLLBACK
[0m12:05:02.200022 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:05:02.201047 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12: Close
[0m12:05:02.201047 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-0bea-18a8-8a3d-2fe889ed56a2
[0m12:05:02.439599 [debug] [Thread-1 (]: Runtime Error in test not_null_fato_nascimento_local_nascimento (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `local_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `cod_municipio`, `data_id`, `idade_mae`, `tipo_parto`]. SQLSTATE: 42703; line 15 pos 6
[0m12:05:02.439599 [error] [Thread-1 (]: 25 of 34 ERROR not_null_fato_nascimento_local_nascimento ....................... [[31mERROR[0m in 1.51s]
[0m12:05:02.439599 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12
[0m12:05:02.439599 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m12:05:02.439599 [info ] [Thread-1 (]: 26 of 34 START test not_null_fato_nascimento_nascimento_id ..................... [RUN]
[0m12:05:02.439599 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_local_nascimento.ef49a76b12, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m12:05:02.439599 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m12:05:02.453717 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m12:05:02.455743 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 12:05:02.439599 => 12:05:02.455142
[0m12:05:02.455743 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m12:05:02.459353 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m12:05:02.459353 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:02.459353 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m12:05:02.459353 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m12:05:02.459353 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:03.309440 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-0cdc-1098-a201-a631b62edb96
[0m12:05:03.789427 [debug] [Thread-1 (]: SQL status: OK in 1.3300000429153442 seconds
[0m12:05:03.789427 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 12:05:02.455743 => 12:05:03.789427
[0m12:05:03.789427 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m12:05:03.789427 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:05:03.789427 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m12:05:03.789427 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-0cdc-1098-a201-a631b62edb96
[0m12:05:04.029500 [info ] [Thread-1 (]: 26 of 34 PASS not_null_fato_nascimento_nascimento_id ........................... [[32mPASS[0m in 1.59s]
[0m12:05:04.039433 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m12:05:04.039433 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m12:05:04.039433 [info ] [Thread-1 (]: 27 of 34 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m12:05:04.039433 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m12:05:04.039433 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m12:05:04.049575 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m12:05:04.049575 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 12:05:04.039433 => 12:05:04.049575
[0m12:05:04.049575 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m12:05:04.056924 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m12:05:04.059467 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:04.059467 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m12:05:04.059467 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m12:05:04.059467 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:04.840212 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-0dc8-1c4d-b576-c1081ecc8d24
[0m12:05:05.319529 [debug] [Thread-1 (]: SQL status: OK in 1.2599999904632568 seconds
[0m12:05:05.319529 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 12:05:04.049575 => 12:05:05.319529
[0m12:05:05.319529 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m12:05:05.319529 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:05:05.319529 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m12:05:05.319529 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-0dc8-1c4d-b576-c1081ecc8d24
[0m12:05:05.568968 [error] [Thread-1 (]: 27 of 34 FAIL 4 not_null_fato_nascimento_peso .................................. [[31mFAIL 4[0m in 1.53s]
[0m12:05:05.573549 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m12:05:05.575460 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m12:05:05.576475 [info ] [Thread-1 (]: 28 of 34 START test not_null_fato_nascimento_sexo .............................. [RUN]
[0m12:05:05.577415 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m12:05:05.578440 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m12:05:05.580768 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m12:05:05.580768 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 12:05:05.579203 => 12:05:05.580768
[0m12:05:05.580768 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m12:05:05.589371 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m12:05:05.590688 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:05.591693 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m12:05:05.591693 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sexo
from `workspace`.`default`.`fato_nascimento`
where sexo is null



      
    ) dbt_internal_test
[0m12:05:05.591693 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:06.439504 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-0ebb-1864-afc5-824eefa3691c
[0m12:05:06.904519 [debug] [Thread-1 (]: SQL status: OK in 1.309999942779541 seconds
[0m12:05:06.914708 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 12:05:05.580768 => 12:05:06.914708
[0m12:05:06.914708 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: ROLLBACK
[0m12:05:06.914708 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:05:06.914708 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: Close
[0m12:05:06.914708 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-0ebb-1864-afc5-824eefa3691c
[0m12:05:07.154120 [info ] [Thread-1 (]: 28 of 34 PASS not_null_fato_nascimento_sexo .................................... [[32mPASS[0m in 1.58s]
[0m12:05:07.155117 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m12:05:07.156114 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m12:05:07.157111 [info ] [Thread-1 (]: 29 of 34 START test not_null_fato_nascimento_tipo_parto ........................ [RUN]
[0m12:05:07.157867 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e)
[0m12:05:07.159079 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m12:05:07.160682 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m12:05:07.169291 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (compile): 12:05:07.159682 => 12:05:07.169291
[0m12:05:07.169291 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m12:05:07.169291 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m12:05:07.169291 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:07.169291 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m12:05:07.169291 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tipo_parto
from `workspace`.`default`.`fato_nascimento`
where tipo_parto is null



      
    ) dbt_internal_test
[0m12:05:07.176909 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:07.949642 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-0fa2-15b6-82e5-a9a281463584
[0m12:05:08.830012 [debug] [Thread-1 (]: SQL status: OK in 1.649999976158142 seconds
[0m12:05:08.830012 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (execute): 12:05:07.169291 => 12:05:08.830012
[0m12:05:08.830012 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: ROLLBACK
[0m12:05:08.830012 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:05:08.830012 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: Close
[0m12:05:08.830012 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-0fa2-15b6-82e5-a9a281463584
[0m12:05:09.070258 [error] [Thread-1 (]: 29 of 34 FAIL 36 not_null_fato_nascimento_tipo_parto ........................... [[31mFAIL 36[0m in 1.91s]
[0m12:05:09.070258 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m12:05:09.079476 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a
[0m12:05:09.079476 [info ] [Thread-1 (]: 30 of 34 START test unique_dim_doenca_doenca_id ................................ [RUN]
[0m12:05:09.079476 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e, now test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a)
[0m12:05:09.079476 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a
[0m12:05:09.089551 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a"
[0m12:05:09.094350 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a (compile): 12:05:09.079476 => 12:05:09.089551
[0m12:05:09.094350 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a
[0m12:05:09.095481 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a"
[0m12:05:09.099588 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:09.099588 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a"
[0m12:05:09.099588 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    doenca_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_doenca`
where doenca_id is not null
group by doenca_id
having count(*) > 1



      
    ) dbt_internal_test
[0m12:05:09.099588 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:09.919827 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-10ce-158c-b013-b21a9787fef1
[0m12:05:10.259597 [debug] [Thread-1 (]: SQL status: OK in 1.159999966621399 seconds
[0m12:05:10.259597 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a (execute): 12:05:09.095481 => 12:05:10.259597
[0m12:05:10.259597 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a: ROLLBACK
[0m12:05:10.259597 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:05:10.259597 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a: Close
[0m12:05:10.259597 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-10ce-158c-b013-b21a9787fef1
[0m12:05:10.492029 [info ] [Thread-1 (]: 30 of 34 PASS unique_dim_doenca_doenca_id ...................................... [[32mPASS[0m in 1.41s]
[0m12:05:10.499594 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a
[0m12:05:10.499594 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m12:05:10.504604 [info ] [Thread-1 (]: 31 of 34 START test unique_dim_localidade_cod_municipio ........................ [RUN]
[0m12:05:10.508961 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_doenca_doenca_id.c66f36639a, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m12:05:10.510215 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m12:05:10.519489 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m12:05:10.519489 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 12:05:10.511213 => 12:05:10.519489
[0m12:05:10.519489 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m12:05:10.519489 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m12:05:10.529583 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:10.529583 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m12:05:10.529583 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_municipio as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_localidade`
where cod_municipio is not null
group by cod_municipio
having count(*) > 1



      
    ) dbt_internal_test
[0m12:05:10.529583 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:11.319520 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-11a4-1338-80da-e6e84bdfab23
[0m12:05:11.699328 [debug] [Thread-1 (]: SQL status: OK in 1.1699999570846558 seconds
[0m12:05:11.699328 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 12:05:10.519489 => 12:05:11.699328
[0m12:05:11.699328 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: ROLLBACK
[0m12:05:11.699328 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:05:11.699328 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: Close
[0m12:05:11.709615 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-11a4-1338-80da-e6e84bdfab23
[0m12:05:11.949847 [error] [Thread-1 (]: 31 of 34 FAIL 78 unique_dim_localidade_cod_municipio ........................... [[31mFAIL 78[0m in 1.45s]
[0m12:05:11.949847 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m12:05:11.949847 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m12:05:11.959387 [info ] [Thread-1 (]: 32 of 34 START test unique_dim_tempo_data_id ................................... [RUN]
[0m12:05:11.959387 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m12:05:11.959387 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m12:05:11.964392 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m12:05:11.964392 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 12:05:11.959387 => 12:05:11.964392
[0m12:05:11.964392 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m12:05:11.969436 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m12:05:11.969436 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:11.969436 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m12:05:11.969436 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m12:05:11.974441 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:12.775327 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-1282-15ca-b0c9-7d451d58a1bc
[0m12:05:13.169278 [debug] [Thread-1 (]: SQL status: OK in 1.190000057220459 seconds
[0m12:05:13.169278 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 12:05:11.964392 => 12:05:13.169278
[0m12:05:13.179491 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m12:05:13.179491 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:05:13.179491 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m12:05:13.179491 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-1282-15ca-b0c9-7d451d58a1bc
[0m12:05:13.419659 [info ] [Thread-1 (]: 32 of 34 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 1.46s]
[0m12:05:13.429630 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m12:05:13.429630 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m12:05:13.429630 [info ] [Thread-1 (]: 33 of 34 START test unique_fato_atendimento_hospitalar_atendimento_id .......... [RUN]
[0m12:05:13.429630 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe)
[0m12:05:13.429630 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m12:05:13.441478 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m12:05:13.441478 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe (compile): 12:05:13.439348 => 12:05:13.441478
[0m12:05:13.441478 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m12:05:13.450174 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m12:05:13.450174 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:13.450174 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"
[0m12:05:13.450174 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    atendimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is not null
group by atendimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m12:05:13.450174 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:14.259546 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-1365-12ca-bc83-627bb880e92b
[0m12:05:14.687074 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    atendimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_atendimento_hospitalar`
where atendimento_id is not null
group by atendimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m12:05:14.689056 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m12:05:14.690059 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m12:05:14.691040 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07920-1386-1a27-8f12-264827366e26
[0m12:05:14.692049 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe (execute): 12:05:13.441478 => 12:05:14.691040
[0m12:05:14.692049 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: ROLLBACK
[0m12:05:14.693046 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:05:14.693046 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe: Close
[0m12:05:14.694044 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-1365-12ca-bc83-627bb880e92b
[0m12:05:14.929835 [debug] [Thread-1 (]: Runtime Error in test unique_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m12:05:14.929835 [error] [Thread-1 (]: 33 of 34 ERROR unique_fato_atendimento_hospitalar_atendimento_id ............... [[31mERROR[0m in 1.50s]
[0m12:05:14.939304 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe
[0m12:05:14.939304 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m12:05:14.939304 [info ] [Thread-1 (]: 34 of 34 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m12:05:14.939304 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_fato_atendimento_hospitalar_atendimento_id.5796fa9ffe, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m12:05:14.939304 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m12:05:14.950965 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m12:05:14.950965 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 12:05:14.947477 => 12:05:14.950965
[0m12:05:14.950965 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m12:05:14.964289 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m12:05:14.964289 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:14.964289 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m12:05:14.964289 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m12:05:14.967249 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:05:15.769570 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07920-144a-1e09-8b30-e3b9027a0f39
[0m12:05:16.239736 [debug] [Thread-1 (]: SQL status: OK in 1.2699999809265137 seconds
[0m12:05:16.249407 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 12:05:14.959514 => 12:05:16.249407
[0m12:05:16.249407 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m12:05:16.249407 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:05:16.249407 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m12:05:16.249407 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07920-144a-1e09-8b30-e3b9027a0f39
[0m12:05:16.489498 [error] [Thread-1 (]: 34 of 34 FAIL 59968 unique_fato_nascimento_nascimento_id ....................... [[31mFAIL 59968[0m in 1.55s]
[0m12:05:16.489498 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m12:05:16.499529 [debug] [MainThread]: On master: ROLLBACK
[0m12:05:16.504069 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:05:17.299601 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07920-1534-1df9-9710-c655a259ef6a
[0m12:05:17.299601 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:05:17.305592 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:05:17.305592 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:05:17.305592 [debug] [MainThread]: On master: ROLLBACK
[0m12:05:17.305592 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:05:17.305592 [debug] [MainThread]: On master: Close
[0m12:05:17.305592 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07920-1534-1df9-9710-c655a259ef6a
[0m12:05:17.549653 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:05:17.549653 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m12:05:17.549653 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m12:05:17.549653 [info ] [MainThread]: 
[0m12:05:17.559238 [info ] [MainThread]: Finished running 34 tests in 0 hours 0 minutes and 57.82 seconds (57.82s).
[0m12:05:17.559238 [debug] [MainThread]: Command end result
[0m12:05:17.569374 [info ] [MainThread]: 
[0m12:05:17.579380 [info ] [MainThread]: [31mCompleted with 14 errors and 0 warnings:[0m
[0m12:05:17.582107 [info ] [MainThread]: 
[0m12:05:17.583201 [error] [MainThread]: [33mRuntime Error in test accepted_values_fato_nascimento_sexo__M__F (models\marts\schema.yaml)[0m
[0m12:05:17.583474 [error] [MainThread]:   [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
[0m12:05:17.583474 [error] [MainThread]:   == SQL (line 24, position 19) ==
[0m12:05:17.587117 [error] [MainThread]:   where value_field not in (
[0m12:05:17.588115 [error] [MainThread]:                     ^^^^^^^^
[0m12:05:17.589112 [error] [MainThread]:       'M','F'
[0m12:05:17.590134 [error] [MainThread]:   ^^^^^^^^^^^
[0m12:05:17.592104 [error] [MainThread]:   )
[0m12:05:17.592903 [error] [MainThread]:   ^
[0m12:05:17.592903 [error] [MainThread]:   
[0m12:05:17.592903 [info ] [MainThread]: 
[0m12:05:17.592903 [error] [MainThread]: [33mRuntime Error in test not_null_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)[0m
[0m12:05:17.592903 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 15 pos 6
[0m12:05:17.599442 [info ] [MainThread]: 
[0m12:05:17.599442 [error] [MainThread]: [33mRuntime Error in test not_null_fato_atendimento_hospitalar_cod_municipio (models\marts\schema.yaml)[0m
[0m12:05:17.599442 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `cod_municipio` cannot be resolved. Did you mean one of the following? [`doenca_id`, `paciente`, `procedimento`, `localidade_id`, `data_atendimento`]. SQLSTATE: 42703; line 15 pos 6
[0m12:05:17.599442 [info ] [MainThread]: 
[0m12:05:17.603316 [error] [MainThread]: [31mFailure in test not_null_fato_nascimento_APGAR1 (models\marts\schema.yaml)[0m
[0m12:05:17.605314 [error] [MainThread]:   Got 677 results, configured to fail if != 0
[0m12:05:17.605314 [info ] [MainThread]: 
[0m12:05:17.606967 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\not_null_fato_nascimento_APGAR1.sql
[0m12:05:17.606967 [info ] [MainThread]: 
[0m12:05:17.609505 [error] [MainThread]: [31mFailure in test not_null_fato_nascimento_APGAR5 (models\marts\schema.yaml)[0m
[0m12:05:17.609505 [error] [MainThread]:   Got 666 results, configured to fail if != 0
[0m12:05:17.609505 [info ] [MainThread]: 
[0m12:05:17.609505 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\not_null_fato_nascimento_APGAR5.sql
[0m12:05:17.609505 [info ] [MainThread]: 
[0m12:05:17.609505 [error] [MainThread]: [33mRuntime Error in test not_null_fato_nascimento_data_nascimento (models\marts\schema.yaml)[0m
[0m12:05:17.609505 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `data_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `idade_mae`, `cod_municipio`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m12:05:17.609505 [info ] [MainThread]: 
[0m12:05:17.609505 [error] [MainThread]: [31mFailure in test not_null_fato_nascimento_gestacao_semanas (models\marts\schema.yaml)[0m
[0m12:05:17.609505 [error] [MainThread]:   Got 173 results, configured to fail if != 0
[0m12:05:17.619985 [info ] [MainThread]: 
[0m12:05:17.620983 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\not_null_fato_nascimento_gestacao_semanas.sql
[0m12:05:17.622979 [info ] [MainThread]: 
[0m12:05:17.623560 [error] [MainThread]: [33mRuntime Error in test not_null_fato_nascimento_hora_nascimento (models\marts\schema.yaml)[0m
[0m12:05:17.623560 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `hora_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `data_id`, `cod_municipio`, `idade_mae`, `sexo`]. SQLSTATE: 42703; line 15 pos 6
[0m12:05:17.623560 [info ] [MainThread]: 
[0m12:05:17.623560 [error] [MainThread]: [33mRuntime Error in test not_null_fato_nascimento_local_nascimento (models\marts\schema.yaml)[0m
[0m12:05:17.623560 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `local_nascimento` cannot be resolved. Did you mean one of the following? [`nascimento_id`, `cod_municipio`, `data_id`, `idade_mae`, `tipo_parto`]. SQLSTATE: 42703; line 15 pos 6
[0m12:05:17.629600 [info ] [MainThread]: 
[0m12:05:17.629600 [error] [MainThread]: [31mFailure in test not_null_fato_nascimento_peso (models\marts\schema.yaml)[0m
[0m12:05:17.629600 [error] [MainThread]:   Got 4 results, configured to fail if != 0
[0m12:05:17.629600 [info ] [MainThread]: 
[0m12:05:17.634619 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\not_null_fato_nascimento_peso.sql
[0m12:05:17.634619 [info ] [MainThread]: 
[0m12:05:17.636736 [error] [MainThread]: [31mFailure in test not_null_fato_nascimento_tipo_parto (models\marts\schema.yaml)[0m
[0m12:05:17.637750 [error] [MainThread]:   Got 36 results, configured to fail if != 0
[0m12:05:17.638755 [info ] [MainThread]: 
[0m12:05:17.639728 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\not_null_fato_nascimento_tipo_parto.sql
[0m12:05:17.641724 [info ] [MainThread]: 
[0m12:05:17.642111 [error] [MainThread]: [31mFailure in test unique_dim_localidade_cod_municipio (models\marts\schema.yaml)[0m
[0m12:05:17.642111 [error] [MainThread]:   Got 78 results, configured to fail if != 0
[0m12:05:17.642111 [info ] [MainThread]: 
[0m12:05:17.642111 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_dim_localidade_cod_municipio.sql
[0m12:05:17.642111 [info ] [MainThread]: 
[0m12:05:17.642111 [error] [MainThread]: [33mRuntime Error in test unique_fato_atendimento_hospitalar_atendimento_id (models\marts\schema.yaml)[0m
[0m12:05:17.649649 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `atendimento_id` cannot be resolved. Did you mean one of the following? [`data_atendimento`, `doenca_id`, `procedimento`, `paciente`, `localidade_id`]. SQLSTATE: 42703; line 16 pos 6
[0m12:05:17.649649 [info ] [MainThread]: 
[0m12:05:17.649649 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m12:05:17.653391 [error] [MainThread]:   Got 59968 results, configured to fail if != 0
[0m12:05:17.654389 [info ] [MainThread]: 
[0m12:05:17.655385 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m12:05:17.656384 [info ] [MainThread]: 
[0m12:05:17.658405 [info ] [MainThread]: Done. PASS=20 WARN=0 ERROR=14 SKIP=0 TOTAL=34
[0m12:05:17.660372 [debug] [MainThread]: Command `dbt test` failed at 12:05:17.660372 after 59.75 seconds
[0m12:05:17.660372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141CABAA410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141CA930B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141C4A1DF50>]}
[0m12:05:17.660372 [debug] [MainThread]: Flushing usage events
[0m12:15:00.798494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E8AABB2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E89B97A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E8ACE3150>]}


============================== 12:15:00.798494 | efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee ==============================
[0m12:15:00.798494 [info ] [MainThread]: Running with dbt=1.5.2
[0m12:15:00.808678 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m12:15:02.135224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E8AAD0C90>]}
[0m12:15:02.151791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E94428210>]}
[0m12:15:02.151791 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m12:15:02.172132 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m12:15:02.288503 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m12:15:02.288503 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\schema.yaml
[0m12:15:02.288503 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m12:15:02.372642 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m12:15:02.388649 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m12:15:02.388649 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m12:15:02.398271 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m12:15:02.398271 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_doenca.sql
[0m12:15:02.508341 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E9562AC90>]}
[0m12:15:02.518490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E956AAA90>]}
[0m12:15:02.518490 [info ] [MainThread]: Found 12 models, 15 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m12:15:02.518490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E91ADF090>]}
[0m12:15:02.518490 [info ] [MainThread]: 
[0m12:15:02.518490 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:15:02.528687 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m12:15:02.528687 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m12:15:02.530760 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m12:15:02.531465 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:15:03.638871 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07921-72ae-1522-b778-03f23ab402c9
[0m12:15:04.378733 [debug] [ThreadPool]: SQL status: OK in 1.850000023841858 seconds
[0m12:15:04.378733 [debug] [ThreadPool]: On list_workspace: Close
[0m12:15:04.378733 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07921-72ae-1522-b778-03f23ab402c9
[0m12:15:04.674312 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m12:15:04.678385 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m12:15:04.688462 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:04.688462 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m12:15:04.688462 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m12:15:04.688462 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:15:05.528698 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07921-73cb-1a2b-a158-dcc3ae15f7c0
[0m12:15:06.042335 [debug] [ThreadPool]: SQL status: OK in 1.350000023841858 seconds
[0m12:15:06.043364 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:15:06.044360 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m12:15:06.045328 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:15:06.045328 [debug] [ThreadPool]: On create_workspace_default: Close
[0m12:15:06.046326 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07921-73cb-1a2b-a158-dcc3ae15f7c0
[0m12:15:06.303228 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m12:15:06.310079 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m12:15:06.310317 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m12:15:06.311315 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:15:07.143944 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07921-74c4-14c5-a5c9-8052b5b4dc06
[0m12:15:07.556257 [debug] [ThreadPool]: SQL status: OK in 1.2400000095367432 seconds
[0m12:15:07.559254 [debug] [ThreadPool]: On list_workspace_default: Close
[0m12:15:07.559254 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07921-74c4-14c5-a5c9-8052b5b4dc06
[0m12:15:07.798588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E9562B690>]}
[0m12:15:07.798588 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:07.798588 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:15:07.798588 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:15:07.798588 [info ] [MainThread]: 
[0m12:15:07.812275 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m12:15:07.813257 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m12:15:07.815253 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m12:15:07.816250 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m12:15:07.820240 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m12:15:07.822234 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 12:15:07.817248 => 12:15:07.821264
[0m12:15:07.822234 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m12:15:07.853334 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m12:15:07.858744 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:07.858744 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m12:15:07.858744 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m12:15:07.858744 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:15:08.678627 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-75ad-194f-a7de-27e31e117ecd
[0m12:15:09.574539 [debug] [Thread-1 (]: SQL status: OK in 1.7200000286102295 seconds
[0m12:15:09.599495 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 12:15:07.823246 => 12:15:09.599495
[0m12:15:09.599495 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m12:15:09.599495 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:09.599495 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m12:15:09.599495 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-75ad-194f-a7de-27e31e117ecd
[0m12:15:09.844551 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E955DEF10>]}
[0m12:15:09.846548 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 2.03s]
[0m12:15:09.847512 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m12:15:09.848510 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m12:15:09.849507 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m12:15:09.851923 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m12:15:09.851923 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m12:15:09.858905 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m12:15:09.859865 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 12:15:09.852920 => 12:15:09.859865
[0m12:15:09.859865 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m12:15:09.859865 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m12:15:09.859865 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:09.868463 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m12:15:09.868463 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m12:15:09.868463 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:15:10.665562 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-76de-1498-a31f-9d6fe4a918af
[0m12:15:11.338488 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m12:15:11.351822 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 12:15:09.859865 => 12:15:11.348680
[0m12:15:11.351822 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m12:15:11.351822 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:11.351822 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m12:15:11.358416 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-76de-1498-a31f-9d6fe4a918af
[0m12:15:11.588728 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E95759E10>]}
[0m12:15:11.588728 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.74s]
[0m12:15:11.598669 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m12:15:11.598669 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m12:15:11.598669 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m12:15:11.598669 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m12:15:11.598669 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m12:15:11.598669 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m12:15:11.608485 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 12:15:11.598669 => 12:15:11.608485
[0m12:15:11.608485 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m12:15:11.613491 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m12:15:11.613491 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:11.613491 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m12:15:11.613491 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m12:15:11.613491 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:15:12.408862 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-77eb-14d0-8395-262972e77d80
[0m12:15:13.088492 [debug] [Thread-1 (]: SQL status: OK in 1.4800000190734863 seconds
[0m12:15:13.088492 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 12:15:11.608485 => 12:15:13.088492
[0m12:15:13.088492 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m12:15:13.088492 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:13.088492 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m12:15:13.088492 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-77eb-14d0-8395-262972e77d80
[0m12:15:13.380307 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E957FE690>]}
[0m12:15:13.380307 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.78s]
[0m12:15:13.388899 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m12:15:13.393408 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m12:15:13.393408 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m12:15:13.398512 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m12:15:13.403860 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m12:15:13.413654 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m12:15:13.415516 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 12:15:13.405779 => 12:15:13.415516
[0m12:15:13.415516 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m12:15:13.418633 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m12:15:13.418633 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:13.418633 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m12:15:13.418633 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m12:15:13.418633 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:15:14.915465 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-7965-1651-93a3-178544a1b6db
[0m12:15:15.573685 [debug] [Thread-1 (]: SQL status: OK in 2.1600000858306885 seconds
[0m12:15:15.578803 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 12:15:13.418633 => 12:15:15.578803
[0m12:15:15.578803 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m12:15:15.578803 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:15.578803 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m12:15:15.578803 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-7965-1651-93a3-178544a1b6db
[0m12:15:15.804492 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E957F0050>]}
[0m12:15:15.805489 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 2.41s]
[0m12:15:15.806069 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m12:15:15.807097 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m12:15:15.808066 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m12:15:15.809063 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m12:15:15.810091 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m12:15:15.814077 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m12:15:15.815057 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 12:15:15.810091 => 12:15:15.815057
[0m12:15:15.816045 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m12:15:15.821058 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m12:15:15.822029 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:15.822029 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m12:15:15.823027 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m12:15:15.824024 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:15:17.299033 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-7ad3-1354-a809-f51229b73c4e
[0m12:15:17.958485 [debug] [Thread-1 (]: SQL status: OK in 2.140000104904175 seconds
[0m12:15:17.968791 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 12:15:15.817070 => 12:15:17.968791
[0m12:15:17.968791 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m12:15:17.968791 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:17.968791 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m12:15:17.968791 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-7ad3-1354-a809-f51229b73c4e
[0m12:15:18.208295 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E957F0110>]}
[0m12:15:18.208295 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 2.40s]
[0m12:15:18.208295 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m12:15:18.208295 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m12:15:18.208295 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m12:15:18.208295 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m12:15:18.208295 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m12:15:18.218781 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m12:15:18.218781 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 12:15:18.208295 => 12:15:18.218781
[0m12:15:18.218781 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m12:15:18.230733 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m12:15:18.231838 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:18.231838 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m12:15:18.231838 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m12:15:18.231838 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:15:19.028587 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-7bdb-1dc6-bc53-40e8a20a561c
[0m12:15:19.758641 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m12:15:19.758641 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 12:15:18.218781 => 12:15:19.758641
[0m12:15:19.758641 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m12:15:19.758641 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:19.763656 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m12:15:19.763656 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-7bdb-1dc6-bc53-40e8a20a561c
[0m12:15:19.988694 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E9579D9D0>]}
[0m12:15:19.991221 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.78s]
[0m12:15:19.991221 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m12:15:19.991221 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m12:15:19.991221 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m12:15:19.991221 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m12:15:19.991221 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m12:15:20.003779 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m12:15:20.005925 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 12:15:19.991221 => 12:15:20.003779
[0m12:15:20.006833 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m12:15:20.013382 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m12:15:20.013382 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:20.013382 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m12:15:20.013382 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH base AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM base

[0m12:15:20.018495 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:15:20.815817 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-7ceb-13d1-9fec-349caec67f4b
[0m12:15:21.518472 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m12:15:21.518472 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 12:15:20.007826 => 12:15:21.518472
[0m12:15:21.518472 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m12:15:21.518472 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:21.518472 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m12:15:21.518472 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-7ceb-13d1-9fec-349caec67f4b
[0m12:15:21.771477 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E9450E9D0>]}
[0m12:15:21.774466 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 1.78s]
[0m12:15:21.775463 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m12:15:21.776461 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m12:15:21.777458 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m12:15:21.779453 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m12:15:21.780450 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m12:15:21.784439 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m12:15:21.786434 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 12:15:21.781447 => 12:15:21.786434
[0m12:15:21.787443 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m12:15:21.793176 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m12:15:21.794145 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:21.795177 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m12:15:21.795177 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m12:15:21.796168 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:15:22.578964 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-7dfa-1448-ae84-cee522b4c31a
[0m12:15:23.278410 [debug] [Thread-1 (]: SQL status: OK in 1.4800000190734863 seconds
[0m12:15:23.278410 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 12:15:21.788576 => 12:15:23.278410
[0m12:15:23.278410 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m12:15:23.283416 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:23.283416 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m12:15:23.283416 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-7dfa-1448-ae84-cee522b4c31a
[0m12:15:23.518639 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E956A5390>]}
[0m12:15:23.518639 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.74s]
[0m12:15:23.518639 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m12:15:23.528186 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m12:15:23.528186 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m12:15:23.528186 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m12:15:23.536285 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m12:15:23.548693 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m12:15:23.560303 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 12:15:23.538686 => 12:15:23.559340
[0m12:15:23.562298 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m12:15:23.570079 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m12:15:23.570079 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:23.570079 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m12:15:23.570079 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m12:15:23.574323 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:15:24.328581 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-7f05-1bd4-bdeb-8469d83a8966
[0m12:15:25.048777 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m12:15:25.048777 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 12:15:23.563296 => 12:15:25.048777
[0m12:15:25.048777 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m12:15:25.048777 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:25.048777 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m12:15:25.053784 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-7f05-1bd4-bdeb-8469d83a8966
[0m12:15:25.288698 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E95827E10>]}
[0m12:15:25.288698 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.76s]
[0m12:15:25.293703 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m12:15:25.294754 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m12:15:25.294754 [info ] [Thread-1 (]: 10 of 12 START sql view model default.dim_localidade ........................... [RUN]
[0m12:15:25.296849 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m12:15:25.296849 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m12:15:25.298405 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m12:15:25.298405 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 12:15:25.298405 => 12:15:25.298405
[0m12:15:25.298405 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m12:15:25.308591 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m12:15:25.308591 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:25.308591 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m12:15:25.308591 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    

WITH locais AS (
    SELECT DISTINCT
        cod_municipio,
        local_nascimento
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    cod_municipio,
    local_nascimento
FROM locais

[0m12:15:25.308591 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:15:26.143853 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-8019-1272-bde3-592bf15d3c63
[0m12:15:26.830784 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m12:15:26.833776 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 12:15:25.304812 => 12:15:26.833776
[0m12:15:26.833776 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m12:15:26.834773 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:26.835770 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m12:15:26.835770 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-8019-1272-bde3-592bf15d3c63
[0m12:15:27.058962 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E945312D0>]}
[0m12:15:27.068508 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.dim_localidade ...................... [[32mOK[0m in 1.76s]
[0m12:15:27.070158 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m12:15:27.070158 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m12:15:27.070158 [info ] [Thread-1 (]: 11 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m12:15:27.070158 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m12:15:27.070158 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m12:15:27.079488 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m12:15:27.079488 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 12:15:27.070158 => 12:15:27.079488
[0m12:15:27.079488 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m12:15:27.088340 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m12:15:27.088340 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:27.088340 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m12:15:27.088340 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

WITH datas AS (
    SELECT DISTINCT
        data_nascimento AS data
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    ROW_NUMBER() OVER (ORDER BY data) AS data_id,
    data,
    EXTRACT(YEAR FROM data) AS ano,
    EXTRACT(MONTH FROM data) AS mes,
    EXTRACT(DAY FROM data) AS dia,
    DAYOFWEEK(data) AS dia_semana
FROM datas

[0m12:15:27.088340 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:15:27.881634 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-8123-16b5-8802-abc73ad1f185
[0m12:15:28.618150 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m12:15:28.621142 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 12:15:27.079488 => 12:15:28.621142
[0m12:15:28.622123 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m12:15:28.622123 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:28.623137 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m12:15:28.623137 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-8123-16b5-8802-abc73ad1f185
[0m12:15:28.868544 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E958079D0>]}
[0m12:15:28.878728 [info ] [Thread-1 (]: 11 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 1.80s]
[0m12:15:28.878728 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m12:15:28.878728 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m12:15:28.878728 [info ] [Thread-1 (]: 12 of 12 START sql table model default.fato_nascimento ......................... [RUN]
[0m12:15:28.882798 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_nascimento)
[0m12:15:28.882798 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m12:15:28.888365 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m12:15:28.890817 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 12:15:28.882798 => 12:15:28.890817
[0m12:15:28.892328 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m12:15:28.898340 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:28.898340 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m12:15:28.898340 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */

      describe extended `workspace`.`default`.`fato_nascimento`
  
[0m12:15:28.898340 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:15:29.678442 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-8235-137f-9986-2a4d6ee7d8c3
[0m12:15:30.208563 [debug] [Thread-1 (]: SQL status: OK in 1.309999942779541 seconds
[0m12:15:30.218858 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 12:15:28.892617 => 12:15:30.218858
[0m12:15:30.218858 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m12:15:30.218858 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:15:30.218858 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m12:15:30.218858 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-8235-137f-9986-2a4d6ee7d8c3
[0m12:15:30.448753 [debug] [Thread-1 (]: Runtime Error in model fato_nascimento (models\marts\fato_nascimento.sql)
  dbt could not find a macro with the name "drop_relation" in any package
[0m12:15:30.448753 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'efe4e06c-f9d4-4ff0-bd28-2a5ab2a137ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E957A0E10>]}
[0m12:15:30.448753 [error] [Thread-1 (]: 12 of 12 ERROR creating sql table model default.fato_nascimento ................ [[31mERROR[0m in 1.57s]
[0m12:15:30.448753 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m12:15:30.448753 [debug] [MainThread]: On master: ROLLBACK
[0m12:15:30.448753 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:15:31.268573 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07921-8325-156a-8a2d-538ccb03f065
[0m12:15:31.273581 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:15:31.273581 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:15:31.273581 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:15:31.273581 [debug] [MainThread]: On master: ROLLBACK
[0m12:15:31.273581 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:15:31.273581 [debug] [MainThread]: On master: Close
[0m12:15:31.273581 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07921-8325-156a-8a2d-538ccb03f065
[0m12:15:31.508796 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:15:31.508796 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m12:15:31.508796 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m12:15:31.508796 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m12:15:31.508796 [info ] [MainThread]: 
[0m12:15:31.518464 [info ] [MainThread]: Finished running 11 view models, 1 table model in 0 hours 0 minutes and 28.99 seconds (28.99s).
[0m12:15:31.518464 [debug] [MainThread]: Command end result
[0m12:15:31.533664 [info ] [MainThread]: 
[0m12:15:31.535772 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:15:31.536868 [info ] [MainThread]: 
[0m12:15:31.538411 [error] [MainThread]: [33mRuntime Error in model fato_nascimento (models\marts\fato_nascimento.sql)[0m
[0m12:15:31.538411 [error] [MainThread]:   dbt could not find a macro with the name "drop_relation" in any package
[0m12:15:31.538411 [info ] [MainThread]: 
[0m12:15:31.538411 [info ] [MainThread]: Done. PASS=11 WARN=0 ERROR=1 SKIP=0 TOTAL=12
[0m12:15:31.538411 [debug] [MainThread]: Command `dbt run` failed at 12:15:31.538411 after 30.76 seconds
[0m12:15:31.538411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E8AABB2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E8ACFD2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E8ACE8A50>]}
[0m12:15:31.538411 [debug] [MainThread]: Flushing usage events
[0m12:16:10.318173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F3B22FE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F3B42A950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F3B4E9510>]}


============================== 12:16:10.324865 | 9f5e8668-90df-4a61-bb81-61dc016c0d40 ==============================
[0m12:16:10.324865 [info ] [MainThread]: Running with dbt=1.5.2
[0m12:16:10.324865 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m12:16:11.683234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F3BAA2850>]}
[0m12:16:11.703392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F44B98210>]}
[0m12:16:11.703392 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m12:16:11.723667 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m12:16:11.828301 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m12:16:11.828301 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m12:16:11.858189 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m12:16:12.012757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F44CA8BD0>]}
[0m12:16:12.028310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F44D20A10>]}
[0m12:16:12.028310 [info ] [MainThread]: Found 12 models, 15 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m12:16:12.028310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F4224ED10>]}
[0m12:16:12.028310 [info ] [MainThread]: 
[0m12:16:12.028310 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:16:12.028310 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m12:16:12.038538 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m12:16:12.038538 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m12:16:12.038538 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:16:12.888914 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07921-9bf6-135b-95cc-1ea7c0529c77
[0m12:16:13.198571 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m12:16:13.198571 [debug] [ThreadPool]: On list_workspace: Close
[0m12:16:13.198571 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07921-9bf6-135b-95cc-1ea7c0529c77
[0m12:16:13.438736 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m12:16:13.448524 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m12:16:13.473681 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:13.473681 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m12:16:13.478728 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m12:16:13.478728 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:16:14.278911 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07921-9ccb-15d1-abdc-3ed34b19c7c7
[0m12:16:14.648583 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m12:16:14.648583 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m12:16:14.648583 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m12:16:14.648583 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m12:16:14.648583 [debug] [ThreadPool]: On create_workspace_default: Close
[0m12:16:14.648583 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07921-9ccb-15d1-abdc-3ed34b19c7c7
[0m12:16:14.882965 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m12:16:14.888603 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m12:16:14.888603 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m12:16:14.888603 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:16:15.678341 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07921-9d9e-10d4-bafc-8cfa07c2911d
[0m12:16:16.048358 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m12:16:16.058223 [debug] [ThreadPool]: On list_workspace_default: Close
[0m12:16:16.058223 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07921-9d9e-10d4-bafc-8cfa07c2911d
[0m12:16:16.313397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F3BABCC50>]}
[0m12:16:16.313397 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:16.318470 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:16:16.318470 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:16:16.318470 [info ] [MainThread]: 
[0m12:16:16.332057 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m12:16:16.332057 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m12:16:16.338619 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m12:16:16.338619 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m12:16:16.338619 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m12:16:16.338619 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 12:16:16.338619 => 12:16:16.338619
[0m12:16:16.338619 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m12:16:16.379919 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m12:16:16.381759 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:16.381759 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m12:16:16.381759 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m12:16:16.381759 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:16:17.178766 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-9e85-1dea-9498-bdbec6c1e35f
[0m12:16:17.828293 [debug] [Thread-1 (]: SQL status: OK in 1.4500000476837158 seconds
[0m12:16:17.848229 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 12:16:16.346512 => 12:16:17.848229
[0m12:16:17.848229 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m12:16:17.848229 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:17.848229 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m12:16:17.848229 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-9e85-1dea-9498-bdbec6c1e35f
[0m12:16:18.088704 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F44CD6F50>]}
[0m12:16:18.088704 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.75s]
[0m12:16:18.088704 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m12:16:18.098697 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m12:16:18.098697 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m12:16:18.098697 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m12:16:18.106513 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m12:16:18.115061 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m12:16:18.115980 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 12:16:18.107512 => 12:16:18.115980
[0m12:16:18.115980 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m12:16:18.118517 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m12:16:18.123527 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:18.123527 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m12:16:18.123527 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m12:16:18.123527 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:16:18.899876 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-9f8c-18d9-83c2-f9317c9a3e8f
[0m12:16:19.558857 [debug] [Thread-1 (]: SQL status: OK in 1.440000057220459 seconds
[0m12:16:19.568658 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 12:16:18.115980 => 12:16:19.568658
[0m12:16:19.568658 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m12:16:19.568658 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:19.568658 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m12:16:19.568658 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-9f8c-18d9-83c2-f9317c9a3e8f
[0m12:16:19.800441 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F44DB6010>]}
[0m12:16:19.800441 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.70s]
[0m12:16:19.800441 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m12:16:19.800441 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m12:16:19.808512 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m12:16:19.808512 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m12:16:19.808512 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m12:16:19.818640 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m12:16:19.818640 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 12:16:19.812914 => 12:16:19.818640
[0m12:16:19.823648 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m12:16:19.828196 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m12:16:19.828196 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:19.828196 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m12:16:19.828196 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m12:16:19.828196 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:16:20.658926 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-a093-193c-a2ef-5a58564d632c
[0m12:16:21.348659 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m12:16:21.348659 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 12:16:19.823648 => 12:16:21.348659
[0m12:16:21.348659 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m12:16:21.348659 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:21.348659 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m12:16:21.348659 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-a093-193c-a2ef-5a58564d632c
[0m12:16:21.583424 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F45EB85D0>]}
[0m12:16:21.583424 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.77s]
[0m12:16:21.583424 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m12:16:21.583424 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m12:16:21.583424 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m12:16:21.588467 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m12:16:21.588467 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m12:16:21.588467 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m12:16:21.588467 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 12:16:21.588467 => 12:16:21.588467
[0m12:16:21.588467 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m12:16:21.602270 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m12:16:21.604008 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:21.604008 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m12:16:21.604008 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42  -- SC
)

SELECT
    ROW_NUMBER() OVER (ORDER BY DTNASC, HORANASC) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento, -- converte BIGINT para DATE
    HORANASC AS hora_nascimento,
    SEXO AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m12:16:21.604008 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:16:22.408743 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-a1a2-1a10-b647-1d3f1d78a42f
[0m12:16:23.078687 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m12:16:23.078687 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 12:16:21.588467 => 12:16:23.078687
[0m12:16:23.078687 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m12:16:23.078687 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:23.078687 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m12:16:23.078687 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-a1a2-1a10-b647-1d3f1d78a42f
[0m12:16:23.328545 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F45F33290>]}
[0m12:16:23.338502 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.74s]
[0m12:16:23.338502 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m12:16:23.338502 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m12:16:23.338502 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m12:16:23.338502 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m12:16:23.338502 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m12:16:23.350869 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m12:16:23.352356 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 12:16:23.338502 => 12:16:23.352356
[0m12:16:23.352356 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m12:16:23.358676 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m12:16:23.358676 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:23.358676 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m12:16:23.358676 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m12:16:23.358676 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:16:24.198706 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-a2b3-1ccd-8772-494cbe762957
[0m12:16:24.833001 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m12:16:24.839191 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 12:16:23.353964 => 12:16:24.839191
[0m12:16:24.839191 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m12:16:24.839191 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:24.848310 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m12:16:24.848310 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-a2b3-1ccd-8772-494cbe762957
[0m12:16:25.078602 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F45F56690>]}
[0m12:16:25.078602 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.74s]
[0m12:16:25.078602 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m12:16:25.078602 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m12:16:25.088642 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m12:16:25.088642 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m12:16:25.088642 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m12:16:25.088642 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m12:16:25.088642 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 12:16:25.088642 => 12:16:25.088642
[0m12:16:25.088642 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m12:16:25.098380 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m12:16:25.098380 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:25.098380 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m12:16:25.098380 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m12:16:25.098380 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:16:25.863243 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-a3b1-134d-9de8-e36541345aa1
[0m12:16:26.588290 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m12:16:26.598270 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 12:16:25.088642 => 12:16:26.598270
[0m12:16:26.598270 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m12:16:26.598270 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:26.598270 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m12:16:26.598270 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-a3b1-134d-9de8-e36541345aa1
[0m12:16:26.833668 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F45F62290>]}
[0m12:16:26.833668 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.75s]
[0m12:16:26.838749 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m12:16:26.838749 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m12:16:26.838749 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m12:16:26.838749 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m12:16:26.838749 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m12:16:26.849712 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m12:16:26.849712 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 12:16:26.848263 => 12:16:26.849712
[0m12:16:26.849712 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m12:16:26.858712 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m12:16:26.858712 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:26.858712 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m12:16:26.858712 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH base AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM base

[0m12:16:26.858712 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:16:27.678709 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-a4c8-1e35-a474-5c149495d458
[0m12:16:28.369539 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m12:16:28.378296 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 12:16:26.849712 => 12:16:28.378296
[0m12:16:28.379287 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m12:16:28.379287 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:28.380310 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m12:16:28.381282 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-a4c8-1e35-a474-5c149495d458
[0m12:16:28.618811 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F44CA3650>]}
[0m12:16:28.618811 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 1.78s]
[0m12:16:28.628395 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m12:16:28.628395 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m12:16:28.628395 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m12:16:28.628395 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m12:16:28.628395 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m12:16:28.638303 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m12:16:28.640188 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 12:16:28.628395 => 12:16:28.640188
[0m12:16:28.641423 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m12:16:28.643910 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m12:16:28.648514 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:28.648514 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m12:16:28.648514 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m12:16:28.648514 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:16:29.448650 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-a5d5-1590-9012-b70f9cd8b571
[0m12:16:30.144363 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m12:16:30.148406 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 12:16:28.641423 => 12:16:30.148406
[0m12:16:30.148406 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m12:16:30.148406 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:30.148406 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m12:16:30.153411 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-a5d5-1590-9012-b70f9cd8b571
[0m12:16:30.388523 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F44E28190>]}
[0m12:16:30.388523 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.76s]
[0m12:16:30.388523 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m12:16:30.388523 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m12:16:30.388523 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m12:16:30.388523 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m12:16:30.398282 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m12:16:30.398282 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m12:16:30.398282 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 12:16:30.398282 => 12:16:30.398282
[0m12:16:30.403561 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m12:16:30.408312 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m12:16:30.408312 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:30.411297 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m12:16:30.411297 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m12:16:30.412434 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:16:31.248463 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-a6e2-1670-bb94-50f9865f8e25
[0m12:16:31.948477 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m12:16:31.948477 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 12:16:30.403561 => 12:16:31.948477
[0m12:16:31.958409 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m12:16:31.958409 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:31.958409 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m12:16:31.958409 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-a6e2-1670-bb94-50f9865f8e25
[0m12:16:32.188892 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F44D37010>]}
[0m12:16:32.188892 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.80s]
[0m12:16:32.198630 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m12:16:32.198630 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m12:16:32.198630 [info ] [Thread-1 (]: 10 of 12 START sql view model default.dim_localidade ........................... [RUN]
[0m12:16:32.198630 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m12:16:32.198630 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m12:16:32.214411 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m12:16:32.214411 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 12:16:32.206604 => 12:16:32.214411
[0m12:16:32.214411 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m12:16:32.218451 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m12:16:32.218451 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:32.218451 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m12:16:32.218451 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    

WITH locais AS (
    SELECT DISTINCT
        cod_municipio,
        local_nascimento
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    cod_municipio,
    local_nascimento
FROM locais

[0m12:16:32.223456 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:16:33.043156 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-a7f7-1a67-9f70-72d26536d3d3
[0m12:16:33.738800 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m12:16:33.748512 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 12:16:32.214411 => 12:16:33.748512
[0m12:16:33.748512 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m12:16:33.748512 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:33.748512 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m12:16:33.748512 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-a7f7-1a67-9f70-72d26536d3d3
[0m12:16:33.988579 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F44E2F710>]}
[0m12:16:33.998652 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.dim_localidade ...................... [[32mOK[0m in 1.79s]
[0m12:16:33.998652 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m12:16:33.998652 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m12:16:33.998652 [info ] [Thread-1 (]: 11 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m12:16:33.998652 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m12:16:33.998652 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m12:16:34.008394 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m12:16:34.010584 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 12:16:33.998652 => 12:16:34.008394
[0m12:16:34.011594 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m12:16:34.016371 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m12:16:34.018413 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:34.018413 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m12:16:34.018413 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

WITH datas AS (
    SELECT DISTINCT
        data_nascimento AS data
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    ROW_NUMBER() OVER (ORDER BY data) AS data_id,
    data,
    EXTRACT(YEAR FROM data) AS ano,
    EXTRACT(MONTH FROM data) AS mes,
    EXTRACT(DAY FROM data) AS dia,
    DAYOFWEEK(data) AS dia_semana
FROM datas

[0m12:16:34.018413 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:16:34.813534 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-a906-1988-86d4-b63e6e7d5ef1
[0m12:16:35.503362 [debug] [Thread-1 (]: SQL status: OK in 1.4800000190734863 seconds
[0m12:16:35.508525 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 12:16:34.011594 => 12:16:35.508525
[0m12:16:35.508525 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m12:16:35.508525 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:35.508525 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m12:16:35.518508 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-a906-1988-86d4-b63e6e7d5ef1
[0m12:16:35.758511 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F44DB6790>]}
[0m12:16:35.763563 [info ] [Thread-1 (]: 11 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 1.76s]
[0m12:16:35.763563 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m12:16:35.763563 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m12:16:35.763563 [info ] [Thread-1 (]: 12 of 12 START sql view model default.fato_nascimento .......................... [RUN]
[0m12:16:35.768630 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_nascimento)
[0m12:16:35.768630 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m12:16:35.768630 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m12:16:35.768630 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 12:16:35.768630 => 12:16:35.768630
[0m12:16:35.768630 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m12:16:35.778363 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m12:16:35.778363 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:35.778363 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m12:16:35.778363 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    i.nascimento_id,
    t.data_id,
    l.cod_municipio,
    i.sexo,
    COALESCE(i.peso, 0) AS peso,
    i.idade_mae,
    COALESCE(i.gestacao_semanas, 0) AS gestacao_semanas,
    i.tipo_parto,
    COALESCE(i.APGAR1, 0) AS APGAR1,
    COALESCE(i.APGAR5, 0) AS APGAR5
FROM `workspace`.`default`.`int_nascimento` AS i
LEFT JOIN `workspace`.`default`.`dim_tempo` AS t
    ON i.data_nascimento = t.data
LEFT JOIN `workspace`.`default`.`dim_localidade` AS l
    ON i.cod_municipio = l.cod_municipio

[0m12:16:35.778363 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:16:36.548706 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-aa11-18bf-9502-9057383ed001
[0m12:16:37.358720 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m12:16:37.370312 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 12:16:35.768630 => 12:16:37.369303
[0m12:16:37.370312 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m12:16:37.371326 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:16:37.371326 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m12:16:37.372323 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-aa11-18bf-9502-9057383ed001
[0m12:16:37.638640 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f5e8668-90df-4a61-bb81-61dc016c0d40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F45EBB2D0>]}
[0m12:16:37.638640 [info ] [Thread-1 (]: 12 of 12 OK created sql view model default.fato_nascimento ..................... [[32mOK[0m in 1.88s]
[0m12:16:37.638640 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m12:16:37.638640 [debug] [MainThread]: On master: ROLLBACK
[0m12:16:37.638640 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:16:38.678397 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07921-ab42-1e8a-8ef2-fcc5c3981e02
[0m12:16:38.678397 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:16:38.678397 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:16:38.678397 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:16:38.678397 [debug] [MainThread]: On master: ROLLBACK
[0m12:16:38.678397 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:16:38.678397 [debug] [MainThread]: On master: Close
[0m12:16:38.678397 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07921-ab42-1e8a-8ef2-fcc5c3981e02
[0m12:16:38.928993 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:16:38.928993 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m12:16:38.928993 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m12:16:38.928993 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m12:16:38.928993 [info ] [MainThread]: 
[0m12:16:38.928993 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 26.90 seconds (26.90s).
[0m12:16:38.938515 [debug] [MainThread]: Command end result
[0m12:16:38.948284 [info ] [MainThread]: 
[0m12:16:38.948284 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:16:38.948284 [info ] [MainThread]: 
[0m12:16:38.954488 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 TOTAL=12
[0m12:16:38.956305 [debug] [MainThread]: Command `dbt run` succeeded at 12:16:38.955485 after 28.65 seconds
[0m12:16:38.956758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F3B261150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F3522DE10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F34F8BB10>]}
[0m12:16:38.958334 [debug] [MainThread]: Flushing usage events
[0m12:17:04.528162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDA4A68D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DD9C3A350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DD9BD80D0>]}


============================== 12:17:04.528162 | 05373565-36b8-4048-9ac2-ec4102eaf7bd ==============================
[0m12:17:04.528162 [info ] [MainThread]: Running with dbt=1.5.2
[0m12:17:04.528162 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m12:17:05.838043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '05373565-36b8-4048-9ac2-ec4102eaf7bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DD9B89E10>]}
[0m12:17:05.848532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '05373565-36b8-4048-9ac2-ec4102eaf7bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DD9B89E10>]}
[0m12:17:05.848532 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m12:17:05.873225 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m12:17:05.988314 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:17:05.988314 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:17:05.993319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '05373565-36b8-4048-9ac2-ec4102eaf7bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DE47E2110>]}
[0m12:17:06.058432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '05373565-36b8-4048-9ac2-ec4102eaf7bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DE3674090>]}
[0m12:17:06.058432 [info ] [MainThread]: Found 12 models, 15 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m12:17:06.058432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05373565-36b8-4048-9ac2-ec4102eaf7bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DE364EF10>]}
[0m12:17:06.058432 [info ] [MainThread]: 
[0m12:17:06.068323 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:17:06.068323 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m12:17:06.068323 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m12:17:06.078583 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m12:17:06.079580 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:17:06.908618 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07921-bc29-1b3f-b500-45d7508f131c
[0m12:17:07.328415 [debug] [ThreadPool]: SQL status: OK in 1.25 seconds
[0m12:17:07.338323 [debug] [ThreadPool]: On list_workspace_default: Close
[0m12:17:07.338323 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07921-bc29-1b3f-b500-45d7508f131c
[0m12:17:07.598586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05373565-36b8-4048-9ac2-ec4102eaf7bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DE2DC5550>]}
[0m12:17:07.608282 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:07.608357 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:17:07.608357 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:17:07.608357 [info ] [MainThread]: 
[0m12:17:07.617348 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m12:17:07.618405 [info ] [Thread-1 (]: 1 of 15 START test accepted_values_fato_nascimento_sexo__M__F .................. [RUN]
[0m12:17:07.620025 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3'
[0m12:17:07.621024 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m12:17:07.635833 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m12:17:07.638189 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (compile): 12:17:07.621024 => 12:17:07.638189
[0m12:17:07.638189 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m12:17:07.658213 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m12:17:07.663217 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:07.663217 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m12:17:07.663217 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'M','F'
)



      
    ) dbt_internal_test
[0m12:17:07.663217 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m12:17:08.466937 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-bd16-1fe0-a7d6-98a9e812213b
[0m12:17:09.178772 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'M','F'
)



      
    ) dbt_internal_test
[0m12:17:09.178772 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

[0m12:17:09.178772 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [CAST_INVALID_INPUT] org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
== SQL (line 24, position 19) ==
where value_field not in (
                  ^^^^^^^^
    'M','F'
^^^^^^^^^^^
)
^

	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
	at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)
	at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toLongExact(UTF8StringUtils.scala:31)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$2(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$2$adapted(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.buildCast(Cast.scala:683)
	at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToLong$1(Cast.scala:948)
	at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:1408)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:730)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:89)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:109)
	at scala.collection.immutable.Vector1.map(Vector.scala:2141)
	at scala.collection.immutable.Vector1.map(Vector.scala:386)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:760)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:109)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:109)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:712)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:109)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.$anonfun$applyOrElse$1(expressions.scala:113)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:230)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:242)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:253)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:113)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:112)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$4(TreeNode.scala:536)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1331)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1705)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:536)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:487)
	at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.apply(expressions.scala:112)
	at org.apache.spark.sql.execution.qrc.Canonicalized$.applyRules(ResultCacheManager.scala:131)
	at org.apache.spark.sql.execution.qrc.Canonicalized$.apply(ResultCacheManager.scala:122)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$lookupCache$4(ResultCacheManager.scala:514)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$lookupCache$4$adapted(ResultCacheManager.scala:514)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:146)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:142)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.recordLatency(ResultCacheManager.scala:305)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.lookupCache(ResultCacheManager.scala:514)
	at org.apache.spark.sql.execution.qrc.ResultCacheManager.lookupHybridResult(ResultCacheManager.scala:413)
	at org.apache.spark.sql.classic.Dataset.lookupHybridResultFromCache(Dataset.scala:1813)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.lookupResult(HybridCloudStoreResultHandler.scala:69)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.$anonfun$collectResult$1(ResultCollector.scala:101)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:585)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult(ResultCollector.scala:101)
	at org.apache.spark.sql.hive.thriftserver.ResultCollector.collectResult$(ResultCollector.scala:98)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.collectResult(HybridCloudStoreResultHandler.scala:33)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler.org$apache$spark$sql$hive$thriftserver$HybridCloudStoreResultHandler$$initFromDataFrame(HybridCloudStoreResultHandler.scala:85)
	at org.apache.spark.sql.hive.thriftserver.HybridCloudStoreResultHandler$.createFromDataFrame(HybridCloudStoreResultHandler.scala:214)
	at org.apache.spark.sql.hive.thriftserver.ResultHandlerFactory.getCollectorHandler(ResultHandlerFactory.scala:473)
	at org.apache.spark.sql.hive.thriftserver.ResultHandlerFactory.createResultHandler(ResultHandlerFactory.scala:443)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$18(SparkExecuteStatementOperation.scala:959)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withSuspendedQueryHangingDetection(SparkExecuteStatementOperation.scala:182)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:956)
	... 53 more

[0m12:17:09.188273 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07921-bd36-1e28-8ff9-920b265631d5
[0m12:17:09.188273 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (execute): 12:17:07.638189 => 12:17:09.188273
[0m12:17:09.188273 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: ROLLBACK
[0m12:17:09.188273 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:09.188273 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: Close
[0m12:17:09.188273 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-bd16-1fe0-a7d6-98a9e812213b
[0m12:17:09.438415 [debug] [Thread-1 (]: Runtime Error in test accepted_values_fato_nascimento_sexo__M__F (models\marts\schema.yaml)
  [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  == SQL (line 24, position 19) ==
  where value_field not in (
                    ^^^^^^^^
      'M','F'
  ^^^^^^^^^^^
  )
  ^
  
[0m12:17:09.438415 [error] [Thread-1 (]: 1 of 15 ERROR accepted_values_fato_nascimento_sexo__M__F ....................... [[31mERROR[0m in 1.82s]
[0m12:17:09.438415 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m12:17:09.438415 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m12:17:09.438415 [info ] [Thread-1 (]: 2 of 15 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m12:17:09.438415 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m12:17:09.438415 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m12:17:09.448357 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m12:17:09.448357 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 12:17:09.438415 => 12:17:09.448357
[0m12:17:09.448357 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m12:17:09.461189 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m12:17:09.461189 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:09.461189 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m12:17:09.461189 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m12:17:09.461189 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:10.248422 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-be25-137b-a6a5-5dc1b98716a1
[0m12:17:10.643441 [debug] [Thread-1 (]: SQL status: OK in 1.1799999475479126 seconds
[0m12:17:10.648555 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 12:17:09.448357 => 12:17:10.648555
[0m12:17:10.648555 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m12:17:10.648555 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:10.648555 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m12:17:10.648555 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-be25-137b-a6a5-5dc1b98716a1
[0m12:17:10.868371 [info ] [Thread-1 (]: 2 of 15 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 1.43s]
[0m12:17:10.879118 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m12:17:10.880143 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m12:17:10.881114 [info ] [Thread-1 (]: 3 of 15 START test not_null_dim_localidade_local_nascimento .................... [RUN]
[0m12:17:10.881770 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305)
[0m12:17:10.882799 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m12:17:10.888463 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m12:17:10.888463 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305 (compile): 12:17:10.882799 => 12:17:10.888463
[0m12:17:10.888463 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m12:17:10.888463 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m12:17:10.888463 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:10.888463 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m12:17:10.888463 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select local_nascimento
from `workspace`.`default`.`dim_localidade`
where local_nascimento is null



      
    ) dbt_internal_test
[0m12:17:10.888463 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:11.688250 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-bf01-1736-88e1-a31dee910f33
[0m12:17:12.068431 [debug] [Thread-1 (]: SQL status: OK in 1.1799999475479126 seconds
[0m12:17:12.068431 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305 (execute): 12:17:10.888463 => 12:17:12.068431
[0m12:17:12.068431 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: ROLLBACK
[0m12:17:12.068431 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:12.078501 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: Close
[0m12:17:12.078501 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-bf01-1736-88e1-a31dee910f33
[0m12:17:12.298642 [info ] [Thread-1 (]: 3 of 15 PASS not_null_dim_localidade_local_nascimento .......................... [[32mPASS[0m in 1.42s]
[0m12:17:12.308459 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m12:17:12.308459 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m12:17:12.308459 [info ] [Thread-1 (]: 4 of 15 START test not_null_dim_tempo_data ..................................... [RUN]
[0m12:17:12.308459 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305, now test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252)
[0m12:17:12.308459 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m12:17:12.323069 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m12:17:12.323069 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (compile): 12:17:12.317331 => 12:17:12.323069
[0m12:17:12.323069 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m12:17:12.328113 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m12:17:12.330147 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:12.330147 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m12:17:12.330147 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data
from `workspace`.`default`.`dim_tempo`
where data is null



      
    ) dbt_internal_test
[0m12:17:12.330147 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:13.130647 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-bfde-1948-9cef-794561bbf4ba
[0m12:17:13.739978 [debug] [Thread-1 (]: SQL status: OK in 1.409999966621399 seconds
[0m12:17:13.748596 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (execute): 12:17:12.323069 => 12:17:13.748596
[0m12:17:13.748596 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: ROLLBACK
[0m12:17:13.748596 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:13.748596 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: Close
[0m12:17:13.748596 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-bfde-1948-9cef-794561bbf4ba
[0m12:17:13.982906 [info ] [Thread-1 (]: 4 of 15 PASS not_null_dim_tempo_data ........................................... [[32mPASS[0m in 1.67s]
[0m12:17:13.982906 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m12:17:13.982906 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m12:17:13.982906 [info ] [Thread-1 (]: 5 of 15 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m12:17:13.988479 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m12:17:13.988479 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m12:17:13.988479 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m12:17:13.998225 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 12:17:13.988479 => 12:17:13.998225
[0m12:17:13.998225 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m12:17:14.001501 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m12:17:14.001501 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:14.001501 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m12:17:14.001501 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m12:17:14.001501 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:14.790320 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-c0da-1884-b1d7-4a78d9c84dab
[0m12:17:15.175500 [debug] [Thread-1 (]: SQL status: OK in 1.1699999570846558 seconds
[0m12:17:15.178553 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 12:17:13.998225 => 12:17:15.178553
[0m12:17:15.178553 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m12:17:15.178553 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:15.178553 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m12:17:15.178553 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-c0da-1884-b1d7-4a78d9c84dab
[0m12:17:15.428800 [info ] [Thread-1 (]: 5 of 15 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 1.44s]
[0m12:17:15.428800 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m12:17:15.438311 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m12:17:15.438311 [info ] [Thread-1 (]: 6 of 15 START test not_null_fato_nascimento_APGAR1 ............................. [RUN]
[0m12:17:15.438311 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074)
[0m12:17:15.438311 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m12:17:15.438311 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m12:17:15.438311 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (compile): 12:17:15.438311 => 12:17:15.438311
[0m12:17:15.448455 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m12:17:15.451593 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m12:17:15.451593 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:15.451593 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m12:17:15.451593 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR1
from `workspace`.`default`.`fato_nascimento`
where APGAR1 is null



      
    ) dbt_internal_test
[0m12:17:15.451593 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:16.258618 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-c1ba-16ab-8680-aea0a3fbaffb
[0m12:17:16.848595 [debug] [Thread-1 (]: SQL status: OK in 1.399999976158142 seconds
[0m12:17:16.858350 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (execute): 12:17:15.448455 => 12:17:16.848595
[0m12:17:16.858350 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: ROLLBACK
[0m12:17:16.858350 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:16.858350 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: Close
[0m12:17:16.858350 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-c1ba-16ab-8680-aea0a3fbaffb
[0m12:17:17.138220 [info ] [Thread-1 (]: 6 of 15 PASS not_null_fato_nascimento_APGAR1 ................................... [[32mPASS[0m in 1.70s]
[0m12:17:17.138220 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m12:17:17.138220 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m12:17:17.138220 [info ] [Thread-1 (]: 7 of 15 START test not_null_fato_nascimento_APGAR5 ............................. [RUN]
[0m12:17:17.143962 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074, now test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994)
[0m12:17:17.144960 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m12:17:17.151968 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m12:17:17.152960 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (compile): 12:17:17.144960 => 12:17:17.152960
[0m12:17:17.153935 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m12:17:17.157924 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m12:17:17.158921 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:17.158921 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m12:17:17.159919 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR5
from `workspace`.`default`.`fato_nascimento`
where APGAR5 is null



      
    ) dbt_internal_test
[0m12:17:17.160917 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:17.938767 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-c2bc-1e90-9ff3-54c1e126d537
[0m12:17:18.563336 [debug] [Thread-1 (]: SQL status: OK in 1.399999976158142 seconds
[0m12:17:18.563336 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (execute): 12:17:17.153935 => 12:17:18.563336
[0m12:17:18.563336 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: ROLLBACK
[0m12:17:18.563336 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:18.563336 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: Close
[0m12:17:18.568438 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-c2bc-1e90-9ff3-54c1e126d537
[0m12:17:18.798385 [info ] [Thread-1 (]: 7 of 15 PASS not_null_fato_nascimento_APGAR5 ................................... [[32mPASS[0m in 1.66s]
[0m12:17:18.798385 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m12:17:18.798385 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m12:17:18.798385 [info ] [Thread-1 (]: 8 of 15 START test not_null_fato_nascimento_cod_municipio ...................... [RUN]
[0m12:17:18.798385 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m12:17:18.808364 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m12:17:18.817353 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m12:17:18.819319 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 12:17:18.808364 => 12:17:18.818350
[0m12:17:18.819319 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m12:17:18.823337 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m12:17:18.824306 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:18.825303 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m12:17:18.826300 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m12:17:18.827298 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:19.613639 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-c3bc-1095-9dec-cec17ef41fcd
[0m12:17:20.458613 [debug] [Thread-1 (]: SQL status: OK in 1.6299999952316284 seconds
[0m12:17:20.468157 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 12:17:18.820345 => 12:17:20.458613
[0m12:17:20.468157 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m12:17:20.468157 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:20.468157 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m12:17:20.468157 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-c3bc-1095-9dec-cec17ef41fcd
[0m12:17:20.714982 [info ] [Thread-1 (]: 8 of 15 PASS not_null_fato_nascimento_cod_municipio ............................ [[32mPASS[0m in 1.92s]
[0m12:17:20.718563 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m12:17:20.718563 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m12:17:20.718563 [info ] [Thread-1 (]: 9 of 15 START test not_null_fato_nascimento_data_id ............................ [RUN]
[0m12:17:20.718563 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m12:17:20.718563 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m12:17:20.718563 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m12:17:20.728310 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 12:17:20.718563 => 12:17:20.728310
[0m12:17:20.728310 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m12:17:20.733617 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m12:17:20.734614 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:20.734614 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m12:17:20.735584 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m12:17:20.736581 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:21.578539 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-c4e8-123b-8176-085ee3fb2513
[0m12:17:22.598731 [debug] [Thread-1 (]: SQL status: OK in 1.8600000143051147 seconds
[0m12:17:22.610792 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 12:17:20.730597 => 12:17:22.610792
[0m12:17:22.610792 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m12:17:22.613296 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:22.613296 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m12:17:22.613296 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-c4e8-123b-8176-085ee3fb2513
[0m12:17:22.848328 [info ] [Thread-1 (]: 9 of 15 PASS not_null_fato_nascimento_data_id .................................. [[32mPASS[0m in 2.13s]
[0m12:17:22.848328 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m12:17:22.848328 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m12:17:22.858428 [info ] [Thread-1 (]: 10 of 15 START test not_null_fato_nascimento_nascimento_id ..................... [RUN]
[0m12:17:22.858428 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m12:17:22.865931 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m12:17:22.879923 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m12:17:22.882884 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 12:17:22.867924 => 12:17:22.881916
[0m12:17:22.883942 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m12:17:22.888549 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m12:17:22.888549 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:22.888549 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m12:17:22.888549 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m12:17:22.888549 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:23.680109 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-c626-1205-a2b3-1f636676eae5
[0m12:17:24.230553 [debug] [Thread-1 (]: SQL status: OK in 1.340000033378601 seconds
[0m12:17:24.230553 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 12:17:22.883942 => 12:17:24.230553
[0m12:17:24.230553 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m12:17:24.230553 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:24.230553 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m12:17:24.230553 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-c626-1205-a2b3-1f636676eae5
[0m12:17:24.469461 [info ] [Thread-1 (]: 10 of 15 PASS not_null_fato_nascimento_nascimento_id ........................... [[32mPASS[0m in 1.61s]
[0m12:17:24.473616 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m12:17:24.473616 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m12:17:24.478233 [info ] [Thread-1 (]: 11 of 15 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m12:17:24.478233 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m12:17:24.484516 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m12:17:24.496512 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m12:17:24.498507 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 12:17:24.486511 => 12:17:24.498507
[0m12:17:24.499475 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m12:17:24.502552 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m12:17:24.502552 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:24.502552 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m12:17:24.502552 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m12:17:24.502552 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:25.298387 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-c71e-1731-9f6a-e05b458bb1b6
[0m12:17:25.888649 [debug] [Thread-1 (]: SQL status: OK in 1.3899999856948853 seconds
[0m12:17:25.888649 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 12:17:24.500501 => 12:17:25.888649
[0m12:17:25.888649 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m12:17:25.888649 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:25.888649 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m12:17:25.888649 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-c71e-1731-9f6a-e05b458bb1b6
[0m12:17:26.118651 [info ] [Thread-1 (]: 11 of 15 PASS not_null_fato_nascimento_peso .................................... [[32mPASS[0m in 1.64s]
[0m12:17:26.118651 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m12:17:26.128515 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m12:17:26.128515 [info ] [Thread-1 (]: 12 of 15 START test not_null_fato_nascimento_sexo .............................. [RUN]
[0m12:17:26.128515 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m12:17:26.128515 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m12:17:26.140581 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m12:17:26.141550 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 12:17:26.133649 => 12:17:26.141550
[0m12:17:26.142547 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m12:17:26.145568 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m12:17:26.146538 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:26.146538 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m12:17:26.147534 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sexo
from `workspace`.`default`.`fato_nascimento`
where sexo is null



      
    ) dbt_internal_test
[0m12:17:26.147534 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:26.938171 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-c81a-1306-a5b4-8127efbc7c99
[0m12:17:27.578127 [debug] [Thread-1 (]: SQL status: OK in 1.4299999475479126 seconds
[0m12:17:27.583138 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 12:17:26.142547 => 12:17:27.583138
[0m12:17:27.583138 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: ROLLBACK
[0m12:17:27.583138 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:27.583138 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: Close
[0m12:17:27.588214 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-c81a-1306-a5b4-8127efbc7c99
[0m12:17:27.828439 [info ] [Thread-1 (]: 12 of 15 PASS not_null_fato_nascimento_sexo .................................... [[32mPASS[0m in 1.70s]
[0m12:17:27.828439 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m12:17:27.838116 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m12:17:27.838116 [info ] [Thread-1 (]: 13 of 15 START test unique_dim_localidade_cod_municipio ........................ [RUN]
[0m12:17:27.838116 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m12:17:27.844097 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m12:17:27.858223 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m12:17:27.858223 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 12:17:27.845337 => 12:17:27.858223
[0m12:17:27.858223 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m12:17:27.868323 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m12:17:27.868323 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:27.868323 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m12:17:27.871784 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_municipio as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_localidade`
where cod_municipio is not null
group by cod_municipio
having count(*) > 1



      
    ) dbt_internal_test
[0m12:17:27.872782 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:28.658556 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-c91f-138e-af0d-0f8aafe53627
[0m12:17:29.028799 [debug] [Thread-1 (]: SQL status: OK in 1.159999966621399 seconds
[0m12:17:29.038222 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 12:17:27.858223 => 12:17:29.028799
[0m12:17:29.038222 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: ROLLBACK
[0m12:17:29.038222 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:29.038222 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: Close
[0m12:17:29.038222 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-c91f-138e-af0d-0f8aafe53627
[0m12:17:29.258452 [error] [Thread-1 (]: 13 of 15 FAIL 78 unique_dim_localidade_cod_municipio ........................... [[31mFAIL 78[0m in 1.42s]
[0m12:17:29.258452 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m12:17:29.258452 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m12:17:29.268614 [info ] [Thread-1 (]: 14 of 15 START test unique_dim_tempo_data_id ................................... [RUN]
[0m12:17:29.268614 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m12:17:29.275722 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m12:17:29.278451 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m12:17:29.278451 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 12:17:29.277089 => 12:17:29.278451
[0m12:17:29.288589 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m12:17:29.291264 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m12:17:29.291264 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:29.291264 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m12:17:29.291264 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m12:17:29.291264 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:30.078557 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-c9f8-11f4-b736-68d9f306b844
[0m12:17:30.458647 [debug] [Thread-1 (]: SQL status: OK in 1.1699999570846558 seconds
[0m12:17:30.468675 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 12:17:29.288589 => 12:17:30.458647
[0m12:17:30.470671 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m12:17:30.472722 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:30.472722 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m12:17:30.472722 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-c9f8-11f4-b736-68d9f306b844
[0m12:17:30.711967 [info ] [Thread-1 (]: 14 of 15 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 1.44s]
[0m12:17:30.712769 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m12:17:30.713798 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m12:17:30.714767 [info ] [Thread-1 (]: 15 of 15 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m12:17:30.715764 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m12:17:30.716790 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m12:17:30.721775 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m12:17:30.723773 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 12:17:30.716790 => 12:17:30.722772
[0m12:17:30.723773 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m12:17:30.724801 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m12:17:30.728414 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:30.728414 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m12:17:30.728414 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m12:17:30.728414 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m12:17:31.520200 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07921-cad4-1409-ade5-3d467acb9eeb
[0m12:17:32.348525 [debug] [Thread-1 (]: SQL status: OK in 1.6200000047683716 seconds
[0m12:17:32.348525 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 12:17:30.723773 => 12:17:32.348525
[0m12:17:32.348525 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m12:17:32.348525 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m12:17:32.348525 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m12:17:32.348525 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07921-cad4-1409-ade5-3d467acb9eeb
[0m12:17:32.578596 [error] [Thread-1 (]: 15 of 15 FAIL 59968 unique_fato_nascimento_nascimento_id ....................... [[31mFAIL 59968[0m in 1.86s]
[0m12:17:32.578596 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m12:17:32.588419 [debug] [MainThread]: On master: ROLLBACK
[0m12:17:32.588419 [debug] [MainThread]: Opening a new connection, currently in state init
[0m12:17:33.368538 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07921-cbec-1d76-a38f-7d63b8a91bca
[0m12:17:33.378645 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:17:33.378645 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m12:17:33.383156 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m12:17:33.383156 [debug] [MainThread]: On master: ROLLBACK
[0m12:17:33.383156 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m12:17:33.383156 [debug] [MainThread]: On master: Close
[0m12:17:33.383156 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07921-cbec-1d76-a38f-7d63b8a91bca
[0m12:17:33.608882 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:17:33.608882 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m12:17:33.608882 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m12:17:33.608882 [info ] [MainThread]: 
[0m12:17:33.608882 [info ] [MainThread]: Finished running 15 tests in 0 hours 0 minutes and 27.54 seconds (27.54s).
[0m12:17:33.618138 [debug] [MainThread]: Command end result
[0m12:17:33.633214 [info ] [MainThread]: 
[0m12:17:33.633214 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m12:17:33.633214 [info ] [MainThread]: 
[0m12:17:33.638247 [error] [MainThread]: [33mRuntime Error in test accepted_values_fato_nascimento_sexo__M__F (models\marts\schema.yaml)[0m
[0m12:17:33.638247 [error] [MainThread]:   [CAST_INVALID_INPUT] The value 'F' of the type "STRING" cannot be cast to "BIGINT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
[0m12:17:33.638247 [error] [MainThread]:   == SQL (line 24, position 19) ==
[0m12:17:33.638247 [error] [MainThread]:   where value_field not in (
[0m12:17:33.638247 [error] [MainThread]:                     ^^^^^^^^
[0m12:17:33.638247 [error] [MainThread]:       'M','F'
[0m12:17:33.638247 [error] [MainThread]:   ^^^^^^^^^^^
[0m12:17:33.645500 [error] [MainThread]:   )
[0m12:17:33.646495 [error] [MainThread]:   ^
[0m12:17:33.647494 [error] [MainThread]:   
[0m12:17:33.648489 [info ] [MainThread]: 
[0m12:17:33.649486 [error] [MainThread]: [31mFailure in test unique_dim_localidade_cod_municipio (models\marts\schema.yaml)[0m
[0m12:17:33.650484 [error] [MainThread]:   Got 78 results, configured to fail if != 0
[0m12:17:33.651483 [info ] [MainThread]: 
[0m12:17:33.652486 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_dim_localidade_cod_municipio.sql
[0m12:17:33.654008 [info ] [MainThread]: 
[0m12:17:33.654008 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m12:17:33.654008 [error] [MainThread]:   Got 59968 results, configured to fail if != 0
[0m12:17:33.654008 [info ] [MainThread]: 
[0m12:17:33.658558 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m12:17:33.658558 [info ] [MainThread]: 
[0m12:17:33.658558 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=3 SKIP=0 TOTAL=15
[0m12:17:33.661177 [debug] [MainThread]: Command `dbt test` failed at 12:17:33.661177 after 29.15 seconds
[0m12:17:33.662175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DD9E6D290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DD9C2FA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DD9DC90D0>]}
[0m12:17:33.663172 [debug] [MainThread]: Flushing usage events
[0m13:12:48.186623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDB3B3E010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDB4115590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDB15EACD0>]}


============================== 13:12:48.191640 | 8e224df0-6745-45dc-95c5-9536730f9677 ==============================
[0m13:12:48.191640 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:12:48.192636 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:12:49.519556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDB3B29E50>]}
[0m13:12:49.539503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDB3B29E50>]}
[0m13:12:49.540500 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m13:12:49.557454 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m13:12:49.666164 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m13:12:49.667161 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\dim_localidade.sql
[0m13:12:49.667161 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m13:12:49.696085 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m13:12:49.767919 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m13:12:49.773900 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging\stg_nascidos_vivos.sql
[0m13:12:49.836731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDBE840390>]}
[0m13:12:49.849673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDBD6038D0>]}
[0m13:12:49.850671 [info ] [MainThread]: Found 12 models, 15 tests, 0 snapshots, 0 analyses, 464 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m13:12:49.851668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDBABF9AD0>]}
[0m13:12:49.853662 [info ] [MainThread]: 
[0m13:12:49.855657 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:12:49.857653 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m13:12:49.858649 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m13:12:49.858649 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m13:12:49.859646 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:12:51.337058 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07929-8577-1f3d-b3c2-e30c240eb64e
[0m13:12:51.609366 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y)\x85w\x1f=\xb3\xc2\xe3\x0c$\x0e\xb6N'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.2703397274017334/900.0')])
[0m13:12:59.543473 [debug] [ThreadPool]: SQL status: OK in 9.680000305175781 seconds
[0m13:12:59.547622 [debug] [ThreadPool]: On list_workspace: Close
[0m13:12:59.547622 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07929-8577-1f3d-b3c2-e30c240eb64e
[0m13:12:59.849023 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m13:12:59.851018 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m13:12:59.860989 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:12:59.861986 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m13:12:59.861986 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m13:12:59.862984 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:13:00.940234 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07929-8b47-1452-81f0-e90789e9533a
[0m13:13:02.512850 [debug] [ThreadPool]: SQL status: OK in 2.6500000953674316 seconds
[0m13:13:02.514844 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m13:13:02.515842 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m13:13:02.515842 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:13:02.516839 [debug] [ThreadPool]: On create_workspace_default: Close
[0m13:13:02.517836 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07929-8b47-1452-81f0-e90789e9533a
[0m13:13:02.764507 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m13:13:02.769496 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m13:13:02.770493 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m13:13:02.770493 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:13:03.587378 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07929-8cdb-19f4-bb97-2bdb663785f3
[0m13:13:04.132341 [debug] [ThreadPool]: SQL status: OK in 1.3600000143051147 seconds
[0m13:13:04.138325 [debug] [ThreadPool]: On list_workspace_default: Close
[0m13:13:04.139322 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07929-8cdb-19f4-bb97-2bdb663785f3
[0m13:13:04.372192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDBE83A9D0>]}
[0m13:13:04.373162 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:13:04.374164 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:13:04.374164 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:13:04.375509 [info ] [MainThread]: 
[0m13:13:04.383001 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m13:13:04.384001 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m13:13:04.385485 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m13:13:04.386485 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m13:13:04.391469 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m13:13:04.393464 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 13:13:04.386485 => 13:13:04.392494
[0m13:13:04.393464 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m13:13:04.428370 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m13:13:04.429367 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:13:04.430364 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m13:13:04.430364 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m13:13:04.431361 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:13:05.282372 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07929-8ddc-1361-ae83-32b73e163ccb
[0m13:13:08.287250 [debug] [Thread-1 (]: SQL status: OK in 3.859999895095825 seconds
[0m13:13:08.302208 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 13:13:04.394484 => 13:13:08.302208
[0m13:13:08.303205 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m13:13:08.303205 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:13:08.304202 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m13:13:08.305200 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07929-8ddc-1361-ae83-32b73e163ccb
[0m13:13:08.582539 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDB444BB10>]}
[0m13:13:08.583537 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 4.20s]
[0m13:13:08.584882 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m13:13:08.585911 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m13:13:08.586880 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m13:13:08.587653 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m13:13:08.588680 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m13:13:08.591672 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m13:13:08.593648 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 13:13:08.588680 => 13:13:08.592670
[0m13:13:08.593648 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m13:13:08.597657 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m13:13:08.599624 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:13:08.599624 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m13:13:08.600643 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m13:13:08.600643 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:13:09.476438 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07929-9060-17a1-a0c6-831118c094d9
[0m13:13:10.767214 [debug] [Thread-1 (]: SQL status: OK in 2.1700000762939453 seconds
[0m13:13:10.770237 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 13:13:08.594665 => 13:13:10.770237
[0m13:13:10.771204 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m13:13:10.771204 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:13:10.772232 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m13:13:10.773199 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07929-9060-17a1-a0c6-831118c094d9
[0m13:13:11.005177 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDBE663490>]}
[0m13:13:11.008171 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 2.42s]
[0m13:13:11.013084 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m13:13:11.015051 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m13:13:11.016041 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m13:13:11.018006 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m13:13:11.018006 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m13:13:11.024987 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m13:13:11.025962 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 13:13:11.018986 => 13:13:11.025962
[0m13:13:11.026959 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m13:13:11.031946 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m13:13:11.032952 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:13:11.032952 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m13:13:11.033940 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m13:13:11.033940 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:13:11.822097 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07929-91c7-1b49-a8f5-129879d945a9
[0m13:13:14.024804 [debug] [Thread-1 (]: SQL status: OK in 2.990000009536743 seconds
[0m13:13:14.027792 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 13:13:11.027956 => 13:13:14.027792
[0m13:13:14.028789 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m13:13:14.028789 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:13:14.029787 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m13:13:14.030785 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07929-91c7-1b49-a8f5-129879d945a9
[0m13:13:14.255437 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDBD66DBD0>]}
[0m13:13:14.256434 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 3.24s]
[0m13:13:14.257442 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m13:13:14.258428 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m13:13:14.258428 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m13:13:14.260423 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m13:13:14.260423 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m13:13:14.267405 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 13:13:14.261420 => 13:13:14.266407
[0m13:13:14.317271 [debug] [Thread-1 (]: Compilation Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m13:13:14.318268 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDBE663E10>]}
[0m13:13:14.319266 [error] [Thread-1 (]: 4 of 12 ERROR creating sql view model default.stg_nascidos_vivos ............... [[31mERROR[0m in 0.06s]
[0m13:13:14.320014 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m13:13:14.321013 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m13:13:14.322011 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m13:13:14.323705 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m13:13:14.323705 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m13:13:14.327666 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m13:13:14.328663 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 13:13:14.324696 => 13:13:14.328663
[0m13:13:14.329661 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m13:13:14.334648 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m13:13:14.336655 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:13:14.336655 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m13:13:14.337640 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m13:13:14.337640 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:13:15.110204 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07929-93bd-1432-b5f9-c0cb4127dfd8
[0m13:13:15.904111 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m13:13:15.908101 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 13:13:14.330658 => 13:13:15.908101
[0m13:13:15.909098 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m13:13:15.910095 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:13:15.911093 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m13:13:15.911093 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07929-93bd-1432-b5f9-c0cb4127dfd8
[0m13:13:16.149692 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDBD604290>]}
[0m13:13:16.151688 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.83s]
[0m13:13:16.153122 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m13:13:16.154149 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m13:13:16.155118 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m13:13:16.155783 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m13:13:16.156811 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m13:13:16.164792 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m13:13:16.166756 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 13:13:16.157781 => 13:13:16.165787
[0m13:13:16.166756 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m13:13:16.171771 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m13:13:16.172740 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:13:16.172740 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m13:13:16.173766 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m13:13:16.173766 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:13:17.037931 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07929-94da-1609-b4c2-954347db4559
[0m13:13:17.912093 [debug] [Thread-1 (]: SQL status: OK in 1.7400000095367432 seconds
[0m13:13:17.921096 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 13:13:16.167782 => 13:13:17.920099
[0m13:13:17.923065 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m13:13:17.925059 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:13:17.927081 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m13:13:17.929076 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07929-94da-1609-b4c2-954347db4559
[0m13:13:18.183500 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDB4449DD0>]}
[0m13:13:18.184498 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 2.03s]
[0m13:13:18.185961 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m13:13:18.186990 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m13:13:18.187963 [info ] [Thread-1 (]: 7 of 12 SKIP relation default.int_nascimento ................................... [[33mSKIP[0m]
[0m13:13:18.188862 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m13:13:18.189869 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m13:13:18.190858 [info ] [Thread-1 (]: 8 of 12 SKIP relation default.stg_tempo ........................................ [[33mSKIP[0m]
[0m13:13:18.191337 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m13:13:18.192337 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m13:13:18.193336 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m13:13:18.195329 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m13:13:18.195329 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m13:13:18.200343 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m13:13:18.202315 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 13:13:18.196326 => 13:13:18.201341
[0m13:13:18.202315 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m13:13:18.207327 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m13:13:18.208324 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:13:18.208324 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m13:13:18.209319 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m13:13:18.209319 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:13:19.192630 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07929-9624-1edb-8262-7aab119c9f4a
[0m13:13:20.160270 [debug] [Thread-1 (]: SQL status: OK in 1.9500000476837158 seconds
[0m13:13:20.163249 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 13:13:18.203336 => 13:13:20.163249
[0m13:13:20.164219 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m13:13:20.164219 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:13:20.165265 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m13:13:20.165265 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07929-9624-1edb-8262-7aab119c9f4a
[0m13:13:20.391768 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8e224df0-6745-45dc-95c5-9536730f9677', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDBE8CA790>]}
[0m13:13:20.393760 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 2.20s]
[0m13:13:20.394757 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m13:13:20.395782 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m13:13:20.396753 [info ] [Thread-1 (]: 10 of 12 SKIP relation default.dim_localidade .................................. [[33mSKIP[0m]
[0m13:13:20.397759 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m13:13:20.398747 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m13:13:20.399270 [info ] [Thread-1 (]: 11 of 12 SKIP relation default.dim_tempo ....................................... [[33mSKIP[0m]
[0m13:13:20.400269 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m13:13:20.401268 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m13:13:20.402264 [info ] [Thread-1 (]: 12 of 12 SKIP relation default.fato_nascimento ................................. [[33mSKIP[0m]
[0m13:13:20.403263 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m13:13:20.405257 [debug] [MainThread]: On master: ROLLBACK
[0m13:13:20.405257 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:13:21.200572 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07929-975e-1395-9a62-38a2171e989e
[0m13:13:21.201197 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:13:21.202197 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:13:21.202197 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:13:21.203223 [debug] [MainThread]: On master: ROLLBACK
[0m13:13:21.203223 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:13:21.204221 [debug] [MainThread]: On master: Close
[0m13:13:21.204221 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07929-975e-1395-9a62-38a2171e989e
[0m13:13:21.427800 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:13:21.429759 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m13:13:21.430779 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m13:13:21.430779 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_atendimento_hospitalar' was properly closed.
[0m13:13:21.431778 [info ] [MainThread]: 
[0m13:13:21.433067 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 31.58 seconds (31.58s).
[0m13:13:21.436089 [debug] [MainThread]: Command end result
[0m13:13:21.448030 [info ] [MainThread]: 
[0m13:13:21.450024 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:13:21.451023 [info ] [MainThread]: 
[0m13:13:21.452022 [error] [MainThread]: [33mCompilation Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m13:13:21.453017 [error] [MainThread]:   'dbt_utils' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m13:13:21.454014 [info ] [MainThread]: 
[0m13:13:21.455012 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=5 TOTAL=12
[0m13:13:21.456034 [debug] [MainThread]: Command `dbt run` failed at 13:13:21.455012 after 33.29 seconds
[0m13:13:21.456034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDB3C0A3D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDB3E9C5D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDB3B5DF90>]}
[0m13:13:21.457030 [debug] [MainThread]: Flushing usage events
[0m13:16:21.597956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256DE9F0C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256DEA76CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256DEAD8D90>]}


============================== 13:16:21.601951 | 9884873a-8fc3-4122-943a-43bf217100f9 ==============================
[0m13:16:21.601951 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:16:21.603918 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:16:21.633865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9884873a-8fc3-4122-943a-43bf217100f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256DE73D910>]}
[0m13:16:21.635854 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:16:21.636831 [debug] [MainThread]: Command `dbt deps` succeeded at 13:16:21.636831 after 0.06 seconds
[0m13:16:21.636831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256DE71EF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256D858BB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256D883C910>]}
[0m13:16:21.637855 [debug] [MainThread]: Flushing usage events
[0m13:17:04.703952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FBC3DB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208803C9810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020880361510>]}


============================== 13:17:04.709673 | 771e0904-b3da-4dfa-8f7b-fb285d9976d8 ==============================
[0m13:17:04.709673 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:17:04.710674 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:17:04.742614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '771e0904-b3da-4dfa-8f7b-fb285d9976d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020880363490>]}
[0m13:17:04.744581 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:17:04.745595 [debug] [MainThread]: Command `dbt deps` succeeded at 13:17:04.745595 after 0.06 seconds
[0m13:17:04.745595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002088022FF10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208F9DBBB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FA05DE90>]}
[0m13:17:04.746575 [debug] [MainThread]: Flushing usage events
[0m13:17:29.960223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3DE4232D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3DE68B150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3DE6D7010>]}


============================== 13:17:29.965551 | 5f65a621-dda8-4715-9ea1-fb69b8d74918 ==============================
[0m13:17:29.965551 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:17:29.966521 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:17:29.998463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5f65a621-dda8-4715-9ea1-fb69b8d74918', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3DE6AB490>]}
[0m13:17:29.999464 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:17:30.000431 [debug] [MainThread]: Command `dbt deps` succeeded at 13:17:30.000431 after 0.06 seconds
[0m13:17:30.000431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3D820BB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3DE6BC050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3DE6BC4D0>]}
[0m13:17:30.001427 [debug] [MainThread]: Flushing usage events
[0m13:20:04.452037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F44E290C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F44BA6AD50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F44E677350>]}


============================== 13:20:04.456026 | 46831415-dba2-4d3e-a678-b1e9a55be80f ==============================
[0m13:20:04.456026 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:20:04.458025 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:20:04.487963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '46831415-dba2-4d3e-a678-b1e9a55be80f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F44E2AB490>]}
[0m13:20:04.488960 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:20:04.489936 [debug] [MainThread]: Command `dbt deps` succeeded at 13:20:04.489936 after 0.06 seconds
[0m13:20:04.490955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F44E48F350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F44DFBFE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F447D9BB10>]}
[0m13:20:04.491931 [debug] [MainThread]: Flushing usage events
[0m13:21:56.012006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C72110490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C730E32D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C72B3F610>]}


============================== 13:21:56.016992 | 621fda78-1a80-42ca-ada4-1202c673223c ==============================
[0m13:21:56.016992 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:21:56.017975 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:21:56.071823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '621fda78-1a80-42ca-ada4-1202c673223c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C72DE6890>]}
[0m13:21:56.073818 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:21:56.075813 [debug] [MainThread]: Command `dbt deps` succeeded at 13:21:56.075813 after 0.08 seconds
[0m13:21:56.076810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C72B3F610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C6CC9BB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C72E24F90>]}
[0m13:21:56.076810 [debug] [MainThread]: Flushing usage events
[0m13:23:19.219334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013A4494D650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013A44963D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013A449DFE90>]}


============================== 13:23:19.224318 | 16fc2920-73ef-4241-8ca2-67692a677dd3 ==============================
[0m13:23:19.224318 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:23:19.225302 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:23:19.258227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '16fc2920-73ef-4241-8ca2-67692a677dd3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013A44C05990>]}
[0m13:23:19.259227 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:23:19.261192 [debug] [MainThread]: Command `dbt deps` succeeded at 13:23:19.261192 after 0.06 seconds
[0m13:23:19.261192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013A3E72BB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013A4493C490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013A449DFF10>]}
[0m13:23:19.262220 [debug] [MainThread]: Flushing usage events
[0m13:28:12.389500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020050140E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200501D5DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020050152C50>]}


============================== 13:28:12.394516 | d2ab0f25-bee6-4d70-a80c-b8d27152f662 ==============================
[0m13:28:12.394516 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:28:12.395490 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:28:12.427425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd2ab0f25-bee6-4d70-a80c-b8d27152f662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200503BB490>]}
[0m13:28:12.465093 [debug] [MainThread]: Command `dbt clean` succeeded at 13:28:12.465093 after 0.09 seconds
[0m13:28:12.466090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020049F9BB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002004A24C8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002004A1DFDD0>]}
[0m13:28:12.467088 [debug] [MainThread]: Flushing usage events
[0m13:28:33.739137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024290F10CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024293657510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242913B6D90>]}


============================== 13:28:33.744124 | 7d3d6f3f-e4f8-4713-a768-239d6761802b ==============================
[0m13:28:33.744124 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:28:33.745101 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:28:33.778006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7d3d6f3f-e4f8-4713-a768-239d6761802b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000242931E6590>]}
[0m13:28:33.780018 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:28:33.780998 [debug] [MainThread]: Command `dbt deps` succeeded at 13:28:33.780998 after 0.06 seconds
[0m13:28:33.782016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002428D17BB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002429355F550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002429355F750>]}
[0m13:28:33.782016 [debug] [MainThread]: Flushing usage events
[0m13:30:25.608055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001751101B2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017510D19FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017510DD1510>]}


============================== 13:30:25.613069 | f66fc115-b124-4f72-bc72-a24e856d2857 ==============================
[0m13:30:25.613069 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:30:25.615036 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:30:25.645953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f66fc115-b124-4f72-bc72-a24e856d2857', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017511658050>]}
[0m13:30:25.647949 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:30:25.648793 [debug] [MainThread]: Command `dbt deps` succeeded at 13:30:25.648793 after 0.06 seconds
[0m13:30:25.649824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017510DDF050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001750AB3BB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001750ADEC910>]}
[0m13:30:25.650791 [debug] [MainThread]: Flushing usage events
[0m13:31:16.526026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B876F550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B8791510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B8AB9510>]}


============================== 13:31:16.531035 | 27156064-b357-4609-ae46-904f91e7b4b1 ==============================
[0m13:31:16.531035 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:31:16.532031 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:31:16.562927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '27156064-b357-4609-ae46-904f91e7b4b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B8746590>]}
[0m13:31:16.563925 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:31:16.565919 [debug] [MainThread]: Command `dbt deps` succeeded at 13:31:16.565919 after 0.06 seconds
[0m13:31:16.566916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B89FA190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B257BB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226B8500650>]}
[0m13:31:16.566916 [debug] [MainThread]: Flushing usage events
[0m13:42:44.944383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298D3D7B090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298D3D2B150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298D3DD0DD0>]}


============================== 13:42:44.949366 | 609cb345-0188-4cec-b051-6f3a8cf60b2c ==============================
[0m13:42:44.949366 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:42:44.950337 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:42:44.983280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '609cb345-0188-4cec-b051-6f3a8cf60b2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298D3D5B490>]}
[0m13:42:44.984278 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m13:42:44.986240 [debug] [MainThread]: Command `dbt deps` succeeded at 13:42:44.986240 after 0.06 seconds
[0m13:42:44.986240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298D3AAF950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298D3CB5E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000298CDB5D690>]}
[0m13:42:44.987264 [debug] [MainThread]: Flushing usage events
[0m13:46:51.390592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B2956A1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B2928F790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B292C0710>]}


============================== 13:46:51.395608 | 84540701-a340-443c-b1fe-d48bc53667c0 ==============================
[0m13:46:51.395608 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:46:51.396588 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:46:51.429517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '84540701-a340-443c-b1fe-d48bc53667c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B2956A8D0>]}
[0m13:46:51.431495 [debug] [MainThread]: Set downloads directory='C:\Users\Marisa\AppData\Local\Temp\dbt-downloads-zr2sazxq'
[0m13:46:51.432482 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m13:46:51.658090 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m13:46:51.661081 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m13:46:51.946011 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m13:46:51.957967 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m13:46:52.523755 [info ] [MainThread]: Installed from version 1.1.1
[0m13:46:52.524752 [info ] [MainThread]: Updated version available: 1.3.0
[0m13:46:52.524752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '84540701-a340-443c-b1fe-d48bc53667c0', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B2953F7D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B2928FA50>]}
[0m13:46:52.525750 [info ] [MainThread]: 
[0m13:46:52.526747 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m13:46:52.528742 [debug] [MainThread]: Command `dbt deps` succeeded at 13:46:52.527745 after 1.16 seconds
[0m13:46:52.528742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B294924D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B2306BB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B2331C910>]}
[0m13:46:52.529739 [debug] [MainThread]: Flushing usage events
[0m13:48:01.923659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C6F51A150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C6F519F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C6FE33AD0>]}


============================== 13:48:01.928640 | 2397e82b-d81f-4aae-ae96-d4c8f982d854 ==============================
[0m13:48:01.928640 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:48:01.929660 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:48:03.308355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2397e82b-d81f-4aae-ae96-d4c8f982d854', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C6F53D550>]}
[0m13:48:03.332291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2397e82b-d81f-4aae-ae96-d4c8f982d854', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C6FE4B1D0>]}
[0m13:48:03.333288 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m13:48:03.368195 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m13:48:03.370190 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:48:03.371187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2397e82b-d81f-4aae-ae96-d4c8f982d854', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C79065610>]}
[0m13:48:05.319706 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_atendimento.sql
[0m13:48:05.334693 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m13:48:05.338685 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_doenca.sql
[0m13:48:05.343653 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m13:48:05.347658 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m13:48:05.351653 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m13:48:05.356607 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m13:48:05.361000 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_atendimento.sql
[0m13:48:05.364992 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_doenca.sql
[0m13:48:05.368981 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_localidade.sql
[0m13:48:05.372998 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m13:48:05.381652 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  
  Warning: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`. The new macro treats null values differently to empty strings. To restore the behaviour of the original macro, add a global variable in dbt_project.yml called `surrogate_key_treat_nulls_as_empty_strings` to your dbt_project.yml file with a value of True. The projeto_health_insights.stg_nascidos_vivos model triggered this warning. 
  
  > in macro default__surrogate_key (macros\sql\surrogate_key.sql)
  > called by macro surrogate_key (macros\sql\surrogate_key.sql)
  > called by model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
[0m13:48:05.383798 [debug] [MainThread]: Command `dbt run` failed at 13:48:05.383798 after 3.48 seconds
[0m13:48:05.384814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C6F882D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C7A098110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028C7A18A490>]}
[0m13:48:05.384814 [debug] [MainThread]: Flushing usage events
[0m13:50:18.543971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E15EC0E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E16233150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E16198150>]}


============================== 13:50:18.548962 | 6c6e3f3e-88ee-423d-bf65-086c3210e0fb ==============================
[0m13:50:18.548962 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:50:18.549934 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:50:19.846165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E15F3A410>]}
[0m13:50:19.865149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E15F3A410>]}
[0m13:50:19.866144 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m13:50:19.894037 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m13:50:19.896032 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:50:19.897029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E1F9A3A90>]}
[0m13:50:21.431154 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_atendimento.sql
[0m13:50:21.447110 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m13:50:21.451127 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_doenca.sql
[0m13:50:21.455117 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m13:50:21.459861 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m13:50:21.463880 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m13:50:21.467872 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m13:50:21.471858 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_atendimento.sql
[0m13:50:21.475851 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_doenca.sql
[0m13:50:21.479838 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_localidade.sql
[0m13:50:21.483827 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m13:50:21.504771 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging\stg_nascidos_vivos.sql
[0m13:50:21.506766 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_tempo.sql
[0m13:50:21.686258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20A94790>]}
[0m13:50:21.700220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20B6B250>]}
[0m13:50:21.701245 [info ] [MainThread]: Found 12 models, 15 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m13:50:21.702216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E12EB4910>]}
[0m13:50:21.704238 [info ] [MainThread]: 
[0m13:50:21.706633 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:50:21.709291 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m13:50:21.709291 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m13:50:21.710289 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m13:50:21.711286 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:50:23.025514 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0792e-c38c-1752-927b-de1b124cd60a
[0m13:50:23.299024 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y.\xc3\x8c\x17R\x92{\xde\x1b\x12L\xd6\n'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.2715153694152832/900.0')])
[0m13:50:30.981470 [debug] [ThreadPool]: SQL status: OK in 9.270000457763672 seconds
[0m13:50:30.986484 [debug] [ThreadPool]: On list_workspace: Close
[0m13:50:30.987481 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0792e-c38c-1752-927b-de1b124cd60a
[0m13:50:31.234411 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m13:50:31.235425 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m13:50:31.246379 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:31.247376 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m13:50:31.247376 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m13:50:31.248374 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:50:32.160767 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0792e-c914-1c1f-b71b-2ee1f28c39fb
[0m13:50:33.775284 [debug] [ThreadPool]: SQL status: OK in 2.5299999713897705 seconds
[0m13:50:33.776281 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m13:50:33.777278 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m13:50:33.777278 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:50:33.778281 [debug] [ThreadPool]: On create_workspace_default: Close
[0m13:50:33.779259 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0792e-c914-1c1f-b71b-2ee1f28c39fb
[0m13:50:34.012622 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m13:50:34.019603 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m13:50:34.019603 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m13:50:34.020601 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:50:34.810503 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0792e-caaf-11c9-9b52-a708f308440d
[0m13:50:35.314711 [debug] [ThreadPool]: SQL status: OK in 1.2899999618530273 seconds
[0m13:50:35.319700 [debug] [ThreadPool]: On list_workspace_default: Close
[0m13:50:35.319700 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0792e-caaf-11c9-9b52-a708f308440d
[0m13:50:35.572105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20B641D0>]}
[0m13:50:35.573104 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:35.573104 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:50:35.574103 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:50:35.575097 [info ] [MainThread]: 
[0m13:50:35.582934 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m13:50:35.583934 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m13:50:35.584931 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m13:50:35.585928 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m13:50:35.590916 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m13:50:35.593493 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 13:50:35.586926 => 13:50:35.593493
[0m13:50:35.594491 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m13:50:35.632389 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m13:50:35.634385 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:35.636378 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m13:50:35.637382 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m13:50:35.637382 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:50:36.494052 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-cba7-1831-ab55-b09e8b190a9e
[0m13:50:38.629198 [debug] [Thread-1 (]: SQL status: OK in 2.990000009536743 seconds
[0m13:50:38.651126 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 13:50:35.594491 => 13:50:38.650131
[0m13:50:38.651126 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m13:50:38.652125 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:38.652125 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m13:50:38.653128 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-cba7-1831-ab55-b09e8b190a9e
[0m13:50:38.880776 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20AD2790>]}
[0m13:50:38.883765 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 3.30s]
[0m13:50:38.888752 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m13:50:38.890718 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m13:50:38.893700 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m13:50:38.898687 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m13:50:38.900682 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m13:50:38.907661 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m13:50:38.909656 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 13:50:38.901677 => 13:50:38.908659
[0m13:50:38.910653 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m13:50:38.916637 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m13:50:38.917635 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:38.917635 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m13:50:38.918660 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m13:50:38.918660 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:50:39.691970 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-cd97-19eb-b04a-2587dbd3fa28
[0m13:50:40.457754 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m13:50:40.461738 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 13:50:38.910653 => 13:50:40.460741
[0m13:50:40.461738 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m13:50:40.462735 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:40.463705 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m13:50:40.463705 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-cd97-19eb-b04a-2587dbd3fa28
[0m13:50:40.695869 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20C58910>]}
[0m13:50:40.696836 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.80s]
[0m13:50:40.698830 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m13:50:40.699858 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m13:50:40.701821 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m13:50:40.703847 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m13:50:40.703847 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m13:50:40.709830 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m13:50:40.711793 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 13:50:40.704840 => 13:50:40.710797
[0m13:50:40.712791 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m13:50:40.717807 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m13:50:40.719772 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:40.719772 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m13:50:40.720769 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m13:50:40.720769 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:50:41.522789 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-ceb0-12d3-bd4e-553a1c2c0b5c
[0m13:50:42.297170 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m13:50:42.301151 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 13:50:40.712791 => 13:50:42.301151
[0m13:50:42.302151 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m13:50:42.302151 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:42.303146 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m13:50:42.303146 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-ceb0-12d3-bd4e-553a1c2c0b5c
[0m13:50:42.544099 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20C70A50>]}
[0m13:50:42.545094 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.84s]
[0m13:50:42.547089 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m13:50:42.547089 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m13:50:42.548086 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m13:50:42.550109 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m13:50:42.550109 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m13:50:42.559085 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m13:50:42.561052 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 13:50:42.551109 => 13:50:42.561052
[0m13:50:42.562077 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m13:50:42.568066 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m13:50:42.569062 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:42.570028 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m13:50:42.570028 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
    md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    CAST(SEXO AS STRING) AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m13:50:42.571024 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:50:43.364048 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-cfc4-1ee4-acec-1ff093721260
[0m13:50:44.494378 [debug] [Thread-1 (]: SQL status: OK in 1.9199999570846558 seconds
[0m13:50:44.504348 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 13:50:42.562077 => 13:50:44.503350
[0m13:50:44.506308 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m13:50:44.508299 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:44.510299 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m13:50:44.511321 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-cfc4-1ee4-acec-1ff093721260
[0m13:50:44.745146 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20ADE750>]}
[0m13:50:44.747138 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 2.20s]
[0m13:50:44.750132 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m13:50:44.751128 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m13:50:44.753123 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m13:50:44.755117 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m13:50:44.756114 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m13:50:44.762097 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m13:50:44.765089 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 13:50:44.757111 => 13:50:44.764093
[0m13:50:44.765089 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m13:50:44.772098 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m13:50:44.774065 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:44.775063 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m13:50:44.775063 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m13:50:44.776060 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:50:45.543112 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-d115-102a-92de-3faa5c2f590d
[0m13:50:46.405704 [debug] [Thread-1 (]: SQL status: OK in 1.6299999952316284 seconds
[0m13:50:46.408703 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 13:50:44.766115 => 13:50:46.408703
[0m13:50:46.409703 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m13:50:46.410663 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:46.410663 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m13:50:46.411697 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-d115-102a-92de-3faa5c2f590d
[0m13:50:46.653722 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20DF0310>]}
[0m13:50:46.654718 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.90s]
[0m13:50:46.655716 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m13:50:46.656714 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m13:50:46.657711 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m13:50:46.658724 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m13:50:46.659720 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m13:50:46.665690 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m13:50:46.666687 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 13:50:46.659720 => 13:50:46.666687
[0m13:50:46.667684 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m13:50:46.671673 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m13:50:46.705064 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:46.706062 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m13:50:46.706062 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m13:50:46.707060 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:50:47.533699 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-d240-1dc0-b286-e6ced29107f1
[0m13:50:48.364441 [debug] [Thread-1 (]: SQL status: OK in 1.659999966621399 seconds
[0m13:50:48.367405 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 13:50:46.667684 => 13:50:48.367405
[0m13:50:48.368402 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m13:50:48.369400 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:48.369400 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m13:50:48.370397 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-d240-1dc0-b286-e6ced29107f1
[0m13:50:48.601383 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20E0A090>]}
[0m13:50:48.604376 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.94s]
[0m13:50:48.608365 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m13:50:48.611359 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m13:50:48.613316 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m13:50:48.617303 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m13:50:48.619296 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m13:50:48.627303 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m13:50:48.628272 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 13:50:48.620325 => 13:50:48.628272
[0m13:50:48.629297 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m13:50:48.635275 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m13:50:48.636251 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:48.637247 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m13:50:48.638259 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH base AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM base

[0m13:50:48.638259 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:50:49.410034 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-d363-1d1b-91dd-768f90de9d06
[0m13:50:50.192382 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m13:50:50.195375 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 13:50:48.629297 => 13:50:50.194377
[0m13:50:50.195375 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m13:50:50.196344 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:50.197341 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m13:50:50.197341 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-d363-1d1b-91dd-768f90de9d06
[0m13:50:50.421050 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20DD3AD0>]}
[0m13:50:50.424041 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 1.80s]
[0m13:50:50.429023 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m13:50:50.430989 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m13:50:50.433807 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m13:50:50.438798 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m13:50:50.440755 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m13:50:50.445768 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m13:50:50.447733 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 13:50:50.441750 => 13:50:50.447733
[0m13:50:50.448731 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m13:50:50.453717 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m13:50:50.455711 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:50.455711 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m13:50:50.456710 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m13:50:50.456710 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:50:51.207352 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-d475-13d7-9582-ec667f5bfa25
[0m13:50:51.932696 [debug] [Thread-1 (]: SQL status: OK in 1.4800000190734863 seconds
[0m13:50:51.935686 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 13:50:50.448731 => 13:50:51.935686
[0m13:50:51.936683 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m13:50:51.936683 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:51.937680 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m13:50:51.938650 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-d475-13d7-9582-ec667f5bfa25
[0m13:50:52.168982 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20E0A910>]}
[0m13:50:52.170977 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.73s]
[0m13:50:52.172961 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m13:50:52.172961 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m13:50:52.173960 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m13:50:52.175953 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m13:50:52.175953 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m13:50:52.180911 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m13:50:52.182906 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 13:50:52.176950 => 13:50:52.181908
[0m13:50:52.182906 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m13:50:52.188889 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m13:50:52.189886 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:52.190884 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m13:50:52.191882 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m13:50:52.191882 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:50:52.981101 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-d583-189b-b2b8-cf4ec1f52362
[0m13:50:53.745653 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m13:50:53.748636 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 13:50:52.183903 => 13:50:53.748636
[0m13:50:53.749633 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m13:50:53.749633 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:53.750630 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m13:50:53.751599 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-d583-189b-b2b8-cf4ec1f52362
[0m13:50:53.998138 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20ADF2D0>]}
[0m13:50:53.999134 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.82s]
[0m13:50:53.999661 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m13:50:54.000688 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m13:50:54.001609 [info ] [Thread-1 (]: 10 of 12 START sql view model default.dim_localidade ........................... [RUN]
[0m13:50:54.002609 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m13:50:54.003635 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m13:50:54.007624 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m13:50:54.009594 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 13:50:54.003635 => 13:50:54.009594
[0m13:50:54.010588 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m13:50:54.018594 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m13:50:54.019641 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:54.020662 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m13:50:54.022636 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    

SELECT
    cod_municipio,
    ANY_VALUE(local_nascimento) AS municipio -- Pega qualquer valor de local_nascimento
FROM `workspace`.`default`.`int_nascimento`
GROUP BY cod_municipio

[0m13:50:54.022636 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:50:55.319292 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-d6e8-1c7d-8e39-ce70b2169dfb
[0m13:50:56.089509 [debug] [Thread-1 (]: SQL status: OK in 2.069999933242798 seconds
[0m13:50:56.095493 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 13:50:54.010588 => 13:50:56.094496
[0m13:50:56.096458 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m13:50:56.097473 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:56.098483 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m13:50:56.099464 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-d6e8-1c7d-8e39-ce70b2169dfb
[0m13:50:56.315612 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20ADFC10>]}
[0m13:50:56.318599 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.dim_localidade ...................... [[32mOK[0m in 2.31s]
[0m13:50:56.319597 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m13:50:56.320623 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m13:50:56.321591 [info ] [Thread-1 (]: 11 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m13:50:56.322402 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m13:50:56.323431 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m13:50:56.328417 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m13:50:56.329387 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 13:50:56.323431 => 13:50:56.329387
[0m13:50:56.330414 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m13:50:56.335398 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m13:50:56.337366 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:56.338362 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m13:50:56.338362 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

WITH datas AS (
    SELECT DISTINCT
        data_nascimento AS data
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    ROW_NUMBER() OVER (ORDER BY data) AS data_id,
    data,
    EXTRACT(YEAR FROM data) AS ano,
    EXTRACT(MONTH FROM data) AS mes,
    EXTRACT(DAY FROM data) AS dia,
    DAYOFWEEK(data) AS dia_semana
FROM datas

[0m13:50:56.339360 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:50:57.157766 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-d7fe-14d1-8c64-d04e252e30b0
[0m13:50:57.922943 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m13:50:57.925933 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 13:50:56.330414 => 13:50:57.924935
[0m13:50:57.925933 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m13:50:57.926954 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:57.927928 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m13:50:57.927928 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-d7fe-14d1-8c64-d04e252e30b0
[0m13:50:58.180148 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20AF4F50>]}
[0m13:50:58.181118 [info ] [Thread-1 (]: 11 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 1.86s]
[0m13:50:58.182115 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m13:50:58.183116 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m13:50:58.184122 [info ] [Thread-1 (]: 12 of 12 START sql view model default.fato_nascimento .......................... [RUN]
[0m13:50:58.185107 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_nascimento)
[0m13:50:58.186132 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m13:50:58.191123 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m13:50:58.192088 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 13:50:58.187100 => 13:50:58.192088
[0m13:50:58.193086 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m13:50:58.199091 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m13:50:58.200089 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:50:58.201063 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m13:50:58.202062 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    i.nascimento_id,
    t.data_id,
    l.cod_municipio,
    i.sexo,
    COALESCE(i.peso, 0) AS peso,
    i.idade_mae,
    COALESCE(i.gestacao_semanas, 0) AS gestacao_semanas,
    i.tipo_parto,
    COALESCE(i.APGAR1, 0) AS APGAR1,
    COALESCE(i.APGAR5, 0) AS APGAR5
FROM `workspace`.`default`.`int_nascimento` AS i
LEFT JOIN `workspace`.`default`.`dim_tempo` AS t
    ON i.data_nascimento = t.data
LEFT JOIN `workspace`.`default`.`dim_localidade` AS l
    ON i.cod_municipio = l.cod_municipio

[0m13:50:58.202062 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:50:59.003238 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-d91a-14ad-96f9-ccabad0656f5
[0m13:50:59.943446 [debug] [Thread-1 (]: SQL status: OK in 1.7400000095367432 seconds
[0m13:50:59.946439 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 13:50:58.194084 => 13:50:59.946439
[0m13:50:59.947407 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m13:50:59.947407 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:50:59.948432 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m13:50:59.948432 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-d91a-14ad-96f9-ccabad0656f5
[0m13:51:00.456067 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c6e3f3e-88ee-423d-bf65-086c3210e0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E20B641D0>]}
[0m13:51:00.459058 [info ] [Thread-1 (]: 12 of 12 OK created sql view model default.fato_nascimento ..................... [[32mOK[0m in 2.27s]
[0m13:51:00.463041 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m13:51:00.467027 [debug] [MainThread]: On master: ROLLBACK
[0m13:51:00.467027 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:51:01.280143 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0792e-da76-106c-9537-7bad0bc75e09
[0m13:51:01.283138 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:51:01.285131 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:01.286127 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:51:01.286127 [debug] [MainThread]: On master: ROLLBACK
[0m13:51:01.287124 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:51:01.287124 [debug] [MainThread]: On master: Close
[0m13:51:01.288122 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0792e-da76-106c-9537-7bad0bc75e09
[0m13:51:01.528066 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:51:01.530058 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m13:51:01.530058 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m13:51:01.531043 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m13:51:01.532013 [info ] [MainThread]: 
[0m13:51:01.533010 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 39.83 seconds (39.83s).
[0m13:51:01.536998 [debug] [MainThread]: Command end result
[0m13:51:01.553992 [info ] [MainThread]: 
[0m13:51:01.554950 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:51:01.555948 [info ] [MainThread]: 
[0m13:51:01.556946 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 TOTAL=12
[0m13:51:01.557943 [debug] [MainThread]: Command `dbt run` succeeded at 13:51:01.557943 after 43.03 seconds
[0m13:51:01.558968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E0FFDC9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E1613B410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E15E9EFD0>]}
[0m13:51:01.558968 [debug] [MainThread]: Flushing usage events
[0m13:51:31.009403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA95672F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA95BD3A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA95388150>]}


============================== 13:51:31.014420 | c92682eb-fb12-4888-adeb-a2ae041608dd ==============================
[0m13:51:31.014420 [info ] [MainThread]: Running with dbt=1.5.2
[0m13:51:31.015395 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m13:51:32.309755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c92682eb-fb12-4888-adeb-a2ae041608dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA95300CD0>]}
[0m13:51:32.329699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c92682eb-fb12-4888-adeb-a2ae041608dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA9ECD8210>]}
[0m13:51:32.329699 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m13:51:32.357624 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m13:51:32.498249 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:51:32.498249 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:51:32.560084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c92682eb-fb12-4888-adeb-a2ae041608dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA95BEAD50>]}
[0m13:51:32.576043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c92682eb-fb12-4888-adeb-a2ae041608dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA9FE27E50>]}
[0m13:51:32.577011 [info ] [MainThread]: Found 12 models, 15 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m13:51:32.578008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c92682eb-fb12-4888-adeb-a2ae041608dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA9C39A910>]}
[0m13:51:32.581000 [info ] [MainThread]: 
[0m13:51:32.581997 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:51:32.584990 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m13:51:32.591001 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m13:51:32.591998 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m13:51:32.592968 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:51:33.562703 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f0792e-edae-16d8-92c9-0d5da4542a50
[0m13:51:33.958585 [debug] [ThreadPool]: SQL status: OK in 1.3700000047683716 seconds
[0m13:51:33.965895 [debug] [ThreadPool]: On list_workspace_default: Close
[0m13:51:33.966893 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f0792e-edae-16d8-92c9-0d5da4542a50
[0m13:51:34.201432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c92682eb-fb12-4888-adeb-a2ae041608dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA94AFD490>]}
[0m13:51:34.202401 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:34.202401 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:51:34.203427 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:51:34.204450 [info ] [MainThread]: 
[0m13:51:34.212414 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m13:51:34.213387 [info ] [Thread-1 (]: 1 of 15 START test accepted_values_fato_nascimento_sexo__M__F .................. [RUN]
[0m13:51:34.215382 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3'
[0m13:51:34.216379 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m13:51:34.233334 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m13:51:34.236350 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (compile): 13:51:34.216379 => 13:51:34.235328
[0m13:51:34.236350 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m13:51:34.260290 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m13:51:34.262257 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:34.262257 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"
[0m13:51:34.263253 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'M','F'
)



      
    ) dbt_internal_test
[0m13:51:34.265259 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:51:35.022819 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-ee93-1b0e-a5f1-0ca9cc1fb2d6
[0m13:51:41.284484 [debug] [Thread-1 (]: SQL status: OK in 7.019999980926514 seconds
[0m13:51:41.572893 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3 (execute): 13:51:34.237346 => 13:51:41.571894
[0m13:51:41.574885 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: ROLLBACK
[0m13:51:41.577879 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:51:41.579874 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3: Close
[0m13:51:41.580868 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-ee93-1b0e-a5f1-0ca9cc1fb2d6
[0m13:51:41.811828 [error] [Thread-1 (]: 1 of 15 FAIL 3 accepted_values_fato_nascimento_sexo__M__F ...................... [[31mFAIL 3[0m in 7.60s]
[0m13:51:41.812836 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3
[0m13:51:41.813849 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m13:51:41.814819 [info ] [Thread-1 (]: 2 of 15 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m13:51:41.815821 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__M__F.1cff82c7a3, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m13:51:41.816813 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m13:51:41.825790 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m13:51:41.827790 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 13:51:41.816813 => 13:51:41.826786
[0m13:51:41.827790 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m13:51:41.831774 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m13:51:41.833769 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:41.833769 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m13:51:41.834767 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m13:51:41.835770 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:42.629878 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-f31c-1305-9b1c-35614f8d4c2a
[0m13:51:43.364291 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m13:51:43.367311 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 13:51:41.828782 => 13:51:43.367311
[0m13:51:43.368309 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m13:51:43.369278 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:51:43.369278 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m13:51:43.370304 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-f31c-1305-9b1c-35614f8d4c2a
[0m13:51:43.614070 [info ] [Thread-1 (]: 2 of 15 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 1.80s]
[0m13:51:43.616064 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m13:51:43.617061 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m13:51:43.617061 [info ] [Thread-1 (]: 3 of 15 START test not_null_dim_localidade_local_nascimento .................... [RUN]
[0m13:51:43.619056 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305)
[0m13:51:43.620053 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m13:51:43.627063 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m13:51:43.628034 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305 (compile): 13:51:43.620053 => 13:51:43.628034
[0m13:51:43.629029 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m13:51:43.632048 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m13:51:43.633046 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:43.634044 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"
[0m13:51:43.635013 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select local_nascimento
from `workspace`.`default`.`dim_localidade`
where local_nascimento is null



      
    ) dbt_internal_test
[0m13:51:43.635013 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:44.398459 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-f429-1cfd-bbf4-3b8cb04472cd
[0m13:51:44.855919 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select local_nascimento
from `workspace`.`default`.`dim_localidade`
where local_nascimento is null



      
    ) dbt_internal_test
[0m13:51:44.856914 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `local_nascimento` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `municipio`]. SQLSTATE: 42703; line 15 pos 6
[0m13:51:44.857912 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `local_nascimento` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `municipio`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `local_nascimento` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `municipio`]. SQLSTATE: 42703; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m13:51:44.858910 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f0792e-f449-1a7a-968a-d0ed2a4c24d4
[0m13:51:44.859908 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305 (execute): 13:51:43.629029 => 13:51:44.858910
[0m13:51:44.859908 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: ROLLBACK
[0m13:51:44.860901 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:51:44.860901 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305: Close
[0m13:51:44.861899 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-f429-1cfd-bbf4-3b8cb04472cd
[0m13:51:45.094054 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_localidade_local_nascimento (models\marts\schema.yaml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `local_nascimento` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `municipio`]. SQLSTATE: 42703; line 15 pos 6
[0m13:51:45.095049 [error] [Thread-1 (]: 3 of 15 ERROR not_null_dim_localidade_local_nascimento ......................... [[31mERROR[0m in 1.48s]
[0m13:51:45.096046 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305
[0m13:51:45.097041 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m13:51:45.098048 [info ] [Thread-1 (]: 4 of 15 START test not_null_dim_tempo_data ..................................... [RUN]
[0m13:51:45.099036 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_local_nascimento.f6d07f1305, now test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252)
[0m13:51:45.100038 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m13:51:45.106017 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m13:51:45.107045 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (compile): 13:51:45.100038 => 13:51:45.107045
[0m13:51:45.108026 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m13:51:45.112029 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m13:51:45.113995 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:45.113995 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m13:51:45.114993 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data
from `workspace`.`default`.`dim_tempo`
where data is null



      
    ) dbt_internal_test
[0m13:51:45.114993 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:45.984979 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-f51b-164b-a3d4-e8c0051ffb18
[0m13:51:47.123364 [debug] [Thread-1 (]: SQL status: OK in 2.009999990463257 seconds
[0m13:51:47.127385 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (execute): 13:51:45.109014 => 13:51:47.127385
[0m13:51:47.128386 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: ROLLBACK
[0m13:51:47.129379 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:51:47.130376 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: Close
[0m13:51:47.131375 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-f51b-164b-a3d4-e8c0051ffb18
[0m13:51:47.362922 [info ] [Thread-1 (]: 4 of 15 PASS not_null_dim_tempo_data ........................................... [[32mPASS[0m in 2.26s]
[0m13:51:47.364916 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m13:51:47.365914 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m13:51:47.366911 [info ] [Thread-1 (]: 5 of 15 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m13:51:47.368782 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m13:51:47.369777 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m13:51:47.378751 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m13:51:47.380719 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 13:51:47.369777 => 13:51:47.379748
[0m13:51:47.380719 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m13:51:47.385743 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m13:51:47.387707 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:47.387707 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m13:51:47.388723 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m13:51:47.388723 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:48.190917 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-f66d-1535-844c-f7df9e30fc29
[0m13:51:50.356356 [debug] [Thread-1 (]: SQL status: OK in 2.9700000286102295 seconds
[0m13:51:50.365357 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 13:51:47.381742 => 13:51:50.364364
[0m13:51:50.365357 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m13:51:50.366354 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:51:50.367323 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m13:51:50.367323 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-f66d-1535-844c-f7df9e30fc29
[0m13:51:50.597614 [info ] [Thread-1 (]: 5 of 15 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 3.23s]
[0m13:51:50.601604 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m13:51:50.603596 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m13:51:50.606589 [info ] [Thread-1 (]: 6 of 15 START test not_null_fato_nascimento_APGAR1 ............................. [RUN]
[0m13:51:50.610683 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074)
[0m13:51:50.611683 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m13:51:50.621650 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m13:51:50.623615 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (compile): 13:51:50.613645 => 13:51:50.622650
[0m13:51:50.623615 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m13:51:50.626921 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m13:51:50.628851 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:50.628851 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m13:51:50.629873 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR1
from `workspace`.`default`.`fato_nascimento`
where APGAR1 is null



      
    ) dbt_internal_test
[0m13:51:50.630843 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:51.414013 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-f857-19c5-b48b-bfd90ee8a8ba
[0m13:51:52.293268 [debug] [Thread-1 (]: SQL status: OK in 1.659999966621399 seconds
[0m13:51:52.302227 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (execute): 13:51:50.624643 => 13:51:52.302227
[0m13:51:52.303197 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: ROLLBACK
[0m13:51:52.303197 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:51:52.304227 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: Close
[0m13:51:52.304227 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-f857-19c5-b48b-bfd90ee8a8ba
[0m13:51:52.587206 [info ] [Thread-1 (]: 6 of 15 PASS not_null_fato_nascimento_APGAR1 ................................... [[32mPASS[0m in 1.98s]
[0m13:51:52.592196 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m13:51:52.594196 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m13:51:52.595205 [info ] [Thread-1 (]: 7 of 15 START test not_null_fato_nascimento_APGAR5 ............................. [RUN]
[0m13:51:52.597176 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074, now test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994)
[0m13:51:52.597176 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m13:51:52.604156 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m13:51:52.606122 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (compile): 13:51:52.598172 => 13:51:52.606122
[0m13:51:52.607119 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m13:51:52.610139 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m13:51:52.611126 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:52.612133 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m13:51:52.612133 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR5
from `workspace`.`default`.`fato_nascimento`
where APGAR5 is null



      
    ) dbt_internal_test
[0m13:51:52.613103 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:53.550066 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-f99e-1868-9311-3d45c728c47c
[0m13:51:54.356879 [debug] [Thread-1 (]: SQL status: OK in 1.7400000095367432 seconds
[0m13:51:54.363851 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (execute): 13:51:52.607119 => 13:51:54.363851
[0m13:51:54.365847 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: ROLLBACK
[0m13:51:54.366814 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:51:54.367816 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: Close
[0m13:51:54.367816 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-f99e-1868-9311-3d45c728c47c
[0m13:51:54.599844 [info ] [Thread-1 (]: 7 of 15 PASS not_null_fato_nascimento_APGAR5 ................................... [[32mPASS[0m in 2.00s]
[0m13:51:54.601664 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m13:51:54.601664 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m13:51:54.602661 [info ] [Thread-1 (]: 8 of 15 START test not_null_fato_nascimento_cod_municipio ...................... [RUN]
[0m13:51:54.604626 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m13:51:54.605656 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m13:51:54.613602 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m13:51:54.615597 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 13:51:54.606621 => 13:51:54.614599
[0m13:51:54.615597 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m13:51:54.620611 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m13:51:54.622579 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:54.622579 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m13:51:54.623576 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m13:51:54.623576 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:55.410137 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-fab8-1420-83d6-c5f2fcd3ba78
[0m13:51:56.908951 [debug] [Thread-1 (]: SQL status: OK in 2.2899999618530273 seconds
[0m13:51:56.912941 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 13:51:54.616594 => 13:51:56.912941
[0m13:51:56.913938 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m13:51:56.913938 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:51:56.914936 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m13:51:56.915932 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-fab8-1420-83d6-c5f2fcd3ba78
[0m13:51:57.145375 [info ] [Thread-1 (]: 8 of 15 PASS not_null_fato_nascimento_cod_municipio ............................ [[32mPASS[0m in 2.54s]
[0m13:51:57.150368 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m13:51:57.152329 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m13:51:57.154360 [info ] [Thread-1 (]: 9 of 15 START test not_null_fato_nascimento_data_id ............................ [RUN]
[0m13:51:57.159345 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m13:51:57.161339 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m13:51:57.176260 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m13:51:57.178255 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 13:51:57.163296 => 13:51:57.178255
[0m13:51:57.179284 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m13:51:57.183270 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m13:51:57.185235 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:57.185235 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m13:51:57.186257 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m13:51:57.186257 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:57.971886 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-fc40-1903-8686-77e00b97c4b2
[0m13:51:59.590140 [debug] [Thread-1 (]: SQL status: OK in 2.4000000953674316 seconds
[0m13:51:59.594131 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 13:51:57.180249 => 13:51:59.593133
[0m13:51:59.595142 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m13:51:59.595142 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:51:59.596094 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m13:51:59.597123 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-fc40-1903-8686-77e00b97c4b2
[0m13:51:59.856474 [info ] [Thread-1 (]: 9 of 15 PASS not_null_fato_nascimento_data_id .................................. [[32mPASS[0m in 2.70s]
[0m13:51:59.857468 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m13:51:59.858485 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m13:51:59.859459 [info ] [Thread-1 (]: 10 of 15 START test not_null_fato_nascimento_nascimento_id ..................... [RUN]
[0m13:51:59.860457 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m13:51:59.861458 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m13:51:59.867437 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m13:51:59.868435 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 13:51:59.861458 => 13:51:59.868435
[0m13:51:59.869433 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m13:51:59.873422 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m13:51:59.875417 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:59.876415 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m13:51:59.877411 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m13:51:59.877411 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:52:00.754597 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-fde8-1d4a-bae4-6204ed3fe48d
[0m13:52:01.412319 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m13:52:01.416309 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 13:51:59.869433 => 13:52:01.415312
[0m13:52:01.416309 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m13:52:01.417306 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:52:01.418304 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m13:52:01.419330 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-fde8-1d4a-bae4-6204ed3fe48d
[0m13:52:01.649177 [info ] [Thread-1 (]: 10 of 15 PASS not_null_fato_nascimento_nascimento_id ........................... [[32mPASS[0m in 1.79s]
[0m13:52:01.653164 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m13:52:01.656156 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m13:52:01.656156 [info ] [Thread-1 (]: 11 of 15 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m13:52:01.658152 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m13:52:01.658152 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m13:52:01.665132 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m13:52:01.667137 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 13:52:01.659149 => 13:52:01.667137
[0m13:52:01.668131 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m13:52:01.671116 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m13:52:01.673113 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:01.673113 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m13:52:01.674114 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m13:52:01.674114 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:52:02.539416 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792e-fef9-1bc6-bb98-49a4c86ac5fa
[0m13:52:03.238888 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m13:52:03.246864 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 13:52:01.668131 => 13:52:03.245867
[0m13:52:03.247856 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m13:52:03.249856 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:52:03.250851 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m13:52:03.251850 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792e-fef9-1bc6-bb98-49a4c86ac5fa
[0m13:52:03.481254 [info ] [Thread-1 (]: 11 of 15 PASS not_null_fato_nascimento_peso .................................... [[32mPASS[0m in 1.82s]
[0m13:52:03.485243 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m13:52:03.487238 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m13:52:03.489193 [info ] [Thread-1 (]: 12 of 15 START test not_null_fato_nascimento_sexo .............................. [RUN]
[0m13:52:03.494183 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m13:52:03.495177 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m13:52:03.503185 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m13:52:03.504179 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 13:52:03.497185 => 13:52:03.504179
[0m13:52:03.505171 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m13:52:03.508169 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m13:52:03.510148 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:03.511133 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m13:52:03.512132 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sexo
from `workspace`.`default`.`fato_nascimento`
where sexo is null



      
    ) dbt_internal_test
[0m13:52:03.513128 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:52:04.309143 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792f-0007-14e2-a322-ae660e028ab9
[0m13:52:05.164571 [debug] [Thread-1 (]: SQL status: OK in 1.649999976158142 seconds
[0m13:52:05.167561 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 13:52:03.505171 => 13:52:05.167561
[0m13:52:05.168575 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: ROLLBACK
[0m13:52:05.168575 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:52:05.169555 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: Close
[0m13:52:05.170553 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792f-0007-14e2-a322-ae660e028ab9
[0m13:52:05.387403 [info ] [Thread-1 (]: 12 of 15 PASS not_null_fato_nascimento_sexo .................................... [[32mPASS[0m in 1.90s]
[0m13:52:05.389397 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m13:52:05.389397 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m13:52:05.390394 [info ] [Thread-1 (]: 13 of 15 START test unique_dim_localidade_cod_municipio ........................ [RUN]
[0m13:52:05.392389 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m13:52:05.392389 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m13:52:05.401366 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m13:52:05.403360 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 13:52:05.393387 => 13:52:05.403360
[0m13:52:05.404359 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m13:52:05.409349 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m13:52:05.410372 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:05.411338 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m13:52:05.411338 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_municipio as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_localidade`
where cod_municipio is not null
group by cod_municipio
having count(*) > 1



      
    ) dbt_internal_test
[0m13:52:05.412364 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:52:06.220810 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792f-012b-1140-ac92-b917b2fb18a4
[0m13:52:06.797089 [debug] [Thread-1 (]: SQL status: OK in 1.3799999952316284 seconds
[0m13:52:06.800080 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 13:52:05.405356 => 13:52:06.800080
[0m13:52:06.801077 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: ROLLBACK
[0m13:52:06.801077 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:52:06.802074 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: Close
[0m13:52:06.803072 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792f-012b-1140-ac92-b917b2fb18a4
[0m13:52:07.079183 [info ] [Thread-1 (]: 13 of 15 PASS unique_dim_localidade_cod_municipio .............................. [[32mPASS[0m in 1.69s]
[0m13:52:07.082704 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m13:52:07.083704 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m13:52:07.085696 [info ] [Thread-1 (]: 14 of 15 START test unique_dim_tempo_data_id ................................... [RUN]
[0m13:52:07.088658 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m13:52:07.089654 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m13:52:07.101621 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m13:52:07.103616 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 13:52:07.091651 => 13:52:07.103616
[0m13:52:07.104614 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m13:52:07.122565 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m13:52:07.134533 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:07.134533 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m13:52:07.135530 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m13:52:07.136527 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:52:07.922947 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792f-022c-1a27-82f5-4c277809c513
[0m13:52:08.778567 [debug] [Thread-1 (]: SQL status: OK in 1.6399999856948853 seconds
[0m13:52:08.781583 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 13:52:07.105611 => 13:52:08.781583
[0m13:52:08.782579 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m13:52:08.783553 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:52:08.783553 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m13:52:08.784551 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792f-022c-1a27-82f5-4c277809c513
[0m13:52:09.110585 [info ] [Thread-1 (]: 14 of 15 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 2.02s]
[0m13:52:09.111583 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m13:52:09.112580 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m13:52:09.113577 [info ] [Thread-1 (]: 15 of 15 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m13:52:09.114575 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m13:52:09.115571 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m13:52:09.121556 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m13:52:09.123550 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 13:52:09.115571 => 13:52:09.123550
[0m13:52:09.124547 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m13:52:09.127540 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m13:52:09.129534 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:09.129534 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m13:52:09.130531 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m13:52:09.131538 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:52:09.970151 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f0792f-0368-1449-9d02-1054c7b3243b
[0m13:52:11.151487 [debug] [Thread-1 (]: SQL status: OK in 2.0199999809265137 seconds
[0m13:52:11.157469 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 13:52:09.124547 => 13:52:11.157469
[0m13:52:11.159464 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m13:52:11.160460 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m13:52:11.161458 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m13:52:11.163453 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f0792f-0368-1449-9d02-1054c7b3243b
[0m13:52:11.406121 [error] [Thread-1 (]: 15 of 15 FAIL 8 unique_fato_nascimento_nascimento_id ........................... [[31mFAIL 8[0m in 2.29s]
[0m13:52:11.408085 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m13:52:11.412105 [debug] [MainThread]: On master: ROLLBACK
[0m13:52:11.413071 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:52:12.225554 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f0792f-04bd-174f-b253-a79d71a097cc
[0m13:52:12.227547 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:52:12.227547 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:12.228573 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:52:12.229540 [debug] [MainThread]: On master: ROLLBACK
[0m13:52:12.229540 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:52:12.230537 [debug] [MainThread]: On master: Close
[0m13:52:12.231536 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f0792f-04bd-174f-b253-a79d71a097cc
[0m13:52:12.513894 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:52:12.513894 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m13:52:12.514891 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m13:52:12.515889 [info ] [MainThread]: 
[0m13:52:12.516898 [info ] [MainThread]: Finished running 15 tests in 0 hours 0 minutes and 39.93 seconds (39.93s).
[0m13:52:12.520898 [debug] [MainThread]: Command end result
[0m13:52:12.536834 [info ] [MainThread]: 
[0m13:52:12.537831 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m13:52:12.538828 [info ] [MainThread]: 
[0m13:52:12.539826 [error] [MainThread]: [31mFailure in test accepted_values_fato_nascimento_sexo__M__F (models\marts\schema.yaml)[0m
[0m13:52:12.541821 [error] [MainThread]:   Got 3 results, configured to fail if != 0
[0m13:52:12.542818 [info ] [MainThread]: 
[0m13:52:12.543828 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\accepted_values_fato_nascimento_sexo__M__F.sql
[0m13:52:12.544813 [info ] [MainThread]: 
[0m13:52:12.545821 [error] [MainThread]: [33mRuntime Error in test not_null_dim_localidade_local_nascimento (models\marts\schema.yaml)[0m
[0m13:52:12.546807 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `local_nascimento` cannot be resolved. Did you mean one of the following? [`cod_municipio`, `municipio`]. SQLSTATE: 42703; line 15 pos 6
[0m13:52:12.547812 [info ] [MainThread]: 
[0m13:52:12.548802 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m13:52:12.549799 [error] [MainThread]:   Got 8 results, configured to fail if != 0
[0m13:52:12.550797 [info ] [MainThread]: 
[0m13:52:12.551794 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m13:52:12.553788 [info ] [MainThread]: 
[0m13:52:12.554786 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=3 SKIP=0 TOTAL=15
[0m13:52:12.556780 [debug] [MainThread]: Command `dbt test` failed at 13:52:12.555785 after 41.57 seconds
[0m13:52:12.556780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA95861710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA8F39C910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA95BD3F10>]}
[0m13:52:12.557801 [debug] [MainThread]: Flushing usage events
[0m14:09:57.544109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FA64BF050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FA6DB6110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FA654EC50>]}


============================== 14:09:57.549135 | 24e86a43-9a99-4e13-b781-3972f3701c75 ==============================
[0m14:09:57.549135 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:09:57.551112 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:09:59.005929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FA6DC0D10>]}
[0m14:09:59.026770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FAFF4F150>]}
[0m14:09:59.027783 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:09:59.055679 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:09:59.195771 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m14:09:59.251910 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\schema.yaml
[0m14:09:59.252937 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m14:09:59.285770 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m14:09:59.301757 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m14:09:59.306716 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m14:09:59.311732 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m14:09:59.334206 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging\stg_nascidos_vivos.sql
[0m14:09:59.469353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FB10D0AD0>]}
[0m14:09:59.486335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FA6DC0590>]}
[0m14:09:59.487330 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:09:59.488308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FA34B4610>]}
[0m14:09:59.490326 [info ] [MainThread]: 
[0m14:09:59.492292 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:09:59.496280 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m14:09:59.496280 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m14:09:59.497278 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m14:09:59.498276 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:10:00.455352 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07931-8162-135a-bdd9-fb469158f055
[0m14:10:00.654477 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y1\x81b\x13Z\xbd\xd9\xfbF\x91X\xf0U'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '1/30'), ('elapsed-seconds', '0.19517278671264648/900.0')])
[0m14:10:05.919132 [info ] [ThreadPool]: databricks-sql-connector adapter: Retrying request after error in 5 seconds: OrderedDict([('method', 'GetSchemas'), ('session-id', b'\x01\xf0y1\x81b\x13Z\xbd\xd9\xfbF\x91X\xf0U'), ('query-id', None), ('http-code', 503), ('error-message', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('original-exception', 'TEMPORARILY_UNAVAILABLE: Failed to execute SQL query. Please resubmit the query. If autoscaling is enabled, your SQL warehouse should scale up soon.'), ('no-retry-reason', None), ('bounded-retry-delay', 5), ('attempt', '2/30'), ('elapsed-seconds', '5.459827899932861/900.0')])
[0m14:10:12.952892 [debug] [ThreadPool]: SQL status: OK in 13.449999809265137 seconds
[0m14:10:12.961860 [debug] [ThreadPool]: On list_workspace: Close
[0m14:10:12.963853 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07931-8162-135a-bdd9-fb469158f055
[0m14:10:13.220817 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m14:10:13.224810 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m14:10:13.239791 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:10:13.239791 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m14:10:13.240760 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m14:10:13.241759 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:10:14.501541 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07931-89cf-1765-8762-1d6ebd99ab9d
[0m14:10:16.187962 [debug] [ThreadPool]: SQL status: OK in 2.950000047683716 seconds
[0m14:10:16.189957 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:10:16.190954 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m14:10:16.191921 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:10:16.191921 [debug] [ThreadPool]: On create_workspace_default: Close
[0m14:10:16.192919 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07931-89cf-1765-8762-1d6ebd99ab9d
[0m14:10:16.568438 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:10:16.574424 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:10:16.574424 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:10:16.575422 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:10:17.981216 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07931-8bca-1017-a008-50a9bbca658f
[0m14:10:19.442616 [debug] [ThreadPool]: SQL status: OK in 2.869999885559082 seconds
[0m14:10:19.446606 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:10:19.446606 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07931-8bca-1017-a008-50a9bbca658f
[0m14:10:19.851108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FAFF7D890>]}
[0m14:10:19.852105 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:10:19.853102 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:10:19.854130 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:10:19.855097 [info ] [MainThread]: 
[0m14:10:19.863115 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m14:10:19.864116 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m14:10:19.866113 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m14:10:19.867108 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m14:10:19.872094 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m14:10:19.874088 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 14:10:19.868105 => 14:10:19.874088
[0m14:10:19.875086 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m14:10:19.910989 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m14:10:19.911986 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:10:19.911986 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m14:10:19.912984 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m14:10:19.913981 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:10:21.209817 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07931-8dc5-10a0-a833-31630241e7be
[0m14:10:23.167037 [debug] [Thread-1 (]: SQL status: OK in 3.25 seconds
[0m14:10:23.185551 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 14:10:19.875086 => 14:10:23.185551
[0m14:10:23.186549 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m14:10:23.186549 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:10:23.187547 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m14:10:23.188544 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07931-8dc5-10a0-a833-31630241e7be
[0m14:10:23.548254 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FB0FF0CD0>]}
[0m14:10:23.550249 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 3.68s]
[0m14:10:23.553272 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m14:10:23.555267 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m14:10:23.557263 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m14:10:23.560223 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m14:10:23.561220 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m14:10:23.572190 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m14:10:23.574191 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 14:10:23.562216 => 14:10:23.573186
[0m14:10:23.575181 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m14:10:23.582191 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m14:10:23.583191 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:10:23.584191 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m14:10:23.585155 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m14:10:23.586152 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:10:24.514493 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07931-8fc8-12b2-a2d3-044255e533a3
[0m14:10:25.282924 [debug] [Thread-1 (]: SQL status: OK in 1.7000000476837158 seconds
[0m14:10:25.286914 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 14:10:23.576201 => 14:10:25.285916
[0m14:10:25.287911 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m14:10:25.288909 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:10:25.288909 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m14:10:25.289935 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07931-8fc8-12b2-a2d3-044255e533a3
[0m14:10:25.538359 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FB115F1D0>]}
[0m14:10:25.539384 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.98s]
[0m14:10:25.541350 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m14:10:25.541350 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m14:10:25.542347 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m14:10:25.544371 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m14:10:25.544371 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m14:10:25.549329 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m14:10:25.551324 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 14:10:25.545368 => 14:10:25.550327
[0m14:10:25.552321 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m14:10:25.560328 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m14:10:25.561296 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:10:25.562323 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m14:10:25.562323 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m14:10:25.563321 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:10:26.373373 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07931-90e6-1d01-9cc6-5e2ef054db44
[0m14:10:27.093683 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m14:10:27.097664 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 14:10:25.552321 => 14:10:27.096665
[0m14:10:27.097664 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m14:10:27.098632 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:10:27.098632 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m14:10:27.099630 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07931-90e6-1d01-9cc6-5e2ef054db44
[0m14:10:27.443783 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FB13FDF10>]}
[0m14:10:27.445777 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.90s]
[0m14:10:27.447804 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m14:10:27.448801 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:10:27.450796 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m14:10:27.452759 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m14:10:27.453755 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m14:10:27.469743 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:10:27.470741 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 14:10:27.454752 => 14:10:27.470741
[0m14:10:27.471736 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m14:10:27.478716 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:10:27.479714 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:10:27.480711 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:10:27.480711 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
   md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    -- Converte os códigos numéricos para strings legíveis
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m14:10:27.481710 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:10:28.835095 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07931-9250-1a1f-8ee2-ce63e0bcf405
[0m14:10:29.523055 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
   md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    -- Converte os códigos numéricos para strings legíveis
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m14:10:29.525016 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ','. SQLSTATE: 42601 (line 27, pos 485)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
   md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,,
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    -- Converte os códigos numéricos para strings legíveis
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m14:10:29.529005 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ','. SQLSTATE: 42601 (line 27, pos 485)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
   md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,,
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    -- Converte os códigos numéricos para strings legíveis
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near ','. SQLSTATE: 42601 (line 27, pos 485)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
   md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,,
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    -- Converte os códigos numéricos para strings legíveis
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more

[0m14:10:29.530999 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07931-9280-11f0-af76-a25dec7abb5a
[0m14:10:29.530999 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 14:10:27.472704 => 14:10:29.530999
[0m14:10:29.531997 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m14:10:29.532994 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:10:29.532994 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m14:10:29.533991 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07931-9250-1a1f-8ee2-ce63e0bcf405
[0m14:10:29.811184 [debug] [Thread-1 (]: Runtime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near ','. SQLSTATE: 42601 (line 27, pos 485)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
  create or replace view `workspace`.`default`.`stg_nascidos_vivos`
    
    
    as
      
  
  WITH nascidos_vivos_raw AS (
      SELECT
          CODMUNNASC,
          LOCNASC,
          DTNASC,
          HORANASC,
          SEXO,
          PESO,
          IDADEMAE,
          GESTACAO,
          PARTO,
          APGAR1,
          APGAR5,
          CODUFNATU
      FROM default.sinasc_2022_sc_clean
      WHERE CODUFNATU = 42 -- SC
  )
  
  SELECT
     md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,,
  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^
      CODMUNNASC AS cod_municipio,
      LOCNASC AS local_nascimento,
      DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
      HORANASC AS hora_nascimento,
      -- Converte os códigos numéricos para strings legíveis
      CASE
          WHEN SEXO = 1 THEN 'Masculino'
          WHEN SEXO = 2 THEN 'Feminino'
          ELSE 'Ignorado'
      END AS sexo,
      PESO AS peso,
      IDADEMAE AS idade_mae,
      GESTACAO AS gestacao_semanas,
      PARTO AS tipo_parto,
      APGAR1,
      APGAR5
  FROM nascidos_vivos_raw
  
[0m14:10:29.812181 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FB140FD50>]}
[0m14:10:29.813184 [error] [Thread-1 (]: 4 of 12 ERROR creating sql view model default.stg_nascidos_vivos ............... [[31mERROR[0m in 2.36s]
[0m14:10:29.814116 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:10:29.815145 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m14:10:29.816122 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m14:10:29.817136 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m14:10:29.818138 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m14:10:29.822125 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m14:10:29.824093 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 14:10:29.819154 => 14:10:29.823126
[0m14:10:29.825090 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m14:10:29.829107 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m14:10:29.831073 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:10:29.831073 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m14:10:29.832104 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m14:10:29.832104 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:10:31.259516 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07931-93aa-1d69-ba7f-821a5720525d
[0m14:10:32.102798 [debug] [Thread-1 (]: SQL status: OK in 2.2699999809265137 seconds
[0m14:10:32.105785 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 14:10:29.825090 => 14:10:32.105785
[0m14:10:32.106783 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m14:10:32.106783 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:10:32.107782 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m14:10:32.108750 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07931-93aa-1d69-ba7f-821a5720525d
[0m14:10:32.573651 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FB140F890>]}
[0m14:10:32.576643 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 2.76s]
[0m14:10:32.582628 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m14:10:32.584585 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m14:10:32.586580 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m14:10:32.589603 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m14:10:32.590600 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m14:10:32.597551 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m14:10:32.599542 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 14:10:32.591600 => 14:10:32.598563
[0m14:10:32.599542 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m14:10:32.604559 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m14:10:32.605534 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:10:32.605534 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m14:10:32.606552 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m14:10:32.606552 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:10:33.796161 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07931-953a-1ec0-94c7-a7a77b4a2aee
[0m14:10:34.741432 [debug] [Thread-1 (]: SQL status: OK in 2.130000114440918 seconds
[0m14:10:34.745421 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 14:10:32.600588 => 14:10:34.744425
[0m14:10:34.746422 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m14:10:34.746422 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:10:34.747416 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m14:10:34.748384 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07931-953a-1ec0-94c7-a7a77b4a2aee
[0m14:10:35.057228 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FB13FDF10>]}
[0m14:10:35.058226 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 2.47s]
[0m14:10:35.060221 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m14:10:35.061249 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m14:10:35.063245 [info ] [Thread-1 (]: 7 of 12 SKIP relation default.int_nascimento ................................... [[33mSKIP[0m]
[0m14:10:35.065208 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m14:10:35.066204 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m14:10:35.068199 [info ] [Thread-1 (]: 8 of 12 SKIP relation default.stg_tempo ........................................ [[33mSKIP[0m]
[0m14:10:35.070195 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m14:10:35.072190 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:10:35.073217 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m14:10:35.076178 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m14:10:35.078171 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:10:35.085183 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:10:35.087148 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 14:10:35.079170 => 14:10:35.087148
[0m14:10:35.088146 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:10:35.093159 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:10:35.095126 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:10:35.095126 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:10:35.096151 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m14:10:35.096151 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:10:35.918254 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07931-9698-103a-aa31-2bda962e2827
[0m14:10:36.696094 [debug] [Thread-1 (]: SQL status: OK in 1.600000023841858 seconds
[0m14:10:36.700100 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 14:10:35.088146 => 14:10:36.699073
[0m14:10:36.700100 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m14:10:36.701103 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:10:36.701103 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m14:10:36.702095 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07931-9698-103a-aa31-2bda962e2827
[0m14:10:36.941475 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24e86a43-9a99-4e13-b781-3972f3701c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FB13FDF10>]}
[0m14:10:36.942471 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.87s]
[0m14:10:36.945465 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:10:36.946462 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m14:10:36.947459 [info ] [Thread-1 (]: 10 of 12 SKIP relation default.dim_localidade .................................. [[33mSKIP[0m]
[0m14:10:36.949455 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m14:10:36.950477 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m14:10:36.951448 [info ] [Thread-1 (]: 11 of 12 SKIP relation default.dim_tempo ....................................... [[33mSKIP[0m]
[0m14:10:36.953473 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m14:10:36.955438 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m14:10:36.956434 [info ] [Thread-1 (]: 12 of 12 SKIP relation default.fato_nascimento ................................. [[33mSKIP[0m]
[0m14:10:36.956937 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m14:10:36.958937 [debug] [MainThread]: On master: ROLLBACK
[0m14:10:36.959965 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:10:38.270344 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07931-97ea-156d-84b1-cf0844561063
[0m14:10:38.271312 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:10:38.271312 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:10:38.272334 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:10:38.272334 [debug] [MainThread]: On master: ROLLBACK
[0m14:10:38.273307 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:10:38.273307 [debug] [MainThread]: On master: Close
[0m14:10:38.273307 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07931-97ea-156d-84b1-cf0844561063
[0m14:10:38.676657 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:10:38.678659 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m14:10:38.680646 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:10:38.681683 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_atendimento_hospitalar' was properly closed.
[0m14:10:38.686634 [info ] [MainThread]: 
[0m14:10:38.689628 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 39.19 seconds (39.19s).
[0m14:10:38.695605 [debug] [MainThread]: Command end result
[0m14:10:38.719874 [info ] [MainThread]: 
[0m14:10:38.720872 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:10:38.721869 [info ] [MainThread]: 
[0m14:10:38.722867 [error] [MainThread]: [33mRuntime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m14:10:38.724861 [error] [MainThread]:   
[0m14:10:38.725873 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near ','. SQLSTATE: 42601 (line 27, pos 485)
[0m14:10:38.726854 [error] [MainThread]:   
[0m14:10:38.728855 [error] [MainThread]:   == SQL ==
[0m14:10:38.729849 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
[0m14:10:38.730849 [error] [MainThread]:   create or replace view `workspace`.`default`.`stg_nascidos_vivos`
[0m14:10:38.731844 [error] [MainThread]:     
[0m14:10:38.732841 [error] [MainThread]:     
[0m14:10:38.733839 [error] [MainThread]:     as
[0m14:10:38.734836 [error] [MainThread]:       
[0m14:10:38.735833 [error] [MainThread]:   
[0m14:10:38.736830 [error] [MainThread]:   WITH nascidos_vivos_raw AS (
[0m14:10:38.737827 [error] [MainThread]:       SELECT
[0m14:10:38.738832 [error] [MainThread]:           CODMUNNASC,
[0m14:10:38.740820 [error] [MainThread]:           LOCNASC,
[0m14:10:38.741818 [error] [MainThread]:           DTNASC,
[0m14:10:38.742843 [error] [MainThread]:           HORANASC,
[0m14:10:38.743812 [error] [MainThread]:           SEXO,
[0m14:10:38.744839 [error] [MainThread]:           PESO,
[0m14:10:38.745836 [error] [MainThread]:           IDADEMAE,
[0m14:10:38.746804 [error] [MainThread]:           GESTACAO,
[0m14:10:38.747802 [error] [MainThread]:           PARTO,
[0m14:10:38.748798 [error] [MainThread]:           APGAR1,
[0m14:10:38.749797 [error] [MainThread]:           APGAR5,
[0m14:10:38.751791 [error] [MainThread]:           CODUFNATU
[0m14:10:38.752788 [error] [MainThread]:       FROM default.sinasc_2022_sc_clean
[0m14:10:38.754783 [error] [MainThread]:       WHERE CODUFNATU = 42 -- SC
[0m14:10:38.756777 [error] [MainThread]:   )
[0m14:10:38.757780 [error] [MainThread]:   
[0m14:10:38.758772 [error] [MainThread]:   SELECT
[0m14:10:38.759786 [error] [MainThread]:      md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,,
[0m14:10:38.760767 [error] [MainThread]:   -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^
[0m14:10:38.761792 [error] [MainThread]:       CODMUNNASC AS cod_municipio,
[0m14:10:38.763758 [error] [MainThread]:       LOCNASC AS local_nascimento,
[0m14:10:38.764755 [error] [MainThread]:       DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
[0m14:10:38.765759 [error] [MainThread]:       HORANASC AS hora_nascimento,
[0m14:10:38.766750 [error] [MainThread]:       -- Converte os códigos numéricos para strings legíveis
[0m14:10:38.768745 [error] [MainThread]:       CASE
[0m14:10:38.777721 [error] [MainThread]:           WHEN SEXO = 1 THEN 'Masculino'
[0m14:10:38.796670 [error] [MainThread]:           WHEN SEXO = 2 THEN 'Feminino'
[0m14:10:38.797667 [error] [MainThread]:           ELSE 'Ignorado'
[0m14:10:38.798665 [error] [MainThread]:       END AS sexo,
[0m14:10:38.799662 [error] [MainThread]:       PESO AS peso,
[0m14:10:38.801658 [error] [MainThread]:       IDADEMAE AS idade_mae,
[0m14:10:38.802654 [error] [MainThread]:       GESTACAO AS gestacao_semanas,
[0m14:10:38.803652 [error] [MainThread]:       PARTO AS tipo_parto,
[0m14:10:38.804649 [error] [MainThread]:       APGAR1,
[0m14:10:38.805646 [error] [MainThread]:       APGAR5
[0m14:10:38.806643 [error] [MainThread]:   FROM nascidos_vivos_raw
[0m14:10:38.808639 [error] [MainThread]:   
[0m14:10:38.809638 [info ] [MainThread]: 
[0m14:10:38.810633 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=5 TOTAL=12
[0m14:10:38.812628 [debug] [MainThread]: Command `dbt run` failed at 14:10:38.811631 after 41.29 seconds
[0m14:10:38.812628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FA67FC5D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FA4310F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016FA677B190>]}
[0m14:10:38.813625 [debug] [MainThread]: Flushing usage events
[0m14:15:33.841408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019708877950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019707F6FE50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019708255250>]}


============================== 14:15:33.847420 | f6bfb94e-aa83-4578-93b2-aabbb1d74a09 ==============================
[0m14:15:33.847420 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:15:33.848417 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:15:35.319828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f6bfb94e-aa83-4578-93b2-aabbb1d74a09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019707FA2ED0>]}
[0m14:15:35.341798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f6bfb94e-aa83-4578-93b2-aabbb1d74a09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019711A454D0>]}
[0m14:15:35.342795 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:15:35.371225 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:15:35.523774 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:15:35.581590 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m14:15:35.615498 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m14:15:35.649429 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging\stg_nascidos_vivos.sql
[0m14:15:35.667387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f6bfb94e-aa83-4578-93b2-aabbb1d74a09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019712A94750>]}
[0m14:15:35.682319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f6bfb94e-aa83-4578-93b2-aabbb1d74a09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019708885690>]}
[0m14:15:35.683317 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:15:35.684001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6bfb94e-aa83-4578-93b2-aabbb1d74a09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001970F00F3D0>]}
[0m14:15:35.686995 [info ] [MainThread]: 
[0m14:15:35.688990 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:15:35.691982 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m14:15:35.691982 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m14:15:35.692979 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m14:15:35.693977 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:15:37.072298 [info ] [ThreadPool]: databricks-sql-connector adapter: Error during request to server: : Invalid access token.: {"method": "OpenSession", "session-id": null, "query-id": null, "http-code": 403, "error-message": ": Invalid access token.", "original-exception": "", "no-retry-reason": "non-retryable error", "bounded-retry-delay": null, "attempt": "1/30", "elapsed-seconds": "1.358337640762329/900.0"}
[0m14:15:37.076279 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.RequestError'>: Error during request to server: : Invalid access token.
[0m14:15:37.078280 [debug] [ThreadPool]: Databricks adapter: attempt: 1/30
[0m14:15:37.079277 [debug] [ThreadPool]: Databricks adapter: bounded-retry-delay: None
[0m14:15:37.082263 [debug] [ThreadPool]: Databricks adapter: elapsed-seconds: 1.358337640762329/900.0
[0m14:15:37.084227 [debug] [ThreadPool]: Databricks adapter: error-message: : Invalid access token.
[0m14:15:37.087252 [debug] [ThreadPool]: Databricks adapter: http-code: 403
[0m14:15:37.088216 [debug] [ThreadPool]: Databricks adapter: method: OpenSession
[0m14:15:37.089212 [debug] [ThreadPool]: Databricks adapter: no-retry-reason: non-retryable error
[0m14:15:37.091206 [debug] [ThreadPool]: Databricks adapter: original-exception: 
[0m14:15:37.092204 [debug] [ThreadPool]: Databricks adapter: query-id: None
[0m14:15:37.093201 [debug] [ThreadPool]: Databricks adapter: session-id: None
[0m14:15:37.094198 [debug] [ThreadPool]: Databricks adapter: Error while running:
GetSchemas(database=`workspace`, schema=None)
[0m14:15:37.095196 [debug] [ThreadPool]: Databricks adapter: Database Error
  Error during request to server: : Invalid access token.
[0m14:15:37.097190 [debug] [ThreadPool]: On list_workspace: No close available on handle
[0m14:15:37.100182 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:15:37.102176 [debug] [MainThread]: Connection 'list_workspace' was properly closed.
[0m14:15:37.102176 [info ] [MainThread]: 
[0m14:15:37.104170 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 1.41 seconds (1.41s).
[0m14:15:37.105168 [error] [MainThread]: Encountered an error:
Runtime Error
  Database Error
    Error during request to server: : Invalid access token.
[0m14:15:37.106762 [debug] [MainThread]: Command `dbt run` failed at 14:15:37.105833 after 3.29 seconds
[0m14:15:37.106762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019708040C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019708254250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019708255E50>]}
[0m14:15:37.107788 [debug] [MainThread]: Flushing usage events
[0m14:18:41.217434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064F5D0E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064F32F510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064F52B150>]}


============================== 14:18:41.222449 | 1495c494-20db-40b1-a48f-cb1963b3027b ==============================
[0m14:18:41.222449 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:18:41.223417 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:18:42.732231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064FB9E110>]}
[0m14:18:42.752208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064F28F010>]}
[0m14:18:42.753175 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:18:42.783123 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:18:42.816007 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m14:18:42.817006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659F38C10>]}
[0m14:18:44.467855 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_atendimento.sql
[0m14:18:44.484812 [debug] [MainThread]: 1699: static parser successfully parsed intermediate\int_nascimento.sql
[0m14:18:44.489768 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_doenca.sql
[0m14:18:44.494754 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_localidade.sql
[0m14:18:44.499769 [debug] [MainThread]: 1699: static parser successfully parsed marts\dim_tempo.sql
[0m14:18:44.504755 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_atendimento_hospitalar.sql
[0m14:18:44.509716 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m14:18:44.514743 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_atendimento.sql
[0m14:18:44.519717 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_doenca.sql
[0m14:18:44.523705 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_localidade.sql
[0m14:18:44.528664 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m14:18:44.551630 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging\stg_nascidos_vivos.sql
[0m14:18:44.553625 [debug] [MainThread]: 1699: static parser successfully parsed staging\stg_tempo.sql
[0m14:18:44.770046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659EE9D10>]}
[0m14:18:44.786974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659FE9B90>]}
[0m14:18:44.787971 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:18:44.788968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020656349AD0>]}
[0m14:18:44.792957 [info ] [MainThread]: 
[0m14:18:44.794951 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:18:44.798941 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m14:18:44.799938 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m14:18:44.801934 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m14:18:44.802931 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:18:46.133907 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07932-bac1-13c6-839e-0337e391cf48
[0m14:18:46.687918 [debug] [ThreadPool]: SQL status: OK in 1.8899999856948853 seconds
[0m14:18:46.692905 [debug] [ThreadPool]: On list_workspace: Close
[0m14:18:46.692905 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07932-bac1-13c6-839e-0337e391cf48
[0m14:18:47.007252 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m14:18:47.010243 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m14:18:47.034051 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:18:47.035045 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m14:18:47.036043 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m14:18:47.036043 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:18:48.189526 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07932-bbf5-10a0-91ef-9f7bc36cf7a2
[0m14:18:48.687424 [debug] [ThreadPool]: SQL status: OK in 1.649999976158142 seconds
[0m14:18:48.690420 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:18:48.691413 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m14:18:48.692415 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:18:48.693410 [debug] [ThreadPool]: On create_workspace_default: Close
[0m14:18:48.694410 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07932-bbf5-10a0-91ef-9f7bc36cf7a2
[0m14:18:48.976452 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:18:48.987422 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:18:48.988388 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:18:48.988388 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:18:49.957248 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07932-bd03-1f95-acb0-28b055b3e7bd
[0m14:18:50.387277 [debug] [ThreadPool]: SQL status: OK in 1.399999976158142 seconds
[0m14:18:50.397281 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:18:50.398279 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07932-bd03-1f95-acb0-28b055b3e7bd
[0m14:18:50.753712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659DA6BD0>]}
[0m14:18:50.754710 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:18:50.754710 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:18:50.755707 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:18:50.756704 [info ] [MainThread]: 
[0m14:18:50.763566 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m14:18:50.764553 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m14:18:50.766549 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m14:18:50.767569 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m14:18:50.773557 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m14:18:50.774526 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 14:18:50.768543 => 14:18:50.774526
[0m14:18:50.775524 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m14:18:50.816441 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m14:18:50.817411 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:18:50.819406 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m14:18:50.820404 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m14:18:50.821401 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:18:52.229912 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-be68-1532-b762-d8f1ebbce3a3
[0m14:18:54.005866 [debug] [Thread-1 (]: SQL status: OK in 3.180000066757202 seconds
[0m14:18:54.022822 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 14:18:50.775524 => 14:18:54.022822
[0m14:18:54.023818 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m14:18:54.024816 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:18:54.024816 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m14:18:54.025813 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-be68-1532-b762-d8f1ebbce3a3
[0m14:18:54.324033 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659E8A210>]}
[0m14:18:54.325030 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 3.56s]
[0m14:18:54.327026 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m14:18:54.327026 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m14:18:54.328023 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m14:18:54.330019 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m14:18:54.330019 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m14:18:54.335005 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m14:18:54.336001 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 14:18:54.331015 => 14:18:54.336001
[0m14:18:54.336998 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m14:18:54.342982 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m14:18:54.344977 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:18:54.344977 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m14:18:54.345974 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m14:18:54.345974 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:18:55.494246 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-c04a-16e2-a38d-73300eab4e0f
[0m14:18:56.237418 [debug] [Thread-1 (]: SQL status: OK in 1.8899999856948853 seconds
[0m14:18:56.241414 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 14:18:54.336998 => 14:18:56.241414
[0m14:18:56.242401 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m14:18:56.243398 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:18:56.243398 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m14:18:56.244395 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-c04a-16e2-a38d-73300eab4e0f
[0m14:18:56.539801 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065A06DC90>]}
[0m14:18:56.540770 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 2.21s]
[0m14:18:56.541767 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m14:18:56.542767 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m14:18:56.543793 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m14:18:56.544759 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m14:18:56.545756 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m14:18:56.551768 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m14:18:56.552767 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 14:18:56.546775 => 14:18:56.552767
[0m14:18:56.553735 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m14:18:56.561713 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m14:18:56.563708 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:18:56.563708 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m14:18:56.564705 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m14:18:56.565702 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:18:57.980700 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-c1b4-102f-bdfb-a98d7aae557b
[0m14:18:58.862733 [debug] [Thread-1 (]: SQL status: OK in 2.299999952316284 seconds
[0m14:18:58.865727 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 14:18:56.553735 => 14:18:58.865727
[0m14:18:58.866724 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m14:18:58.866724 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:18:58.867719 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m14:18:58.867719 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-c1b4-102f-bdfb-a98d7aae557b
[0m14:18:59.171991 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065A0FEE50>]}
[0m14:18:59.173983 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 2.63s]
[0m14:18:59.177939 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m14:18:59.180934 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:18:59.182927 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m14:18:59.186916 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m14:18:59.188912 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m14:18:59.203895 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:18:59.205862 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 14:18:59.189909 => 14:18:59.205862
[0m14:18:59.206860 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m14:18:59.213842 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:18:59.214844 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:18:59.215835 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:18:59.216838 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
   md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    -- Converte os códigos numéricos para strings legíveis
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m14:18:59.216838 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:19:00.480122 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-c33b-1fdd-8742-7899128ef070
[0m14:19:01.430163 [debug] [Thread-1 (]: SQL status: OK in 2.2100000381469727 seconds
[0m14:19:01.437178 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 14:18:59.206860 => 14:19:01.437178
[0m14:19:01.438167 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m14:19:01.439165 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:19:01.439165 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m14:19:01.440162 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-c33b-1fdd-8742-7899128ef070
[0m14:19:01.831396 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065A0F0910>]}
[0m14:19:01.832392 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 2.65s]
[0m14:19:01.833638 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:19:01.834666 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m14:19:01.835636 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m14:19:01.836633 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m14:19:01.837662 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m14:19:01.842616 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m14:19:01.843614 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 14:19:01.837662 => 14:19:01.843614
[0m14:19:01.844611 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m14:19:01.851436 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m14:19:01.852431 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:19:01.852431 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m14:19:01.853429 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m14:19:01.854398 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:19:03.129308 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-c4e1-14b8-b0f7-13b8667b54cd
[0m14:19:03.999394 [debug] [Thread-1 (]: SQL status: OK in 2.1500000953674316 seconds
[0m14:19:04.002386 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 14:19:01.845436 => 14:19:04.002386
[0m14:19:04.003383 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m14:19:04.004380 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:19:04.004380 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m14:19:04.005378 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-c4e1-14b8-b0f7-13b8667b54cd
[0m14:19:04.380112 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065A0F0910>]}
[0m14:19:04.382110 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 2.54s]
[0m14:19:04.384105 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m14:19:04.385125 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m14:19:04.386099 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m14:19:04.387097 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m14:19:04.388122 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m14:19:04.392111 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m14:19:04.393081 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 14:19:04.388122 => 14:19:04.393081
[0m14:19:04.394106 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m14:19:04.398094 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m14:19:04.400062 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:19:04.401060 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m14:19:04.401060 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m14:19:04.402057 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:19:05.811969 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-c672-1b9c-8f13-d6ae9a7a2629
[0m14:19:06.950181 [debug] [Thread-1 (]: SQL status: OK in 2.549999952316284 seconds
[0m14:19:06.957163 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 14:19:04.394106 => 14:19:06.956164
[0m14:19:06.958159 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m14:19:06.959157 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:19:06.959157 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m14:19:06.960154 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-c672-1b9c-8f13-d6ae9a7a2629
[0m14:19:07.363140 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065A104F90>]}
[0m14:19:07.364138 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 2.98s]
[0m14:19:07.365134 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m14:19:07.366620 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m14:19:07.367609 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m14:19:07.368606 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m14:19:07.370601 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m14:19:07.374615 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m14:19:07.376584 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 14:19:07.370601 => 14:19:07.376584
[0m14:19:07.377581 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m14:19:07.381602 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m14:19:07.383565 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:19:07.383565 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m14:19:07.384596 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH base AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM base

[0m14:19:07.385561 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:19:08.882206 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-c84b-1d0f-bbda-abfa9f650d2d
[0m14:19:09.740973 [debug] [Thread-1 (]: SQL status: OK in 2.359999895095825 seconds
[0m14:19:09.744991 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 14:19:07.377581 => 14:19:09.743993
[0m14:19:09.744991 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m14:19:09.745959 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:19:09.745959 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m14:19:09.746987 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-c84b-1d0f-bbda-abfa9f650d2d
[0m14:19:10.050173 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065A08C850>]}
[0m14:19:10.051171 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 2.68s]
[0m14:19:10.052141 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m14:19:10.053138 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m14:19:10.054164 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m14:19:10.055132 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m14:19:10.056140 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m14:19:10.065127 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m14:19:10.067115 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 14:19:10.057128 => 14:19:10.067115
[0m14:19:10.068127 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m14:19:10.073087 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m14:19:10.075144 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:19:10.076109 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m14:19:10.076109 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m14:19:10.077129 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:19:11.143160 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-c9aa-1138-af08-c34c3a2ed4a9
[0m14:19:11.983157 [debug] [Thread-1 (]: SQL status: OK in 1.909999966621399 seconds
[0m14:19:11.988149 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 14:19:10.068127 => 14:19:11.988149
[0m14:19:11.989144 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m14:19:11.990141 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:19:11.991109 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m14:19:11.992107 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-c9aa-1138-af08-c34c3a2ed4a9
[0m14:19:12.330944 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065A113490>]}
[0m14:19:12.331966 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 2.28s]
[0m14:19:12.333142 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m14:19:12.334171 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:19:12.335139 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m14:19:12.336137 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m14:19:12.337167 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:19:12.340154 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:19:12.341154 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 14:19:12.337167 => 14:19:12.341154
[0m14:19:12.342121 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:19:12.347138 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:19:12.348145 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:19:12.349135 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:19:12.349135 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m14:19:12.350126 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:19:13.412707 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-cb05-1102-b2a1-952ec93ff0c2
[0m14:19:14.463882 [debug] [Thread-1 (]: SQL status: OK in 2.109999895095825 seconds
[0m14:19:14.470865 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 14:19:12.342121 => 14:19:14.469865
[0m14:19:14.471860 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m14:19:14.473855 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:19:14.474852 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m14:19:14.476817 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-cb05-1102-b2a1-952ec93ff0c2
[0m14:19:14.971782 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659E7B450>]}
[0m14:19:14.972779 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 2.64s]
[0m14:19:14.974746 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:19:14.975776 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m14:19:14.976769 [info ] [Thread-1 (]: 10 of 12 START sql view model default.dim_localidade ........................... [RUN]
[0m14:19:14.978735 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m14:19:14.979732 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m14:19:14.985744 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m14:19:14.986713 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 14:19:14.979732 => 14:19:14.986713
[0m14:19:14.987710 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m14:19:14.994692 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m14:19:14.995689 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:19:14.996714 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m14:19:14.996714 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    

SELECT
    cod_municipio,
    ANY_VALUE(local_nascimento) AS municipio -- Pega qualquer valor de local_nascimento
FROM `workspace`.`default`.`int_nascimento`
GROUP BY cod_municipio

[0m14:19:14.997710 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:19:16.055880 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-cc96-14f2-88d5-60ab734c163f
[0m14:19:17.051125 [debug] [Thread-1 (]: SQL status: OK in 2.049999952316284 seconds
[0m14:19:17.061135 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 14:19:14.988706 => 14:19:17.060138
[0m14:19:17.063092 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m14:19:17.065089 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:19:17.066083 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m14:19:17.068079 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-cc96-14f2-88d5-60ab734c163f
[0m14:19:17.319297 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659F8C350>]}
[0m14:19:17.320292 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.dim_localidade ...................... [[32mOK[0m in 2.34s]
[0m14:19:17.320856 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m14:19:17.321886 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m14:19:17.322867 [info ] [Thread-1 (]: 11 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m14:19:17.323863 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m14:19:17.324876 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m14:19:17.328865 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m14:19:17.330860 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 14:19:17.324876 => 14:19:17.330860
[0m14:19:17.331858 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m14:19:17.336816 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m14:19:17.338811 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:19:17.339809 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m14:19:17.339809 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

WITH datas AS (
    SELECT DISTINCT
        data_nascimento AS data
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    ROW_NUMBER() OVER (ORDER BY data) AS data_id,
    data,
    EXTRACT(YEAR FROM data) AS ano,
    EXTRACT(MONTH FROM data) AS mes,
    EXTRACT(DAY FROM data) AS dia,
    DAYOFWEEK(data) AS dia_semana
FROM datas

[0m14:19:17.340837 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:19:18.480295 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-cdf0-1892-9df8-2bf205988130
[0m14:19:19.528316 [debug] [Thread-1 (]: SQL status: OK in 2.190000057220459 seconds
[0m14:19:19.531297 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 14:19:17.331858 => 14:19:19.530297
[0m14:19:19.531297 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m14:19:19.532267 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:19:19.533276 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m14:19:19.534259 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-cdf0-1892-9df8-2bf205988130
[0m14:19:19.826317 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065A0A2CD0>]}
[0m14:19:19.827284 [info ] [Thread-1 (]: 11 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 2.50s]
[0m14:19:19.828280 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m14:19:19.829277 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m14:19:19.830306 [info ] [Thread-1 (]: 12 of 12 START sql view model default.fato_nascimento .......................... [RUN]
[0m14:19:19.831240 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_nascimento)
[0m14:19:19.832269 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m14:19:19.838225 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m14:19:19.839222 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 14:19:19.833267 => 14:19:19.839222
[0m14:19:19.840220 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m14:19:19.848226 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m14:19:19.849195 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:19:19.850193 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m14:19:19.851191 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    i.nascimento_id,
    t.data_id,
    l.cod_municipio,
    i.sexo,
    COALESCE(i.peso, 0) AS peso,
    i.idade_mae,
    COALESCE(i.gestacao_semanas, 0) AS gestacao_semanas,
    i.tipo_parto,
    COALESCE(i.APGAR1, 0) AS APGAR1,
    COALESCE(i.APGAR5, 0) AS APGAR5
FROM `workspace`.`default`.`int_nascimento` AS i
LEFT JOIN `workspace`.`default`.`dim_tempo` AS t
    ON i.data_nascimento = t.data
LEFT JOIN `workspace`.`default`.`dim_localidade` AS l
    ON i.cod_municipio = l.cod_municipio

[0m14:19:19.852187 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:19:21.097565 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-cf86-1fd4-b0d5-c1a82dc54b37
[0m14:19:22.171682 [debug] [Thread-1 (]: SQL status: OK in 2.319999933242798 seconds
[0m14:19:22.180624 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 14:19:19.840220 => 14:19:22.179660
[0m14:19:22.182618 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m14:19:22.184612 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:19:22.186608 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m14:19:22.188602 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-cf86-1fd4-b0d5-c1a82dc54b37
[0m14:19:22.606740 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1495c494-20db-40b1-a48f-cb1963b3027b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065A0181D0>]}
[0m14:19:22.607737 [info ] [Thread-1 (]: 12 of 12 OK created sql view model default.fato_nascimento ..................... [[32mOK[0m in 2.78s]
[0m14:19:22.608735 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m14:19:22.611175 [debug] [MainThread]: On master: ROLLBACK
[0m14:19:22.611175 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:19:24.036221 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07932-d147-14f1-b2f7-4a8e758d0771
[0m14:19:24.038217 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:19:24.039215 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:19:24.039215 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:19:24.040211 [debug] [MainThread]: On master: ROLLBACK
[0m14:19:24.041208 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:19:24.041208 [debug] [MainThread]: On master: Close
[0m14:19:24.042205 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07932-d147-14f1-b2f7-4a8e758d0771
[0m14:19:24.458826 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:19:24.460821 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m14:19:24.462815 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:19:24.463812 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m14:19:24.467802 [info ] [MainThread]: 
[0m14:19:24.469763 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 39.67 seconds (39.67s).
[0m14:19:24.479733 [debug] [MainThread]: Command end result
[0m14:19:24.498681 [info ] [MainThread]: 
[0m14:19:24.499678 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:19:24.501674 [info ] [MainThread]: 
[0m14:19:24.502670 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 TOTAL=12
[0m14:19:24.504666 [debug] [MainThread]: Command `dbt run` succeeded at 14:19:24.503667 after 43.31 seconds
[0m14:19:24.505663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064FB838D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206493FC8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064914BB10>]}
[0m14:19:24.506660 [debug] [MainThread]: Flushing usage events
[0m14:20:13.044176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222BD9BE310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222BDA26D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222BD6FFD50>]}


============================== 14:20:13.048165 | 62ba9dff-6b1b-47f8-a42b-79922b9bd776 ==============================
[0m14:20:13.048165 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:20:13.050133 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:20:14.517242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '62ba9dff-6b1b-47f8-a42b-79922b9bd776', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222BDFC7A10>]}
[0m14:20:14.538186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '62ba9dff-6b1b-47f8-a42b-79922b9bd776', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222C81B9DD0>]}
[0m14:20:14.539162 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:20:14.567086 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:20:14.725146 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:20:14.726140 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:20:14.797949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '62ba9dff-6b1b-47f8-a42b-79922b9bd776', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222BDFE0850>]}
[0m14:20:14.815872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '62ba9dff-6b1b-47f8-a42b-79922b9bd776', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222C822BE50>]}
[0m14:20:14.815872 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:20:14.817228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '62ba9dff-6b1b-47f8-a42b-79922b9bd776', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222C478A450>]}
[0m14:20:14.820252 [info ] [MainThread]: 
[0m14:20:14.822246 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:20:14.825209 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:20:14.831193 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:20:14.832190 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:20:14.832190 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:20:16.064067 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07932-f056-1d4e-a190-a06dfd9a6161
[0m14:20:16.769037 [debug] [ThreadPool]: SQL status: OK in 1.940000057220459 seconds
[0m14:20:16.778040 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:20:16.779038 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07932-f056-1d4e-a190-a06dfd9a6161
[0m14:20:17.019546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '62ba9dff-6b1b-47f8-a42b-79922b9bd776', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222BCEED350>]}
[0m14:20:17.020566 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:17.021568 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:20:17.022568 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:20:17.024532 [info ] [MainThread]: 
[0m14:20:17.032510 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:20:17.033507 [info ] [Thread-1 (]: 1 of 21 START test accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado  [RUN]
[0m14:20:17.035502 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d'
[0m14:20:17.035502 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:20:17.054482 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:20:17.056446 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (compile): 14:20:17.036500 => 14:20:17.055478
[0m14:20:17.056446 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:20:17.082376 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:20:17.084370 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:17.084370 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:20:17.085368 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'Masculino','Feminino','Ignorado'
)



      
    ) dbt_internal_test
[0m14:20:17.086366 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:20:18.179780 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-f193-1517-a7d3-142d4bd1da70
[0m14:20:24.082799 [debug] [Thread-1 (]: SQL status: OK in 7.0 seconds
[0m14:20:24.374766 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (execute): 14:20:17.057443 => 14:20:24.372773
[0m14:20:24.376761 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: ROLLBACK
[0m14:20:24.378758 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:24.379753 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: Close
[0m14:20:24.381748 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-f193-1517-a7d3-142d4bd1da70
[0m14:20:24.645836 [info ] [Thread-1 (]: 1 of 21 PASS accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado  [[32mPASS[0m in 7.61s]
[0m14:20:24.650786 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:20:24.652779 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:20:24.655774 [info ] [Thread-1 (]: 2 of 21 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m14:20:24.659763 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m14:20:24.660758 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:20:24.677740 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:20:24.678737 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 14:20:24.662754 => 14:20:24.678737
[0m14:20:24.679735 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:20:24.683723 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:20:24.686688 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:24.687684 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:20:24.687684 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m14:20:24.688681 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:25.521358 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-f604-1262-9202-342011457048
[0m14:20:26.215157 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m14:20:26.225121 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 14:20:24.679735 => 14:20:26.224124
[0m14:20:26.227087 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m14:20:26.228114 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:26.230109 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m14:20:26.231105 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-f604-1262-9202-342011457048
[0m14:20:26.463024 [info ] [Thread-1 (]: 2 of 21 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 1.80s]
[0m14:20:26.465986 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:20:26.466982 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:20:26.468978 [info ] [Thread-1 (]: 3 of 21 START test not_null_dim_localidade_municipio ........................... [RUN]
[0m14:20:26.472997 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771)
[0m14:20:26.474993 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:20:26.489921 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:20:26.492913 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (compile): 14:20:26.475960 => 14:20:26.491916
[0m14:20:26.493910 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:20:26.497898 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:20:26.500890 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:26.500890 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:20:26.501889 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select municipio
from `workspace`.`default`.`dim_localidade`
where municipio is null



      
    ) dbt_internal_test
[0m14:20:26.502885 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:27.311401 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-f715-1fbf-b8bf-f4e40ed836c8
[0m14:20:28.445917 [debug] [Thread-1 (]: SQL status: OK in 1.940000057220459 seconds
[0m14:20:28.454852 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (execute): 14:20:26.493910 => 14:20:28.453884
[0m14:20:28.456848 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: ROLLBACK
[0m14:20:28.457876 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:28.458872 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: Close
[0m14:20:28.459870 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-f715-1fbf-b8bf-f4e40ed836c8
[0m14:20:28.816987 [info ] [Thread-1 (]: 3 of 21 PASS not_null_dim_localidade_municipio ................................. [[32mPASS[0m in 2.35s]
[0m14:20:28.829952 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:20:28.830949 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:20:28.831947 [info ] [Thread-1 (]: 4 of 21 START test not_null_dim_tempo_ano ...................................... [RUN]
[0m14:20:28.832945 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771, now test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6)
[0m14:20:28.833942 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:20:28.840923 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:20:28.841920 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (compile): 14:20:28.833942 => 14:20:28.841920
[0m14:20:28.842918 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:20:28.849900 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:20:28.851894 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:28.852891 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:20:28.852891 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select ano
from `workspace`.`default`.`dim_tempo`
where ano is null



      
    ) dbt_internal_test
[0m14:20:28.853888 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:30.037651 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-f89e-1db2-a046-2f18de2cedb3
[0m14:20:30.895852 [debug] [Thread-1 (]: SQL status: OK in 2.0399999618530273 seconds
[0m14:20:30.898872 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (execute): 14:20:28.842918 => 14:20:30.898872
[0m14:20:30.899869 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: ROLLBACK
[0m14:20:30.900851 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:30.900851 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: Close
[0m14:20:30.901837 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-f89e-1db2-a046-2f18de2cedb3
[0m14:20:31.149713 [info ] [Thread-1 (]: 4 of 21 PASS not_null_dim_tempo_ano ............................................ [[32mPASS[0m in 2.32s]
[0m14:20:31.154764 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:20:31.156759 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:20:31.159714 [info ] [Thread-1 (]: 5 of 21 START test not_null_dim_tempo_data ..................................... [RUN]
[0m14:20:31.164736 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6, now test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252)
[0m14:20:31.166703 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:20:31.181652 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:20:31.183647 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (compile): 14:20:31.168691 => 14:20:31.182649
[0m14:20:31.184680 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:20:31.188662 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:20:31.189659 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:31.190655 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:20:31.192625 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data
from `workspace`.`default`.`dim_tempo`
where data is null



      
    ) dbt_internal_test
[0m14:20:31.193620 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:31.990987 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-f9e0-1523-b9fb-694820b03b73
[0m14:20:32.715213 [debug] [Thread-1 (]: SQL status: OK in 1.5199999809265137 seconds
[0m14:20:32.720199 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (execute): 14:20:31.184680 => 14:20:32.720199
[0m14:20:32.722194 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: ROLLBACK
[0m14:20:32.723192 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:32.724191 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: Close
[0m14:20:32.725189 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-f9e0-1523-b9fb-694820b03b73
[0m14:20:32.956139 [info ] [Thread-1 (]: 5 of 21 PASS not_null_dim_tempo_data ........................................... [[32mPASS[0m in 1.79s]
[0m14:20:32.962125 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:20:32.964118 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:20:32.966112 [info ] [Thread-1 (]: 6 of 21 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m14:20:32.969104 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m14:20:32.970099 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:20:32.981079 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:20:32.982067 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 14:20:32.971098 => 14:20:32.982067
[0m14:20:32.983065 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:20:32.987085 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:20:32.989049 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:32.989049 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:20:32.990075 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m14:20:32.990075 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:33.782627 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-faf0-18e2-a4ec-a3838c7a74d4
[0m14:20:34.388915 [debug] [Thread-1 (]: SQL status: OK in 1.399999976158142 seconds
[0m14:20:34.392876 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 14:20:32.984062 => 14:20:34.391907
[0m14:20:34.392876 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m14:20:34.393902 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:34.394870 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m14:20:34.395879 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-faf0-18e2-a4ec-a3838c7a74d4
[0m14:20:34.628822 [info ] [Thread-1 (]: 6 of 21 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 1.66s]
[0m14:20:34.630830 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:20:34.631824 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:20:34.631824 [info ] [Thread-1 (]: 7 of 21 START test not_null_dim_tempo_dia ...................................... [RUN]
[0m14:20:34.633810 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306)
[0m14:20:34.633810 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:20:34.640819 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:20:34.641788 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (compile): 14:20:34.634807 => 14:20:34.641788
[0m14:20:34.642810 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:20:34.646774 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:20:34.648780 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:34.649767 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:20:34.649767 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select dia
from `workspace`.`default`.`dim_tempo`
where dia is null



      
    ) dbt_internal_test
[0m14:20:34.650763 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:35.449831 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-fbee-1c1e-9fe2-699537eab6c5
[0m14:20:36.074276 [debug] [Thread-1 (]: SQL status: OK in 1.4199999570846558 seconds
[0m14:20:36.085340 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (execute): 14:20:34.642810 => 14:20:36.084343
[0m14:20:36.087336 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: ROLLBACK
[0m14:20:36.089330 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:36.091293 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: Close
[0m14:20:36.093322 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-fbee-1c1e-9fe2-699537eab6c5
[0m14:20:36.376372 [info ] [Thread-1 (]: 7 of 21 PASS not_null_dim_tempo_dia ............................................ [[32mPASS[0m in 1.74s]
[0m14:20:36.378388 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:20:36.378388 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:20:36.379364 [info ] [Thread-1 (]: 8 of 21 START test not_null_dim_tempo_mes ...................................... [RUN]
[0m14:20:36.381361 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306, now test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972)
[0m14:20:36.382356 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:20:36.387364 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:20:36.389343 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (compile): 14:20:36.382356 => 14:20:36.389343
[0m14:20:36.390335 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:20:36.393327 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:20:36.395323 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:36.397318 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:20:36.398314 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select mes
from `workspace`.`default`.`dim_tempo`
where mes is null



      
    ) dbt_internal_test
[0m14:20:36.399324 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:37.654932 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-fd3c-13c9-b200-009877394839
[0m14:20:38.377990 [debug] [Thread-1 (]: SQL status: OK in 1.9800000190734863 seconds
[0m14:20:38.382978 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (execute): 14:20:36.390335 => 14:20:38.382978
[0m14:20:38.383974 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: ROLLBACK
[0m14:20:38.384971 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:38.385968 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: Close
[0m14:20:38.387000 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-fd3c-13c9-b200-009877394839
[0m14:20:38.679907 [info ] [Thread-1 (]: 8 of 21 PASS not_null_dim_tempo_mes ............................................ [[32mPASS[0m in 2.30s]
[0m14:20:38.683897 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:20:38.686852 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:20:38.688846 [info ] [Thread-1 (]: 9 of 21 START test not_null_fato_nascimento_APGAR1 ............................. [RUN]
[0m14:20:38.692837 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972, now test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074)
[0m14:20:38.695829 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:20:38.708820 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:20:38.710787 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (compile): 14:20:38.696824 => 14:20:38.710787
[0m14:20:38.711812 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:20:38.720786 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:20:38.721754 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:38.721754 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:20:38.722780 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR1
from `workspace`.`default`.`fato_nascimento`
where APGAR1 is null



      
    ) dbt_internal_test
[0m14:20:38.722780 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:39.593476 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-fe68-10a2-aaac-cc7740bd9a06
[0m14:20:40.260445 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m14:20:40.264435 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (execute): 14:20:38.711812 => 14:20:40.264435
[0m14:20:40.265432 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: ROLLBACK
[0m14:20:40.266430 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:40.267398 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: Close
[0m14:20:40.268425 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-fe68-10a2-aaac-cc7740bd9a06
[0m14:20:40.505999 [info ] [Thread-1 (]: 9 of 21 PASS not_null_fato_nascimento_APGAR1 ................................... [[32mPASS[0m in 1.81s]
[0m14:20:40.506997 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:20:40.507994 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:20:40.507994 [info ] [Thread-1 (]: 10 of 21 START test not_null_fato_nascimento_APGAR5 ............................ [RUN]
[0m14:20:40.509989 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074, now test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994)
[0m14:20:40.509989 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:20:40.516970 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:20:40.517968 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (compile): 14:20:40.510986 => 14:20:40.517968
[0m14:20:40.518965 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:20:40.522981 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:20:40.523981 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:40.523981 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:20:40.524976 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR5
from `workspace`.`default`.`fato_nascimento`
where APGAR5 is null



      
    ) dbt_internal_test
[0m14:20:40.525953 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:41.314474 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07932-ff6d-1d50-add6-0bb34df0bc8c
[0m14:20:41.968150 [debug] [Thread-1 (]: SQL status: OK in 1.440000057220459 seconds
[0m14:20:41.978120 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (execute): 14:20:40.519963 => 14:20:41.977121
[0m14:20:41.979115 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: ROLLBACK
[0m14:20:41.981110 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:41.982106 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: Close
[0m14:20:41.983104 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07932-ff6d-1d50-add6-0bb34df0bc8c
[0m14:20:42.204789 [info ] [Thread-1 (]: 10 of 21 PASS not_null_fato_nascimento_APGAR5 .................................. [[32mPASS[0m in 1.70s]
[0m14:20:42.209809 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:20:42.211803 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:20:42.213801 [info ] [Thread-1 (]: 11 of 21 START test not_null_fato_nascimento_cod_municipio ..................... [RUN]
[0m14:20:42.216794 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m14:20:42.219749 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:20:42.232710 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:20:42.234706 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 14:20:42.221741 => 14:20:42.233707
[0m14:20:42.235702 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:20:42.241714 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:20:42.242714 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:42.242714 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:20:42.243681 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m14:20:42.243681 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:43.049183 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-0076-1d65-b05a-aa65ef2237c1
[0m14:20:44.172187 [debug] [Thread-1 (]: SQL status: OK in 1.9299999475479126 seconds
[0m14:20:44.175206 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 14:20:42.236700 => 14:20:44.174208
[0m14:20:44.175206 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m14:20:44.176175 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:44.176175 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m14:20:44.177200 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-0076-1d65-b05a-aa65ef2237c1
[0m14:20:44.407280 [info ] [Thread-1 (]: 11 of 21 PASS not_null_fato_nascimento_cod_municipio ........................... [[32mPASS[0m in 2.19s]
[0m14:20:44.412265 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:20:44.413260 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:20:44.415256 [info ] [Thread-1 (]: 12 of 21 START test not_null_fato_nascimento_data_id ........................... [RUN]
[0m14:20:44.417251 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m14:20:44.418248 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:20:44.428249 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:20:44.429217 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 14:20:44.419245 => 14:20:44.429217
[0m14:20:44.430215 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:20:44.434203 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:20:44.435201 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:44.435201 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:20:44.436199 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m14:20:44.437196 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:45.333942 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-01c6-1855-bd6f-57539bfb2cff
[0m14:20:46.511862 [debug] [Thread-1 (]: SQL status: OK in 2.0799999237060547 seconds
[0m14:20:46.514841 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 14:20:44.431211 => 14:20:46.514841
[0m14:20:46.515840 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m14:20:46.515840 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:46.516809 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m14:20:46.517806 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-01c6-1855-bd6f-57539bfb2cff
[0m14:20:46.766147 [info ] [Thread-1 (]: 12 of 21 PASS not_null_fato_nascimento_data_id ................................. [[32mPASS[0m in 2.35s]
[0m14:20:46.768635 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:20:46.768635 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:20:46.769632 [info ] [Thread-1 (]: 13 of 21 START test not_null_fato_nascimento_gestacao_semanas .................. [RUN]
[0m14:20:46.771596 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2)
[0m14:20:46.772594 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:20:46.781599 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:20:46.783563 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (compile): 14:20:46.773593 => 14:20:46.783563
[0m14:20:46.784562 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:20:46.788552 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:20:46.789548 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:46.790545 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:20:46.791544 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select gestacao_semanas
from `workspace`.`default`.`fato_nascimento`
where gestacao_semanas is null



      
    ) dbt_internal_test
[0m14:20:46.792542 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:47.898636 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-0358-1107-a5c8-d69925fd3ce7
[0m14:20:48.580880 [debug] [Thread-1 (]: SQL status: OK in 1.7899999618530273 seconds
[0m14:20:48.589864 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (execute): 14:20:46.784562 => 14:20:48.588858
[0m14:20:48.591842 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: ROLLBACK
[0m14:20:48.593852 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:48.595833 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: Close
[0m14:20:48.596830 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-0358-1107-a5c8-d69925fd3ce7
[0m14:20:48.870501 [info ] [Thread-1 (]: 13 of 21 PASS not_null_fato_nascimento_gestacao_semanas ........................ [[32mPASS[0m in 2.10s]
[0m14:20:48.875066 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:20:48.877009 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:20:48.880048 [info ] [Thread-1 (]: 14 of 21 START test not_null_fato_nascimento_idade_mae ......................... [RUN]
[0m14:20:48.884988 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2, now test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42)
[0m14:20:48.886981 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:20:48.903962 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:20:48.905926 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (compile): 14:20:48.887976 => 14:20:48.905926
[0m14:20:48.906924 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:20:48.911912 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:20:48.913906 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:48.913906 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:20:48.914931 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select idade_mae
from `workspace`.`default`.`fato_nascimento`
where idade_mae is null



      
    ) dbt_internal_test
[0m14:20:48.915901 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:49.762521 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-0476-17e8-a501-13d811f628a6
[0m14:20:50.498527 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m14:20:50.502515 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (execute): 14:20:48.906924 => 14:20:50.501518
[0m14:20:50.503513 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: ROLLBACK
[0m14:20:50.503513 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:50.504510 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: Close
[0m14:20:50.505507 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-0476-17e8-a501-13d811f628a6
[0m14:20:50.844210 [info ] [Thread-1 (]: 14 of 21 PASS not_null_fato_nascimento_idade_mae ............................... [[32mPASS[0m in 1.96s]
[0m14:20:50.846177 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:20:50.847204 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:20:50.847204 [info ] [Thread-1 (]: 15 of 21 START test not_null_fato_nascimento_nascimento_id ..................... [RUN]
[0m14:20:50.848685 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m14:20:50.849715 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:20:50.857664 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:20:50.859659 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 14:20:50.850711 => 14:20:50.858692
[0m14:20:50.859659 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:20:50.864674 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:20:50.865643 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:50.866669 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:20:50.866669 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m14:20:50.867664 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:51.682979 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-059b-1b98-819c-7dd23fa135dc
[0m14:20:52.300712 [debug] [Thread-1 (]: SQL status: OK in 1.4299999475479126 seconds
[0m14:20:52.303703 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 14:20:50.860659 => 14:20:52.302711
[0m14:20:52.303703 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m14:20:52.304709 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:52.305670 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m14:20:52.305670 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-059b-1b98-819c-7dd23fa135dc
[0m14:20:52.550808 [info ] [Thread-1 (]: 15 of 21 PASS not_null_fato_nascimento_nascimento_id ........................... [[32mPASS[0m in 1.70s]
[0m14:20:52.551777 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:20:52.552802 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:20:52.552802 [info ] [Thread-1 (]: 16 of 21 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m14:20:52.554797 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m14:20:52.554797 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:20:52.562777 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:20:52.564743 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 14:20:52.555788 => 14:20:52.563747
[0m14:20:52.565740 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:20:52.568753 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:20:52.569729 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:52.570748 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:20:52.570748 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m14:20:52.571747 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:53.422077 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-06a6-10b9-b0b8-70fc4aef0703
[0m14:20:54.070853 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m14:20:54.074207 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 14:20:52.565740 => 14:20:54.074207
[0m14:20:54.076175 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m14:20:54.076175 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:54.077201 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m14:20:54.078196 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-06a6-10b9-b0b8-70fc4aef0703
[0m14:20:54.314896 [info ] [Thread-1 (]: 16 of 21 PASS not_null_fato_nascimento_peso .................................... [[32mPASS[0m in 1.76s]
[0m14:20:54.317889 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:20:54.318885 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:20:54.319883 [info ] [Thread-1 (]: 17 of 21 START test not_null_fato_nascimento_sexo .............................. [RUN]
[0m14:20:54.322876 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m14:20:54.323872 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:20:54.332848 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:20:54.334877 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 14:20:54.324869 => 14:20:54.334877
[0m14:20:54.335868 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:20:54.338860 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:20:54.339834 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:54.340855 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:20:54.341823 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sexo
from `workspace`.`default`.`fato_nascimento`
where sexo is null



      
    ) dbt_internal_test
[0m14:20:54.341823 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:55.308663 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-07bb-17ed-b28b-344909fb4a53
[0m14:20:55.946574 [debug] [Thread-1 (]: SQL status: OK in 1.600000023841858 seconds
[0m14:20:55.952556 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 14:20:54.335868 => 14:20:55.951593
[0m14:20:55.953553 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: ROLLBACK
[0m14:20:55.954551 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:55.956545 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: Close
[0m14:20:55.956545 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-07bb-17ed-b28b-344909fb4a53
[0m14:20:56.244962 [info ] [Thread-1 (]: 17 of 21 PASS not_null_fato_nascimento_sexo .................................... [[32mPASS[0m in 1.92s]
[0m14:20:56.248922 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:20:56.251915 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:20:56.254908 [info ] [Thread-1 (]: 18 of 21 START test not_null_fato_nascimento_tipo_parto ........................ [RUN]
[0m14:20:56.258931 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e)
[0m14:20:56.261886 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:20:56.274849 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:20:56.276844 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (compile): 14:20:56.262883 => 14:20:56.275846
[0m14:20:56.276844 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:20:56.281858 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:20:56.282869 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:56.283828 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:20:56.284821 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tipo_parto
from `workspace`.`default`.`fato_nascimento`
where tipo_parto is null



      
    ) dbt_internal_test
[0m14:20:56.285820 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:57.462139 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-08fa-1852-b63a-4d12da4aeac0
[0m14:20:58.724308 [debug] [Thread-1 (]: SQL status: OK in 2.440000057220459 seconds
[0m14:20:58.727327 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (execute): 14:20:56.277840 => 14:20:58.726322
[0m14:20:58.727327 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: ROLLBACK
[0m14:20:58.728322 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:20:58.729293 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: Close
[0m14:20:58.729293 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-08fa-1852-b63a-4d12da4aeac0
[0m14:20:58.991989 [error] [Thread-1 (]: 18 of 21 FAIL 11 not_null_fato_nascimento_tipo_parto ........................... [[31mFAIL 11[0m in 2.73s]
[0m14:20:58.993985 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:20:58.995980 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:20:58.996976 [info ] [Thread-1 (]: 19 of 21 START test unique_dim_localidade_cod_municipio ........................ [RUN]
[0m14:20:58.999002 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m14:20:58.999968 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:20:59.012932 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:20:59.014926 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 14:20:59.000964 => 14:20:59.014926
[0m14:20:59.015932 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:20:59.018915 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:20:59.019913 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:20:59.020911 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:20:59.022906 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_municipio as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_localidade`
where cod_municipio is not null
group by cod_municipio
having count(*) > 1



      
    ) dbt_internal_test
[0m14:20:59.022906 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:20:59.817152 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-0a75-16bc-9ed3-ad85f1ad5a85
[0m14:21:00.445402 [debug] [Thread-1 (]: SQL status: OK in 1.4199999570846558 seconds
[0m14:21:00.448421 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 14:20:59.015932 => 14:21:00.447397
[0m14:21:00.448421 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: ROLLBACK
[0m14:21:00.449421 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:21:00.450417 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: Close
[0m14:21:00.450417 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-0a75-16bc-9ed3-ad85f1ad5a85
[0m14:21:00.685901 [info ] [Thread-1 (]: 19 of 21 PASS unique_dim_localidade_cod_municipio .............................. [[32mPASS[0m in 1.69s]
[0m14:21:00.687868 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:21:00.688865 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:21:00.689870 [info ] [Thread-1 (]: 20 of 21 START test unique_dim_tempo_data_id ................................... [RUN]
[0m14:21:00.690862 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m14:21:00.691867 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:21:00.696844 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:21:00.697845 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 14:21:00.691867 => 14:21:00.697845
[0m14:21:00.698840 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:21:00.702829 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:21:00.704823 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:21:00.705821 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:21:00.706818 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:21:00.707814 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:21:01.518998 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-0b79-1ec6-ba06-87233c761fa5
[0m14:21:02.265745 [debug] [Thread-1 (]: SQL status: OK in 1.559999942779541 seconds
[0m14:21:02.268737 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 14:21:00.699837 => 14:21:02.268737
[0m14:21:02.269737 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m14:21:02.270746 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:21:02.270746 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m14:21:02.271729 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-0b79-1ec6-ba06-87233c761fa5
[0m14:21:02.529284 [info ] [Thread-1 (]: 20 of 21 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 1.84s]
[0m14:21:02.533274 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:21:02.535271 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:21:02.537293 [info ] [Thread-1 (]: 21 of 21 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m14:21:02.540255 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m14:21:02.542247 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:21:02.551244 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:21:02.553216 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 14:21:02.543246 => 14:21:02.552242
[0m14:21:02.553216 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:21:02.556209 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:21:02.559202 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:21:02.560199 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:21:02.560199 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:21:02.561196 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:21:03.334241 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-0c8e-1413-ad5e-e8cab52ed517
[0m14:21:04.358298 [debug] [Thread-1 (]: SQL status: OK in 1.7999999523162842 seconds
[0m14:21:04.367050 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 14:21:02.554215 => 14:21:04.366052
[0m14:21:04.369042 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m14:21:04.371038 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:21:04.372035 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m14:21:04.374029 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-0c8e-1413-ad5e-e8cab52ed517
[0m14:21:04.609673 [error] [Thread-1 (]: 21 of 21 FAIL 1 unique_fato_nascimento_nascimento_id ........................... [[31mFAIL 1[0m in 2.07s]
[0m14:21:04.612696 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:21:04.615657 [debug] [MainThread]: On master: ROLLBACK
[0m14:21:04.616653 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:21:06.285972 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07933-0e1f-1234-a712-22397f3b1d85
[0m14:21:06.287934 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:21:06.288945 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:21:06.289927 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:21:06.290947 [debug] [MainThread]: On master: ROLLBACK
[0m14:21:06.291924 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:21:06.292919 [debug] [MainThread]: On master: Close
[0m14:21:06.293916 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07933-0e1f-1234-a712-22397f3b1d85
[0m14:21:06.534653 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:21:06.535650 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:21:06.535650 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m14:21:06.536647 [info ] [MainThread]: 
[0m14:21:06.537645 [info ] [MainThread]: Finished running 21 tests in 0 hours 0 minutes and 51.72 seconds (51.72s).
[0m14:21:06.544656 [debug] [MainThread]: Command end result
[0m14:21:06.565569 [info ] [MainThread]: 
[0m14:21:06.566567 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m14:21:06.567565 [info ] [MainThread]: 
[0m14:21:06.568589 [error] [MainThread]: [31mFailure in test not_null_fato_nascimento_tipo_parto (models\marts\schema.yaml)[0m
[0m14:21:06.569580 [error] [MainThread]:   Got 11 results, configured to fail if != 0
[0m14:21:06.570557 [info ] [MainThread]: 
[0m14:21:06.571554 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\not_null_fato_nascimento_tipo_parto.sql
[0m14:21:06.572552 [info ] [MainThread]: 
[0m14:21:06.573548 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m14:21:06.574545 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m14:21:06.575543 [info ] [MainThread]: 
[0m14:21:06.576540 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m14:21:06.576540 [info ] [MainThread]: 
[0m14:21:06.578535 [info ] [MainThread]: Done. PASS=19 WARN=0 ERROR=2 SKIP=0 TOTAL=21
[0m14:21:06.579532 [debug] [MainThread]: Command `dbt test` failed at 14:21:06.579532 after 53.55 seconds
[0m14:21:06.580558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222BD77B8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222BD6DF750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000222BD6DE750>]}
[0m14:21:06.581549 [debug] [MainThread]: Flushing usage events
[0m14:24:33.246411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525141ADD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015253C560D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525395A410>]}


============================== 14:24:33.251424 | 60dc1c52-ea55-490e-b598-e67b6274db36 ==============================
[0m14:24:33.251424 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:24:33.252393 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:24:34.699053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015254274F90>]}
[0m14:24:34.720972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015253990850>]}
[0m14:24:34.721970 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:24:34.751918 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:24:34.903812 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m14:24:34.904809 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\marts\fato_nascimento.sql
[0m14:24:34.905807 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m14:24:34.994569 [debug] [MainThread]: 1699: static parser successfully parsed marts\fato_nascimento.sql
[0m14:24:35.010526 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m14:24:35.034462 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging\stg_nascidos_vivos.sql
[0m14:24:35.138691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525D4AD910>]}
[0m14:24:35.154648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525E6244D0>]}
[0m14:24:35.155646 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:24:35.156460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525A9FF310>]}
[0m14:24:35.159476 [info ] [MainThread]: 
[0m14:24:35.160453 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:24:35.163484 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m14:24:35.163484 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m14:24:35.164482 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m14:24:35.165480 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:36.124616 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07933-8b60-1169-87b7-4653566079b5
[0m14:24:36.638682 [debug] [ThreadPool]: SQL status: OK in 1.4700000286102295 seconds
[0m14:24:36.644668 [debug] [ThreadPool]: On list_workspace: Close
[0m14:24:36.645658 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07933-8b60-1169-87b7-4653566079b5
[0m14:24:36.869506 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m14:24:36.871501 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m14:24:36.890418 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:36.891416 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m14:24:36.891416 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m14:24:36.892413 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:24:37.709398 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07933-8c54-1964-84f7-4069f6d80851
[0m14:24:38.086217 [debug] [ThreadPool]: SQL status: OK in 1.190000057220459 seconds
[0m14:24:38.088243 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:24:38.088243 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m14:24:38.089229 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:24:38.089229 [debug] [ThreadPool]: On create_workspace_default: Close
[0m14:24:38.090205 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07933-8c54-1964-84f7-4069f6d80851
[0m14:24:38.327667 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:24:38.332654 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:24:38.332654 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:24:38.333652 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:39.153774 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07933-8d2f-1a9a-8a34-3380bd02d212
[0m14:24:39.584947 [debug] [ThreadPool]: SQL status: OK in 1.25 seconds
[0m14:24:39.591930 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:24:39.593894 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07933-8d2f-1a9a-8a34-3380bd02d212
[0m14:24:39.815641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525E52F890>]}
[0m14:24:39.816637 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:39.817604 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:24:39.818633 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:24:39.819599 [info ] [MainThread]: 
[0m14:24:39.826567 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m14:24:39.827567 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m14:24:39.828563 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m14:24:39.829584 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m14:24:39.837553 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m14:24:39.839534 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 14:24:39.830565 => 14:24:39.838537
[0m14:24:39.839534 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m14:24:39.876464 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m14:24:39.877461 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:39.878461 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m14:24:39.878461 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m14:24:39.879456 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:24:40.689408 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-8e19-1c55-89da-2b7531f2a044
[0m14:24:41.454744 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m14:24:41.472696 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 14:24:39.840531 => 14:24:41.471699
[0m14:24:41.472696 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m14:24:41.473693 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:41.474713 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m14:24:41.475701 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-8e19-1c55-89da-2b7531f2a044
[0m14:24:41.710748 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525397E490>]}
[0m14:24:41.712742 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.88s]
[0m14:24:41.715707 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m14:24:41.716731 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m14:24:41.718730 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m14:24:41.720721 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m14:24:41.722688 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m14:24:41.730702 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m14:24:41.733686 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 14:24:41.723684 => 14:24:41.732689
[0m14:24:41.734687 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m14:24:41.742663 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m14:24:41.743650 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:41.744656 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m14:24:41.745625 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m14:24:41.745625 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:42.519360 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-8f33-143f-b668-5b07ca72f75c
[0m14:24:43.295169 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m14:24:43.298161 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 14:24:41.734687 => 14:24:43.297164
[0m14:24:43.298161 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m14:24:43.299161 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:43.299161 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m14:24:43.300159 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-8f33-143f-b668-5b07ca72f75c
[0m14:24:43.663436 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525E612CD0>]}
[0m14:24:43.665431 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.94s]
[0m14:24:43.668423 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m14:24:43.670418 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m14:24:43.671413 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m14:24:43.676402 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m14:24:43.677397 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m14:24:43.686406 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m14:24:43.688369 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 14:24:43.678396 => 14:24:43.687370
[0m14:24:43.689394 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m14:24:43.696347 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m14:24:43.697372 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:43.697372 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m14:24:43.698369 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m14:24:43.698369 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:45.083832 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-90ba-1b2a-bdf9-959723ca376d
[0m14:24:45.902859 [debug] [Thread-1 (]: SQL status: OK in 2.200000047683716 seconds
[0m14:24:45.905854 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 14:24:43.689394 => 14:24:45.904854
[0m14:24:45.905854 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m14:24:45.906849 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:45.906849 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m14:24:45.907846 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-90ba-1b2a-bdf9-959723ca376d
[0m14:24:46.298114 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525E5826D0>]}
[0m14:24:46.301108 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 2.62s]
[0m14:24:46.305096 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m14:24:46.307098 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:24:46.308087 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m14:24:46.311079 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m14:24:46.313074 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m14:24:46.325069 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:24:46.328034 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 14:24:46.313074 => 14:24:46.327038
[0m14:24:46.329031 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m14:24:46.335042 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:24:46.337009 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:46.337009 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:24:46.338035 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
    ...
SELECT
    md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    -- Converte os códigos numéricos para strings legíveis
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m14:24:46.339004 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:47.749807 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-9235-16a9-bd5b-13ddd0e3209e
[0m14:24:48.366207 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
    ...
SELECT
    md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    -- Converte os códigos numéricos para strings legíveis
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m14:24:48.368200 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '.'. SQLSTATE: 42601 (line 27, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
    ...
----^^^
SELECT
    md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    -- Converte os códigos numéricos para strings legíveis
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

[0m14:24:48.370196 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '.'. SQLSTATE: 42601 (line 27, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
    ...
----^^^
SELECT
    md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    -- Converte os códigos numéricos para strings legíveis
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near '.'. SQLSTATE: 42601 (line 27, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
)

SELECT
    ...
----^^^
SELECT
    md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    -- Converte os códigos numéricos para strings legíveis
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM nascidos_vivos_raw

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:487)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:153)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:117)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$4(QueryRuntimePrediction.scala:446)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:445)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)
	at com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)
	at com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)
	at com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:444)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:440)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:790)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:781)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:569)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:901)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:296)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:896)
	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)
	at com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)
	... 3 more

[0m14:24:48.373186 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07933-9272-19e3-812b-64e78e4642d5
[0m14:24:48.374183 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 14:24:46.329031 => 14:24:48.374183
[0m14:24:48.375180 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m14:24:48.376178 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:48.377173 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m14:24:48.379138 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-9235-16a9-bd5b-13ddd0e3209e
[0m14:24:48.687823 [debug] [Thread-1 (]: Runtime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near '.'. SQLSTATE: 42601 (line 27, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
  create or replace view `workspace`.`default`.`stg_nascidos_vivos`
    
    
    as
      
  
  WITH nascidos_vivos_raw AS (
      SELECT
          CODMUNNASC,
          LOCNASC,
          DTNASC,
          HORANASC,
          SEXO,
          PESO,
          IDADEMAE,
          GESTACAO,
          PARTO,
          APGAR1,
          APGAR5,
          CODUFNATU
      FROM default.sinasc_2022_sc_clean
      WHERE CODUFNATU = 42 -- SC
  )
  
  SELECT
      ...
  ----^^^
  SELECT
      md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
      CODMUNNASC AS cod_municipio,
      LOCNASC AS local_nascimento,
      DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
      HORANASC AS hora_nascimento,
      -- Converte os códigos numéricos para strings legíveis
      CASE
          WHEN SEXO = 1 THEN 'Masculino'
          WHEN SEXO = 2 THEN 'Feminino'
          ELSE 'Ignorado'
      END AS sexo,
      PESO AS peso,
      IDADEMAE AS idade_mae,
      GESTACAO AS gestacao_semanas,
      PARTO AS tipo_parto,
      APGAR1,
      APGAR5
  FROM nascidos_vivos_raw
  
[0m14:24:48.688833 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525E7F8910>]}
[0m14:24:48.689817 [error] [Thread-1 (]: 4 of 12 ERROR creating sql view model default.stg_nascidos_vivos ............... [[31mERROR[0m in 2.38s]
[0m14:24:48.690799 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:24:48.691784 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m14:24:48.692782 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m14:24:48.693780 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m14:24:48.694805 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m14:24:48.699790 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m14:24:48.701758 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 14:24:48.695796 => 14:24:48.700788
[0m14:24:48.702754 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m14:24:48.718714 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m14:24:48.720707 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:48.721705 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m14:24:48.721705 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m14:24:48.722704 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:50.049847 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-93a2-1cfd-a8e4-caee14b7f2c9
[0m14:24:50.828778 [debug] [Thread-1 (]: SQL status: OK in 2.109999895095825 seconds
[0m14:24:50.832768 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 14:24:48.702754 => 14:24:50.832768
[0m14:24:50.833766 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m14:24:50.834764 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:50.835730 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m14:24:50.835730 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-93a2-1cfd-a8e4-caee14b7f2c9
[0m14:24:51.158420 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525E7C6350>]}
[0m14:24:51.161420 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 2.46s]
[0m14:24:51.166367 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m14:24:51.169400 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m14:24:51.171386 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m14:24:51.175345 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m14:24:51.176341 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m14:24:51.185315 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m14:24:51.187310 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 14:24:51.177337 => 14:24:51.186312
[0m14:24:51.188306 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m14:24:51.194290 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m14:24:51.195287 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:51.196285 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m14:24:51.197283 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m14:24:51.197283 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:52.564980 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-9520-15c0-b169-d353c0d1491d
[0m14:24:53.355980 [debug] [Thread-1 (]: SQL status: OK in 2.1600000858306885 seconds
[0m14:24:53.358997 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 14:24:51.188306 => 14:24:53.358997
[0m14:24:53.359997 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m14:24:53.360963 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:53.360963 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m14:24:53.361995 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-9520-15c0-b169-d353c0d1491d
[0m14:24:53.758255 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525E7DDB90>]}
[0m14:24:53.761247 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 2.58s]
[0m14:24:53.766198 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m14:24:53.768192 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m14:24:53.771224 [info ] [Thread-1 (]: 7 of 12 SKIP relation default.int_nascimento ................................... [[33mSKIP[0m]
[0m14:24:53.774177 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m14:24:53.777168 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m14:24:53.780160 [info ] [Thread-1 (]: 8 of 12 SKIP relation default.stg_tempo ........................................ [[33mSKIP[0m]
[0m14:24:53.784149 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m14:24:53.786143 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:24:53.788137 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m14:24:53.792125 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m14:24:53.793123 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:24:53.800132 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:24:53.802100 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 14:24:53.794138 => 14:24:53.801100
[0m14:24:53.803098 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:24:53.808081 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:24:53.810085 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:53.810085 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:24:53.811074 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m14:24:53.812070 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:24:55.303015 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-96bd-10f6-b4cf-9f705f5962e5
[0m14:24:56.224338 [debug] [Thread-1 (]: SQL status: OK in 2.4100000858306885 seconds
[0m14:24:56.227360 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 14:24:53.803098 => 14:24:56.226363
[0m14:24:56.227360 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m14:24:56.228357 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:24:56.229327 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m14:24:56.229327 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-96bd-10f6-b4cf-9f705f5962e5
[0m14:24:56.657695 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60dc1c52-ea55-490e-b598-e67b6274db36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001525E717850>]}
[0m14:24:56.660687 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 2.87s]
[0m14:24:56.664675 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:24:56.666636 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m14:24:56.668630 [info ] [Thread-1 (]: 10 of 12 SKIP relation default.dim_localidade .................................. [[33mSKIP[0m]
[0m14:24:56.672617 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m14:24:56.674611 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m14:24:56.676605 [info ] [Thread-1 (]: 11 of 12 SKIP relation default.dim_tempo ....................................... [[33mSKIP[0m]
[0m14:24:56.678598 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m14:24:56.679625 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m14:24:56.680622 [info ] [Thread-1 (]: 12 of 12 SKIP relation default.fato_nascimento ................................. [[33mSKIP[0m]
[0m14:24:56.681590 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m14:24:56.683585 [debug] [MainThread]: On master: ROLLBACK
[0m14:24:56.684611 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:24:57.966708 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07933-9855-1ae7-bbcf-a404aab3fbd7
[0m14:24:57.969704 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:24:57.971733 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:24:57.973727 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:24:57.974688 [debug] [MainThread]: On master: ROLLBACK
[0m14:24:57.975681 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:24:57.976680 [debug] [MainThread]: On master: Close
[0m14:24:57.977685 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07933-9855-1ae7-bbcf-a404aab3fbd7
[0m14:24:58.310257 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:24:58.311282 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m14:24:58.311282 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:24:58.312279 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_atendimento_hospitalar' was properly closed.
[0m14:24:58.313277 [info ] [MainThread]: 
[0m14:24:58.314244 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 23.15 seconds (23.15s).
[0m14:24:58.317266 [debug] [MainThread]: Command end result
[0m14:24:58.334192 [info ] [MainThread]: 
[0m14:24:58.336187 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:24:58.337184 [info ] [MainThread]: 
[0m14:24:58.337184 [error] [MainThread]: [33mRuntime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m14:24:58.339183 [error] [MainThread]:   
[0m14:24:58.340178 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near '.'. SQLSTATE: 42601 (line 27, pos 4)
[0m14:24:58.342171 [error] [MainThread]:   
[0m14:24:58.343168 [error] [MainThread]:   == SQL ==
[0m14:24:58.344165 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
[0m14:24:58.345162 [error] [MainThread]:   create or replace view `workspace`.`default`.`stg_nascidos_vivos`
[0m14:24:58.345162 [error] [MainThread]:     
[0m14:24:58.346159 [error] [MainThread]:     
[0m14:24:58.347157 [error] [MainThread]:     as
[0m14:24:58.348154 [error] [MainThread]:       
[0m14:24:58.349151 [error] [MainThread]:   
[0m14:24:58.350149 [error] [MainThread]:   WITH nascidos_vivos_raw AS (
[0m14:24:58.351146 [error] [MainThread]:       SELECT
[0m14:24:58.352153 [error] [MainThread]:           CODMUNNASC,
[0m14:24:58.353146 [error] [MainThread]:           LOCNASC,
[0m14:24:58.354139 [error] [MainThread]:           DTNASC,
[0m14:24:58.355135 [error] [MainThread]:           HORANASC,
[0m14:24:58.356134 [error] [MainThread]:           SEXO,
[0m14:24:58.357131 [error] [MainThread]:           PESO,
[0m14:24:58.358128 [error] [MainThread]:           IDADEMAE,
[0m14:24:58.359131 [error] [MainThread]:           GESTACAO,
[0m14:24:58.360122 [error] [MainThread]:           PARTO,
[0m14:24:58.361120 [error] [MainThread]:           APGAR1,
[0m14:24:58.362117 [error] [MainThread]:           APGAR5,
[0m14:24:58.363115 [error] [MainThread]:           CODUFNATU
[0m14:24:58.364119 [error] [MainThread]:       FROM default.sinasc_2022_sc_clean
[0m14:24:58.364119 [error] [MainThread]:       WHERE CODUFNATU = 42 -- SC
[0m14:24:58.366107 [error] [MainThread]:   )
[0m14:24:58.367105 [error] [MainThread]:   
[0m14:24:58.368130 [error] [MainThread]:   SELECT
[0m14:24:58.369099 [error] [MainThread]:       ...
[0m14:24:58.370096 [error] [MainThread]:   ----^^^
[0m14:24:58.371094 [error] [MainThread]:   SELECT
[0m14:24:58.373088 [error] [MainThread]:       md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
[0m14:24:58.374085 [error] [MainThread]:       CODMUNNASC AS cod_municipio,
[0m14:24:58.375093 [error] [MainThread]:       LOCNASC AS local_nascimento,
[0m14:24:58.376079 [error] [MainThread]:       DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
[0m14:24:58.377091 [error] [MainThread]:       HORANASC AS hora_nascimento,
[0m14:24:58.378074 [error] [MainThread]:       -- Converte os códigos numéricos para strings legíveis
[0m14:24:58.379071 [error] [MainThread]:       CASE
[0m14:24:58.380074 [error] [MainThread]:           WHEN SEXO = 1 THEN 'Masculino'
[0m14:24:58.381067 [error] [MainThread]:           WHEN SEXO = 2 THEN 'Feminino'
[0m14:24:58.382064 [error] [MainThread]:           ELSE 'Ignorado'
[0m14:24:58.384058 [error] [MainThread]:       END AS sexo,
[0m14:24:58.385055 [error] [MainThread]:       PESO AS peso,
[0m14:24:58.386053 [error] [MainThread]:       IDADEMAE AS idade_mae,
[0m14:24:58.387052 [error] [MainThread]:       GESTACAO AS gestacao_semanas,
[0m14:24:58.388047 [error] [MainThread]:       PARTO AS tipo_parto,
[0m14:24:58.389045 [error] [MainThread]:       APGAR1,
[0m14:24:58.390072 [error] [MainThread]:       APGAR5
[0m14:24:58.391068 [error] [MainThread]:   FROM nascidos_vivos_raw
[0m14:24:58.392063 [error] [MainThread]:   
[0m14:24:58.393035 [info ] [MainThread]: 
[0m14:24:58.395037 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=5 TOTAL=12
[0m14:24:58.396026 [debug] [MainThread]: Command `dbt run` failed at 14:24:58.396026 after 25.17 seconds
[0m14:24:58.397053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000152539A11D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000152539A3150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015253A34BD0>]}
[0m14:24:58.398021 [debug] [MainThread]: Flushing usage events
[0m14:26:51.412807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002307A8A1110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002307A892650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002307A8901D0>]}


============================== 14:26:51.416826 | 25e0e812-3b61-4eef-8b87-904d0001f8f0 ==============================
[0m14:26:51.416826 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:26:51.418791 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:26:52.889717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002307A9144D0>]}
[0m14:26:52.911659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002307F6C1A50>]}
[0m14:26:52.912656 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:26:52.943577 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:26:53.096165 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:26:53.097162 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m14:26:53.184958 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m14:26:53.217840 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging\stg_nascidos_vivos.sql
[0m14:26:53.234823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230053C4ED0>]}
[0m14:26:53.251749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023005400B90>]}
[0m14:26:53.252747 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:26:53.253864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023077884750>]}
[0m14:26:53.256886 [info ] [MainThread]: 
[0m14:26:53.257869 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:26:53.261856 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m14:26:53.262842 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m14:26:53.263839 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m14:26:53.263839 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:54.778937 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07933-dde3-11b8-8e65-cb68933d2a98
[0m14:26:55.326848 [debug] [ThreadPool]: SQL status: OK in 2.059999942779541 seconds
[0m14:26:55.335826 [debug] [ThreadPool]: On list_workspace: Close
[0m14:26:55.336822 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07933-dde3-11b8-8e65-cb68933d2a98
[0m14:26:55.627313 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m14:26:55.629307 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m14:26:55.642298 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:26:55.643269 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m14:26:55.643269 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m14:26:55.643269 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:26:57.282874 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07933-df64-1020-9d45-8b26b64ac97b
[0m14:26:57.995448 [debug] [ThreadPool]: SQL status: OK in 2.3499999046325684 seconds
[0m14:26:57.999434 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:26:58.000431 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m14:26:58.001461 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:26:58.002458 [debug] [ThreadPool]: On create_workspace_default: Close
[0m14:26:58.003455 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07933-df64-1020-9d45-8b26b64ac97b
[0m14:26:58.439876 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:26:58.454830 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:26:58.455797 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:26:58.456793 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:27:00.305814 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07933-e143-1b16-b4f3-2a1fce51b50c
[0m14:27:00.914315 [debug] [ThreadPool]: SQL status: OK in 2.4600000381469727 seconds
[0m14:27:00.920299 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:27:00.921296 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07933-e143-1b16-b4f3-2a1fce51b50c
[0m14:27:01.390088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230043D6B90>]}
[0m14:27:01.391084 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:01.392082 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:27:01.393079 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:27:01.394051 [info ] [MainThread]: 
[0m14:27:01.400964 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m14:27:01.401963 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m14:27:01.403972 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m14:27:01.404978 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m14:27:01.409943 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m14:27:01.411936 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 14:27:01.405952 => 14:27:01.410968
[0m14:27:01.412933 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m14:27:01.447839 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m14:27:01.449834 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:01.449834 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m14:27:01.450840 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m14:27:01.451830 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:27:03.639253 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-e311-1b4a-a14d-264a6de0d3d5
[0m14:27:04.700714 [debug] [Thread-1 (]: SQL status: OK in 3.25 seconds
[0m14:27:04.716640 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 14:27:01.412933 => 14:27:04.716640
[0m14:27:04.716640 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m14:27:04.717637 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:04.718646 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m14:27:04.718646 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-e311-1b4a-a14d-264a6de0d3d5
[0m14:27:05.017746 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023005543750>]}
[0m14:27:05.019741 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 3.61s]
[0m14:27:05.022732 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m14:27:05.023731 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m14:27:05.025726 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m14:27:05.028751 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m14:27:05.029743 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m14:27:05.039688 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m14:27:05.042679 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 14:27:05.030741 => 14:27:05.041683
[0m14:27:05.043707 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m14:27:05.053649 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m14:27:05.057638 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:05.058637 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m14:27:05.059632 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m14:27:05.059632 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:06.386215 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-e4e9-1719-870c-a33c06f712d6
[0m14:27:07.219548 [debug] [Thread-1 (]: SQL status: OK in 2.1600000858306885 seconds
[0m14:27:07.228556 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 14:27:05.044704 => 14:27:07.227561
[0m14:27:07.229549 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m14:27:07.231545 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:07.232511 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m14:27:07.233509 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-e4e9-1719-870c-a33c06f712d6
[0m14:27:07.580385 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300555DE10>]}
[0m14:27:07.581388 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 2.55s]
[0m14:27:07.582351 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m14:27:07.583376 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m14:27:07.584345 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m14:27:07.586340 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m14:27:07.587337 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m14:27:07.592328 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m14:27:07.594332 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 14:27:07.587337 => 14:27:07.594332
[0m14:27:07.595317 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m14:27:07.603321 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m14:27:07.604291 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:07.604291 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m14:27:07.605289 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m14:27:07.605289 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:09.251206 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-e68c-1e56-bebc-a95f507b1678
[0m14:27:10.048719 [debug] [Thread-1 (]: SQL status: OK in 2.440000057220459 seconds
[0m14:27:10.057687 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 14:27:07.595317 => 14:27:10.057687
[0m14:27:10.059653 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m14:27:10.060650 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:10.061647 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m14:27:10.063641 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-e68c-1e56-bebc-a95f507b1678
[0m14:27:10.414595 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230056CB890>]}
[0m14:27:10.415587 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 2.83s]
[0m14:27:10.416583 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m14:27:10.417581 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:27:10.418580 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m14:27:10.419589 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m14:27:10.420604 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m14:27:10.429550 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:27:10.431555 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 14:27:10.420604 => 14:27:10.430547
[0m14:27:10.432541 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m14:27:10.436559 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:27:10.437529 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:10.438530 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:27:10.439523 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
),

final AS (
    SELECT
        md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
        CODMUNNASC AS cod_municipio,
        LOCNASC AS local_nascimento,
        DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
        HORANASC AS hora_nascimento,
        CASE
            WHEN SEXO = 1 THEN 'Masculino'
            WHEN SEXO = 2 THEN 'Feminino'
            ELSE 'Ignorado'
        END AS sexo,
        PESO AS peso,
        IDADEMAE AS idade_mae,
        GESTACAO AS gestacao_semanas,
        PARTO AS tipo_parto,
        APGAR1,
        APGAR5
    FROM nascidos_vivos_raw
)

SELECT * FROM final

[0m14:27:10.439523 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:12.061599 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-e839-17e1-91a1-70ab194922e3
[0m14:27:12.781396 [debug] [Thread-1 (]: SQL status: OK in 2.3399999141693115 seconds
[0m14:27:12.788378 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 14:27:10.432541 => 14:27:12.787408
[0m14:27:12.789374 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m14:27:12.790372 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:12.791368 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m14:27:12.792366 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-e839-17e1-91a1-70ab194922e3
[0m14:27:13.038655 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230056C0690>]}
[0m14:27:13.040689 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 2.62s]
[0m14:27:13.044674 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:27:13.045640 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m14:27:13.048629 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m14:27:13.052653 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m14:27:13.054648 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m14:27:13.066582 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m14:27:13.069573 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 14:27:13.055612 => 14:27:13.068575
[0m14:27:13.070570 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m14:27:13.076582 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m14:27:13.078548 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:13.079546 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m14:27:13.079546 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m14:27:13.080543 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:13.899535 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-e96d-100d-b908-8828ee423549
[0m14:27:14.551140 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m14:27:14.553135 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 14:27:13.071567 => 14:27:14.553135
[0m14:27:14.554132 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m14:27:14.555103 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:14.555103 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m14:27:14.556127 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-e96d-100d-b908-8828ee423549
[0m14:27:14.773549 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230043E5E90>]}
[0m14:27:14.774515 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.72s]
[0m14:27:14.775513 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m14:27:14.776529 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m14:27:14.777507 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m14:27:14.779502 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m14:27:14.780499 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m14:27:14.787506 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m14:27:14.789474 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 14:27:14.780499 => 14:27:14.788478
[0m14:27:14.789474 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m14:27:14.795471 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m14:27:14.797454 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:14.797454 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m14:27:14.798451 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m14:27:14.799449 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:15.584422 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-ea6d-1f3a-965f-d76307425884
[0m14:27:16.313684 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m14:27:16.320663 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 14:27:14.790486 => 14:27:16.320663
[0m14:27:16.322657 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m14:27:16.323687 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:16.324652 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m14:27:16.325649 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-ea6d-1f3a-965f-d76307425884
[0m14:27:16.622147 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023005690B50>]}
[0m14:27:16.623144 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.84s]
[0m14:27:16.625139 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m14:27:16.625139 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m14:27:16.626135 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m14:27:16.628158 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m14:27:16.628158 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m14:27:16.632147 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m14:27:16.634115 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 14:27:16.629146 => 14:27:16.634115
[0m14:27:16.635112 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m14:27:16.642120 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m14:27:16.644096 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:16.644096 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m14:27:16.645114 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH base AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM base

[0m14:27:16.645114 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:17.710288 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-eb9d-16c8-a8fe-0b98f0ac640d
[0m14:27:18.484913 [debug] [Thread-1 (]: SQL status: OK in 1.840000033378601 seconds
[0m14:27:18.488932 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 14:27:16.636116 => 14:27:18.488932
[0m14:27:18.489930 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m14:27:18.490898 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:18.490898 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m14:27:18.491927 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-eb9d-16c8-a8fe-0b98f0ac640d
[0m14:27:18.812092 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230056E5E90>]}
[0m14:27:18.813090 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 2.18s]
[0m14:27:18.814087 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m14:27:18.815091 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m14:27:18.816096 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m14:27:18.817079 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m14:27:18.818080 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m14:27:18.823063 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m14:27:18.825058 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 14:27:18.819084 => 14:27:18.824060
[0m14:27:18.826055 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m14:27:18.830044 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m14:27:18.831042 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:18.832039 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m14:27:18.833040 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m14:27:18.833040 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:20.150207 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-ed0a-1819-9f94-5698653cf16b
[0m14:27:20.959328 [debug] [Thread-1 (]: SQL status: OK in 2.130000114440918 seconds
[0m14:27:20.970297 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 14:27:18.826055 => 14:27:20.969301
[0m14:27:20.972292 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m14:27:20.975321 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:20.977278 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m14:27:20.979274 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-ed0a-1819-9f94-5698653cf16b
[0m14:27:21.408218 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230056E5B50>]}
[0m14:27:21.410213 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 2.59s]
[0m14:27:21.414201 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m14:27:21.415198 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:27:21.416195 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m14:27:21.418190 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m14:27:21.419218 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:27:21.425200 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:27:21.426199 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 14:27:21.420215 => 14:27:21.426199
[0m14:27:21.427191 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:27:21.431182 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:27:21.433150 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:21.433150 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:27:21.434146 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m14:27:21.434146 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:22.499125 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-ee8c-1e09-ac4d-5360bfc5c2a1
[0m14:27:23.228329 [debug] [Thread-1 (]: SQL status: OK in 1.7899999618530273 seconds
[0m14:27:23.231348 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 14:27:21.427191 => 14:27:23.231348
[0m14:27:23.232355 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m14:27:23.232355 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:23.233343 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m14:27:23.234313 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-ee8c-1e09-ac4d-5360bfc5c2a1
[0m14:27:23.461315 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300436F690>]}
[0m14:27:23.463313 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 2.04s]
[0m14:27:23.465307 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:27:23.466303 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m14:27:23.467299 [info ] [Thread-1 (]: 10 of 12 START sql view model default.dim_localidade ........................... [RUN]
[0m14:27:23.469326 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m14:27:23.470293 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m14:27:23.476313 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m14:27:23.477301 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 14:27:23.470293 => 14:27:23.477301
[0m14:27:23.478271 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m14:27:23.483257 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m14:27:23.485252 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:23.486281 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m14:27:23.486281 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    

SELECT
    cod_municipio,
    ANY_VALUE(local_nascimento) AS municipio -- Pega qualquer valor de local_nascimento
FROM `workspace`.`default`.`int_nascimento`
GROUP BY cod_municipio

[0m14:27:23.487275 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:24.286952 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-ef9d-1348-b62c-490cc40d5f29
[0m14:27:25.013676 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m14:27:25.020663 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 14:27:23.478271 => 14:27:25.019666
[0m14:27:25.021660 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m14:27:25.023656 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:25.024640 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m14:27:25.026611 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-ef9d-1348-b62c-490cc40d5f29
[0m14:27:25.261031 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300438E510>]}
[0m14:27:25.264022 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.dim_localidade ...................... [[32mOK[0m in 1.79s]
[0m14:27:25.269041 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m14:27:25.271008 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m14:27:25.273002 [info ] [Thread-1 (]: 11 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m14:27:25.276989 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m14:27:25.278982 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m14:27:25.287985 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m14:27:25.289951 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 14:27:25.279980 => 14:27:25.288955
[0m14:27:25.289951 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m14:27:25.297930 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m14:27:25.298927 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:25.299953 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m14:27:25.299953 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

WITH datas AS (
    SELECT DISTINCT
        data_nascimento AS data
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    ROW_NUMBER() OVER (ORDER BY data) AS data_id,
    data,
    EXTRACT(YEAR FROM data) AS ano,
    EXTRACT(MONTH FROM data) AS mes,
    EXTRACT(DAY FROM data) AS dia,
    DAYOFWEEK(data) AS dia_semana
FROM datas

[0m14:27:25.300949 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:26.162876 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-f0b0-1ebe-a566-0ba7711fd02a
[0m14:27:26.872145 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m14:27:26.875136 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 14:27:25.290977 => 14:27:26.875136
[0m14:27:26.875136 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m14:27:26.876133 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:26.876133 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m14:27:26.877131 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-f0b0-1ebe-a566-0ba7711fd02a
[0m14:27:27.112422 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230054EE010>]}
[0m14:27:27.115413 [info ] [Thread-1 (]: 11 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 1.84s]
[0m14:27:27.119404 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m14:27:27.122424 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m14:27:27.124419 [info ] [Thread-1 (]: 12 of 12 START sql view model default.fato_nascimento .......................... [RUN]
[0m14:27:27.127415 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_nascimento)
[0m14:27:27.129374 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m14:27:27.141340 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m14:27:27.143337 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 14:27:27.130373 => 14:27:27.142337
[0m14:27:27.143337 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m14:27:27.150316 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m14:27:27.151315 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:27.152310 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m14:27:27.153313 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    i.nascimento_id,
    t.data_id,
    l.cod_municipio,
    i.sexo,
    COALESCE(i.peso, 0) AS peso,
    i.idade_mae,
    COALESCE(i.gestacao_semanas, 0) AS gestacao_semanas,
    COALESCE(i.tipo_parto, 0) AS tipo_parto,
    COALESCE(i.APGAR1, 0) AS APGAR1,
    COALESCE(i.APGAR5, 0) AS APGAR5
FROM `workspace`.`default`.`int_nascimento` AS i
LEFT JOIN `workspace`.`default`.`dim_tempo` AS t
    ON i.data_nascimento = t.data
LEFT JOIN `workspace`.`default`.`dim_localidade` AS l
    ON i.cod_municipio = l.cod_municipio

[0m14:27:27.153313 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:27:28.006865 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07933-f1cb-1664-bb21-38bc35553eb7
[0m14:27:28.964070 [debug] [Thread-1 (]: SQL status: OK in 1.809999942779541 seconds
[0m14:27:28.970055 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 14:27:27.144338 => 14:27:28.970055
[0m14:27:28.971081 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m14:27:28.972049 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:27:28.973054 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m14:27:28.974043 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07933-f1cb-1664-bb21-38bc35553eb7
[0m14:27:29.296950 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e0e812-3b61-4eef-8b87-904d0001f8f0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002300567BB90>]}
[0m14:27:29.297919 [info ] [Thread-1 (]: 12 of 12 OK created sql view model default.fato_nascimento ..................... [[32mOK[0m in 2.17s]
[0m14:27:29.300942 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m14:27:29.302936 [debug] [MainThread]: On master: ROLLBACK
[0m14:27:29.303933 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:27:30.176550 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07933-f321-10d6-97b6-74fff18a4d53
[0m14:27:30.177548 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:27:30.177548 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:27:30.178546 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:27:30.179541 [debug] [MainThread]: On master: ROLLBACK
[0m14:27:30.179541 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:27:30.179541 [debug] [MainThread]: On master: Close
[0m14:27:30.180539 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07933-f321-10d6-97b6-74fff18a4d53
[0m14:27:30.416435 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:27:30.417433 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m14:27:30.418430 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:27:30.419428 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m14:27:30.420424 [info ] [MainThread]: 
[0m14:27:30.421421 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 37.16 seconds (37.16s).
[0m14:27:30.424823 [debug] [MainThread]: Command end result
[0m14:27:30.439756 [info ] [MainThread]: 
[0m14:27:30.440753 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:27:30.442748 [info ] [MainThread]: 
[0m14:27:30.443746 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 TOTAL=12
[0m14:27:30.444743 [debug] [MainThread]: Command `dbt run` succeeded at 14:27:30.444743 after 39.05 seconds
[0m14:27:30.444743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002307AC0EA90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230746BABD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230746FBB10>]}
[0m14:27:30.445763 [debug] [MainThread]: Flushing usage events
[0m14:28:02.216666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E6EADF50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E7157A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E494AF10>]}


============================== 14:28:02.221680 | ed459108-8241-4953-b860-8576a36a6feb ==============================
[0m14:28:02.221680 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:28:02.222649 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:28:03.614057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ed459108-8241-4953-b860-8576a36a6feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E7259190>]}
[0m14:28:03.636996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ed459108-8241-4953-b860-8576a36a6feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E71617D0>]}
[0m14:28:03.636996 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:28:03.665927 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:28:03.820483 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:28:03.821508 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:28:03.886370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ed459108-8241-4953-b860-8576a36a6feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277F1B31E90>]}
[0m14:28:03.905292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ed459108-8241-4953-b860-8576a36a6feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277F19FB850>]}
[0m14:28:03.906289 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:28:03.907285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed459108-8241-4953-b860-8576a36a6feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E36B4910>]}
[0m14:28:03.911303 [info ] [MainThread]: 
[0m14:28:03.912824 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:28:03.915845 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:28:03.923822 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:28:03.924822 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:28:03.924822 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:28:05.481387 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07934-0827-136d-985e-f8a40ce71cf6
[0m14:28:05.912986 [debug] [ThreadPool]: SQL status: OK in 1.9900000095367432 seconds
[0m14:28:05.918970 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:28:05.919968 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07934-0827-136d-985e-f8a40ce71cf6
[0m14:28:06.160324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed459108-8241-4953-b860-8576a36a6feb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277F0960310>]}
[0m14:28:06.163347 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:06.164345 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:28:06.167329 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:28:06.169311 [info ] [MainThread]: 
[0m14:28:06.181259 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:28:06.182256 [info ] [Thread-1 (]: 1 of 21 START test accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado  [RUN]
[0m14:28:06.184252 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d'
[0m14:28:06.185248 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:28:06.201233 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:28:06.204199 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (compile): 14:28:06.186246 => 14:28:06.203200
[0m14:28:06.205195 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:28:06.230155 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:28:06.232134 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:06.232134 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:28:06.233120 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'Masculino','Feminino','Ignorado'
)



      
    ) dbt_internal_test
[0m14:28:06.234120 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:28:07.036244 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-091a-1005-9585-18e2a8e66c08
[0m14:28:07.815518 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m14:28:07.822503 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (execute): 14:28:06.205195 => 14:28:07.821472
[0m14:28:07.823497 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: ROLLBACK
[0m14:28:07.824495 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:07.825491 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: Close
[0m14:28:07.826489 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-091a-1005-9585-18e2a8e66c08
[0m14:28:08.059299 [info ] [Thread-1 (]: 1 of 21 PASS accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado  [[32mPASS[0m in 1.88s]
[0m14:28:08.062291 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:28:08.063288 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:28:08.064284 [info ] [Thread-1 (]: 2 of 21 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m14:28:08.066281 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m14:28:08.067278 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:28:08.082748 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:28:08.084743 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 14:28:08.068823 => 14:28:08.083746
[0m14:28:08.084743 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:28:08.089730 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:28:08.090727 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:08.091752 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:28:08.091752 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m14:28:08.092749 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:08.873558 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-0a33-14bf-80e4-c9588e9e8764
[0m14:28:09.445052 [debug] [Thread-1 (]: SQL status: OK in 1.350000023841858 seconds
[0m14:28:09.450034 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 14:28:08.085740 => 14:28:09.449036
[0m14:28:09.451001 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m14:28:09.451998 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:09.452997 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m14:28:09.454023 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-0a33-14bf-80e4-c9588e9e8764
[0m14:28:09.693128 [info ] [Thread-1 (]: 2 of 21 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 1.63s]
[0m14:28:09.695124 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:28:09.695124 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:28:09.696149 [info ] [Thread-1 (]: 3 of 21 START test not_null_dim_localidade_municipio ........................... [RUN]
[0m14:28:09.697278 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771)
[0m14:28:09.698278 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:28:09.705260 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:28:09.707255 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (compile): 14:28:09.699297 => 14:28:09.706257
[0m14:28:09.707255 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:28:09.711245 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:28:09.713248 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:09.714251 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:28:09.714251 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select municipio
from `workspace`.`default`.`dim_localidade`
where municipio is null



      
    ) dbt_internal_test
[0m14:28:09.715260 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:10.538549 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-0b30-1478-a7fa-608a54933aac
[0m14:28:11.203457 [debug] [Thread-1 (]: SQL status: OK in 1.4900000095367432 seconds
[0m14:28:11.213390 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (execute): 14:28:09.708252 => 14:28:11.212394
[0m14:28:11.216382 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: ROLLBACK
[0m14:28:11.217379 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:11.218373 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: Close
[0m14:28:11.219379 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-0b30-1478-a7fa-608a54933aac
[0m14:28:11.450357 [info ] [Thread-1 (]: 3 of 21 PASS not_null_dim_localidade_municipio ................................. [[32mPASS[0m in 1.75s]
[0m14:28:11.454347 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:28:11.456345 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:28:11.457346 [info ] [Thread-1 (]: 4 of 21 START test not_null_dim_tempo_ano ...................................... [RUN]
[0m14:28:11.462293 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771, now test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6)
[0m14:28:11.463288 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:28:11.470297 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:28:11.472271 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (compile): 14:28:11.464285 => 14:28:11.471294
[0m14:28:11.472271 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:28:11.480276 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:28:11.481274 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:11.482243 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:28:11.482243 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select ano
from `workspace`.`default`.`dim_tempo`
where ano is null



      
    ) dbt_internal_test
[0m14:28:11.483241 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:12.282416 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-0c39-130e-8977-b9940542f077
[0m14:28:12.877482 [debug] [Thread-1 (]: SQL status: OK in 1.399999976158142 seconds
[0m14:28:12.880502 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (execute): 14:28:11.473289 => 14:28:12.879476
[0m14:28:12.880502 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: ROLLBACK
[0m14:28:12.881498 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:12.882469 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: Close
[0m14:28:12.882469 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-0c39-130e-8977-b9940542f077
[0m14:28:13.215000 [info ] [Thread-1 (]: 4 of 21 PASS not_null_dim_tempo_ano ............................................ [[32mPASS[0m in 1.75s]
[0m14:28:13.219951 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:28:13.221943 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:28:13.222940 [info ] [Thread-1 (]: 5 of 21 START test not_null_dim_tempo_data ..................................... [RUN]
[0m14:28:13.225933 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6, now test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252)
[0m14:28:13.226929 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:28:13.237927 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:28:13.239893 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (compile): 14:28:13.228923 => 14:28:13.238900
[0m14:28:13.239893 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:28:13.244880 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:28:13.245877 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:13.246873 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:28:13.247872 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data
from `workspace`.`default`.`dim_tempo`
where data is null



      
    ) dbt_internal_test
[0m14:28:13.247872 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:14.486696 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-0d68-1a3b-a6cd-15e29e5d5531
[0m14:28:15.165345 [debug] [Thread-1 (]: SQL status: OK in 1.9199999570846558 seconds
[0m14:28:15.169334 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (execute): 14:28:13.240919 => 14:28:15.168337
[0m14:28:15.170331 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: ROLLBACK
[0m14:28:15.171328 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:15.172326 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: Close
[0m14:28:15.173324 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-0d68-1a3b-a6cd-15e29e5d5531
[0m14:28:15.423798 [info ] [Thread-1 (]: 5 of 21 PASS not_null_dim_tempo_data ........................................... [[32mPASS[0m in 2.20s]
[0m14:28:15.426818 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:28:15.429782 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:28:15.431778 [info ] [Thread-1 (]: 6 of 21 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m14:28:15.435765 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m14:28:15.437759 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:28:15.448731 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:28:15.450725 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 14:28:15.438759 => 14:28:15.449758
[0m14:28:15.451721 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:28:15.454744 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:28:15.456736 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:15.456736 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:28:15.457733 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m14:28:15.458703 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:16.593556 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-0ebd-19e3-ac92-e1cfca24b576
[0m14:28:17.141431 [debug] [Thread-1 (]: SQL status: OK in 1.6799999475479126 seconds
[0m14:28:17.144395 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 14:28:15.451721 => 14:28:17.144395
[0m14:28:17.144395 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m14:28:17.145392 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:17.146389 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m14:28:17.146389 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-0ebd-19e3-ac92-e1cfca24b576
[0m14:28:17.388564 [info ] [Thread-1 (]: 6 of 21 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 1.95s]
[0m14:28:17.392748 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:28:17.394745 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:28:17.396738 [info ] [Thread-1 (]: 7 of 21 START test not_null_dim_tempo_dia ...................................... [RUN]
[0m14:28:17.402716 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306)
[0m14:28:17.403712 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:28:17.410688 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:28:17.412656 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (compile): 14:28:17.404684 => 14:28:17.411685
[0m14:28:17.412656 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:28:17.417643 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:28:17.418639 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:17.419637 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:28:17.420648 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select dia
from `workspace`.`default`.`dim_tempo`
where dia is null



      
    ) dbt_internal_test
[0m14:28:17.420648 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:18.204690 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-0fc1-124b-8c9b-7ea23a73a2e8
[0m14:28:18.787109 [debug] [Thread-1 (]: SQL status: OK in 1.3700000047683716 seconds
[0m14:28:18.792094 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (execute): 14:28:17.413681 => 14:28:18.792094
[0m14:28:18.793136 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: ROLLBACK
[0m14:28:18.794119 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:18.795117 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: Close
[0m14:28:18.796084 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-0fc1-124b-8c9b-7ea23a73a2e8
[0m14:28:19.032305 [info ] [Thread-1 (]: 7 of 21 PASS not_null_dim_tempo_dia ............................................ [[32mPASS[0m in 1.63s]
[0m14:28:19.037291 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:28:19.038288 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:28:19.040284 [info ] [Thread-1 (]: 8 of 21 START test not_null_dim_tempo_mes ...................................... [RUN]
[0m14:28:19.042279 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306, now test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972)
[0m14:28:19.044272 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:28:19.053249 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:28:19.055242 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (compile): 14:28:19.045270 => 14:28:19.055242
[0m14:28:19.056239 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:28:19.059261 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:28:19.061246 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:19.061246 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:28:19.062256 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select mes
from `workspace`.`default`.`dim_tempo`
where mes is null



      
    ) dbt_internal_test
[0m14:28:19.062256 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:19.836057 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-10ba-1458-b073-05b4d6b363ac
[0m14:28:20.405578 [debug] [Thread-1 (]: SQL status: OK in 1.340000033378601 seconds
[0m14:28:20.411562 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (execute): 14:28:19.056239 => 14:28:20.411562
[0m14:28:20.412559 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: ROLLBACK
[0m14:28:20.413556 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:20.414553 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: Close
[0m14:28:20.415551 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-10ba-1458-b073-05b4d6b363ac
[0m14:28:20.659697 [info ] [Thread-1 (]: 8 of 21 PASS not_null_dim_tempo_mes ............................................ [[32mPASS[0m in 1.62s]
[0m14:28:20.660602 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:28:20.661631 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:28:20.662600 [info ] [Thread-1 (]: 9 of 21 START test not_null_fato_nascimento_APGAR1 ............................. [RUN]
[0m14:28:20.663597 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972, now test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074)
[0m14:28:20.664610 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:28:20.671607 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:28:20.673570 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (compile): 14:28:20.664610 => 14:28:20.672601
[0m14:28:20.673570 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:28:20.679582 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:28:20.681576 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:20.681576 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:28:20.682545 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR1
from `workspace`.`default`.`fato_nascimento`
where APGAR1 is null



      
    ) dbt_internal_test
[0m14:28:20.683543 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:21.489045 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-11b6-1e6c-bc6a-2f11f37e1406
[0m14:28:22.080377 [debug] [Thread-1 (]: SQL status: OK in 1.399999976158142 seconds
[0m14:28:22.083369 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (execute): 14:28:20.674614 => 14:28:22.083369
[0m14:28:22.083369 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: ROLLBACK
[0m14:28:22.084367 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:22.085366 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: Close
[0m14:28:22.085366 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-11b6-1e6c-bc6a-2f11f37e1406
[0m14:28:22.389754 [info ] [Thread-1 (]: 9 of 21 PASS not_null_fato_nascimento_APGAR1 ................................... [[32mPASS[0m in 1.73s]
[0m14:28:22.394740 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:28:22.396735 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:28:22.398729 [info ] [Thread-1 (]: 10 of 21 START test not_null_fato_nascimento_APGAR5 ............................ [RUN]
[0m14:28:22.402719 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074, now test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994)
[0m14:28:22.404714 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:28:22.414753 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:28:22.415720 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (compile): 14:28:22.405712 => 14:28:22.415720
[0m14:28:22.416750 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:28:22.421705 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:28:22.423699 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:22.424725 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:28:22.424725 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR5
from `workspace`.`default`.`fato_nascimento`
where APGAR5 is null



      
    ) dbt_internal_test
[0m14:28:22.425721 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:23.531696 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-12e2-1cc3-b268-f0f5aca52e48
[0m14:28:24.235741 [debug] [Thread-1 (]: SQL status: OK in 1.809999942779541 seconds
[0m14:28:24.239700 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (execute): 14:28:22.417744 => 14:28:24.238703
[0m14:28:24.240711 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: ROLLBACK
[0m14:28:24.241726 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:24.242723 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: Close
[0m14:28:24.243722 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-12e2-1cc3-b268-f0f5aca52e48
[0m14:28:24.627896 [info ] [Thread-1 (]: 10 of 21 PASS not_null_fato_nascimento_APGAR5 .................................. [[32mPASS[0m in 2.23s]
[0m14:28:24.631913 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:28:24.633902 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:28:24.633902 [info ] [Thread-1 (]: 11 of 21 START test not_null_fato_nascimento_cod_municipio ..................... [RUN]
[0m14:28:24.635897 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m14:28:24.635897 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:28:24.642850 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:28:24.643847 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 14:28:24.636898 => 14:28:24.643847
[0m14:28:24.644844 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:28:24.648834 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:28:24.649831 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:24.650829 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:28:24.651828 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m14:28:24.651828 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:25.711076 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-143b-1657-b2e4-fda5aff6caed
[0m14:28:26.601438 [debug] [Thread-1 (]: SQL status: OK in 1.9500000476837158 seconds
[0m14:28:26.604403 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 14:28:24.644844 => 14:28:26.604403
[0m14:28:26.605427 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m14:28:26.606396 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:26.606396 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m14:28:26.607422 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-143b-1657-b2e4-fda5aff6caed
[0m14:28:26.836330 [info ] [Thread-1 (]: 11 of 21 PASS not_null_fato_nascimento_cod_municipio ........................... [[32mPASS[0m in 2.20s]
[0m14:28:26.837327 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:28:26.838325 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:28:26.839322 [info ] [Thread-1 (]: 12 of 21 START test not_null_fato_nascimento_data_id ........................... [RUN]
[0m14:28:26.839907 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m14:28:26.840937 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:28:26.846919 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:28:26.848887 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 14:28:26.841908 => 14:28:26.847919
[0m14:28:26.848887 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:28:26.852903 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:28:26.853872 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:26.854870 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:28:26.854870 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m14:28:26.855868 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:27.700314 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-1569-1ba8-b07e-02ca58673634
[0m14:28:28.666149 [debug] [Thread-1 (]: SQL status: OK in 1.809999942779541 seconds
[0m14:28:28.670137 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 14:28:26.849911 => 14:28:28.669140
[0m14:28:28.670137 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m14:28:28.671134 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:28.672132 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m14:28:28.672132 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-1569-1ba8-b07e-02ca58673634
[0m14:28:28.897869 [info ] [Thread-1 (]: 12 of 21 PASS not_null_fato_nascimento_data_id ................................. [[32mPASS[0m in 2.06s]
[0m14:28:28.899862 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:28:28.900831 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:28:28.900831 [info ] [Thread-1 (]: 13 of 21 START test not_null_fato_nascimento_gestacao_semanas .................. [RUN]
[0m14:28:28.902175 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2)
[0m14:28:28.903205 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:28:28.910156 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:28:28.912170 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (compile): 14:28:28.904202 => 14:28:28.911154
[0m14:28:28.912170 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:28:28.916169 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:28:28.917166 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:28.917166 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:28:28.918136 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select gestacao_semanas
from `workspace`.`default`.`fato_nascimento`
where gestacao_semanas is null



      
    ) dbt_internal_test
[0m14:28:28.919133 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:29.750324 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-169e-1058-b1b2-8f55a1531667
[0m14:28:30.347911 [debug] [Thread-1 (]: SQL status: OK in 1.4299999475479126 seconds
[0m14:28:30.350914 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (execute): 14:28:28.913177 => 14:28:30.350914
[0m14:28:30.351907 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: ROLLBACK
[0m14:28:30.352877 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:30.352877 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: Close
[0m14:28:30.353901 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-169e-1058-b1b2-8f55a1531667
[0m14:28:30.583283 [info ] [Thread-1 (]: 13 of 21 PASS not_null_fato_nascimento_gestacao_semanas ........................ [[32mPASS[0m in 1.68s]
[0m14:28:30.586245 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:28:30.587241 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:28:30.589237 [info ] [Thread-1 (]: 14 of 21 START test not_null_fato_nascimento_idade_mae ......................... [RUN]
[0m14:28:30.592202 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2, now test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42)
[0m14:28:30.593199 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:28:30.611150 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:28:30.614143 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (compile): 14:28:30.595214 => 14:28:30.612147
[0m14:28:30.615140 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:28:30.619156 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:28:30.620126 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:30.621166 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:28:30.621166 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select idade_mae
from `workspace`.`default`.`fato_nascimento`
where idade_mae is null



      
    ) dbt_internal_test
[0m14:28:30.622149 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:31.416473 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-17a1-1eba-8a06-7905d4d32313
[0m14:28:32.077496 [debug] [Thread-1 (]: SQL status: OK in 1.4600000381469727 seconds
[0m14:28:32.081457 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (execute): 14:28:30.615140 => 14:28:32.080459
[0m14:28:32.082453 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: ROLLBACK
[0m14:28:32.083450 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:32.083450 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: Close
[0m14:28:32.084448 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-17a1-1eba-8a06-7905d4d32313
[0m14:28:32.355218 [info ] [Thread-1 (]: 14 of 21 PASS not_null_fato_nascimento_idade_mae ............................... [[32mPASS[0m in 1.76s]
[0m14:28:32.356215 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:28:32.357242 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:28:32.358211 [info ] [Thread-1 (]: 15 of 21 START test not_null_fato_nascimento_nascimento_id ..................... [RUN]
[0m14:28:32.359208 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m14:28:32.360204 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:28:32.368184 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:28:32.370178 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 14:28:32.360204 => 14:28:32.370178
[0m14:28:32.371176 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:28:32.374167 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:28:32.375164 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:32.376162 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:28:32.376162 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m14:28:32.377159 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:33.358038 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-18c9-135f-a884-2741ab859365
[0m14:28:34.041320 [debug] [Thread-1 (]: SQL status: OK in 1.659999966621399 seconds
[0m14:28:34.044284 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 14:28:32.371176 => 14:28:34.044284
[0m14:28:34.044284 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m14:28:34.045281 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:34.046278 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m14:28:34.046278 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-18c9-135f-a884-2741ab859365
[0m14:28:34.280278 [info ] [Thread-1 (]: 15 of 21 PASS not_null_fato_nascimento_nascimento_id ........................... [[32mPASS[0m in 1.92s]
[0m14:28:34.283302 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:28:34.285323 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:28:34.287291 [info ] [Thread-1 (]: 16 of 21 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m14:28:34.290253 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m14:28:34.292248 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:28:34.306236 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:28:34.308233 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 14:28:34.293243 => 14:28:34.308233
[0m14:28:34.309230 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:28:34.313221 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:28:34.315228 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:34.316181 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:28:34.317179 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m14:28:34.318176 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:35.149266 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-19da-1303-bf3b-a2f369bdb1c2
[0m14:28:35.814944 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m14:28:35.818905 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 14:28:34.310228 => 14:28:35.817908
[0m14:28:35.818905 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m14:28:35.819902 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:35.820899 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m14:28:35.820899 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-19da-1303-bf3b-a2f369bdb1c2
[0m14:28:36.059777 [info ] [Thread-1 (]: 16 of 21 PASS not_null_fato_nascimento_peso .................................... [[32mPASS[0m in 1.77s]
[0m14:28:36.061772 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:28:36.063766 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:28:36.065792 [info ] [Thread-1 (]: 17 of 21 START test not_null_fato_nascimento_sexo .............................. [RUN]
[0m14:28:36.068753 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m14:28:36.069780 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:28:36.081746 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:28:36.082742 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 14:28:36.071743 => 14:28:36.082742
[0m14:28:36.083741 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:28:36.088725 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:28:36.089694 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:36.090691 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:28:36.090691 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sexo
from `workspace`.`default`.`fato_nascimento`
where sexo is null



      
    ) dbt_internal_test
[0m14:28:36.091688 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:37.024481 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-1ae3-1109-a2d6-844b700d7613
[0m14:28:37.669855 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m14:28:37.675870 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 14:28:36.084708 => 14:28:37.675870
[0m14:28:37.677833 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: ROLLBACK
[0m14:28:37.678861 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:37.680856 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: Close
[0m14:28:37.681854 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-1ae3-1109-a2d6-844b700d7613
[0m14:28:37.917991 [info ] [Thread-1 (]: 17 of 21 PASS not_null_fato_nascimento_sexo .................................... [[32mPASS[0m in 1.85s]
[0m14:28:37.921950 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:28:37.923973 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:28:37.924941 [info ] [Thread-1 (]: 18 of 21 START test not_null_fato_nascimento_tipo_parto ........................ [RUN]
[0m14:28:37.927935 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e)
[0m14:28:37.928931 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:28:37.936907 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:28:37.938902 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (compile): 14:28:37.929958 => 14:28:37.938902
[0m14:28:37.939899 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:28:37.942918 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:28:37.943934 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:37.943934 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:28:37.944886 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tipo_parto
from `workspace`.`default`.`fato_nascimento`
where tipo_parto is null



      
    ) dbt_internal_test
[0m14:28:37.944886 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:39.075272 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-1c29-1514-8328-baa54cab6f73
[0m14:28:39.783140 [debug] [Thread-1 (]: SQL status: OK in 1.840000033378601 seconds
[0m14:28:39.788127 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (execute): 14:28:37.939899 => 14:28:39.787129
[0m14:28:39.789124 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: ROLLBACK
[0m14:28:39.790121 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:39.791118 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: Close
[0m14:28:39.792115 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-1c29-1514-8328-baa54cab6f73
[0m14:28:40.157349 [info ] [Thread-1 (]: 18 of 21 PASS not_null_fato_nascimento_tipo_parto .............................. [[32mPASS[0m in 2.23s]
[0m14:28:40.161298 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:28:40.161298 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:28:40.162295 [info ] [Thread-1 (]: 19 of 21 START test unique_dim_localidade_cod_municipio ........................ [RUN]
[0m14:28:40.163297 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m14:28:40.164291 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:28:40.176288 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:28:40.177256 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 14:28:40.164291 => 14:28:40.177256
[0m14:28:40.178253 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:28:40.181273 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:28:40.182242 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:40.183239 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:28:40.183239 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_municipio as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_localidade`
where cod_municipio is not null
group by cod_municipio
having count(*) > 1



      
    ) dbt_internal_test
[0m14:28:40.184236 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:41.201639 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-1d72-1cd3-a15c-5be86c2c63dc
[0m14:28:41.838533 [debug] [Thread-1 (]: SQL status: OK in 1.649999976158142 seconds
[0m14:28:41.844518 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 14:28:40.178253 => 14:28:41.843521
[0m14:28:41.845515 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: ROLLBACK
[0m14:28:41.846513 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:41.847510 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: Close
[0m14:28:41.848508 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-1d72-1cd3-a15c-5be86c2c63dc
[0m14:28:42.114397 [info ] [Thread-1 (]: 19 of 21 PASS unique_dim_localidade_cod_municipio .............................. [[32mPASS[0m in 1.95s]
[0m14:28:42.116782 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:28:42.117809 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:28:42.118777 [info ] [Thread-1 (]: 20 of 21 START test unique_dim_tempo_data_id ................................... [RUN]
[0m14:28:42.120802 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m14:28:42.121806 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:28:42.130774 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:28:42.132740 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 14:28:42.122766 => 14:28:42.132740
[0m14:28:42.133768 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:28:42.138723 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:28:42.140716 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:42.141716 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:28:42.142713 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:28:42.143737 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:43.150168 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-1e9e-15b5-a6c9-f206f7ca89c5
[0m14:28:43.924160 [debug] [Thread-1 (]: SQL status: OK in 1.7799999713897705 seconds
[0m14:28:43.933172 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 14:28:42.134733 => 14:28:43.932173
[0m14:28:43.935165 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m14:28:43.937159 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:43.938152 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m14:28:43.940155 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-1e9e-15b5-a6c9-f206f7ca89c5
[0m14:28:44.175248 [info ] [Thread-1 (]: 20 of 21 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 2.05s]
[0m14:28:44.179201 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:28:44.182195 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:28:44.184186 [info ] [Thread-1 (]: 21 of 21 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m14:28:44.188891 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m14:28:44.190886 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:28:44.203879 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:28:44.205847 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 14:28:44.192878 => 14:28:44.205847
[0m14:28:44.206872 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:28:44.212854 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:28:44.214819 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:44.214819 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:28:44.215817 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:28:44.215817 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:28:45.296156 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-1fe7-107f-a495-ee97b1ff6412
[0m14:28:46.089284 [debug] [Thread-1 (]: SQL status: OK in 1.8700000047683716 seconds
[0m14:28:46.092106 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 14:28:44.207868 => 14:28:46.092106
[0m14:28:46.093095 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m14:28:46.094065 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:28:46.094065 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m14:28:46.095062 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-1fe7-107f-a495-ee97b1ff6412
[0m14:28:46.330709 [error] [Thread-1 (]: 21 of 21 FAIL 1 unique_fato_nascimento_nascimento_id ........................... [[31mFAIL 1[0m in 2.14s]
[0m14:28:46.335694 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:28:46.341717 [debug] [MainThread]: On master: ROLLBACK
[0m14:28:46.343711 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:28:47.149984 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07934-20ff-19b6-8fb3-e68bb9e3d026
[0m14:28:47.152976 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:28:47.154973 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:28:47.156967 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:28:47.157962 [debug] [MainThread]: On master: ROLLBACK
[0m14:28:47.159956 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:28:47.161950 [debug] [MainThread]: On master: Close
[0m14:28:47.162950 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07934-20ff-19b6-8fb3-e68bb9e3d026
[0m14:28:47.405408 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:28:47.406415 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:28:47.408374 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m14:28:47.411366 [info ] [MainThread]: 
[0m14:28:47.414376 [info ] [MainThread]: Finished running 21 tests in 0 hours 0 minutes and 43.50 seconds (43.50s).
[0m14:28:47.425324 [debug] [MainThread]: Command end result
[0m14:28:47.450259 [info ] [MainThread]: 
[0m14:28:47.451255 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:28:47.452172 [info ] [MainThread]: 
[0m14:28:47.453171 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m14:28:47.454179 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m14:28:47.455166 [info ] [MainThread]: 
[0m14:28:47.456164 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m14:28:47.457162 [info ] [MainThread]: 
[0m14:28:47.458164 [info ] [MainThread]: Done. PASS=20 WARN=0 ERROR=1 SKIP=0 TOTAL=21
[0m14:28:47.459168 [debug] [MainThread]: Command `dbt test` failed at 14:28:47.459168 after 45.26 seconds
[0m14:28:47.460175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E7024BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E7193190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E7190590>]}
[0m14:28:47.461151 [debug] [MainThread]: Flushing usage events
[0m14:30:19.003773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DE4FC3E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DE4F2BC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DE5229510>]}


============================== 14:30:19.009757 | f707869d-03f0-4725-a4f7-87e2e42d8d0f ==============================
[0m14:30:19.009757 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:30:19.010753 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:30:20.437979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DE5186490>]}
[0m14:30:20.457922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DE57B11D0>]}
[0m14:30:20.458920 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:30:20.487842 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:30:20.624984 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:30:20.625987 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m14:30:20.714750 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m14:30:20.749659 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging\stg_nascidos_vivos.sql
[0m14:30:20.765614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEF9EBDD0>]}
[0m14:30:20.783539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEE9B2210>]}
[0m14:30:20.784545 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:30:20.785532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEBF4D450>]}
[0m14:30:20.788525 [info ] [MainThread]: 
[0m14:30:20.790520 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:30:20.792514 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m14:30:20.793540 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m14:30:20.793540 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m14:30:20.794546 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:30:22.586333 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07934-59d5-1ecb-998b-bea9490ba928
[0m14:30:23.409731 [debug] [ThreadPool]: SQL status: OK in 2.619999885559082 seconds
[0m14:30:23.415746 [debug] [ThreadPool]: On list_workspace: Close
[0m14:30:23.417711 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07934-59d5-1ecb-998b-bea9490ba928
[0m14:30:23.838391 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m14:30:23.839393 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m14:30:23.850362 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:23.850362 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m14:30:23.851363 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m14:30:23.851363 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:30:25.228452 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07934-5b47-1ea7-a3e9-9057cdf3a4e3
[0m14:30:25.679594 [debug] [ThreadPool]: SQL status: OK in 1.8300000429153442 seconds
[0m14:30:25.682586 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:30:25.684580 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m14:30:25.685578 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:30:25.686576 [debug] [ThreadPool]: On create_workspace_default: Close
[0m14:30:25.688599 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07934-5b47-1ea7-a3e9-9057cdf3a4e3
[0m14:30:25.993759 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:30:26.003762 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:30:26.004758 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:30:26.005757 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:30:27.936677 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07934-5ccf-1b94-b872-a8b4c7416c0c
[0m14:30:28.551836 [debug] [ThreadPool]: SQL status: OK in 2.549999952316284 seconds
[0m14:30:28.555824 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:30:28.555824 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07934-5ccf-1b94-b872-a8b4c7416c0c
[0m14:30:29.128144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DE5170090>]}
[0m14:30:29.129139 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:29.131135 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:30:29.133128 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:30:29.134126 [info ] [MainThread]: 
[0m14:30:29.144100 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m14:30:29.145096 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m14:30:29.147091 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m14:30:29.148088 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m14:30:29.152077 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m14:30:29.155079 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 14:30:29.149085 => 14:30:29.154075
[0m14:30:29.156070 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m14:30:29.191997 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m14:30:29.192978 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:29.193965 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m14:30:29.194962 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m14:30:29.195960 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:30:31.106849 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-5ed6-1a7f-ba0b-f47e368a12cb
[0m14:30:32.380474 [debug] [Thread-1 (]: SQL status: OK in 3.180000066757202 seconds
[0m14:30:32.397428 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 14:30:29.157064 => 14:30:32.397428
[0m14:30:32.398426 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m14:30:32.399423 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:32.399423 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m14:30:32.400421 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-5ed6-1a7f-ba0b-f47e368a12cb
[0m14:30:32.902747 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEE9B4390>]}
[0m14:30:32.905739 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 3.75s]
[0m14:30:32.909691 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m14:30:32.911717 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m14:30:32.912715 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m14:30:32.915693 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m14:30:32.916672 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m14:30:32.922655 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m14:30:32.925646 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 14:30:32.917668 => 14:30:32.924652
[0m14:30:32.926643 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m14:30:32.931659 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m14:30:32.932655 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:32.933653 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m14:30:32.933653 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m14:30:32.934650 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:34.621766 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-60dd-1710-aa45-2ef57eba47eb
[0m14:30:35.450943 [debug] [Thread-1 (]: SQL status: OK in 2.5199999809265137 seconds
[0m14:30:35.453907 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 14:30:32.926643 => 14:30:35.453907
[0m14:30:35.454904 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m14:30:35.454904 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:35.455902 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m14:30:35.455902 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-60dd-1710-aa45-2ef57eba47eb
[0m14:30:35.900015 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEF9D5A90>]}
[0m14:30:35.903008 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 2.99s]
[0m14:30:35.906997 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m14:30:35.908990 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m14:30:35.910985 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m14:30:35.913977 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m14:30:35.915005 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m14:30:35.922980 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m14:30:35.923949 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 14:30:35.916003 => 14:30:35.923949
[0m14:30:35.924945 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m14:30:35.933950 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m14:30:35.934950 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:35.934950 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m14:30:35.935916 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m14:30:35.935916 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:37.873118 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-62e7-1398-820e-7fbc2f51937f
[0m14:30:38.624147 [debug] [Thread-1 (]: SQL status: OK in 2.690000057220459 seconds
[0m14:30:38.627136 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 14:30:35.925942 => 14:30:38.627136
[0m14:30:38.628133 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m14:30:38.628133 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:38.629130 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m14:30:38.630147 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-62e7-1398-820e-7fbc2f51937f
[0m14:30:39.094359 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEFB0A4D0>]}
[0m14:30:39.096355 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 3.18s]
[0m14:30:39.099346 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m14:30:39.100343 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:30:39.102343 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m14:30:39.105330 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m14:30:39.106329 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m14:30:39.119284 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:30:39.121260 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 14:30:39.107324 => 14:30:39.120284
[0m14:30:39.122248 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m14:30:39.126264 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:30:39.127234 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:39.128231 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:30:39.128231 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
),

final AS (
    SELECT
        md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PARTO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(GESTACAO as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
        CODMUNNASC AS cod_municipio,
        LOCNASC AS local_nascimento,
        DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
        HORANASC AS hora_nascimento,
        CASE
            WHEN SEXO = 1 THEN 'Masculino'
            WHEN SEXO = 2 THEN 'Feminino'
            ELSE 'Ignorado'
        END AS sexo,
        PESO AS peso,
        IDADEMAE AS idade_mae,
        GESTACAO AS gestacao_semanas,
        PARTO AS tipo_parto,
        APGAR1,
        APGAR5
    FROM nascidos_vivos_raw
)

SELECT * FROM final

[0m14:30:39.129257 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:40.249736 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-6463-10b7-8799-16895dddd908
[0m14:30:41.093703 [debug] [Thread-1 (]: SQL status: OK in 1.9600000381469727 seconds
[0m14:30:41.098697 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 14:30:39.122248 => 14:30:41.098697
[0m14:30:41.100657 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m14:30:41.101655 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:41.102652 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m14:30:41.103649 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-6463-10b7-8799-16895dddd908
[0m14:30:41.483599 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEE531A90>]}
[0m14:30:41.486592 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 2.38s]
[0m14:30:41.488587 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:30:41.489582 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m14:30:41.490576 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m14:30:41.491587 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m14:30:41.492571 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m14:30:41.496587 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m14:30:41.497565 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 14:30:41.492571 => 14:30:41.497565
[0m14:30:41.498555 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m14:30:41.503570 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m14:30:41.504539 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:41.505536 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m14:30:41.505536 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m14:30:41.506533 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:42.572680 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-65c6-1b8f-9bb1-0ad1293bfb14
[0m14:30:43.349109 [debug] [Thread-1 (]: SQL status: OK in 1.840000033378601 seconds
[0m14:30:43.353070 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 14:30:41.498555 => 14:30:43.352101
[0m14:30:43.353070 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m14:30:43.354068 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:43.355065 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m14:30:43.355065 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-65c6-1b8f-9bb1-0ad1293bfb14
[0m14:30:43.893504 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEFCAAE90>]}
[0m14:30:43.894500 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 2.40s]
[0m14:30:43.897521 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m14:30:43.898489 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m14:30:43.900513 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m14:30:43.903478 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m14:30:43.904473 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m14:30:43.915481 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m14:30:43.917441 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 14:30:43.905473 => 14:30:43.917441
[0m14:30:43.918466 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m14:30:43.926414 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m14:30:43.928408 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:43.928408 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m14:30:43.929406 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m14:30:43.930403 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:46.394861 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-67bd-15c6-8fa2-785219ab4ae3
[0m14:30:47.331442 [debug] [Thread-1 (]: SQL status: OK in 3.4000000953674316 seconds
[0m14:30:47.334426 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 14:30:43.919435 => 14:30:47.334426
[0m14:30:47.335422 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m14:30:47.336393 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:47.336393 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m14:30:47.337394 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-67bd-15c6-8fa2-785219ab4ae3
[0m14:30:47.665879 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEF9C2150>]}
[0m14:30:47.668872 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 3.76s]
[0m14:30:47.672821 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m14:30:47.675814 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m14:30:47.677807 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m14:30:47.678804 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m14:30:47.679831 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m14:30:47.683790 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m14:30:47.685785 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 14:30:47.679831 => 14:30:47.684787
[0m14:30:47.685785 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m14:30:47.693763 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m14:30:47.694760 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:47.695758 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m14:30:47.695758 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH base AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM base

[0m14:30:47.696755 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:48.886882 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-6979-161c-8d84-25cfa5d99095
[0m14:30:49.667977 [debug] [Thread-1 (]: SQL status: OK in 1.9700000286102295 seconds
[0m14:30:49.673990 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 14:30:47.686783 => 14:30:49.672995
[0m14:30:49.674956 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m14:30:49.675954 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:49.676980 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m14:30:49.677949 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-6979-161c-8d84-25cfa5d99095
[0m14:30:50.326724 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEE9B5A10>]}
[0m14:30:50.327721 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 2.65s]
[0m14:30:50.329716 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m14:30:50.330713 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m14:30:50.331710 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m14:30:50.332708 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m14:30:50.333734 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m14:30:50.339717 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m14:30:50.340715 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 14:30:50.333734 => 14:30:50.340715
[0m14:30:50.341712 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m14:30:50.348664 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m14:30:50.349661 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:50.350658 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m14:30:50.350658 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m14:30:50.351656 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:51.662523 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-6b2c-168a-ad1d-b95b4a78bc78
[0m14:30:52.383921 [debug] [Thread-1 (]: SQL status: OK in 2.0299999713897705 seconds
[0m14:30:52.389887 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 14:30:50.341712 => 14:30:52.388889
[0m14:30:52.389887 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m14:30:52.390883 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:52.391860 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m14:30:52.391860 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-6b2c-168a-ad1d-b95b4a78bc78
[0m14:30:52.729048 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEFA91A10>]}
[0m14:30:52.732039 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 2.40s]
[0m14:30:52.734944 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m14:30:52.735957 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:30:52.737940 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m14:30:52.739930 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m14:30:52.740928 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:30:52.748873 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:30:52.749870 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 14:30:52.741926 => 14:30:52.749870
[0m14:30:52.750868 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:30:52.755880 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:30:52.756875 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:52.757849 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:30:52.757849 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m14:30:52.758874 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:54.145139 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-6ca9-1197-9bde-9fdce6b7be83
[0m14:30:55.003123 [debug] [Thread-1 (]: SQL status: OK in 2.240000009536743 seconds
[0m14:30:55.006116 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 14:30:52.750868 => 14:30:55.006116
[0m14:30:55.007111 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m14:30:55.007111 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:55.008109 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m14:30:55.008109 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-6ca9-1197-9bde-9fdce6b7be83
[0m14:30:55.428023 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEE931950>]}
[0m14:30:55.429021 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 2.69s]
[0m14:30:55.430018 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:30:55.431016 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m14:30:55.432014 [info ] [Thread-1 (]: 10 of 12 START sql view model default.dim_localidade ........................... [RUN]
[0m14:30:55.434036 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m14:30:55.434036 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m14:30:55.438025 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m14:30:55.439992 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 14:30:55.435005 => 14:30:55.439992
[0m14:30:55.440989 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m14:30:55.445005 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m14:30:55.445975 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:55.446995 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m14:30:55.446995 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    

SELECT
    cod_municipio,
    ANY_VALUE(local_nascimento) AS municipio -- Pega qualquer valor de local_nascimento
FROM `workspace`.`default`.`int_nascimento`
GROUP BY cod_municipio

[0m14:30:55.447970 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:56.750225 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-6e3b-1e86-bab0-446644c48005
[0m14:30:57.626238 [debug] [Thread-1 (]: SQL status: OK in 2.180000066757202 seconds
[0m14:30:57.629984 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 14:30:55.440989 => 14:30:57.629984
[0m14:30:57.630980 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m14:30:57.631983 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:30:57.632945 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m14:30:57.633942 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-6e3b-1e86-bab0-446644c48005
[0m14:30:57.991335 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEFCCC390>]}
[0m14:30:57.993360 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.dim_localidade ...................... [[32mOK[0m in 2.56s]
[0m14:30:57.996353 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m14:30:57.997321 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m14:30:57.998317 [info ] [Thread-1 (]: 11 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m14:30:58.002307 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m14:30:58.003303 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m14:30:58.011311 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m14:30:58.013276 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 14:30:58.004300 => 14:30:58.012309
[0m14:30:58.013276 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m14:30:58.020257 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m14:30:58.021253 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:30:58.022251 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m14:30:58.022251 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

WITH datas AS (
    SELECT DISTINCT
        data_nascimento AS data
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    ROW_NUMBER() OVER (ORDER BY data) AS data_id,
    data,
    EXTRACT(YEAR FROM data) AS ano,
    EXTRACT(MONTH FROM data) AS mes,
    EXTRACT(DAY FROM data) AS dia,
    DAYOFWEEK(data) AS dia_semana
FROM datas

[0m14:30:58.023248 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:30:59.266890 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-6fc2-13b9-9d13-e4995fa7ddcd
[0m14:31:00.016311 [debug] [Thread-1 (]: SQL status: OK in 1.9900000095367432 seconds
[0m14:31:00.021296 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 14:30:58.014301 => 14:31:00.021296
[0m14:31:00.022339 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m14:31:00.023291 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:00.024287 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m14:31:00.025285 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-6fc2-13b9-9d13-e4995fa7ddcd
[0m14:31:00.439936 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEFC1BB10>]}
[0m14:31:00.440933 [info ] [Thread-1 (]: 11 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 2.44s]
[0m14:31:00.441931 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m14:31:00.442990 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m14:31:00.443720 [info ] [Thread-1 (]: 12 of 12 START sql view model default.fato_nascimento .......................... [RUN]
[0m14:31:00.444533 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_nascimento)
[0m14:31:00.445561 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m14:31:00.452514 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m14:31:00.454509 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 14:31:00.445561 => 14:31:00.453511
[0m14:31:00.454509 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m14:31:00.459495 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m14:31:00.461490 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:00.461490 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m14:31:00.462487 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    i.nascimento_id,
    t.data_id,
    l.cod_municipio,
    i.sexo,
    COALESCE(i.peso, 0) AS peso,
    i.idade_mae,
    COALESCE(i.gestacao_semanas, 0) AS gestacao_semanas,
    COALESCE(i.tipo_parto, 0) AS tipo_parto,
    COALESCE(i.APGAR1, 0) AS APGAR1,
    COALESCE(i.APGAR5, 0) AS APGAR5
FROM `workspace`.`default`.`int_nascimento` AS i
LEFT JOIN `workspace`.`default`.`dim_tempo` AS t
    ON i.data_nascimento = t.data
LEFT JOIN `workspace`.`default`.`dim_localidade` AS l
    ON i.cod_municipio = l.cod_municipio

[0m14:31:00.462487 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:01.993036 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-7153-1483-9aab-ef8e93c1c8e2
[0m14:31:03.022525 [debug] [Thread-1 (]: SQL status: OK in 2.559999942779541 seconds
[0m14:31:03.025545 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 14:31:00.455505 => 14:31:03.025545
[0m14:31:03.026514 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m14:31:03.026514 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:03.027512 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m14:31:03.027512 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-7153-1483-9aab-ef8e93c1c8e2
[0m14:31:03.370074 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f707869d-03f0-4725-a4f7-87e2e42d8d0f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DEFC6F250>]}
[0m14:31:03.374062 [info ] [Thread-1 (]: 12 of 12 OK created sql view model default.fato_nascimento ..................... [[32mOK[0m in 2.93s]
[0m14:31:03.378054 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m14:31:03.384428 [debug] [MainThread]: On master: ROLLBACK
[0m14:31:03.386425 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:31:04.894092 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07934-72f2-1b48-bb7b-9fe1fee12346
[0m14:31:04.896088 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:31:04.897085 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:04.898083 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:31:04.899108 [debug] [MainThread]: On master: ROLLBACK
[0m14:31:04.900106 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:31:04.901103 [debug] [MainThread]: On master: Close
[0m14:31:04.902100 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07934-72f2-1b48-bb7b-9fe1fee12346
[0m14:31:05.219042 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:31:05.220013 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m14:31:05.221010 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:31:05.221010 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m14:31:05.224031 [info ] [MainThread]: 
[0m14:31:05.224998 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 44.43 seconds (44.43s).
[0m14:31:05.231003 [debug] [MainThread]: Command end result
[0m14:31:05.246938 [info ] [MainThread]: 
[0m14:31:05.248476 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:31:05.249441 [info ] [MainThread]: 
[0m14:31:05.250439 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 TOTAL=12
[0m14:31:05.251436 [debug] [MainThread]: Command `dbt run` succeeded at 14:31:05.251436 after 46.27 seconds
[0m14:31:05.251436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DE2B00E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDEF6C910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDECBBB10>]}
[0m14:31:05.252433 [debug] [MainThread]: Flushing usage events
[0m14:31:34.514852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4F3A438D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4F67DF610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4F6CC9510>]}


============================== 14:31:34.519839 | 9ba48791-f77e-4343-8606-5aafe8e2fd60 ==============================
[0m14:31:34.519839 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:31:34.521816 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:31:35.971045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9ba48791-f77e-4343-8606-5aafe8e2fd60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4F5DB0450>]}
[0m14:31:35.994011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9ba48791-f77e-4343-8606-5aafe8e2fd60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4F73B9710>]}
[0m14:31:35.994983 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:31:36.023904 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:31:36.171582 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:31:36.172579 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:31:36.237406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9ba48791-f77e-4343-8606-5aafe8e2fd60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D481741E90>]}
[0m14:31:36.254868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9ba48791-f77e-4343-8606-5aafe8e2fd60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D480643F90>]}
[0m14:31:36.255865 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:31:36.256863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ba48791-f77e-4343-8606-5aafe8e2fd60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4FDB52F10>]}
[0m14:31:36.259882 [info ] [MainThread]: 
[0m14:31:36.261877 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:31:36.264842 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:31:36.271851 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:31:36.272827 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:31:36.272827 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:31:37.545885 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07934-8687-1a2a-b3c2-515914850851
[0m14:31:38.030027 [debug] [ThreadPool]: SQL status: OK in 1.7599999904632568 seconds
[0m14:31:38.036490 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:31:38.036490 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07934-8687-1a2a-b3c2-515914850851
[0m14:31:38.635343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ba48791-f77e-4343-8606-5aafe8e2fd60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4F62BD9D0>]}
[0m14:31:38.637338 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:38.639333 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:31:38.641327 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:31:38.644319 [info ] [MainThread]: 
[0m14:31:38.655461 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:31:38.656490 [info ] [Thread-1 (]: 1 of 21 START test accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado  [RUN]
[0m14:31:38.658454 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d'
[0m14:31:38.660448 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:31:38.677405 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:31:38.679395 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (compile): 14:31:38.661445 => 14:31:38.678429
[0m14:31:38.679395 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:31:38.705326 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:31:38.707335 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:38.707335 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:31:38.708341 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'Masculino','Feminino','Ignorado'
)



      
    ) dbt_internal_test
[0m14:31:38.709315 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:31:39.528322 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-87c0-1d34-8c40-6b5b44226b01
[0m14:31:40.293851 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m14:31:40.300801 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (execute): 14:31:38.680393 => 14:31:40.299831
[0m14:31:40.301798 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: ROLLBACK
[0m14:31:40.301798 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:40.302795 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: Close
[0m14:31:40.303793 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-87c0-1d34-8c40-6b5b44226b01
[0m14:31:40.556620 [info ] [Thread-1 (]: 1 of 21 PASS accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado  [[32mPASS[0m in 1.90s]
[0m14:31:40.557617 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:31:40.558646 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:31:40.559655 [info ] [Thread-1 (]: 2 of 21 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m14:31:40.560656 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m14:31:40.561682 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:31:40.572652 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:31:40.573621 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 14:31:40.562651 => 14:31:40.573621
[0m14:31:40.574647 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:31:40.578607 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:31:40.580602 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:40.580602 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:31:40.581628 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m14:31:40.581628 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:41.361384 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-88d7-17ca-8eb5-e2d6bee4d913
[0m14:31:42.039000 [debug] [Thread-1 (]: SQL status: OK in 1.4600000381469727 seconds
[0m14:31:42.043987 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 14:31:40.574647 => 14:31:42.042990
[0m14:31:42.044985 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m14:31:42.045982 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:42.046980 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m14:31:42.047978 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-88d7-17ca-8eb5-e2d6bee4d913
[0m14:31:42.304045 [info ] [Thread-1 (]: 2 of 21 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 1.74s]
[0m14:31:42.306010 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:31:42.307007 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:31:42.308034 [info ] [Thread-1 (]: 3 of 21 START test not_null_dim_localidade_municipio ........................... [RUN]
[0m14:31:42.309001 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771)
[0m14:31:42.309999 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:31:42.319003 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:31:42.320969 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (compile): 14:31:42.310996 => 14:31:42.319972
[0m14:31:42.320969 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:31:42.325982 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:31:42.326952 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:42.327950 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:31:42.327950 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select municipio
from `workspace`.`default`.`dim_localidade`
where municipio is null



      
    ) dbt_internal_test
[0m14:31:42.328976 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:44.068650 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-8a6c-1bea-ac18-6e4c4d9e0c4d
[0m14:31:45.066898 [debug] [Thread-1 (]: SQL status: OK in 2.740000009536743 seconds
[0m14:31:45.073904 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (execute): 14:31:42.321967 => 14:31:45.073904
[0m14:31:45.074907 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: ROLLBACK
[0m14:31:45.075899 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:45.076897 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: Close
[0m14:31:45.078893 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-8a6c-1bea-ac18-6e4c4d9e0c4d
[0m14:31:45.344878 [info ] [Thread-1 (]: 3 of 21 PASS not_null_dim_localidade_municipio ................................. [[32mPASS[0m in 3.03s]
[0m14:31:45.348860 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:31:45.351822 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:31:45.353817 [info ] [Thread-1 (]: 4 of 21 START test not_null_dim_tempo_ano ...................................... [RUN]
[0m14:31:45.357805 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771, now test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6)
[0m14:31:45.360834 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:31:45.374787 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:31:45.376751 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (compile): 14:31:45.361831 => 14:31:45.376751
[0m14:31:45.377780 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:31:45.385727 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:31:45.386725 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:45.387751 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:31:45.387751 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select ano
from `workspace`.`default`.`dim_tempo`
where ano is null



      
    ) dbt_internal_test
[0m14:31:45.388719 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:46.182082 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-8bb7-15c5-8bfe-bfa4bf93cede
[0m14:31:46.797101 [debug] [Thread-1 (]: SQL status: OK in 1.409999966621399 seconds
[0m14:31:46.802053 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (execute): 14:31:45.378746 => 14:31:46.801056
[0m14:31:46.803052 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: ROLLBACK
[0m14:31:46.804049 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:46.804049 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: Close
[0m14:31:46.805078 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-8bb7-15c5-8bfe-bfa4bf93cede
[0m14:31:47.022011 [info ] [Thread-1 (]: 4 of 21 PASS not_null_dim_tempo_ano ............................................ [[32mPASS[0m in 1.67s]
[0m14:31:47.023976 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:31:47.024972 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:31:47.025970 [info ] [Thread-1 (]: 5 of 21 START test not_null_dim_tempo_data ..................................... [RUN]
[0m14:31:47.027996 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6, now test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252)
[0m14:31:47.028962 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:31:47.037939 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:31:47.039933 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (compile): 14:31:47.029961 => 14:31:47.039933
[0m14:31:47.040959 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:31:47.043949 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:31:47.044946 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:47.045915 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:31:47.045915 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data
from `workspace`.`default`.`dim_tempo`
where data is null



      
    ) dbt_internal_test
[0m14:31:47.046914 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:47.828052 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-8cb2-1f81-8c10-bb94d51fdb85
[0m14:31:48.411474 [debug] [Thread-1 (]: SQL status: OK in 1.3600000143051147 seconds
[0m14:31:48.416461 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (execute): 14:31:47.040959 => 14:31:48.415464
[0m14:31:48.417458 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: ROLLBACK
[0m14:31:48.418425 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:48.419457 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: Close
[0m14:31:48.420420 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-8cb2-1f81-8c10-bb94d51fdb85
[0m14:31:48.655910 [info ] [Thread-1 (]: 5 of 21 PASS not_null_dim_tempo_data ........................................... [[32mPASS[0m in 1.63s]
[0m14:31:48.657906 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:31:48.658933 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:31:48.659933 [info ] [Thread-1 (]: 6 of 21 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m14:31:48.662891 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m14:31:48.663889 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:31:48.673891 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:31:48.674888 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 14:31:48.663889 => 14:31:48.674888
[0m14:31:48.675885 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:31:48.679873 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:31:48.680873 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:48.680873 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:31:48.681840 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m14:31:48.682837 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:49.472640 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-8dae-134e-a34e-9b28140d618c
[0m14:31:49.948986 [debug] [Thread-1 (]: SQL status: OK in 1.2699999809265137 seconds
[0m14:31:49.955001 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 14:31:48.675885 => 14:31:49.954001
[0m14:31:49.955990 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m14:31:49.956964 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:49.957989 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m14:31:49.958959 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-8dae-134e-a34e-9b28140d618c
[0m14:31:50.207964 [info ] [Thread-1 (]: 6 of 21 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 1.55s]
[0m14:31:50.212984 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:31:50.214971 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:31:50.217933 [info ] [Thread-1 (]: 7 of 21 START test not_null_dim_tempo_dia ...................................... [RUN]
[0m14:31:50.221925 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306)
[0m14:31:50.224915 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:31:50.236910 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:31:50.239872 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (compile): 14:31:50.225910 => 14:31:50.237878
[0m14:31:50.240868 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:31:50.244858 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:31:50.246853 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:50.246853 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:31:50.247849 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select dia
from `workspace`.`default`.`dim_tempo`
where dia is null



      
    ) dbt_internal_test
[0m14:31:50.247849 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:51.276963 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-8eb3-13e6-bde0-81d36d8495a2
[0m14:31:52.128928 [debug] [Thread-1 (]: SQL status: OK in 1.8799999952316284 seconds
[0m14:31:52.132919 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (execute): 14:31:50.241895 => 14:31:52.132919
[0m14:31:52.133914 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: ROLLBACK
[0m14:31:52.134912 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:52.135909 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: Close
[0m14:31:52.136906 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-8eb3-13e6-bde0-81d36d8495a2
[0m14:31:52.520569 [info ] [Thread-1 (]: 7 of 21 PASS not_null_dim_tempo_dia ............................................ [[32mPASS[0m in 2.30s]
[0m14:31:52.522563 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:31:52.522563 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:31:52.523560 [info ] [Thread-1 (]: 8 of 21 START test not_null_dim_tempo_mes ...................................... [RUN]
[0m14:31:52.525556 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306, now test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972)
[0m14:31:52.526552 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:31:52.532536 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:31:52.534530 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (compile): 14:31:52.526552 => 14:31:52.534530
[0m14:31:52.535528 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:31:52.539517 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:31:52.541512 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:52.541512 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:31:52.542511 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select mes
from `workspace`.`default`.`dim_tempo`
where mes is null



      
    ) dbt_internal_test
[0m14:31:52.543514 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:53.637968 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-9013-1745-a5ff-31fbed7d428d
[0m14:31:54.306854 [debug] [Thread-1 (]: SQL status: OK in 1.7599999904632568 seconds
[0m14:31:54.316791 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (execute): 14:31:52.535528 => 14:31:54.315792
[0m14:31:54.319821 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: ROLLBACK
[0m14:31:54.321815 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:54.322812 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: Close
[0m14:31:54.324799 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-9013-1745-a5ff-31fbed7d428d
[0m14:31:54.557905 [info ] [Thread-1 (]: 8 of 21 PASS not_null_dim_tempo_mes ............................................ [[32mPASS[0m in 2.03s]
[0m14:31:54.560866 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:31:54.561861 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:31:54.562859 [info ] [Thread-1 (]: 9 of 21 START test not_null_fato_nascimento_APGAR1 ............................. [RUN]
[0m14:31:54.565461 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972, now test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074)
[0m14:31:54.566459 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:31:54.580395 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:31:54.582387 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (compile): 14:31:54.567455 => 14:31:54.581391
[0m14:31:54.583384 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:31:54.591395 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:31:54.593358 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:54.594354 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:31:54.594354 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR1
from `workspace`.`default`.`fato_nascimento`
where APGAR1 is null



      
    ) dbt_internal_test
[0m14:31:54.595351 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:55.476032 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-9142-186e-90f5-fb1730629010
[0m14:31:56.065776 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m14:31:56.068772 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (execute): 14:31:54.584412 => 14:31:56.068772
[0m14:31:56.069737 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: ROLLBACK
[0m14:31:56.069737 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:56.070762 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: Close
[0m14:31:56.071732 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-9142-186e-90f5-fb1730629010
[0m14:31:56.305282 [info ] [Thread-1 (]: 9 of 21 PASS not_null_fato_nascimento_APGAR1 ................................... [[32mPASS[0m in 1.74s]
[0m14:31:56.306278 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:31:56.307276 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:31:56.307276 [info ] [Thread-1 (]: 10 of 21 START test not_null_fato_nascimento_APGAR5 ............................ [RUN]
[0m14:31:56.308405 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074, now test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994)
[0m14:31:56.309434 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:31:56.314420 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:31:56.316387 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (compile): 14:31:56.309434 => 14:31:56.316387
[0m14:31:56.317384 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:31:56.320378 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:31:56.321401 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:56.322372 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:31:56.323373 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR5
from `workspace`.`default`.`fato_nascimento`
where APGAR5 is null



      
    ) dbt_internal_test
[0m14:31:56.323373 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:31:57.723124 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-925b-1def-bb3e-cd9cea8710a7
[0m14:31:58.531117 [debug] [Thread-1 (]: SQL status: OK in 2.2100000381469727 seconds
[0m14:31:58.539092 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (execute): 14:31:56.317384 => 14:31:58.538096
[0m14:31:58.540093 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: ROLLBACK
[0m14:31:58.542085 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:31:58.543109 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: Close
[0m14:31:58.544079 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-925b-1def-bb3e-cd9cea8710a7
[0m14:31:58.938287 [info ] [Thread-1 (]: 10 of 21 PASS not_null_fato_nascimento_APGAR5 .................................. [[32mPASS[0m in 2.63s]
[0m14:31:58.941314 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:31:58.943309 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:31:58.944312 [info ] [Thread-1 (]: 11 of 21 START test not_null_fato_nascimento_cod_municipio ..................... [RUN]
[0m14:31:58.947293 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m14:31:58.949258 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:31:58.963221 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:31:58.966212 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 14:31:58.950255 => 14:31:58.965215
[0m14:31:58.967209 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:31:58.972195 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:31:58.974190 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:31:58.974190 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:31:58.975188 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m14:31:58.976185 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:00.707675 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-9443-1cc4-a747-c83a0af37bb6
[0m14:32:01.626013 [debug] [Thread-1 (]: SQL status: OK in 2.6500000953674316 seconds
[0m14:32:01.631966 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 14:31:58.968206 => 14:32:01.631966
[0m14:32:01.633961 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m14:32:01.635955 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:01.637951 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m14:32:01.638947 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-9443-1cc4-a747-c83a0af37bb6
[0m14:32:02.109960 [info ] [Thread-1 (]: 11 of 21 PASS not_null_fato_nascimento_cod_municipio ........................... [[32mPASS[0m in 3.16s]
[0m14:32:02.114946 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:32:02.117957 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:32:02.119932 [info ] [Thread-1 (]: 12 of 21 START test not_null_fato_nascimento_data_id ........................... [RUN]
[0m14:32:02.122923 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m14:32:02.123919 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:32:02.133893 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:32:02.136885 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 14:32:02.124917 => 14:32:02.135887
[0m14:32:02.137882 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:32:02.141871 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:32:02.142868 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:02.143865 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:32:02.143865 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m14:32:02.144863 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:04.208168 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-9674-13e2-b1ae-baab10573a2b
[0m14:32:05.150542 [debug] [Thread-1 (]: SQL status: OK in 3.0 seconds
[0m14:32:05.153533 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 14:32:02.137882 => 14:32:05.153533
[0m14:32:05.154502 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m14:32:05.155501 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:05.156497 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m14:32:05.156497 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-9674-13e2-b1ae-baab10573a2b
[0m14:32:05.392041 [info ] [Thread-1 (]: 12 of 21 PASS not_null_fato_nascimento_data_id ................................. [[32mPASS[0m in 3.27s]
[0m14:32:05.394000 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:32:05.395995 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:32:05.396992 [info ] [Thread-1 (]: 13 of 21 START test not_null_fato_nascimento_gestacao_semanas .................. [RUN]
[0m14:32:05.399017 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2)
[0m14:32:05.400018 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:32:05.407962 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:32:05.409956 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (compile): 14:32:05.401014 => 14:32:05.408988
[0m14:32:05.409956 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:32:05.412976 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:32:05.413973 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:05.414943 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:32:05.415940 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select gestacao_semanas
from `workspace`.`default`.`fato_nascimento`
where gestacao_semanas is null



      
    ) dbt_internal_test
[0m14:32:05.415940 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:06.235294 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-97ab-1de5-98a0-f9bc827b171a
[0m14:32:06.820783 [debug] [Thread-1 (]: SQL status: OK in 1.399999976158142 seconds
[0m14:32:06.826766 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (execute): 14:32:05.409956 => 14:32:06.825769
[0m14:32:06.827764 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: ROLLBACK
[0m14:32:06.827764 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:06.828792 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: Close
[0m14:32:06.829789 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-97ab-1de5-98a0-f9bc827b171a
[0m14:32:07.093041 [info ] [Thread-1 (]: 13 of 21 PASS not_null_fato_nascimento_gestacao_semanas ........................ [[32mPASS[0m in 1.70s]
[0m14:32:07.093881 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:32:07.094909 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:32:07.095890 [info ] [Thread-1 (]: 14 of 21 START test not_null_fato_nascimento_idade_mae ......................... [RUN]
[0m14:32:07.096876 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2, now test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42)
[0m14:32:07.097873 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:32:07.106849 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:32:07.108843 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (compile): 14:32:07.097873 => 14:32:07.107847
[0m14:32:07.109842 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:32:07.113859 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:32:07.114827 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:07.114827 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:32:07.115854 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select idade_mae
from `workspace`.`default`.`fato_nascimento`
where idade_mae is null



      
    ) dbt_internal_test
[0m14:32:07.116822 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:08.436216 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-98e0-1ccf-8ead-fcef28a9466e
[0m14:32:10.510402 [debug] [Thread-1 (]: SQL status: OK in 3.390000104904175 seconds
[0m14:32:10.513394 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (execute): 14:32:07.110839 => 14:32:10.513394
[0m14:32:10.514391 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: ROLLBACK
[0m14:32:10.514391 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:10.515389 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: Close
[0m14:32:10.515389 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-98e0-1ccf-8ead-fcef28a9466e
[0m14:32:10.943922 [info ] [Thread-1 (]: 14 of 21 PASS not_null_fato_nascimento_idade_mae ............................... [[32mPASS[0m in 3.85s]
[0m14:32:10.945947 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:32:10.945947 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:32:10.946941 [info ] [Thread-1 (]: 15 of 21 START test not_null_fato_nascimento_nascimento_id ..................... [RUN]
[0m14:32:10.947893 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m14:32:10.948924 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:32:10.955876 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:32:10.957871 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 14:32:10.948924 => 14:32:10.956902
[0m14:32:10.957871 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:32:10.961860 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:32:10.963854 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:10.964853 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:32:10.965867 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m14:32:10.965867 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:12.234293 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-9b3e-12af-a92f-7aca35a05daa
[0m14:32:13.146262 [debug] [Thread-1 (]: SQL status: OK in 2.180000066757202 seconds
[0m14:32:13.156271 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 14:32:10.958896 => 14:32:13.155273
[0m14:32:13.158230 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m14:32:13.161258 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:13.162251 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m14:32:13.164214 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-9b3e-12af-a92f-7aca35a05daa
[0m14:32:13.719501 [info ] [Thread-1 (]: 15 of 21 PASS not_null_fato_nascimento_nascimento_id ........................... [[32mPASS[0m in 2.77s]
[0m14:32:13.723527 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:32:13.725522 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:32:13.727517 [info ] [Thread-1 (]: 16 of 21 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m14:32:13.732467 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m14:32:13.734459 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:32:13.745458 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:32:13.748421 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 14:32:13.735456 => 14:32:13.747424
[0m14:32:13.749418 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:32:13.754404 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:32:13.756398 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:13.756398 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:32:13.757396 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m14:32:13.757396 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:15.627725 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-9d2c-18dc-8429-f23c7e6da6ce
[0m14:32:16.243575 [debug] [Thread-1 (]: SQL status: OK in 2.490000009536743 seconds
[0m14:32:16.247581 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 14:32:13.750416 => 14:32:16.246570
[0m14:32:16.247581 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m14:32:16.248589 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:16.249558 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m14:32:16.249558 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-9d2c-18dc-8429-f23c7e6da6ce
[0m14:32:16.616717 [info ] [Thread-1 (]: 16 of 21 PASS not_null_fato_nascimento_peso .................................... [[32mPASS[0m in 2.89s]
[0m14:32:16.618717 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:32:16.618717 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:32:16.619743 [info ] [Thread-1 (]: 17 of 21 START test not_null_fato_nascimento_sexo .............................. [RUN]
[0m14:32:16.621740 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m14:32:16.621740 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:32:16.629687 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:32:16.631682 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 14:32:16.622735 => 14:32:16.631682
[0m14:32:16.633677 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:32:16.637694 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:32:16.639661 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:16.639661 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:32:16.640686 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sexo
from `workspace`.`default`.`fato_nascimento`
where sexo is null



      
    ) dbt_internal_test
[0m14:32:16.640686 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:17.758979 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-9e79-13c7-bfc2-cf05b23214f9
[0m14:32:18.508106 [debug] [Thread-1 (]: SQL status: OK in 1.8700000047683716 seconds
[0m14:32:18.511077 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 14:32:16.634674 => 14:32:18.511077
[0m14:32:18.512074 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: ROLLBACK
[0m14:32:18.513072 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:18.513072 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: Close
[0m14:32:18.514081 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-9e79-13c7-bfc2-cf05b23214f9
[0m14:32:18.878915 [info ] [Thread-1 (]: 17 of 21 PASS not_null_fato_nascimento_sexo .................................... [[32mPASS[0m in 2.26s]
[0m14:32:18.882904 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:32:18.884899 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:32:18.887852 [info ] [Thread-1 (]: 18 of 21 START test not_null_fato_nascimento_tipo_parto ........................ [RUN]
[0m14:32:18.891843 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e)
[0m14:32:18.892839 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:32:18.906801 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:32:18.908796 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (compile): 14:32:18.894833 => 14:32:18.907800
[0m14:32:18.908796 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:32:18.912812 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:32:18.913810 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:18.914810 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:32:18.914810 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tipo_parto
from `workspace`.`default`.`fato_nascimento`
where tipo_parto is null



      
    ) dbt_internal_test
[0m14:32:18.915805 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:20.211146 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-9ff8-1fd5-b71b-03f7a1e98151
[0m14:32:20.851973 [debug] [Thread-1 (]: SQL status: OK in 1.940000057220459 seconds
[0m14:32:20.854960 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (execute): 14:32:18.909820 => 14:32:20.854960
[0m14:32:20.855937 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: ROLLBACK
[0m14:32:20.856933 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:20.856933 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: Close
[0m14:32:20.857930 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-9ff8-1fd5-b71b-03f7a1e98151
[0m14:32:21.107729 [info ] [Thread-1 (]: 18 of 21 PASS not_null_fato_nascimento_tipo_parto .............................. [[32mPASS[0m in 2.22s]
[0m14:32:21.109723 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:32:21.109723 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:32:21.110716 [info ] [Thread-1 (]: 19 of 21 START test unique_dim_localidade_cod_municipio ........................ [RUN]
[0m14:32:21.112711 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m14:32:21.112711 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:32:21.125683 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:32:21.127645 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 14:32:21.113708 => 14:32:21.126678
[0m14:32:21.128644 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:32:21.133660 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:32:21.134627 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:21.135651 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:32:21.136622 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_municipio as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_localidade`
where cod_municipio is not null
group by cod_municipio
having count(*) > 1



      
    ) dbt_internal_test
[0m14:32:21.136622 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:22.163344 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-a128-1b1e-b0d5-3d27e986ab20
[0m14:32:22.647289 [debug] [Thread-1 (]: SQL status: OK in 1.5099999904632568 seconds
[0m14:32:22.650279 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 14:32:21.129640 => 14:32:22.649281
[0m14:32:22.650279 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: ROLLBACK
[0m14:32:22.651279 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:22.652245 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: Close
[0m14:32:22.652245 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-a128-1b1e-b0d5-3d27e986ab20
[0m14:32:22.882825 [info ] [Thread-1 (]: 19 of 21 PASS unique_dim_localidade_cod_municipio .............................. [[32mPASS[0m in 1.77s]
[0m14:32:22.886852 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:32:22.888814 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:32:22.891802 [info ] [Thread-1 (]: 20 of 21 START test unique_dim_tempo_data_id ................................... [RUN]
[0m14:32:22.897784 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m14:32:22.898781 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:32:22.910778 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:32:22.912743 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 14:32:22.900778 => 14:32:22.911776
[0m14:32:22.912743 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:32:22.916760 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:32:22.917759 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:22.918756 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:32:22.919724 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:32:22.919724 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:24.213454 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-a248-147c-9e87-da4c13a1039a
[0m14:32:25.010289 [debug] [Thread-1 (]: SQL status: OK in 2.0899999141693115 seconds
[0m14:32:25.013277 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 14:32:22.913769 => 14:32:25.012280
[0m14:32:25.014246 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m14:32:25.014246 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:25.015276 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m14:32:25.015276 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-a248-147c-9e87-da4c13a1039a
[0m14:32:25.379643 [info ] [Thread-1 (]: 20 of 21 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 2.48s]
[0m14:32:25.384536 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:32:25.386499 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:32:25.388527 [info ] [Thread-1 (]: 21 of 21 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m14:32:25.393482 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m14:32:25.395476 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:32:25.408469 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:32:25.412429 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 14:32:25.396475 => 14:32:25.412429
[0m14:32:25.413458 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:32:25.417414 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:32:25.419409 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:25.419409 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:32:25.420406 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:32:25.420406 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:26.564601 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-a3c8-1b07-8a22-1d781127c4dc
[0m14:32:27.889093 [debug] [Thread-1 (]: SQL status: OK in 2.4700000286102295 seconds
[0m14:32:27.899058 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 14:32:25.414427 => 14:32:27.897063
[0m14:32:27.901051 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m14:32:27.902047 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:32:27.904073 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m14:32:27.905042 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-a3c8-1b07-8a22-1d781127c4dc
[0m14:32:28.303083 [error] [Thread-1 (]: 21 of 21 FAIL 1 unique_fato_nascimento_nascimento_id ........................... [[31mFAIL 1[0m in 2.91s]
[0m14:32:28.305090 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:32:28.307074 [debug] [MainThread]: On master: ROLLBACK
[0m14:32:28.308071 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:32:30.019814 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07934-a5af-160f-9618-05f8bc4dec46
[0m14:32:30.020778 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:32:30.020778 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:32:30.021776 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:32:30.022773 [debug] [MainThread]: On master: ROLLBACK
[0m14:32:30.022773 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:32:30.023770 [debug] [MainThread]: On master: Close
[0m14:32:30.023770 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07934-a5af-160f-9618-05f8bc4dec46
[0m14:32:30.520508 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:32:30.521502 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:32:30.521502 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m14:32:30.522476 [info ] [MainThread]: 
[0m14:32:30.523502 [info ] [MainThread]: Finished running 21 tests in 0 hours 0 minutes and 54.26 seconds (54.26s).
[0m14:32:30.529486 [debug] [MainThread]: Command end result
[0m14:32:30.547438 [info ] [MainThread]: 
[0m14:32:30.549404 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:32:30.550401 [info ] [MainThread]: 
[0m14:32:30.551398 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m14:32:30.552397 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m14:32:30.553405 [info ] [MainThread]: 
[0m14:32:30.554398 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m14:32:30.555387 [info ] [MainThread]: 
[0m14:32:30.556400 [info ] [MainThread]: Done. PASS=20 WARN=0 ERROR=1 SKIP=0 TOTAL=21
[0m14:32:30.557383 [debug] [MainThread]: Command `dbt test` failed at 14:32:30.557383 after 56.06 seconds
[0m14:32:30.558380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4F6AD2B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4F6A9E490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D4F6D686D0>]}
[0m14:32:30.558380 [debug] [MainThread]: Flushing usage events
[0m14:34:23.540338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBBFBB7690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBBD6F1190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBBFB42F10>]}


============================== 14:34:23.545325 | fa420c1b-65f8-4b05-8319-910ce4c527c3 ==============================
[0m14:34:23.545325 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:34:23.546294 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:34:24.948897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBBFAA2DD0>]}
[0m14:34:24.973829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBBFAB2C90>]}
[0m14:34:24.973829 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:34:25.004719 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:34:25.141353 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:34:25.142351 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m14:34:25.229147 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m14:34:25.265050 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging\stg_nascidos_vivos.sql
[0m14:34:25.281977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBCA7216D0>]}
[0m14:34:25.297947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBC9582990>]}
[0m14:34:25.297947 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:34:25.299220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBC6B3DE90>]}
[0m14:34:25.302216 [info ] [MainThread]: 
[0m14:34:25.305229 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:34:25.308199 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m14:34:25.309196 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m14:34:25.309196 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m14:34:25.310193 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:26.774869 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07934-eb5c-147f-b779-4fedf4afd194
[0m14:34:27.430364 [debug] [ThreadPool]: SQL status: OK in 2.119999885559082 seconds
[0m14:34:27.434387 [debug] [ThreadPool]: On list_workspace: Close
[0m14:34:27.435355 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07934-eb5c-147f-b779-4fedf4afd194
[0m14:34:27.874716 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m14:34:27.876712 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m14:34:27.894692 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:27.895689 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m14:34:27.896658 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m14:34:27.897655 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:34:28.991047 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07934-ecc2-1a20-a38f-1a101ca8d414
[0m14:34:29.348606 [debug] [ThreadPool]: SQL status: OK in 1.4500000476837158 seconds
[0m14:34:29.349603 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:34:29.350600 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m14:34:29.350600 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:34:29.351597 [debug] [ThreadPool]: On create_workspace_default: Close
[0m14:34:29.352594 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07934-ecc2-1a20-a38f-1a101ca8d414
[0m14:34:29.582106 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:34:29.588090 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:34:29.589087 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:34:29.590084 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:30.378285 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07934-ed94-10a9-9f7e-9b9915a13c08
[0m14:34:30.775017 [debug] [ThreadPool]: SQL status: OK in 1.1799999475479126 seconds
[0m14:34:30.779032 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:34:30.780030 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07934-ed94-10a9-9f7e-9b9915a13c08
[0m14:34:31.001594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBBFD45E90>]}
[0m14:34:31.002592 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:31.002592 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:34:31.003589 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:34:31.004593 [info ] [MainThread]: 
[0m14:34:31.014559 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m14:34:31.015558 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m14:34:31.018211 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m14:34:31.018211 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m14:34:31.022200 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m14:34:31.023171 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 14:34:31.019207 => 14:34:31.023171
[0m14:34:31.024165 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m14:34:31.063061 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m14:34:31.064058 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:31.065056 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m14:34:31.066053 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m14:34:31.067050 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:34:31.889377 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-ee7b-1b5e-ae9d-510ca028ff63
[0m14:34:32.611096 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m14:34:32.639044 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 14:34:31.025164 => 14:34:32.639044
[0m14:34:32.639044 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m14:34:32.640043 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:32.641011 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m14:34:32.641011 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-ee7b-1b5e-ae9d-510ca028ff63
[0m14:34:32.974333 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBC95A0F10>]}
[0m14:34:32.976297 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.96s]
[0m14:34:32.980286 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m14:34:32.982281 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m14:34:32.983278 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m14:34:32.986864 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m14:34:32.987861 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m14:34:32.994811 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m14:34:32.997803 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 14:34:32.988829 => 14:34:32.996806
[0m14:34:32.998800 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m14:34:33.003786 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m14:34:33.004783 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:33.005781 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m14:34:33.005781 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m14:34:33.006778 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:34.537086 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-efe2-13fe-a5cd-86e826f386b7
[0m14:34:35.182946 [debug] [Thread-1 (]: SQL status: OK in 2.180000066757202 seconds
[0m14:34:35.185938 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 14:34:32.998800 => 14:34:35.184941
[0m14:34:35.185938 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m14:34:35.186943 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:35.186943 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m14:34:35.187933 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-efe2-13fe-a5cd-86e826f386b7
[0m14:34:35.415594 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBCA6DFE50>]}
[0m14:34:35.418621 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 2.43s]
[0m14:34:35.422576 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m14:34:35.424568 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m14:34:35.427599 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m14:34:35.431552 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m14:34:35.433544 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m14:34:35.440524 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m14:34:35.441527 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 14:34:35.434544 => 14:34:35.441527
[0m14:34:35.442519 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m14:34:35.450525 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m14:34:35.452493 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:35.452493 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m14:34:35.453518 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m14:34:35.453518 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:36.280397 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-f119-10bf-9bdc-cef1a27da005
[0m14:34:36.933234 [debug] [Thread-1 (]: SQL status: OK in 1.4800000190734863 seconds
[0m14:34:36.939216 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 14:34:35.443516 => 14:34:36.939216
[0m14:34:36.941211 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m14:34:36.942207 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:36.943206 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m14:34:36.945200 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-f119-10bf-9bdc-cef1a27da005
[0m14:34:37.177675 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBCA745B90>]}
[0m14:34:37.179667 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.75s]
[0m14:34:37.181663 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m14:34:37.182630 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:34:37.183627 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m14:34:37.185781 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m14:34:37.185781 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m14:34:37.200739 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:34:37.202705 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 14:34:37.186778 => 14:34:37.201736
[0m14:34:37.202705 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m14:34:37.207721 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:34:37.209687 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:37.209687 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:34:37.210713 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
),

final AS (
    SELECT
        md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(GESTACAO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PARTO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(APGAR1 as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(APGAR5 as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
        CODMUNNASC AS cod_municipio,
        LOCNASC AS local_nascimento,
        DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
        HORANASC AS hora_nascimento,
        CASE
            WHEN SEXO = 1 THEN 'Masculino'
            WHEN SEXO = 2 THEN 'Feminino'
            ELSE 'Ignorado'
        END AS sexo,
        PESO AS peso,
        IDADEMAE AS idade_mae,
        GESTACAO AS gestacao_semanas,
        PARTO AS tipo_parto,
        APGAR1,
        APGAR5
    FROM nascidos_vivos_raw
)

SELECT * FROM final

[0m14:34:37.211702 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:37.985975 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-f21f-146d-86b6-148567ddf1ce
[0m14:34:38.662778 [debug] [Thread-1 (]: SQL status: OK in 1.4500000476837158 seconds
[0m14:34:38.668762 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 14:34:37.203733 => 14:34:38.667798
[0m14:34:38.669761 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m14:34:38.670764 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:38.671754 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m14:34:38.671754 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-f21f-146d-86b6-148567ddf1ce
[0m14:34:38.906875 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBCA5A3B10>]}
[0m14:34:38.908867 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.72s]
[0m14:34:38.910862 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:34:38.911860 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m14:34:38.912729 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m14:34:38.914955 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m14:34:38.915923 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m14:34:38.922934 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m14:34:38.924900 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 14:34:38.916922 => 14:34:38.923902
[0m14:34:38.925927 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m14:34:38.932876 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m14:34:38.934874 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:38.935898 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m14:34:38.936866 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m14:34:38.936866 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:39.729946 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-f327-13f4-8640-804421e5ba6f
[0m14:34:40.349460 [debug] [Thread-1 (]: SQL status: OK in 1.409999966621399 seconds
[0m14:34:40.360419 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 14:34:38.925927 => 14:34:40.359434
[0m14:34:40.360419 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m14:34:40.361432 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:40.361432 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m14:34:40.362414 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-f327-13f4-8640-804421e5ba6f
[0m14:34:40.587782 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBC954C4D0>]}
[0m14:34:40.589777 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.67s]
[0m14:34:40.591772 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m14:34:40.593803 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m14:34:40.594763 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m14:34:40.597757 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m14:34:40.599785 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m14:34:40.609757 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m14:34:40.612750 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 14:34:40.600758 => 14:34:40.611752
[0m14:34:40.613744 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m14:34:40.620693 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m14:34:40.621691 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:40.622689 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m14:34:40.622689 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m14:34:40.623685 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:41.531425 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-f43b-15d8-84cd-4c2778eb93e2
[0m14:34:42.377125 [debug] [Thread-1 (]: SQL status: OK in 1.75 seconds
[0m14:34:42.381111 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 14:34:40.613744 => 14:34:42.381111
[0m14:34:42.382108 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m14:34:42.383107 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:42.384104 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m14:34:42.384104 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-f43b-15d8-84cd-4c2778eb93e2
[0m14:34:42.633359 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBCA891210>]}
[0m14:34:42.634355 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 2.04s]
[0m14:34:42.636349 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m14:34:42.637348 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m14:34:42.639374 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m14:34:42.640339 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m14:34:42.641368 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m14:34:42.648350 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m14:34:42.649347 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 14:34:42.642363 => 14:34:42.649347
[0m14:34:42.650342 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m14:34:42.661310 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m14:34:42.662280 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:42.662280 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m14:34:42.663277 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH base AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM base

[0m14:34:42.663277 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:43.598755 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-f572-1c56-988e-724bfe450ecf
[0m14:34:44.365199 [debug] [Thread-1 (]: SQL status: OK in 1.7000000476837158 seconds
[0m14:34:44.368191 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 14:34:42.651340 => 14:34:44.367194
[0m14:34:44.368191 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m14:34:44.369188 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:44.370186 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m14:34:44.370186 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-f572-1c56-988e-724bfe450ecf
[0m14:34:44.700032 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBCA81D810>]}
[0m14:34:44.702026 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 2.06s]
[0m14:34:44.707045 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m14:34:44.709050 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m14:34:44.710999 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m14:34:44.711996 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m14:34:44.713016 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m14:34:44.719007 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m14:34:44.720972 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 14:34:44.713992 => 14:34:44.720003
[0m14:34:44.720972 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m14:34:44.726955 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m14:34:44.727953 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:44.728951 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m14:34:44.728951 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m14:34:44.729948 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:45.790807 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-f6c4-1b58-8002-0a5da64ae58b
[0m14:34:46.500848 [debug] [Thread-1 (]: SQL status: OK in 1.7699999809265137 seconds
[0m14:34:46.505863 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 14:34:44.722015 => 14:34:46.504863
[0m14:34:46.506830 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m14:34:46.507827 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:46.508825 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m14:34:46.509854 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-f6c4-1b58-8002-0a5da64ae58b
[0m14:34:46.745851 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBCA6822D0>]}
[0m14:34:46.746820 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 2.03s]
[0m14:34:46.747822 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m14:34:46.748841 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:34:46.749840 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m14:34:46.750815 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m14:34:46.751804 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:34:46.755794 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:34:46.756791 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 14:34:46.751804 => 14:34:46.756791
[0m14:34:46.757789 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:34:46.762804 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:34:46.763807 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:46.764798 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:34:46.764798 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m14:34:46.765797 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:47.596801 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-f7d8-1313-8a9d-89c2e31aba64
[0m14:34:48.315947 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m14:34:48.319937 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 14:34:46.758787 => 14:34:48.318939
[0m14:34:48.319937 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m14:34:48.320900 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:48.321912 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m14:34:48.321912 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-f7d8-1313-8a9d-89c2e31aba64
[0m14:34:48.554311 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBCA81D190>]}
[0m14:34:48.555273 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.80s]
[0m14:34:48.556291 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:34:48.557303 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m14:34:48.558264 [info ] [Thread-1 (]: 10 of 12 START sql view model default.dim_localidade ........................... [RUN]
[0m14:34:48.560259 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m14:34:48.560259 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m14:34:48.565269 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m14:34:48.567240 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 14:34:48.561281 => 14:34:48.566269
[0m14:34:48.568239 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m14:34:48.572260 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m14:34:48.573253 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:48.574222 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m14:34:48.575219 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    

SELECT
    cod_municipio,
    ANY_VALUE(local_nascimento) AS municipio -- Pega qualquer valor de local_nascimento
FROM `workspace`.`default`.`int_nascimento`
GROUP BY cod_municipio

[0m14:34:48.575219 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:49.326496 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-f8e1-1dec-a8d0-3fa8c2da9408
[0m14:34:50.010603 [debug] [Thread-1 (]: SQL status: OK in 1.440000057220459 seconds
[0m14:34:50.014593 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 14:34:48.568239 => 14:34:50.014593
[0m14:34:50.015591 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m14:34:50.016588 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:50.017585 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m14:34:50.018583 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-f8e1-1dec-a8d0-3fa8c2da9408
[0m14:34:50.267401 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBCA627650>]}
[0m14:34:50.268368 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.dim_localidade ...................... [[32mOK[0m in 1.71s]
[0m14:34:50.269632 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m14:34:50.270663 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m14:34:50.271671 [info ] [Thread-1 (]: 11 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m14:34:50.273653 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m14:34:50.273653 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m14:34:50.279609 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m14:34:50.280605 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 14:34:50.274651 => 14:34:50.280605
[0m14:34:50.281602 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m14:34:50.287617 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m14:34:50.288585 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:50.289611 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m14:34:50.289611 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

WITH datas AS (
    SELECT DISTINCT
        data_nascimento AS data
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    ROW_NUMBER() OVER (ORDER BY data) AS data_id,
    data,
    EXTRACT(YEAR FROM data) AS ano,
    EXTRACT(MONTH FROM data) AS mes,
    EXTRACT(DAY FROM data) AS dia,
    DAYOFWEEK(data) AS dia_semana
FROM datas

[0m14:34:50.290607 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:52.177711 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-fa60-1e39-841e-fe81e166d4a2
[0m14:34:53.151482 [debug] [Thread-1 (]: SQL status: OK in 2.859999895095825 seconds
[0m14:34:53.157497 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 14:34:50.281602 => 14:34:53.157497
[0m14:34:53.159498 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m14:34:53.160494 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:53.162484 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m14:34:53.163487 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-fa60-1e39-841e-fe81e166d4a2
[0m14:34:53.527721 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBCA857E50>]}
[0m14:34:53.528719 [info ] [Thread-1 (]: 11 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 3.26s]
[0m14:34:53.533706 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m14:34:53.534703 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m14:34:53.535703 [info ] [Thread-1 (]: 12 of 12 START sql view model default.fato_nascimento .......................... [RUN]
[0m14:34:53.540687 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_nascimento)
[0m14:34:53.541684 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m14:34:53.550660 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m14:34:53.551657 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 14:34:53.541684 => 14:34:53.551657
[0m14:34:53.552655 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m14:34:53.557641 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m14:34:53.558638 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:53.558638 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m14:34:53.559636 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    i.nascimento_id,
    t.data_id,
    l.cod_municipio,
    i.sexo,
    COALESCE(i.peso, 0) AS peso,
    i.idade_mae,
    COALESCE(i.gestacao_semanas, 0) AS gestacao_semanas,
    COALESCE(i.tipo_parto, 0) AS tipo_parto,
    COALESCE(i.APGAR1, 0) AS APGAR1,
    COALESCE(i.APGAR5, 0) AS APGAR5
FROM `workspace`.`default`.`int_nascimento` AS i
LEFT JOIN `workspace`.`default`.`dim_tempo` AS t
    ON i.data_nascimento = t.data
LEFT JOIN `workspace`.`default`.`dim_localidade` AS l
    ON i.cod_municipio = l.cod_municipio

[0m14:34:53.559636 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:34:55.244340 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07934-fc43-11d0-a8ff-8b9f3cb19b8b
[0m14:34:56.108085 [debug] [Thread-1 (]: SQL status: OK in 2.549999952316284 seconds
[0m14:34:56.111073 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 14:34:53.552655 => 14:34:56.111073
[0m14:34:56.111073 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m14:34:56.112071 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:34:56.112071 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m14:34:56.113068 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07934-fc43-11d0-a8ff-8b9f3cb19b8b
[0m14:34:56.350011 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa420c1b-65f8-4b05-8319-910ce4c527c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBC95BDED0>]}
[0m14:34:56.352006 [info ] [Thread-1 (]: 12 of 12 OK created sql view model default.fato_nascimento ..................... [[32mOK[0m in 2.81s]
[0m14:34:56.354002 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m14:34:56.357989 [debug] [MainThread]: On master: ROLLBACK
[0m14:34:56.358986 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:34:57.133816 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07934-fd88-1f27-84c7-927ab69389b8
[0m14:34:57.135777 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:34:57.136773 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:34:57.137813 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:34:57.138801 [debug] [MainThread]: On master: ROLLBACK
[0m14:34:57.139799 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:34:57.140794 [debug] [MainThread]: On master: Close
[0m14:34:57.141759 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07934-fd88-1f27-84c7-927ab69389b8
[0m14:34:57.366104 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:57.366104 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m14:34:57.367135 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:34:57.367135 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m14:34:57.369085 [info ] [MainThread]: 
[0m14:34:57.370065 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 32.06 seconds (32.06s).
[0m14:34:57.374081 [debug] [MainThread]: Command end result
[0m14:34:57.389014 [info ] [MainThread]: 
[0m14:34:57.391009 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:34:57.392005 [info ] [MainThread]: 
[0m14:34:57.393012 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 TOTAL=12
[0m14:34:57.394001 [debug] [MainThread]: Command `dbt run` succeeded at 14:34:57.394001 after 33.87 seconds
[0m14:34:57.395026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBC0001D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBB9B9C910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BBB98EBB10>]}
[0m14:34:57.395026 [debug] [MainThread]: Flushing usage events
[0m14:35:07.314526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A69488E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A690762D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A69161F50>]}


============================== 14:35:07.318521 | 8e1b0fee-50c1-4167-83f1-cb280d327b96 ==============================
[0m14:35:07.318521 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:35:07.320483 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:35:08.710313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8e1b0fee-50c1-4167-83f1-cb280d327b96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A69076190>]}
[0m14:35:08.732261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8e1b0fee-50c1-4167-83f1-cb280d327b96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A73C181D0>]}
[0m14:35:08.733259 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:35:08.762177 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:35:08.909786 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:08.910787 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:08.976609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8e1b0fee-50c1-4167-83f1-cb280d327b96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A73DDF6D0>]}
[0m14:35:08.993534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8e1b0fee-50c1-4167-83f1-cb280d327b96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A73C8BE10>]}
[0m14:35:08.993534 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:35:08.994559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8e1b0fee-50c1-4167-83f1-cb280d327b96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A66174910>]}
[0m14:35:08.998548 [info ] [MainThread]: 
[0m14:35:08.999532 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:35:09.002533 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:35:09.008493 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:35:09.009514 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:35:09.010492 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:10.959902 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07935-05b6-11c1-993e-095c2177e5a1
[0m14:35:11.708859 [debug] [ThreadPool]: SQL status: OK in 2.700000047683716 seconds
[0m14:35:11.716818 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:35:11.717814 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07935-05b6-11c1-993e-095c2177e5a1
[0m14:35:12.045211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8e1b0fee-50c1-4167-83f1-cb280d327b96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A6894D6D0>]}
[0m14:35:12.046209 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:12.046209 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:35:12.047207 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:35:12.058177 [info ] [MainThread]: 
[0m14:35:12.068541 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:35:12.069570 [info ] [Thread-1 (]: 1 of 21 START test accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado  [RUN]
[0m14:35:12.070535 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d'
[0m14:35:12.071532 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:35:12.092487 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:35:12.094516 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (compile): 14:35:12.072529 => 14:35:12.094516
[0m14:35:12.095491 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:35:12.119405 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:35:12.121398 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:12.122402 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:35:12.122402 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'Masculino','Feminino','Ignorado'
)



      
    ) dbt_internal_test
[0m14:35:12.123393 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:35:13.556266 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-0737-1cda-8fb3-6a56bdc17a2c
[0m14:35:14.471695 [debug] [Thread-1 (]: SQL status: OK in 2.3499999046325684 seconds
[0m14:35:14.480701 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (execute): 14:35:12.095491 => 14:35:14.479702
[0m14:35:14.481696 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: ROLLBACK
[0m14:35:14.482663 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:14.483659 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: Close
[0m14:35:14.484657 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-0737-1cda-8fb3-6a56bdc17a2c
[0m14:35:15.065904 [info ] [Thread-1 (]: 1 of 21 PASS accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado  [[32mPASS[0m in 3.00s]
[0m14:35:15.068895 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:35:15.069894 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:35:15.070857 [info ] [Thread-1 (]: 2 of 21 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m14:35:15.073850 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m14:35:15.074848 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:35:15.087811 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:35:15.090804 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 14:35:15.075844 => 14:35:15.089807
[0m14:35:15.091801 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:35:15.094815 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:35:15.096787 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:15.096787 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:35:15.097805 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m14:35:15.097805 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:17.701964 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-0985-12a7-8a45-a1c24be1fd22
[0m14:35:18.813732 [debug] [Thread-1 (]: SQL status: OK in 3.7200000286102295 seconds
[0m14:35:18.819745 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 14:35:15.091801 => 14:35:18.818749
[0m14:35:18.820747 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m14:35:18.821743 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:18.823735 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m14:35:18.824733 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-0985-12a7-8a45-a1c24be1fd22
[0m14:35:19.159384 [info ] [Thread-1 (]: 2 of 21 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 4.09s]
[0m14:35:19.164410 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:35:19.166405 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:35:19.168357 [info ] [Thread-1 (]: 3 of 21 START test not_null_dim_localidade_municipio ........................... [RUN]
[0m14:35:19.171351 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771)
[0m14:35:19.173343 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:35:19.184313 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:35:19.185314 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (compile): 14:35:19.173343 => 14:35:19.185314
[0m14:35:19.186308 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:35:19.190326 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:35:19.191305 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:19.192293 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:35:19.193290 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select municipio
from `workspace`.`default`.`dim_localidade`
where municipio is null



      
    ) dbt_internal_test
[0m14:35:19.194287 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:20.736995 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-0b72-165f-891a-0b84740cd5ba
[0m14:35:21.691953 [debug] [Thread-1 (]: SQL status: OK in 2.5 seconds
[0m14:35:21.694918 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (execute): 14:35:19.186308 => 14:35:21.694918
[0m14:35:21.695913 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: ROLLBACK
[0m14:35:21.696911 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:21.696911 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: Close
[0m14:35:21.697908 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-0b72-165f-891a-0b84740cd5ba
[0m14:35:22.399633 [info ] [Thread-1 (]: 3 of 21 PASS not_null_dim_localidade_municipio ................................. [[32mPASS[0m in 3.23s]
[0m14:35:22.401623 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:35:22.402620 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:35:22.402620 [info ] [Thread-1 (]: 4 of 21 START test not_null_dim_tempo_ano ...................................... [RUN]
[0m14:35:22.404615 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771, now test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6)
[0m14:35:22.404615 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:35:22.410599 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:35:22.412595 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (compile): 14:35:22.405613 => 14:35:22.411596
[0m14:35:22.412595 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:35:22.420601 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:35:22.421569 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:22.422567 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:35:22.422567 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select ano
from `workspace`.`default`.`dim_tempo`
where ano is null



      
    ) dbt_internal_test
[0m14:35:22.423592 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:24.679579 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-0dd6-1f62-8a0d-278e08acd4d6
[0m14:35:25.343136 [debug] [Thread-1 (]: SQL status: OK in 2.9200000762939453 seconds
[0m14:35:25.352113 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (execute): 14:35:22.413591 => 14:35:25.351121
[0m14:35:25.353108 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: ROLLBACK
[0m14:35:25.354140 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:25.355136 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: Close
[0m14:35:25.356134 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-0dd6-1f62-8a0d-278e08acd4d6
[0m14:35:25.797668 [info ] [Thread-1 (]: 4 of 21 PASS not_null_dim_tempo_ano ............................................ [[32mPASS[0m in 3.39s]
[0m14:35:25.802651 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:35:25.803649 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:35:25.804646 [info ] [Thread-1 (]: 5 of 21 START test not_null_dim_tempo_data ..................................... [RUN]
[0m14:35:25.805645 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6, now test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252)
[0m14:35:25.806642 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:35:25.813651 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:35:25.815618 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (compile): 14:35:25.806642 => 14:35:25.814641
[0m14:35:25.816614 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:35:25.820630 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:35:25.821600 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:25.822597 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:35:25.822597 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data
from `workspace`.`default`.`dim_tempo`
where data is null



      
    ) dbt_internal_test
[0m14:35:25.823595 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:26.880527 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-0f3b-13c5-9056-af5a85c3ee43
[0m14:35:27.532354 [debug] [Thread-1 (]: SQL status: OK in 1.7100000381469727 seconds
[0m14:35:27.535346 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (execute): 14:35:25.817622 => 14:35:27.535346
[0m14:35:27.536346 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: ROLLBACK
[0m14:35:27.537312 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:27.537312 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: Close
[0m14:35:27.538339 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-0f3b-13c5-9056-af5a85c3ee43
[0m14:35:27.854132 [info ] [Thread-1 (]: 5 of 21 PASS not_null_dim_tempo_data ........................................... [[32mPASS[0m in 2.05s]
[0m14:35:27.859151 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:35:27.861146 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:35:27.863149 [info ] [Thread-1 (]: 6 of 21 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m14:35:27.868096 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m14:35:27.870125 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:35:27.880092 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:35:27.882055 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 14:35:27.871118 => 14:35:27.882055
[0m14:35:27.883085 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:35:27.888070 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:35:27.889038 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:27.890035 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:35:27.891032 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m14:35:27.891032 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:29.295629 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-10af-1a74-8350-5ab89144c9ee
[0m14:35:29.882382 [debug] [Thread-1 (]: SQL status: OK in 1.9900000095367432 seconds
[0m14:35:29.888327 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 14:35:27.883085 => 14:35:29.888327
[0m14:35:29.889352 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m14:35:29.890320 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:29.890320 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m14:35:29.891349 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-10af-1a74-8350-5ab89144c9ee
[0m14:35:30.278195 [info ] [Thread-1 (]: 6 of 21 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 2.41s]
[0m14:35:30.282222 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:35:30.284207 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:35:30.285204 [info ] [Thread-1 (]: 7 of 21 START test not_null_dim_tempo_dia ...................................... [RUN]
[0m14:35:30.289162 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306)
[0m14:35:30.290160 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:35:30.300132 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:35:30.301129 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (compile): 14:35:30.291156 => 14:35:30.301129
[0m14:35:30.302126 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:35:30.306116 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:35:30.308111 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:30.308111 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:35:30.309108 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select dia
from `workspace`.`default`.`dim_tempo`
where dia is null



      
    ) dbt_internal_test
[0m14:35:30.310105 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:31.458957 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-11f4-1ee6-b539-d644772ff107
[0m14:35:32.105149 [debug] [Thread-1 (]: SQL status: OK in 1.7999999523162842 seconds
[0m14:35:32.114094 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (execute): 14:35:30.302126 => 14:35:32.114094
[0m14:35:32.116114 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: ROLLBACK
[0m14:35:32.117115 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:32.118112 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: Close
[0m14:35:32.120104 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-11f4-1ee6-b539-d644772ff107
[0m14:35:32.368794 [info ] [Thread-1 (]: 7 of 21 PASS not_null_dim_tempo_dia ............................................ [[32mPASS[0m in 2.08s]
[0m14:35:32.373744 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:35:32.375768 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:35:32.377732 [info ] [Thread-1 (]: 8 of 21 START test not_null_dim_tempo_mes ...................................... [RUN]
[0m14:35:32.381722 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306, now test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972)
[0m14:35:32.382749 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:35:32.392691 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:35:32.394693 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (compile): 14:35:32.383741 => 14:35:32.393688
[0m14:35:32.394693 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:35:32.398673 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:35:32.400668 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:32.400668 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:35:32.401666 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select mes
from `workspace`.`default`.`dim_tempo`
where mes is null



      
    ) dbt_internal_test
[0m14:35:32.401666 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:33.696684 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-1336-183d-8f64-3787e09889f2
[0m14:35:34.347600 [debug] [Thread-1 (]: SQL status: OK in 1.9500000476837158 seconds
[0m14:35:34.350756 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (execute): 14:35:32.395711 => 14:35:34.349791
[0m14:35:34.350756 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: ROLLBACK
[0m14:35:34.351781 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:34.352751 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: Close
[0m14:35:34.352751 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-1336-183d-8f64-3787e09889f2
[0m14:35:34.590295 [info ] [Thread-1 (]: 8 of 21 PASS not_null_dim_tempo_mes ............................................ [[32mPASS[0m in 2.21s]
[0m14:35:34.592291 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:35:34.593288 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:35:34.593288 [info ] [Thread-1 (]: 9 of 21 START test not_null_fato_nascimento_APGAR1 ............................. [RUN]
[0m14:35:34.595316 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972, now test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074)
[0m14:35:34.595316 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:35:34.602292 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:35:34.604257 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (compile): 14:35:34.596311 => 14:35:34.603267
[0m14:35:34.604257 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:35:34.613234 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:35:34.614231 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:34.615229 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:35:34.615229 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR1
from `workspace`.`default`.`fato_nascimento`
where APGAR1 is null



      
    ) dbt_internal_test
[0m14:35:34.616226 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:35.655665 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-1471-13f9-a4c0-cc6105991086
[0m14:35:36.312200 [debug] [Thread-1 (]: SQL status: OK in 1.7000000476837158 seconds
[0m14:35:36.320240 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (execute): 14:35:34.605284 => 14:35:36.319242
[0m14:35:36.322232 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: ROLLBACK
[0m14:35:36.324227 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:36.325223 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: Close
[0m14:35:36.326221 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-1471-13f9-a4c0-cc6105991086
[0m14:35:36.591581 [info ] [Thread-1 (]: 9 of 21 PASS not_null_fato_nascimento_APGAR1 ................................... [[32mPASS[0m in 2.00s]
[0m14:35:36.593574 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:35:36.594574 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:35:36.594574 [info ] [Thread-1 (]: 10 of 21 START test not_null_fato_nascimento_APGAR5 ............................ [RUN]
[0m14:35:36.596568 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074, now test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994)
[0m14:35:36.597564 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:35:36.604545 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:35:36.606541 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (compile): 14:35:36.597564 => 14:35:36.605543
[0m14:35:36.606541 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:35:36.612540 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:35:36.613527 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:36.614550 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:35:36.614550 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR5
from `workspace`.`default`.`fato_nascimento`
where APGAR5 is null



      
    ) dbt_internal_test
[0m14:35:36.615544 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:37.699218 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-15b2-1d8c-ae6a-d39e95bc1dd3
[0m14:35:38.343840 [debug] [Thread-1 (]: SQL status: OK in 1.7300000190734863 seconds
[0m14:35:38.347823 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (execute): 14:35:36.607538 => 14:35:38.346806
[0m14:35:38.347823 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: ROLLBACK
[0m14:35:38.348827 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:38.349796 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: Close
[0m14:35:38.349796 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-15b2-1d8c-ae6a-d39e95bc1dd3
[0m14:35:38.738369 [info ] [Thread-1 (]: 10 of 21 PASS not_null_fato_nascimento_APGAR5 .................................. [[32mPASS[0m in 2.14s]
[0m14:35:38.739366 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:35:38.740363 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:35:38.740363 [info ] [Thread-1 (]: 11 of 21 START test not_null_fato_nascimento_cod_municipio ..................... [RUN]
[0m14:35:38.741731 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m14:35:38.742759 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:35:38.749740 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:35:38.750739 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 14:35:38.742759 => 14:35:38.750739
[0m14:35:38.751707 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:35:38.754732 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:35:38.755696 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:38.756694 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:35:38.756694 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m14:35:38.757691 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:40.090769 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-1709-1c18-bd00-357f63f2ce01
[0m14:35:41.115057 [debug] [Thread-1 (]: SQL status: OK in 2.359999895095825 seconds
[0m14:35:41.123036 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 14:35:38.751707 => 14:35:41.122037
[0m14:35:41.124032 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m14:35:41.125029 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:41.126026 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m14:35:41.127023 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-1709-1c18-bd00-357f63f2ce01
[0m14:35:42.470534 [info ] [Thread-1 (]: 11 of 21 PASS not_null_fato_nascimento_cod_municipio ........................... [[32mPASS[0m in 3.73s]
[0m14:35:42.473528 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:35:42.474496 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:35:42.475520 [info ] [Thread-1 (]: 12 of 21 START test not_null_fato_nascimento_data_id ........................... [RUN]
[0m14:35:42.477488 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m14:35:42.479481 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:35:42.489454 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:35:42.490452 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 14:35:42.480480 => 14:35:42.490452
[0m14:35:42.491449 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:35:42.495438 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:35:42.496435 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:42.497433 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:35:42.497433 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m14:35:42.498430 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:43.773915 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-1948-1e80-be0b-8817870d8767
[0m14:35:44.757855 [debug] [Thread-1 (]: SQL status: OK in 2.259999990463257 seconds
[0m14:35:44.765871 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 14:35:42.492446 => 14:35:44.765871
[0m14:35:44.766856 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m14:35:44.767825 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:44.768822 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m14:35:44.769820 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-1948-1e80-be0b-8817870d8767
[0m14:35:45.068488 [info ] [Thread-1 (]: 12 of 21 PASS not_null_fato_nascimento_data_id ................................. [[32mPASS[0m in 2.59s]
[0m14:35:45.076467 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:35:45.077464 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:35:45.078462 [info ] [Thread-1 (]: 13 of 21 START test not_null_fato_nascimento_gestacao_semanas .................. [RUN]
[0m14:35:45.080478 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2)
[0m14:35:45.081465 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:35:45.088435 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:35:45.090429 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (compile): 14:35:45.081465 => 14:35:45.089432
[0m14:35:45.090429 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:35:45.094418 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:35:45.095415 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:45.095415 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:35:45.096413 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select gestacao_semanas
from `workspace`.`default`.`fato_nascimento`
where gestacao_semanas is null



      
    ) dbt_internal_test
[0m14:35:45.096413 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:46.506316 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-1aeb-1607-90d6-2cc67338d6b0
[0m14:35:47.274173 [debug] [Thread-1 (]: SQL status: OK in 2.180000066757202 seconds
[0m14:35:47.277164 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (execute): 14:35:45.091426 => 14:35:47.277164
[0m14:35:47.278162 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: ROLLBACK
[0m14:35:47.278162 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:47.279162 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: Close
[0m14:35:47.280161 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-1aeb-1607-90d6-2cc67338d6b0
[0m14:35:47.584645 [info ] [Thread-1 (]: 13 of 21 PASS not_null_fato_nascimento_gestacao_semanas ........................ [[32mPASS[0m in 2.50s]
[0m14:35:47.588607 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:35:47.591598 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:35:47.593628 [info ] [Thread-1 (]: 14 of 21 START test not_null_fato_nascimento_idade_mae ......................... [RUN]
[0m14:35:47.598578 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2, now test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42)
[0m14:35:47.600571 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:35:47.619518 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:35:47.620514 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (compile): 14:35:47.602565 => 14:35:47.620514
[0m14:35:47.621513 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:35:47.626499 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:35:47.628493 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:47.628493 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:35:47.628493 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select idade_mae
from `workspace`.`default`.`fato_nascimento`
where idade_mae is null



      
    ) dbt_internal_test
[0m14:35:47.629491 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:49.044943 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-1c50-1229-9574-cedb5cfbf059
[0m14:35:49.848485 [debug] [Thread-1 (]: SQL status: OK in 2.2200000286102295 seconds
[0m14:35:49.853439 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (execute): 14:35:47.622510 => 14:35:49.852445
[0m14:35:49.854453 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: ROLLBACK
[0m14:35:49.855434 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:49.855434 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: Close
[0m14:35:49.856462 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-1c50-1229-9574-cedb5cfbf059
[0m14:35:50.256441 [info ] [Thread-1 (]: 14 of 21 PASS not_null_fato_nascimento_idade_mae ............................... [[32mPASS[0m in 2.66s]
[0m14:35:50.261427 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:35:50.263457 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:35:50.264450 [info ] [Thread-1 (]: 15 of 21 START test not_null_fato_nascimento_nascimento_id ..................... [RUN]
[0m14:35:50.268442 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m14:35:50.270407 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:35:50.283364 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:35:50.285361 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 14:35:50.271398 => 14:35:50.284392
[0m14:35:50.286358 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:35:50.290624 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:35:50.293586 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:50.293586 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:35:50.294583 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m14:35:50.295581 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:51.391378 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-1dd6-19b6-bfb6-3601e051c0dc
[0m14:35:52.150700 [debug] [Thread-1 (]: SQL status: OK in 1.8600000143051147 seconds
[0m14:35:52.152901 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 14:35:50.286358 => 14:35:52.152901
[0m14:35:52.153896 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m14:35:52.154895 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:52.154895 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m14:35:52.155876 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-1dd6-19b6-bfb6-3601e051c0dc
[0m14:35:52.527509 [info ] [Thread-1 (]: 15 of 21 PASS not_null_fato_nascimento_nascimento_id ........................... [[32mPASS[0m in 2.26s]
[0m14:35:52.530530 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:35:52.532496 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:35:52.533492 [info ] [Thread-1 (]: 16 of 21 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m14:35:52.535486 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m14:35:52.536510 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:35:52.547482 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:35:52.548478 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 14:35:52.537482 => 14:35:52.548478
[0m14:35:52.549447 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:35:52.552471 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:35:52.553464 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:52.554434 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:35:52.554434 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m14:35:52.555459 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:53.863552 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-1f40-13c7-afee-2411f65a5954
[0m14:35:54.490846 [debug] [Thread-1 (]: SQL status: OK in 1.940000057220459 seconds
[0m14:35:54.499785 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 14:35:52.549447 => 14:35:54.499785
[0m14:35:54.500779 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m14:35:54.501782 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:54.501782 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m14:35:54.502773 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-1f40-13c7-afee-2411f65a5954
[0m14:35:54.914495 [info ] [Thread-1 (]: 16 of 21 PASS not_null_fato_nascimento_peso .................................... [[32mPASS[0m in 2.38s]
[0m14:35:54.915493 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:35:54.916519 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:35:54.916519 [info ] [Thread-1 (]: 17 of 21 START test not_null_fato_nascimento_sexo .............................. [RUN]
[0m14:35:54.917487 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m14:35:54.918513 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:35:54.925494 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:35:54.926463 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 14:35:54.919482 => 14:35:54.926463
[0m14:35:54.926463 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:35:54.931450 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:35:54.932447 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:54.933445 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:35:54.934442 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sexo
from `workspace`.`default`.`fato_nascimento`
where sexo is null



      
    ) dbt_internal_test
[0m14:35:54.935453 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:55.969185 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-2089-1a08-8254-6478e7bf7d02
[0m14:35:56.742572 [debug] [Thread-1 (]: SQL status: OK in 1.809999942779541 seconds
[0m14:35:56.744768 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 14:35:54.927488 => 14:35:56.744768
[0m14:35:56.745763 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: ROLLBACK
[0m14:35:56.746733 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:56.746733 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: Close
[0m14:35:56.747742 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-2089-1a08-8254-6478e7bf7d02
[0m14:35:57.042848 [info ] [Thread-1 (]: 17 of 21 PASS not_null_fato_nascimento_sexo .................................... [[32mPASS[0m in 2.12s]
[0m14:35:57.044586 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:35:57.045614 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:35:57.046611 [info ] [Thread-1 (]: 18 of 21 START test not_null_fato_nascimento_tipo_parto ........................ [RUN]
[0m14:35:57.049572 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e)
[0m14:35:57.050569 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:35:57.062569 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:35:57.065529 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (compile): 14:35:57.051599 => 14:35:57.064561
[0m14:35:57.066525 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:35:57.070514 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:35:57.072509 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:57.072509 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:35:57.073534 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tipo_parto
from `workspace`.`default`.`fato_nascimento`
where tipo_parto is null



      
    ) dbt_internal_test
[0m14:35:57.073534 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:35:58.412967 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-220a-182f-8882-131ae479bd22
[0m14:35:59.155682 [debug] [Thread-1 (]: SQL status: OK in 2.0799999237060547 seconds
[0m14:35:59.160666 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (execute): 14:35:57.067524 => 14:35:59.159671
[0m14:35:59.161633 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: ROLLBACK
[0m14:35:59.162631 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:35:59.163628 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: Close
[0m14:35:59.164625 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-220a-182f-8882-131ae479bd22
[0m14:35:59.514721 [info ] [Thread-1 (]: 18 of 21 PASS not_null_fato_nascimento_tipo_parto .............................. [[32mPASS[0m in 2.47s]
[0m14:35:59.517713 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:35:59.518709 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:35:59.520706 [info ] [Thread-1 (]: 19 of 21 START test unique_dim_localidade_cod_municipio ........................ [RUN]
[0m14:35:59.522699 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m14:35:59.524694 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:35:59.542644 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:35:59.544638 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 14:35:59.525690 => 14:35:59.544638
[0m14:35:59.545636 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:35:59.548627 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:35:59.549624 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:35:59.550623 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:35:59.551620 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_municipio as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_localidade`
where cod_municipio is not null
group by cod_municipio
having count(*) > 1



      
    ) dbt_internal_test
[0m14:35:59.552618 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:00.388594 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-233b-1d3b-bc53-c47724a2b3c8
[0m14:36:00.863486 [debug] [Thread-1 (]: SQL status: OK in 1.309999942779541 seconds
[0m14:36:00.870983 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 14:35:59.545636 => 14:36:00.869984
[0m14:36:00.871979 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: ROLLBACK
[0m14:36:00.872977 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:36:00.873973 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: Close
[0m14:36:00.874971 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-233b-1d3b-bc53-c47724a2b3c8
[0m14:36:01.109857 [info ] [Thread-1 (]: 19 of 21 PASS unique_dim_localidade_cod_municipio .............................. [[32mPASS[0m in 1.59s]
[0m14:36:01.111880 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:36:01.111880 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:36:01.112849 [info ] [Thread-1 (]: 20 of 21 START test unique_dim_tempo_data_id ................................... [RUN]
[0m14:36:01.113549 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m14:36:01.114579 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:36:01.120533 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:36:01.122527 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 14:36:01.114579 => 14:36:01.121530
[0m14:36:01.122527 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:36:01.126518 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:36:01.128512 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:36:01.129509 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:36:01.129509 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:36:01.130535 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:01.898029 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-2422-1579-990e-67ff8d4ad8e3
[0m14:36:02.539676 [debug] [Thread-1 (]: SQL status: OK in 1.409999966621399 seconds
[0m14:36:02.548683 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 14:36:01.123525 => 14:36:02.547671
[0m14:36:02.550650 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m14:36:02.552645 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:36:02.554635 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m14:36:02.556662 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-2422-1579-990e-67ff8d4ad8e3
[0m14:36:02.784293 [info ] [Thread-1 (]: 20 of 21 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 1.67s]
[0m14:36:02.786287 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:36:02.787283 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:36:02.788281 [info ] [Thread-1 (]: 21 of 21 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m14:36:02.790276 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m14:36:02.791272 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:36:02.800250 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:36:02.802244 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 14:36:02.791272 => 14:36:02.801246
[0m14:36:02.803240 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:36:02.808227 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:36:02.809224 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:36:02.810222 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:36:02.810222 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:36:02.811219 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:36:03.596427 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-2526-1293-acd2-6e90ef486912
[0m14:36:04.704896 [debug] [Thread-1 (]: SQL status: OK in 1.8899999856948853 seconds
[0m14:36:04.709883 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 14:36:02.803240 => 14:36:04.708885
[0m14:36:04.710880 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m14:36:04.711876 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:36:04.712874 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m14:36:04.712874 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-2526-1293-acd2-6e90ef486912
[0m14:36:04.943876 [error] [Thread-1 (]: 21 of 21 FAIL 1 unique_fato_nascimento_nascimento_id ........................... [[31mFAIL 1[0m in 2.15s]
[0m14:36:04.947866 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:36:04.954843 [debug] [MainThread]: On master: ROLLBACK
[0m14:36:04.955840 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:36:05.791118 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07935-2673-1e9b-95b1-62748d04ad6a
[0m14:36:05.793113 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:36:05.794109 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:36:05.794109 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:36:05.795106 [debug] [MainThread]: On master: ROLLBACK
[0m14:36:05.796132 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:36:05.796132 [debug] [MainThread]: On master: Close
[0m14:36:05.797132 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07935-2673-1e9b-95b1-62748d04ad6a
[0m14:36:06.054837 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:36:06.054837 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:36:06.055849 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m14:36:06.056831 [info ] [MainThread]: 
[0m14:36:06.057830 [info ] [MainThread]: Finished running 21 tests in 0 hours 0 minutes and 57.06 seconds (57.06s).
[0m14:36:06.063839 [debug] [MainThread]: Command end result
[0m14:36:06.080768 [info ] [MainThread]: 
[0m14:36:06.082761 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:36:06.083760 [info ] [MainThread]: 
[0m14:36:06.084756 [error] [MainThread]: [31mFailure in test unique_fato_nascimento_nascimento_id (models\marts\schema.yaml)[0m
[0m14:36:06.085753 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m14:36:06.086751 [info ] [MainThread]: 
[0m14:36:06.087748 [info ] [MainThread]:   compiled Code at target\compiled\projeto_health_insights\models\marts\schema.yaml\unique_fato_nascimento_nascimento_id.sql
[0m14:36:06.088754 [info ] [MainThread]: 
[0m14:36:06.089750 [info ] [MainThread]: Done. PASS=20 WARN=0 ERROR=1 SKIP=0 TOTAL=21
[0m14:36:06.091738 [debug] [MainThread]: Command `dbt test` failed at 14:36:06.091738 after 58.80 seconds
[0m14:36:06.092735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A66BDAC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A6916FC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A6916D250>]}
[0m14:36:06.093732 [debug] [MainThread]: Flushing usage events
[0m14:38:18.829584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027ED18F7AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027ED19C2F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027ED163F6D0>]}


============================== 14:38:18.836559 | f67b47c2-d82a-471e-9bcc-f4a377d6e5f1 ==============================
[0m14:38:18.836559 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:38:18.837551 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:38:20.417912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027ED1652510>]}
[0m14:38:20.439854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027ED1F3B390>]}
[0m14:38:20.440849 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:38:20.475756 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:38:20.620372 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:38:20.621339 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m14:38:20.714090 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m14:38:20.755009 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging\stg_nascidos_vivos.sql
[0m14:38:20.771966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EDC151AD0>]}
[0m14:38:20.787893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EDC1BC690>]}
[0m14:38:20.788890 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:38:20.789558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027ECE644750>]}
[0m14:38:20.792581 [info ] [MainThread]: 
[0m14:38:20.793552 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:38:20.796555 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m14:38:20.797562 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m14:38:20.798537 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m14:38:20.798537 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:38:22.114661 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07935-77b0-18c5-b7fa-86e6c4e960a2
[0m14:38:22.523382 [debug] [ThreadPool]: SQL status: OK in 1.7200000286102295 seconds
[0m14:38:22.529366 [debug] [ThreadPool]: On list_workspace: Close
[0m14:38:22.529366 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07935-77b0-18c5-b7fa-86e6c4e960a2
[0m14:38:22.775105 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m14:38:22.777135 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m14:38:22.791062 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:38:22.792059 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m14:38:22.793057 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m14:38:22.793057 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:38:23.630184 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07935-789a-1cfe-9c1a-3601655f4a41
[0m14:38:24.095146 [debug] [ThreadPool]: SQL status: OK in 1.2999999523162842 seconds
[0m14:38:24.099466 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:38:24.100615 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m14:38:24.101643 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:38:24.103608 [debug] [ThreadPool]: On create_workspace_default: Close
[0m14:38:24.105168 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07935-789a-1cfe-9c1a-3601655f4a41
[0m14:38:24.354645 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:38:24.359634 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:38:24.360632 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:38:24.360632 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:38:25.729564 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07935-79dc-13bb-96d2-6f2c30f94529
[0m14:38:26.195447 [debug] [ThreadPool]: SQL status: OK in 1.8300000429153442 seconds
[0m14:38:26.213362 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:38:26.214360 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07935-79dc-13bb-96d2-6f2c30f94529
[0m14:38:26.461516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EDC2E3D90>]}
[0m14:38:26.463508 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:38:26.465503 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:38:26.468495 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:38:26.470454 [info ] [MainThread]: 
[0m14:38:26.492872 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m14:38:26.494866 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m14:38:26.500849 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m14:38:26.502844 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m14:38:26.512816 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m14:38:26.514810 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 14:38:26.503839 => 14:38:26.514810
[0m14:38:26.515806 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m14:38:26.561684 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m14:38:26.562681 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:38:26.563678 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m14:38:26.563678 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m14:38:26.564675 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:38:27.392321 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-7ada-11c1-a388-39af6c30881e
[0m14:38:28.132371 [debug] [Thread-1 (]: SQL status: OK in 1.5700000524520874 seconds
[0m14:38:28.151348 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 14:38:26.515806 => 14:38:28.150323
[0m14:38:28.151348 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m14:38:28.152345 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:38:28.152345 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m14:38:28.153342 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-7ada-11c1-a388-39af6c30881e
[0m14:38:28.404071 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EDC2BB250>]}
[0m14:38:28.405068 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.90s]
[0m14:38:28.406065 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m14:38:28.407063 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m14:38:28.408061 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m14:38:28.410066 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m14:38:28.412069 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m14:38:28.416067 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m14:38:28.417037 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 14:38:28.412069 => 14:38:28.417037
[0m14:38:28.418062 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m14:38:28.423020 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m14:38:28.426013 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:38:28.427014 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m14:38:28.428007 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m14:38:28.428007 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:29.248013 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-7bf5-19a4-b538-d726471427a8
[0m14:38:29.966003 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m14:38:29.971987 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 14:38:28.418062 => 14:38:29.971987
[0m14:38:29.972984 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m14:38:29.973981 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:38:29.974979 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m14:38:29.975974 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-7bf5-19a4-b538-d726471427a8
[0m14:38:30.207547 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EDC2E3D50>]}
[0m14:38:30.208544 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.80s]
[0m14:38:30.210540 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m14:38:30.211539 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m14:38:30.213531 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m14:38:30.214529 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m14:38:30.215526 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m14:38:30.219515 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m14:38:30.221510 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 14:38:30.215526 => 14:38:30.220513
[0m14:38:30.221510 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m14:38:30.230486 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m14:38:30.232481 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:38:30.232481 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m14:38:30.233484 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m14:38:30.233484 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:31.006171 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-7d02-1ced-a21c-41e907bc40e0
[0m14:38:31.661133 [debug] [Thread-1 (]: SQL status: OK in 1.4299999475479126 seconds
[0m14:38:31.665123 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 14:38:30.222507 => 14:38:31.665123
[0m14:38:31.666150 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m14:38:31.666150 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:38:31.667148 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m14:38:31.667148 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-7d02-1ced-a21c-41e907bc40e0
[0m14:38:31.918392 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EDC44BE50>]}
[0m14:38:31.920387 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 1.70s]
[0m14:38:31.924378 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m14:38:31.926413 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:38:31.930359 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m14:38:31.934350 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m14:38:31.936342 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m14:38:31.954322 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:38:31.955321 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 14:38:31.937339 => 14:38:31.955321
[0m14:38:31.956288 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m14:38:31.965265 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:38:31.966262 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:38:31.967259 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:38:31.968256 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
),

final AS (
    SELECT
        md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(GESTACAO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PARTO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(APGAR1 as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(APGAR5 as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(source_id as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
        CODMUNNASC AS cod_municipio,
        LOCNASC AS local_nascimento,
        DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
        HORANASC AS hora_nascimento,
        CASE
            WHEN SEXO = 1 THEN 'Masculino'
            WHEN SEXO = 2 THEN 'Feminino'
            ELSE 'Ignorado'
        END AS sexo,
        PESO AS peso,
        IDADEMAE AS idade_mae,
        GESTACAO AS gestacao_semanas,
        PARTO AS tipo_parto,
        APGAR1,
        APGAR5
    FROM nascidos_vivos_raw
)

SELECT * FROM final

[0m14:38:31.968256 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:32.870730 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-7e1d-1d22-9c72-5cbb1b5c0393
[0m14:38:33.228217 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
),

final AS (
    SELECT
        md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(GESTACAO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PARTO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(APGAR1 as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(APGAR5 as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(source_id as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
        CODMUNNASC AS cod_municipio,
        LOCNASC AS local_nascimento,
        DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
        HORANASC AS hora_nascimento,
        CASE
            WHEN SEXO = 1 THEN 'Masculino'
            WHEN SEXO = 2 THEN 'Feminino'
            ELSE 'Ignorado'
        END AS sexo,
        PESO AS peso,
        IDADEMAE AS idade_mae,
        GESTACAO AS gestacao_semanas,
        PARTO AS tipo_parto,
        APGAR1,
        APGAR5
    FROM nascidos_vivos_raw
)

SELECT * FROM final

[0m14:38:33.229200 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `source_id` cannot be resolved. Did you mean one of the following? [`APGAR1`, `APGAR5`, `DTNASC`, `LOCNASC`, `PARTO`]. SQLSTATE: 42703; line 28 pos 847
[0m14:38:33.230182 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `source_id` cannot be resolved. Did you mean one of the following? [`APGAR1`, `APGAR5`, `DTNASC`, `LOCNASC`, `PARTO`]. SQLSTATE: 42703; line 28 pos 847
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1036)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:785)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:744)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:572)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:236)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:75)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:549)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:585)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `source_id` cannot be resolved. Did you mean one of the following? [`APGAR1`, `APGAR5`, `DTNASC`, `LOCNASC`, `PARTO`]. SQLSTATE: 42703; line 28 pos 847
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:997)
	... 53 more

[0m14:38:33.231179 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01f07935-7e3e-1cdc-9038-0c9d6e8de8ed
[0m14:38:33.232176 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 14:38:31.957285 => 14:38:33.232176
[0m14:38:33.233174 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m14:38:33.233174 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:38:33.234170 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m14:38:33.234170 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-7e1d-1d22-9c72-5cbb1b5c0393
[0m14:38:33.484984 [debug] [Thread-1 (]: Runtime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `source_id` cannot be resolved. Did you mean one of the following? [`APGAR1`, `APGAR5`, `DTNASC`, `LOCNASC`, `PARTO`]. SQLSTATE: 42703; line 28 pos 847
[0m14:38:33.485951 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EDC44BED0>]}
[0m14:38:33.486952 [error] [Thread-1 (]: 4 of 12 ERROR creating sql view model default.stg_nascidos_vivos ............... [[31mERROR[0m in 1.55s]
[0m14:38:33.487947 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:38:33.488943 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m14:38:33.489942 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m14:38:33.491965 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m14:38:33.492934 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m14:38:33.497942 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m14:38:33.499914 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 14:38:33.492934 => 14:38:33.498930
[0m14:38:33.499914 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m14:38:33.504929 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m14:38:33.505927 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:38:33.506896 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m14:38:33.506896 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m14:38:33.507921 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:34.651477 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-7f2e-1422-a667-235cd73b119b
[0m14:38:35.287582 [debug] [Thread-1 (]: SQL status: OK in 1.7799999713897705 seconds
[0m14:38:35.290575 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 14:38:33.500913 => 14:38:35.289577
[0m14:38:35.290575 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m14:38:35.291572 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:38:35.292569 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m14:38:35.292569 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-7f2e-1422-a667-235cd73b119b
[0m14:38:35.511163 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EDC1FF410>]}
[0m14:38:35.512159 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 2.02s]
[0m14:38:35.514824 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m14:38:35.515792 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m14:38:35.516788 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m14:38:35.518783 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m14:38:35.519781 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m14:38:35.527758 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m14:38:35.528755 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 14:38:35.520778 => 14:38:35.528755
[0m14:38:35.529754 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m14:38:35.536762 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m14:38:35.537731 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:38:35.538729 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m14:38:35.538729 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m14:38:35.539754 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:36.435563 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-803e-14f9-8867-0b4fb6b9c376
[0m14:38:37.175032 [debug] [Thread-1 (]: SQL status: OK in 1.6299999952316284 seconds
[0m14:38:37.181045 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 14:38:35.530751 => 14:38:37.180050
[0m14:38:37.182013 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m14:38:37.183039 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:38:37.184006 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m14:38:37.185005 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-803e-14f9-8867-0b4fb6b9c376
[0m14:38:37.425591 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EDC102110>]}
[0m14:38:37.426591 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.91s]
[0m14:38:37.427590 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m14:38:37.428614 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m14:38:37.429586 [info ] [Thread-1 (]: 7 of 12 SKIP relation default.int_nascimento ................................... [[33mSKIP[0m]
[0m14:38:37.430581 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m14:38:37.431578 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m14:38:37.432576 [info ] [Thread-1 (]: 8 of 12 SKIP relation default.stg_tempo ........................................ [[33mSKIP[0m]
[0m14:38:37.433574 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m14:38:37.434571 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:38:37.435568 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m14:38:37.436565 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m14:38:37.437562 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:38:37.441580 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:38:37.442579 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 14:38:37.437562 => 14:38:37.441580
[0m14:38:37.442579 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:38:37.449558 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:38:37.451527 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:38:37.452523 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:38:37.452523 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m14:38:37.453520 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:38:38.262081 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-8154-1e27-bacb-cd9014852efa
[0m14:38:38.982179 [debug] [Thread-1 (]: SQL status: OK in 1.5299999713897705 seconds
[0m14:38:38.984173 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 14:38:37.443574 => 14:38:38.984173
[0m14:38:38.986141 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m14:38:38.986141 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:38:38.987171 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m14:38:38.988135 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-8154-1e27-bacb-cd9014852efa
[0m14:38:39.221760 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f67b47c2-d82a-471e-9bcc-f4a377d6e5f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027EDC10D010>]}
[0m14:38:39.222757 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.79s]
[0m14:38:39.224752 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:38:39.225748 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m14:38:39.226745 [info ] [Thread-1 (]: 10 of 12 SKIP relation default.dim_localidade .................................. [[33mSKIP[0m]
[0m14:38:39.227743 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m14:38:39.228739 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m14:38:39.229737 [info ] [Thread-1 (]: 11 of 12 SKIP relation default.dim_tempo ....................................... [[33mSKIP[0m]
[0m14:38:39.230735 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m14:38:39.231732 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m14:38:39.232760 [info ] [Thread-1 (]: 12 of 12 SKIP relation default.fato_nascimento ................................. [[33mSKIP[0m]
[0m14:38:39.234725 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m14:38:39.237716 [debug] [MainThread]: On master: ROLLBACK
[0m14:38:39.238714 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:38:40.044032 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07935-8262-1d96-8aa2-02737a3015e5
[0m14:38:40.045996 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:38:40.046996 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:38:40.047994 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:38:40.048989 [debug] [MainThread]: On master: ROLLBACK
[0m14:38:40.049987 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:38:40.050984 [debug] [MainThread]: On master: Close
[0m14:38:40.052979 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07935-8262-1d96-8aa2-02737a3015e5
[0m14:38:40.304450 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:38:40.305448 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m14:38:40.306446 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:38:40.307477 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_atendimento_hospitalar' was properly closed.
[0m14:38:40.309463 [info ] [MainThread]: 
[0m14:38:40.310434 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 19.52 seconds (19.52s).
[0m14:38:40.315447 [debug] [MainThread]: Command end result
[0m14:38:40.340354 [info ] [MainThread]: 
[0m14:38:40.341863 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:38:40.342864 [info ] [MainThread]: 
[0m14:38:40.343860 [error] [MainThread]: [33mRuntime Error in model stg_nascidos_vivos (models\staging\stg_nascidos_vivos.sql)[0m
[0m14:38:40.344868 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `source_id` cannot be resolved. Did you mean one of the following? [`APGAR1`, `APGAR5`, `DTNASC`, `LOCNASC`, `PARTO`]. SQLSTATE: 42703; line 28 pos 847
[0m14:38:40.345855 [info ] [MainThread]: 
[0m14:38:40.346853 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=5 TOTAL=12
[0m14:38:40.347849 [debug] [MainThread]: Command `dbt run` failed at 14:38:40.347849 after 21.54 seconds
[0m14:38:40.348848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027ED18CAA90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027ED18F4BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027ED16D80D0>]}
[0m14:38:40.348848 [debug] [MainThread]: Flushing usage events
[0m14:40:02.896179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DC5A4B2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DC59AE150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DC597EE10>]}


============================== 14:40:02.901194 | b637d977-76b1-4ae7-941b-23c3143c2592 ==============================
[0m14:40:02.901194 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:40:02.902191 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:40:04.346980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DC5FDB7D0>]}
[0m14:40:04.367952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DCF0C8210>]}
[0m14:40:04.368991 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:40:04.400488 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:40:04.546100 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:40:04.547097 [debug] [MainThread]: Partial parsing: updated file: projeto_health_insights://models\staging\stg_nascidos_vivos.sql
[0m14:40:04.636101 [debug] [MainThread]: 1603: static parser failed on staging\stg_nascidos_vivos.sql
[0m14:40:04.672513 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging\stg_nascidos_vivos.sql
[0m14:40:04.690467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DC5FDF8D0>]}
[0m14:40:04.705399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DCF1FFD90>]}
[0m14:40:04.706425 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:40:04.707393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DCC75ED90>]}
[0m14:40:04.710385 [info ] [MainThread]: 
[0m14:40:04.712380 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:40:04.714374 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace'
[0m14:40:04.715371 [debug] [ThreadPool]: Using databricks connection "list_workspace"
[0m14:40:04.716369 [debug] [ThreadPool]: On list_workspace: GetSchemas(database=`workspace`, schema=None)
[0m14:40:04.716369 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:40:05.645293 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07935-b56a-1ce7-b2b1-1cf20d553546
[0m14:40:05.957028 [debug] [ThreadPool]: SQL status: OK in 1.2400000095367432 seconds
[0m14:40:05.963395 [debug] [ThreadPool]: On list_workspace: Close
[0m14:40:05.964392 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07935-b56a-1ce7-b2b1-1cf20d553546
[0m14:40:06.200042 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_workspace, now create_workspace_default)
[0m14:40:06.202067 [debug] [ThreadPool]: Creating schema "database: "workspace"
schema: "default"
"
[0m14:40:06.215028 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:06.215028 [debug] [ThreadPool]: Using databricks connection "create_workspace_default"
[0m14:40:06.215997 [debug] [ThreadPool]: On create_workspace_default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "create_workspace_default"} */
create schema if not exists `workspace`.`default`
  
[0m14:40:06.215997 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:40:07.029858 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07935-b63b-1a81-84cc-d290cb0cf9aa
[0m14:40:07.446766 [debug] [ThreadPool]: SQL status: OK in 1.2300000190734863 seconds
[0m14:40:07.448788 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m14:40:07.448788 [debug] [ThreadPool]: On create_workspace_default: ROLLBACK
[0m14:40:07.449797 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:40:07.449797 [debug] [ThreadPool]: On create_workspace_default: Close
[0m14:40:07.450786 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07935-b63b-1a81-84cc-d290cb0cf9aa
[0m14:40:07.679145 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:40:07.697096 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:40:07.699126 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:40:07.700088 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:40:08.529099 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07935-b721-163d-881f-621eba222f5e
[0m14:40:08.944682 [debug] [ThreadPool]: SQL status: OK in 1.2400000095367432 seconds
[0m14:40:08.953653 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:40:08.954617 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07935-b721-163d-881f-621eba222f5e
[0m14:40:09.209184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DCF1D2150>]}
[0m14:40:09.212177 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:09.214137 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:40:09.217126 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:40:09.218123 [info ] [MainThread]: 
[0m14:40:09.227899 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m14:40:09.228897 [info ] [Thread-1 (]: 1 of 12 START sql view model default.stg_atendimento ........................... [RUN]
[0m14:40:09.230893 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m14:40:09.231890 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m14:40:09.236876 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m14:40:09.238871 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 14:40:09.232886 => 14:40:09.238871
[0m14:40:09.239868 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m14:40:09.274774 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_atendimento"
[0m14:40:09.275771 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:09.276769 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_atendimento"
[0m14:40:09.276769 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_atendimento"} */
create or replace view `workspace`.`default`.`stg_atendimento`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`atendimento`

[0m14:40:09.277766 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:40:10.211199 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-b820-1700-b885-9226b06444f2
[0m14:40:10.933448 [debug] [Thread-1 (]: SQL status: OK in 1.659999966621399 seconds
[0m14:40:10.951368 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 14:40:09.239868 => 14:40:10.951368
[0m14:40:10.952393 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: ROLLBACK
[0m14:40:10.953362 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:10.953362 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_atendimento: Close
[0m14:40:10.954382 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-b820-1700-b885-9226b06444f2
[0m14:40:11.219955 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DD02AFB10>]}
[0m14:40:11.223942 [info ] [Thread-1 (]: 1 of 12 OK created sql view model default.stg_atendimento ...................... [[32mOK[0m in 1.99s]
[0m14:40:11.227932 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m14:40:11.229925 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m14:40:11.231920 [info ] [Thread-1 (]: 2 of 12 START sql view model default.stg_doenca ................................ [RUN]
[0m14:40:11.236906 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m14:40:11.237905 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m14:40:11.244884 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m14:40:11.246879 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 14:40:11.238903 => 14:40:11.245880
[0m14:40:11.246879 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m14:40:11.253859 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_doenca"
[0m14:40:11.254860 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:11.255854 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_doenca"
[0m14:40:11.256852 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_doenca"} */
create or replace view `workspace`.`default`.`stg_doenca`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`doenca`

[0m14:40:11.256852 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:12.136660 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-b946-16f9-99ea-2aa04e7f047c
[0m14:40:12.797695 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m14:40:12.803651 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 14:40:11.247876 => 14:40:12.802656
[0m14:40:12.804648 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: ROLLBACK
[0m14:40:12.805648 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:12.806645 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_doenca: Close
[0m14:40:12.807642 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-b946-16f9-99ea-2aa04e7f047c
[0m14:40:13.064106 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DCF1E5BD0>]}
[0m14:40:13.067097 [info ] [Thread-1 (]: 2 of 12 OK created sql view model default.stg_doenca ........................... [[32mOK[0m in 1.83s]
[0m14:40:13.073087 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m14:40:13.075076 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m14:40:13.078068 [info ] [Thread-1 (]: 3 of 12 START sql view model default.stg_localidade ............................ [RUN]
[0m14:40:13.083019 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m14:40:13.085012 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m14:40:13.097012 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m14:40:13.099970 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 14:40:13.087008 => 14:40:13.098973
[0m14:40:13.100967 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m14:40:13.110968 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_localidade"
[0m14:40:13.112935 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:13.112935 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_localidade"
[0m14:40:13.113933 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_localidade"} */
create or replace view `workspace`.`default`.`stg_localidade`
  
  
  as
    select
    id,
    paciente,
    data_atendimento,
    procedimento
from `workspace`.`raw`.`localidade`

[0m14:40:13.113933 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:14.194325 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-ba5b-1d86-b89c-2d151bb9b328
[0m14:40:14.906717 [debug] [Thread-1 (]: SQL status: OK in 1.7899999618530273 seconds
[0m14:40:14.917723 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 14:40:13.101967 => 14:40:14.916724
[0m14:40:14.920677 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: ROLLBACK
[0m14:40:14.922670 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:14.923668 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_localidade: Close
[0m14:40:14.924666 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-ba5b-1d86-b89c-2d151bb9b328
[0m14:40:15.172855 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DD04EB390>]}
[0m14:40:15.174849 [info ] [Thread-1 (]: 3 of 12 OK created sql view model default.stg_localidade ....................... [[32mOK[0m in 2.09s]
[0m14:40:15.178808 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m14:40:15.179804 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:40:15.181798 [info ] [Thread-1 (]: 4 of 12 START sql view model default.stg_nascidos_vivos ........................ [RUN]
[0m14:40:15.183822 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m14:40:15.184822 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m14:40:15.199763 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:40:15.200747 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 14:40:15.185788 => 14:40:15.200747
[0m14:40:15.200747 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m14:40:15.207757 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:40:15.208727 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:15.209731 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:40:15.210721 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_nascidos_vivos"} */
create or replace view `workspace`.`default`.`stg_nascidos_vivos`
  
  
  as
    

WITH nascidos_vivos_raw AS (
    SELECT
        CODMUNNASC,
        LOCNASC,
        DTNASC,
        HORANASC,
        SEXO,
        PESO,
        IDADEMAE,
        GESTACAO,
        PARTO,
        APGAR1,
        APGAR5,
        CODUFNATU
    FROM default.sinasc_2022_sc_clean
    WHERE CODUFNATU = 42 -- SC
),

with_row_number AS (
    SELECT
        *,
        ROW_NUMBER() OVER(
            PARTITION BY DTNASC, HORANASC, CODMUNNASC, LOCNASC, PESO, SEXO, IDADEMAE, GESTACAO, PARTO, APGAR1, APGAR5
            ORDER BY DTNASC, HORANASC
        ) as rn
    FROM nascidos_vivos_raw
)

SELECT
    md5(cast(concat(coalesce(cast(DTNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(HORANASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(CODMUNNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(LOCNASC as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PESO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(SEXO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(IDADEMAE as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(GESTACAO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(PARTO as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(APGAR1 as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(APGAR5 as string), '_dbt_utils_surrogate_key_null_'), '-', coalesce(cast(rn as string), '_dbt_utils_surrogate_key_null_')) as string)) AS nascimento_id,
    CODMUNNASC AS cod_municipio,
    LOCNASC AS local_nascimento,
    DATE_FROM_UNIX_DATE(DTNASC) AS data_nascimento,
    HORANASC AS hora_nascimento,
    CASE
        WHEN SEXO = 1 THEN 'Masculino'
        WHEN SEXO = 2 THEN 'Feminino'
        ELSE 'Ignorado'
    END AS sexo,
    PESO AS peso,
    IDADEMAE AS idade_mae,
    GESTACAO AS gestacao_semanas,
    PARTO AS tipo_parto,
    APGAR1,
    APGAR5
FROM with_row_number

[0m14:40:15.211719 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:16.004186 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-bb97-14df-987d-e71e41e2c995
[0m14:40:16.677535 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m14:40:16.681525 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 14:40:15.201745 => 14:40:16.681525
[0m14:40:16.682522 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: ROLLBACK
[0m14:40:16.683518 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:16.684516 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_nascidos_vivos: Close
[0m14:40:16.685514 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-bb97-14df-987d-e71e41e2c995
[0m14:40:16.937425 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DCF20FA10>]}
[0m14:40:16.941377 [info ] [Thread-1 (]: 4 of 12 OK created sql view model default.stg_nascidos_vivos ................... [[32mOK[0m in 1.75s]
[0m14:40:16.946400 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:40:16.948360 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m14:40:16.951349 [info ] [Thread-1 (]: 5 of 12 START sql view model default.dim_doenca ................................ [RUN]
[0m14:40:16.956268 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m14:40:16.959259 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m14:40:16.969230 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m14:40:16.972221 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 14:40:16.961251 => 14:40:16.971226
[0m14:40:16.973219 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m14:40:16.980199 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_doenca"
[0m14:40:16.981198 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:16.982194 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_doenca"
[0m14:40:16.983192 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_doenca"} */
create or replace view `workspace`.`default`.`dim_doenca`
  
  
  as
    -- Dimensão Doença
select
    id as doenca_id,
    paciente,
    data_atendimento,
    procedimento as descricao_doenca
from `workspace`.`default`.`stg_doenca`

[0m14:40:16.983192 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:17.973043 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-bcc1-1f98-9524-ef047935058d
[0m14:40:18.620287 [debug] [Thread-1 (]: SQL status: OK in 1.6399999856948853 seconds
[0m14:40:18.630258 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 14:40:16.974216 => 14:40:18.630258
[0m14:40:18.632253 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: ROLLBACK
[0m14:40:18.633248 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:18.634248 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_doenca: Close
[0m14:40:18.636241 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-bcc1-1f98-9524-ef047935058d
[0m14:40:18.878004 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DD044EAD0>]}
[0m14:40:18.879001 [info ] [Thread-1 (]: 5 of 12 OK created sql view model default.dim_doenca ........................... [[32mOK[0m in 1.92s]
[0m14:40:18.880966 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m14:40:18.881993 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m14:40:18.882993 [info ] [Thread-1 (]: 6 of 12 START sql view model default.int_atendimento ........................... [RUN]
[0m14:40:18.885982 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m14:40:18.885982 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m14:40:18.894926 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m14:40:18.895923 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 14:40:18.886980 => 14:40:18.895923
[0m14:40:18.896922 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m14:40:18.903934 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_atendimento"
[0m14:40:18.905926 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:18.905926 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_atendimento"
[0m14:40:18.906926 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_atendimento"} */
create or replace view `workspace`.`default`.`int_atendimento`
  
  
  as
    -- Modelo intermediário de atendimento
select
    a.id as atendimento_id,
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    d.id as doenca_id,
    l.id as localidade_id
from `workspace`.`default`.`stg_atendimento` a
left join `workspace`.`default`.`stg_doenca` d
    on a.paciente = d.paciente
left join `workspace`.`default`.`stg_localidade` l
    on a.paciente = l.paciente

[0m14:40:18.907893 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:19.780397 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-bdd3-1f35-bbfb-84cdf044a94a
[0m14:40:20.501781 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m14:40:20.511745 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 14:40:18.897948 => 14:40:20.510762
[0m14:40:20.513741 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: ROLLBACK
[0m14:40:20.514708 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:20.515704 [debug] [Thread-1 (]: On model.projeto_health_insights.int_atendimento: Close
[0m14:40:20.516702 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-bdd3-1f35-bbfb-84cdf044a94a
[0m14:40:20.763242 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DD04E1510>]}
[0m14:40:20.765238 [info ] [Thread-1 (]: 6 of 12 OK created sql view model default.int_atendimento ...................... [[32mOK[0m in 1.88s]
[0m14:40:20.767230 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m14:40:20.768227 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m14:40:20.770256 [info ] [Thread-1 (]: 7 of 12 START sql view model default.int_nascimento ............................ [RUN]
[0m14:40:20.772220 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m14:40:20.773214 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m14:40:20.781222 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m14:40:20.783187 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 14:40:20.774211 => 14:40:20.782223
[0m14:40:20.784184 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m14:40:20.795155 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.int_nascimento"
[0m14:40:20.796156 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:20.797149 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.int_nascimento"
[0m14:40:20.798145 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.int_nascimento"} */
create or replace view `workspace`.`default`.`int_nascimento`
  
  
  as
    

WITH base AS (
    SELECT *
    FROM `workspace`.`default`.`stg_nascidos_vivos`
)

SELECT
    nascimento_id,
    cod_municipio,
    local_nascimento,
    data_nascimento,
    hora_nascimento,
    sexo,
    peso,
    idade_mae,
    gestacao_semanas,
    tipo_parto,
    APGAR1,
    APGAR5
FROM base

[0m14:40:20.799143 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:21.651284 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-bef5-1d56-86e3-37b6dd67cd3c
[0m14:40:22.336493 [debug] [Thread-1 (]: SQL status: OK in 1.5399999618530273 seconds
[0m14:40:22.345434 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 14:40:20.785180 => 14:40:22.344437
[0m14:40:22.346431 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: ROLLBACK
[0m14:40:22.347463 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:22.348461 [debug] [Thread-1 (]: On model.projeto_health_insights.int_nascimento: Close
[0m14:40:22.349457 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-bef5-1d56-86e3-37b6dd67cd3c
[0m14:40:22.578295 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DD03F3E50>]}
[0m14:40:22.580290 [info ] [Thread-1 (]: 7 of 12 OK created sql view model default.int_nascimento ....................... [[32mOK[0m in 1.81s]
[0m14:40:22.582318 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m14:40:22.584307 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m14:40:22.585281 [info ] [Thread-1 (]: 8 of 12 START sql view model default.stg_tempo ................................. [RUN]
[0m14:40:22.588302 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m14:40:22.589298 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m14:40:22.599269 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m14:40:22.600234 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 14:40:22.590263 => 14:40:22.600234
[0m14:40:22.601261 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m14:40:22.607245 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.stg_tempo"
[0m14:40:22.609212 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:22.609212 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.stg_tempo"
[0m14:40:22.610208 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.stg_tempo"} */
create or replace view `workspace`.`default`.`stg_tempo`
  
  
  as
    

SELECT DISTINCT
    data_nascimento AS data,
    CAST(date_format(data_nascimento, 'yyyyMMdd') AS INT) AS data_id,
    year(data_nascimento) AS ano,
    month(data_nascimento) AS mes,
    day(data_nascimento) AS dia,
    quarter(data_nascimento) AS trimestre,
    dayofweek(data_nascimento) AS dia_semana
FROM `workspace`.`default`.`stg_nascidos_vivos`

[0m14:40:22.610208 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:23.537946 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-c014-12c1-a60e-cf0e5b619d6b
[0m14:40:24.306302 [debug] [Thread-1 (]: SQL status: OK in 1.7000000476837158 seconds
[0m14:40:24.314282 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 14:40:22.602259 => 14:40:24.313284
[0m14:40:24.316275 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: ROLLBACK
[0m14:40:24.317272 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:24.318295 [debug] [Thread-1 (]: On model.projeto_health_insights.stg_tempo: Close
[0m14:40:24.318295 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-c014-12c1-a60e-cf0e5b619d6b
[0m14:40:24.558211 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DCF1A7C50>]}
[0m14:40:24.559208 [info ] [Thread-1 (]: 8 of 12 OK created sql view model default.stg_tempo ............................ [[32mOK[0m in 1.97s]
[0m14:40:24.560206 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m14:40:24.561202 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:40:24.561202 [info ] [Thread-1 (]: 9 of 12 START sql view model default.fato_atendimento_hospitalar ............... [RUN]
[0m14:40:24.563206 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m14:40:24.564197 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:40:24.568217 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:40:24.570178 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 14:40:24.564197 => 14:40:24.569181
[0m14:40:24.570178 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:40:24.575193 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:40:24.576163 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:24.577181 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:40:24.577181 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_atendimento_hospitalar"} */
create or replace view `workspace`.`default`.`fato_atendimento_hospitalar`
  
  
  as
    -- Fato Atendimento Hospitalar
select
    a.paciente,
    a.data_atendimento,
    a.procedimento,
    a.doenca_id,
    a.localidade_id
from `workspace`.`default`.`int_atendimento` a

[0m14:40:24.578194 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:25.349495 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-c12a-165c-a7ce-2e676103cbf4
[0m14:40:26.232299 [debug] [Thread-1 (]: SQL status: OK in 1.649999976158142 seconds
[0m14:40:26.240276 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 14:40:24.571176 => 14:40:26.240276
[0m14:40:26.241270 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: ROLLBACK
[0m14:40:26.242268 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:26.242268 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_atendimento_hospitalar: Close
[0m14:40:26.243264 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-c12a-165c-a7ce-2e676103cbf4
[0m14:40:26.486466 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DD0353190>]}
[0m14:40:26.489458 [info ] [Thread-1 (]: 9 of 12 OK created sql view model default.fato_atendimento_hospitalar .......... [[32mOK[0m in 1.92s]
[0m14:40:26.493447 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:40:26.495407 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m14:40:26.498435 [info ] [Thread-1 (]: 10 of 12 START sql view model default.dim_localidade ........................... [RUN]
[0m14:40:26.502385 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m14:40:26.504380 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m14:40:26.511359 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m14:40:26.513354 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 14:40:26.505377 => 14:40:26.512356
[0m14:40:26.514351 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m14:40:26.520335 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_localidade"
[0m14:40:26.521334 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:26.522330 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_localidade"
[0m14:40:26.522330 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_localidade"} */
create or replace view `workspace`.`default`.`dim_localidade`
  
  
  as
    

SELECT
    cod_municipio,
    ANY_VALUE(local_nascimento) AS municipio -- Pega qualquer valor de local_nascimento
FROM `workspace`.`default`.`int_nascimento`
GROUP BY cod_municipio

[0m14:40:26.523355 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:27.328265 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-c257-190c-b5fb-242a7b6e114a
[0m14:40:28.115838 [debug] [Thread-1 (]: SQL status: OK in 1.590000033378601 seconds
[0m14:40:28.118830 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 14:40:26.514351 => 14:40:28.117806
[0m14:40:28.118830 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: ROLLBACK
[0m14:40:28.119830 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:28.119830 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_localidade: Close
[0m14:40:28.120825 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-c257-190c-b5fb-242a7b6e114a
[0m14:40:28.357146 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DD030F4D0>]}
[0m14:40:28.360099 [info ] [Thread-1 (]: 10 of 12 OK created sql view model default.dim_localidade ...................... [[32mOK[0m in 1.86s]
[0m14:40:28.363343 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m14:40:28.364339 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m14:40:28.365337 [info ] [Thread-1 (]: 11 of 12 START sql view model default.dim_tempo ................................ [RUN]
[0m14:40:28.368329 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m14:40:28.369326 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m14:40:28.376335 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m14:40:28.378302 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 14:40:28.370322 => 14:40:28.377334
[0m14:40:28.378302 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m14:40:28.384285 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.dim_tempo"
[0m14:40:28.386280 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:28.387291 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.dim_tempo"
[0m14:40:28.387291 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.dim_tempo"} */
create or replace view `workspace`.`default`.`dim_tempo`
  
  
  as
    

WITH datas AS (
    SELECT DISTINCT
        data_nascimento AS data
    FROM `workspace`.`default`.`int_nascimento`
)

SELECT
    ROW_NUMBER() OVER (ORDER BY data) AS data_id,
    data,
    EXTRACT(YEAR FROM data) AS ano,
    EXTRACT(MONTH FROM data) AS mes,
    EXTRACT(DAY FROM data) AS dia,
    DAYOFWEEK(data) AS dia_semana
FROM datas

[0m14:40:28.388302 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:29.190352 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-c370-1939-8a75-8de065d5f3a9
[0m14:40:30.015944 [debug] [Thread-1 (]: SQL status: OK in 1.6299999952316284 seconds
[0m14:40:30.025947 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 14:40:28.379327 => 14:40:30.025947
[0m14:40:30.027913 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: ROLLBACK
[0m14:40:30.028939 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:30.029938 [debug] [Thread-1 (]: On model.projeto_health_insights.dim_tempo: Close
[0m14:40:30.030936 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-c370-1939-8a75-8de065d5f3a9
[0m14:40:30.365318 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DD030F310>]}
[0m14:40:30.366285 [info ] [Thread-1 (]: 11 of 12 OK created sql view model default.dim_tempo ........................... [[32mOK[0m in 2.00s]
[0m14:40:30.367282 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m14:40:30.368308 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m14:40:30.369306 [info ] [Thread-1 (]: 12 of 12 START sql view model default.fato_nascimento .......................... [RUN]
[0m14:40:30.371272 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now model.projeto_health_insights.fato_nascimento)
[0m14:40:30.372298 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m14:40:30.380279 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m14:40:30.381260 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 14:40:30.372298 => 14:40:30.381260
[0m14:40:30.382270 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m14:40:30.388227 [debug] [Thread-1 (]: Writing runtime sql for node "model.projeto_health_insights.fato_nascimento"
[0m14:40:30.390221 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:30.391227 [debug] [Thread-1 (]: Using databricks connection "model.projeto_health_insights.fato_nascimento"
[0m14:40:30.392215 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "model.projeto_health_insights.fato_nascimento"} */
create or replace view `workspace`.`default`.`fato_nascimento`
  
  
  as
    

SELECT
    i.nascimento_id,
    t.data_id,
    l.cod_municipio,
    i.sexo,
    COALESCE(i.peso, 0) AS peso,
    i.idade_mae,
    COALESCE(i.gestacao_semanas, 0) AS gestacao_semanas,
    COALESCE(i.tipo_parto, 0) AS tipo_parto,
    COALESCE(i.APGAR1, 0) AS APGAR1,
    COALESCE(i.APGAR5, 0) AS APGAR5
FROM `workspace`.`default`.`int_nascimento` AS i
LEFT JOIN `workspace`.`default`.`dim_tempo` AS t
    ON i.data_nascimento = t.data
LEFT JOIN `workspace`.`default`.`dim_localidade` AS l
    ON i.cod_municipio = l.cod_municipio

[0m14:40:30.393213 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:31.192632 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-c4a3-1772-bb05-9007648f94cf
[0m14:40:32.057244 [debug] [Thread-1 (]: SQL status: OK in 1.6699999570846558 seconds
[0m14:40:32.061233 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 14:40:30.382270 => 14:40:32.061233
[0m14:40:32.062230 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: ROLLBACK
[0m14:40:32.063227 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:32.064225 [debug] [Thread-1 (]: On model.projeto_health_insights.fato_nascimento: Close
[0m14:40:32.064225 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-c4a3-1772-bb05-9007648f94cf
[0m14:40:32.330864 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b637d977-76b1-4ae7-941b-23c3143c2592', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DD052BF50>]}
[0m14:40:32.331861 [info ] [Thread-1 (]: 12 of 12 OK created sql view model default.fato_nascimento ..................... [[32mOK[0m in 1.96s]
[0m14:40:32.332861 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m14:40:32.334853 [debug] [MainThread]: On master: ROLLBACK
[0m14:40:32.335851 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:40:33.248827 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07935-c5dd-1273-b1ff-adf5b4a29b9c
[0m14:40:33.251818 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:40:33.253813 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:33.255771 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:40:33.257766 [debug] [MainThread]: On master: ROLLBACK
[0m14:40:33.259762 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:40:33.261753 [debug] [MainThread]: On master: Close
[0m14:40:33.264747 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07935-c5dd-1273-b1ff-adf5b4a29b9c
[0m14:40:33.500266 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:40:33.502258 [debug] [MainThread]: Connection 'create_workspace_default' was properly closed.
[0m14:40:33.503260 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:40:33.505214 [debug] [MainThread]: Connection 'model.projeto_health_insights.fato_nascimento' was properly closed.
[0m14:40:33.508204 [info ] [MainThread]: 
[0m14:40:33.510198 [info ] [MainThread]: Finished running 12 view models in 0 hours 0 minutes and 28.80 seconds (28.80s).
[0m14:40:33.516213 [debug] [MainThread]: Command end result
[0m14:40:33.534162 [info ] [MainThread]: 
[0m14:40:33.535280 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:40:33.536280 [info ] [MainThread]: 
[0m14:40:33.537287 [info ] [MainThread]: Done. PASS=12 WARN=0 ERROR=0 SKIP=0 TOTAL=12
[0m14:40:33.538277 [debug] [MainThread]: Command `dbt run` succeeded at 14:40:33.538277 after 30.66 seconds
[0m14:40:33.540270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DC2EA2750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DC57942D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019DBF50BB10>]}
[0m14:40:33.541267 [debug] [MainThread]: Flushing usage events
[0m14:40:43.285281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020959C98C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020959C765D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020959FEC3D0>]}


============================== 14:40:43.290269 | 05a91c52-427d-4ce9-aa65-7df1ab76f0d9 ==============================
[0m14:40:43.290269 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:40:43.291266 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:40:44.713686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '05a91c52-427d-4ce9-aa65-7df1ab76f0d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002095A597D50>]}
[0m14:40:44.734602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '05a91c52-427d-4ce9-aa65-7df1ab76f0d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020959F826D0>]}
[0m14:40:44.735613 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:40:44.765519 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:40:44.922091 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:40:44.923117 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:40:45.029803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '05a91c52-427d-4ce9-aa65-7df1ab76f0d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020959FECAD0>]}
[0m14:40:45.045760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '05a91c52-427d-4ce9-aa65-7df1ab76f0d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209647FBD50>]}
[0m14:40:45.046758 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:40:45.047183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05a91c52-427d-4ce9-aa65-7df1ab76f0d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020960D5D9D0>]}
[0m14:40:45.050207 [info ] [MainThread]: 
[0m14:40:45.052173 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:40:45.055165 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:40:45.062146 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:40:45.063144 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:40:45.063144 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:40:45.910100 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07935-cd69-198b-a628-3d5c6676d443
[0m14:40:46.637005 [debug] [ThreadPool]: SQL status: OK in 1.5700000524520874 seconds
[0m14:40:46.646612 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:40:46.647610 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07935-cd69-198b-a628-3d5c6676d443
[0m14:40:47.402927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05a91c52-427d-4ce9-aa65-7df1ab76f0d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209594AD610>]}
[0m14:40:47.403913 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:47.405889 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:40:47.406916 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:40:47.407866 [info ] [MainThread]: 
[0m14:40:47.417327 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:40:47.418328 [info ] [Thread-1 (]: 1 of 21 START test accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado  [RUN]
[0m14:40:47.420323 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d'
[0m14:40:47.421321 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:40:47.445255 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:40:47.446252 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (compile): 14:40:47.422317 => 14:40:47.446252
[0m14:40:47.447249 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:40:47.471187 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:40:47.473180 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:47.474177 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:40:47.475175 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        sexo as value_field,
        count(*) as n_records

    from `workspace`.`default`.`fato_nascimento`
    group by sexo

)

select *
from all_values
where value_field not in (
    'Masculino','Feminino','Ignorado'
)



      
    ) dbt_internal_test
[0m14:40:47.476172 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:40:48.667477 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-cef2-1899-a912-54f8b6cebd33
[0m14:40:49.404885 [debug] [Thread-1 (]: SQL status: OK in 1.9299999475479126 seconds
[0m14:40:49.409873 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (execute): 14:40:47.447249 => 14:40:49.409873
[0m14:40:49.410869 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: ROLLBACK
[0m14:40:49.410869 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:49.411867 [debug] [Thread-1 (]: On test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d: Close
[0m14:40:49.412864 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-cef2-1899-a912-54f8b6cebd33
[0m14:40:49.643721 [info ] [Thread-1 (]: 1 of 21 PASS accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado  [[32mPASS[0m in 2.22s]
[0m14:40:49.647748 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:40:49.649741 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:40:49.651702 [info ] [Thread-1 (]: 2 of 21 START test not_null_dim_localidade_cod_municipio ....................... [RUN]
[0m14:40:49.655690 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m14:40:49.658683 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:40:49.674638 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:40:49.677628 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 14:40:49.659682 => 14:40:49.676632
[0m14:40:49.677628 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:40:49.681617 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:40:49.682644 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:49.683641 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:40:49.683641 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`dim_localidade`
where cod_municipio is null



      
    ) dbt_internal_test
[0m14:40:49.684639 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:50.738775 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-d040-1e01-a7ed-69487f19da7b
[0m14:40:51.264893 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m14:40:51.266916 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 14:40:49.678625 => 14:40:51.266916
[0m14:40:51.267913 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: ROLLBACK
[0m14:40:51.268883 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:51.268883 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f: Close
[0m14:40:51.269915 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-d040-1e01-a7ed-69487f19da7b
[0m14:40:51.506020 [info ] [Thread-1 (]: 2 of 21 PASS not_null_dim_localidade_cod_municipio ............................. [[32mPASS[0m in 1.85s]
[0m14:40:51.510009 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:40:51.513000 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:40:51.514993 [info ] [Thread-1 (]: 3 of 21 START test not_null_dim_localidade_municipio ........................... [RUN]
[0m14:40:51.517985 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771)
[0m14:40:51.518982 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:40:51.530978 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:40:51.532943 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (compile): 14:40:51.519979 => 14:40:51.531966
[0m14:40:51.532943 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:40:51.536966 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:40:51.537959 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:51.538927 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:40:51.538927 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select municipio
from `workspace`.`default`.`dim_localidade`
where municipio is null



      
    ) dbt_internal_test
[0m14:40:51.539925 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:52.400522 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-d148-1e62-a262-b0f7149ea69d
[0m14:40:53.123104 [debug] [Thread-1 (]: SQL status: OK in 1.5800000429153442 seconds
[0m14:40:53.132071 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (execute): 14:40:51.533970 => 14:40:53.131074
[0m14:40:53.134070 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: ROLLBACK
[0m14:40:53.135034 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:53.136031 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771: Close
[0m14:40:53.137056 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-d148-1e62-a262-b0f7149ea69d
[0m14:40:53.424078 [info ] [Thread-1 (]: 3 of 21 PASS not_null_dim_localidade_municipio ................................. [[32mPASS[0m in 1.91s]
[0m14:40:53.426078 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:40:53.427040 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:40:53.429036 [info ] [Thread-1 (]: 4 of 21 START test not_null_dim_tempo_ano ...................................... [RUN]
[0m14:40:53.431031 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771, now test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6)
[0m14:40:53.433024 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:40:53.443030 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:40:53.445990 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (compile): 14:40:53.433024 => 14:40:53.444993
[0m14:40:53.446988 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:40:53.453967 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:40:53.455963 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:53.455963 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:40:53.456960 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select ano
from `workspace`.`default`.`dim_tempo`
where ano is null



      
    ) dbt_internal_test
[0m14:40:53.456960 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:54.272151 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-d266-1166-b3b3-fb6557803843
[0m14:40:54.843350 [debug] [Thread-1 (]: SQL status: OK in 1.3899999856948853 seconds
[0m14:40:54.847342 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (execute): 14:40:53.447985 => 14:40:54.846315
[0m14:40:54.847342 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: ROLLBACK
[0m14:40:54.848338 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:54.848338 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6: Close
[0m14:40:54.849334 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-d266-1166-b3b3-fb6557803843
[0m14:40:55.113104 [info ] [Thread-1 (]: 4 of 21 PASS not_null_dim_tempo_ano ............................................ [[32mPASS[0m in 1.68s]
[0m14:40:55.115098 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:40:55.115098 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:40:55.116095 [info ] [Thread-1 (]: 5 of 21 START test not_null_dim_tempo_data ..................................... [RUN]
[0m14:40:55.117199 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6, now test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252)
[0m14:40:55.118230 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:40:55.125180 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:40:55.127175 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (compile): 14:40:55.119226 => 14:40:55.127175
[0m14:40:55.128173 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:40:55.132161 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:40:55.134156 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:55.134156 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:40:55.135153 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data
from `workspace`.`default`.`dim_tempo`
where data is null



      
    ) dbt_internal_test
[0m14:40:55.135153 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:56.053717 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-d373-18a5-8b1e-9fbb73e6ad8d
[0m14:40:56.635951 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m14:40:56.645439 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (execute): 14:40:55.129201 => 14:40:56.644450
[0m14:40:56.647403 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: ROLLBACK
[0m14:40:56.648402 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:56.650394 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252: Close
[0m14:40:56.651391 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-d373-18a5-8b1e-9fbb73e6ad8d
[0m14:40:56.888231 [info ] [Thread-1 (]: 5 of 21 PASS not_null_dim_tempo_data ........................................... [[32mPASS[0m in 1.77s]
[0m14:40:56.891251 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:40:56.893247 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:40:56.894243 [info ] [Thread-1 (]: 6 of 21 START test not_null_dim_tempo_data_id .................................. [RUN]
[0m14:40:56.897206 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m14:40:56.898201 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:40:56.907177 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:40:56.909171 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 14:40:56.899199 => 14:40:56.908173
[0m14:40:56.909171 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:40:56.913162 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:40:56.915155 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:56.916154 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:40:56.917150 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`dim_tempo`
where data_id is null



      
    ) dbt_internal_test
[0m14:40:56.918147 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:57.683020 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-d470-12c5-9c05-3d8cdf6585eb
[0m14:40:58.158720 [debug] [Thread-1 (]: SQL status: OK in 1.2400000095367432 seconds
[0m14:40:58.161711 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 14:40:56.910169 => 14:40:58.161711
[0m14:40:58.162710 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: ROLLBACK
[0m14:40:58.163724 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:58.163724 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e: Close
[0m14:40:58.164703 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-d470-12c5-9c05-3d8cdf6585eb
[0m14:40:58.405968 [info ] [Thread-1 (]: 6 of 21 PASS not_null_dim_tempo_data_id ........................................ [[32mPASS[0m in 1.51s]
[0m14:40:58.408927 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:40:58.409923 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:40:58.410922 [info ] [Thread-1 (]: 7 of 21 START test not_null_dim_tempo_dia ...................................... [RUN]
[0m14:40:58.413155 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306)
[0m14:40:58.415151 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:40:58.426635 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:40:58.428641 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (compile): 14:40:58.416147 => 14:40:58.428641
[0m14:40:58.429626 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:40:58.433616 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:40:58.434614 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:40:58.435611 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:40:58.436609 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select dia
from `workspace`.`default`.`dim_tempo`
where dia is null



      
    ) dbt_internal_test
[0m14:40:58.436609 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:40:59.242258 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-d55c-1d98-9442-ecda36c13b78
[0m14:40:59.837064 [debug] [Thread-1 (]: SQL status: OK in 1.399999976158142 seconds
[0m14:40:59.840057 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (execute): 14:40:58.429626 => 14:40:59.840057
[0m14:40:59.841054 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: ROLLBACK
[0m14:40:59.841054 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:40:59.842052 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306: Close
[0m14:40:59.843097 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-d55c-1d98-9442-ecda36c13b78
[0m14:41:00.080269 [info ] [Thread-1 (]: 7 of 21 PASS not_null_dim_tempo_dia ............................................ [[32mPASS[0m in 1.67s]
[0m14:41:00.082263 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:41:00.082263 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:41:00.083260 [info ] [Thread-1 (]: 8 of 21 START test not_null_dim_tempo_mes ...................................... [RUN]
[0m14:41:00.085257 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306, now test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972)
[0m14:41:00.086287 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:41:00.092249 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:41:00.094232 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (compile): 14:41:00.086287 => 14:41:00.093264
[0m14:41:00.094232 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:41:00.097253 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:41:00.099218 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:00.099218 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:41:00.100216 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select mes
from `workspace`.`default`.`dim_tempo`
where mes is null



      
    ) dbt_internal_test
[0m14:41:00.101214 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:00.908876 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-d65b-1e84-a76a-3e97db97743f
[0m14:41:01.485142 [debug] [Thread-1 (]: SQL status: OK in 1.3799999952316284 seconds
[0m14:41:01.495107 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (execute): 14:41:00.094232 => 14:41:01.494118
[0m14:41:01.497102 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: ROLLBACK
[0m14:41:01.498068 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:01.499097 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972: Close
[0m14:41:01.500092 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-d65b-1e84-a76a-3e97db97743f
[0m14:41:01.722665 [info ] [Thread-1 (]: 8 of 21 PASS not_null_dim_tempo_mes ............................................ [[32mPASS[0m in 1.64s]
[0m14:41:01.724058 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:41:01.725086 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:41:01.726056 [info ] [Thread-1 (]: 9 of 21 START test not_null_fato_nascimento_APGAR1 ............................. [RUN]
[0m14:41:01.727053 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972, now test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074)
[0m14:41:01.728050 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:41:01.735059 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:41:01.736029 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (compile): 14:41:01.728050 => 14:41:01.736029
[0m14:41:01.737027 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:41:01.743037 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:41:01.745005 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:01.745005 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:41:01.746001 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR1
from `workspace`.`default`.`fato_nascimento`
where APGAR1 is null



      
    ) dbt_internal_test
[0m14:41:01.747027 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:02.568330 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-d758-1308-a02e-55953c329e2f
[0m14:41:03.192698 [debug] [Thread-1 (]: SQL status: OK in 1.4500000476837158 seconds
[0m14:41:03.198713 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (execute): 14:41:01.738023 => 14:41:03.197683
[0m14:41:03.199680 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: ROLLBACK
[0m14:41:03.200677 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:03.201675 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074: Close
[0m14:41:03.202672 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-d758-1308-a02e-55953c329e2f
[0m14:41:03.439700 [info ] [Thread-1 (]: 9 of 21 PASS not_null_fato_nascimento_APGAR1 ................................... [[32mPASS[0m in 1.71s]
[0m14:41:03.441151 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:41:03.442180 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:41:03.443178 [info ] [Thread-1 (]: 10 of 21 START test not_null_fato_nascimento_APGAR5 ............................ [RUN]
[0m14:41:03.444174 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074, now test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994)
[0m14:41:03.444174 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:41:03.450173 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:41:03.451171 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (compile): 14:41:03.445174 => 14:41:03.451171
[0m14:41:03.451171 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:41:03.456116 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:41:03.457141 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:03.458131 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:41:03.458131 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select APGAR5
from `workspace`.`default`.`fato_nascimento`
where APGAR5 is null



      
    ) dbt_internal_test
[0m14:41:03.459134 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:04.239446 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-d858-102d-977d-f2f0dabb8a9b
[0m14:41:04.842005 [debug] [Thread-1 (]: SQL status: OK in 1.3799999952316284 seconds
[0m14:41:04.844996 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (execute): 14:41:03.452153 => 14:41:04.844996
[0m14:41:04.845967 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: ROLLBACK
[0m14:41:04.845967 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:04.846964 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994: Close
[0m14:41:04.846964 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-d858-102d-977d-f2f0dabb8a9b
[0m14:41:05.101093 [info ] [Thread-1 (]: 10 of 21 PASS not_null_fato_nascimento_APGAR5 .................................. [[32mPASS[0m in 1.66s]
[0m14:41:05.103129 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:41:05.105155 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:41:05.106120 [info ] [Thread-1 (]: 11 of 21 START test not_null_fato_nascimento_cod_municipio ..................... [RUN]
[0m14:41:05.109145 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m14:41:05.110108 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:41:05.119115 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:41:05.120110 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 14:41:05.111106 => 14:41:05.120110
[0m14:41:05.121111 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:41:05.126095 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:41:05.127063 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:05.127063 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:41:05.128060 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select cod_municipio
from `workspace`.`default`.`fato_nascimento`
where cod_municipio is null



      
    ) dbt_internal_test
[0m14:41:05.129057 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:05.941101 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-d95c-12cb-a037-7b3ec549a0e1
[0m14:41:06.772593 [debug] [Thread-1 (]: SQL status: OK in 1.6399999856948853 seconds
[0m14:41:06.778578 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 14:41:05.122102 => 14:41:06.777581
[0m14:41:06.779579 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: ROLLBACK
[0m14:41:06.780573 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:06.780573 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11: Close
[0m14:41:06.781597 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-d95c-12cb-a037-7b3ec549a0e1
[0m14:41:07.036813 [info ] [Thread-1 (]: 11 of 21 PASS not_null_fato_nascimento_cod_municipio ........................... [[32mPASS[0m in 1.93s]
[0m14:41:07.038836 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:41:07.038836 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:41:07.039833 [info ] [Thread-1 (]: 12 of 21 START test not_null_fato_nascimento_data_id ........................... [RUN]
[0m14:41:07.041828 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m14:41:07.042797 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:41:07.049778 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:41:07.051771 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 14:41:07.042797 => 14:41:07.051771
[0m14:41:07.052770 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:41:07.055792 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:41:07.056789 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:07.057755 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:41:07.058754 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select data_id
from `workspace`.`default`.`fato_nascimento`
where data_id is null



      
    ) dbt_internal_test
[0m14:41:07.059752 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:07.857178 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-da7c-1b19-ad72-2e40840d7482
[0m14:41:08.793002 [debug] [Thread-1 (]: SQL status: OK in 1.7300000190734863 seconds
[0m14:41:08.799985 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 14:41:07.052770 => 14:41:08.799985
[0m14:41:08.801979 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: ROLLBACK
[0m14:41:08.803973 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:08.804975 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244: Close
[0m14:41:08.806966 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-da7c-1b19-ad72-2e40840d7482
[0m14:41:09.063179 [info ] [Thread-1 (]: 12 of 21 PASS not_null_fato_nascimento_data_id ................................. [[32mPASS[0m in 2.02s]
[0m14:41:09.066170 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:41:09.068163 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:41:09.069132 [info ] [Thread-1 (]: 13 of 21 START test not_null_fato_nascimento_gestacao_semanas .................. [RUN]
[0m14:41:09.072154 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2)
[0m14:41:09.073153 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:41:09.084116 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:41:09.085087 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (compile): 14:41:09.075114 => 14:41:09.085087
[0m14:41:09.086112 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:41:09.089103 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:41:09.091074 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:09.091074 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:41:09.092067 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select gestacao_semanas
from `workspace`.`default`.`fato_nascimento`
where gestacao_semanas is null



      
    ) dbt_internal_test
[0m14:41:09.093065 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:10.069074 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-dbcf-11ad-baa4-719f8863a154
[0m14:41:10.790486 [debug] [Thread-1 (]: SQL status: OK in 1.7000000476837158 seconds
[0m14:41:10.794475 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (execute): 14:41:09.086112 => 14:41:10.793502
[0m14:41:10.794475 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: ROLLBACK
[0m14:41:10.795473 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:10.796471 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2: Close
[0m14:41:10.797468 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-dbcf-11ad-baa4-719f8863a154
[0m14:41:11.366054 [info ] [Thread-1 (]: 13 of 21 PASS not_null_fato_nascimento_gestacao_semanas ........................ [[32mPASS[0m in 2.29s]
[0m14:41:11.368015 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:41:11.369012 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:41:11.370009 [info ] [Thread-1 (]: 14 of 21 START test not_null_fato_nascimento_idade_mae ......................... [RUN]
[0m14:41:11.371008 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2, now test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42)
[0m14:41:11.372006 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:41:11.383004 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:41:11.384000 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (compile): 14:41:11.372006 => 14:41:11.384000
[0m14:41:11.385004 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:41:11.387992 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:41:11.388960 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:11.389962 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:41:11.390955 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select idade_mae
from `workspace`.`default`.`fato_nascimento`
where idade_mae is null



      
    ) dbt_internal_test
[0m14:41:11.390955 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:12.514401 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-dd40-1002-b5bd-68d1def26536
[0m14:41:13.171789 [debug] [Thread-1 (]: SQL status: OK in 1.7799999713897705 seconds
[0m14:41:13.176776 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (execute): 14:41:11.385004 => 14:41:13.176776
[0m14:41:13.178770 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: ROLLBACK
[0m14:41:13.179736 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:13.180734 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42: Close
[0m14:41:13.181762 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-dd40-1002-b5bd-68d1def26536
[0m14:41:13.424696 [info ] [Thread-1 (]: 14 of 21 PASS not_null_fato_nascimento_idade_mae ............................... [[32mPASS[0m in 2.05s]
[0m14:41:13.427659 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:41:13.428656 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:41:13.429654 [info ] [Thread-1 (]: 15 of 21 START test not_null_fato_nascimento_nascimento_id ..................... [RUN]
[0m14:41:13.431650 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m14:41:13.433642 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:41:13.442647 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:41:13.443616 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 14:41:13.434640 => 14:41:13.443616
[0m14:41:13.444662 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:41:13.448603 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:41:13.450596 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:13.450596 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:41:13.451615 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select nascimento_id
from `workspace`.`default`.`fato_nascimento`
where nascimento_id is null



      
    ) dbt_internal_test
[0m14:41:13.452591 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:14.414091 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-de66-19ce-b2ad-7c341d84f843
[0m14:41:15.003102 [debug] [Thread-1 (]: SQL status: OK in 1.5499999523162842 seconds
[0m14:41:15.006132 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 14:41:13.444662 => 14:41:15.006132
[0m14:41:15.007117 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: ROLLBACK
[0m14:41:15.008107 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:15.008107 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a: Close
[0m14:41:15.009118 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-de66-19ce-b2ad-7c341d84f843
[0m14:41:15.288233 [info ] [Thread-1 (]: 15 of 21 PASS not_null_fato_nascimento_nascimento_id ........................... [[32mPASS[0m in 1.86s]
[0m14:41:15.291196 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:41:15.292190 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:41:15.293188 [info ] [Thread-1 (]: 16 of 21 START test not_null_fato_nascimento_peso .............................. [RUN]
[0m14:41:15.294186 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m14:41:15.295183 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:41:15.302165 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:41:15.304160 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 14:41:15.295183 => 14:41:15.303162
[0m14:41:15.304160 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:41:15.308175 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:41:15.309159 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:15.309159 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:41:15.310145 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select peso
from `workspace`.`default`.`fato_nascimento`
where peso is null



      
    ) dbt_internal_test
[0m14:41:15.310145 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:16.188534 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-df76-1d70-9c77-e4b75a7036fb
[0m14:41:16.805435 [debug] [Thread-1 (]: SQL status: OK in 1.5 seconds
[0m14:41:16.810453 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 14:41:15.305184 => 14:41:16.809456
[0m14:41:16.811451 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: ROLLBACK
[0m14:41:16.812451 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:16.813445 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b: Close
[0m14:41:16.814411 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-df76-1d70-9c77-e4b75a7036fb
[0m14:41:17.051651 [info ] [Thread-1 (]: 16 of 21 PASS not_null_fato_nascimento_peso .................................... [[32mPASS[0m in 1.76s]
[0m14:41:17.054670 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:41:17.055676 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:41:17.056667 [info ] [Thread-1 (]: 17 of 21 START test not_null_fato_nascimento_sexo .............................. [RUN]
[0m14:41:17.057634 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m14:41:17.058663 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:41:17.069603 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:41:17.071598 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 14:41:17.059659 => 14:41:17.070599
[0m14:41:17.072593 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:41:17.077580 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:41:17.078578 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:17.079576 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:41:17.080572 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sexo
from `workspace`.`default`.`fato_nascimento`
where sexo is null



      
    ) dbt_internal_test
[0m14:41:17.081570 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:17.938071 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-e082-1cdd-a70b-fe7a1ce68a85
[0m14:41:18.527872 [debug] [Thread-1 (]: SQL status: OK in 1.4500000476837158 seconds
[0m14:41:18.530834 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 14:41:17.072593 => 14:41:18.529836
[0m14:41:18.530834 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: ROLLBACK
[0m14:41:18.531831 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:18.531831 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156: Close
[0m14:41:18.532828 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-e082-1cdd-a70b-fe7a1ce68a85
[0m14:41:18.768817 [info ] [Thread-1 (]: 17 of 21 PASS not_null_fato_nascimento_sexo .................................... [[32mPASS[0m in 1.71s]
[0m14:41:18.772809 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:41:18.774801 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:41:18.775802 [info ] [Thread-1 (]: 18 of 21 START test not_null_fato_nascimento_tipo_parto ........................ [RUN]
[0m14:41:18.778791 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e)
[0m14:41:18.780787 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:41:18.791785 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:41:18.792783 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (compile): 14:41:18.781785 => 14:41:18.792783
[0m14:41:18.793780 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:41:18.797739 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:41:18.798767 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:18.799735 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:41:18.799735 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select tipo_parto
from `workspace`.`default`.`fato_nascimento`
where tipo_parto is null



      
    ) dbt_internal_test
[0m14:41:18.800732 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:19.687175 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-e18c-12ce-a5a2-33d401bae218
[0m14:41:20.265327 [debug] [Thread-1 (]: SQL status: OK in 1.4700000286102295 seconds
[0m14:41:20.269314 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (execute): 14:41:18.793780 => 14:41:20.268317
[0m14:41:20.270309 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: ROLLBACK
[0m14:41:20.271280 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:20.271280 [debug] [Thread-1 (]: On test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e: Close
[0m14:41:20.272277 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-e18c-12ce-a5a2-33d401bae218
[0m14:41:20.512483 [info ] [Thread-1 (]: 18 of 21 PASS not_null_fato_nascimento_tipo_parto .............................. [[32mPASS[0m in 1.73s]
[0m14:41:20.516507 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:41:20.518469 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:41:20.520462 [info ] [Thread-1 (]: 19 of 21 START test unique_dim_localidade_cod_municipio ........................ [RUN]
[0m14:41:20.525448 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m14:41:20.527442 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:41:20.546418 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:41:20.548383 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 14:41:20.529436 => 14:41:20.548383
[0m14:41:20.549408 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:41:20.553398 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:41:20.554396 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:20.556362 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:41:20.557360 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    cod_municipio as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_localidade`
where cod_municipio is not null
group by cod_municipio
having count(*) > 1



      
    ) dbt_internal_test
[0m14:41:20.558358 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:21.511078 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-e2a2-1a99-92a5-a8c17aad4b4b
[0m14:41:22.178323 [debug] [Thread-1 (]: SQL status: OK in 1.6200000047683716 seconds
[0m14:41:22.181315 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 14:41:20.549408 => 14:41:22.180317
[0m14:41:22.181315 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: ROLLBACK
[0m14:41:22.182312 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:22.182312 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2: Close
[0m14:41:22.183309 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-e2a2-1a99-92a5-a8c17aad4b4b
[0m14:41:22.450927 [info ] [Thread-1 (]: 19 of 21 PASS unique_dim_localidade_cod_municipio .............................. [[32mPASS[0m in 1.93s]
[0m14:41:22.452923 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:41:22.453951 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:41:22.454948 [info ] [Thread-1 (]: 20 of 21 START test unique_dim_tempo_data_id ................................... [RUN]
[0m14:41:22.457908 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m14:41:22.459903 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:41:22.469874 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:41:22.471871 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 14:41:22.459903 => 14:41:22.470907
[0m14:41:22.472868 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:41:22.477883 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:41:22.479849 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:22.479849 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:41:22.480875 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    data_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`dim_tempo`
where data_id is not null
group by data_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:41:22.480875 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:23.290557 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-e3b2-1e52-b062-d62630d19721
[0m14:41:24.109905 [debug] [Thread-1 (]: SQL status: OK in 1.6299999952316284 seconds
[0m14:41:24.119877 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 14:41:22.472868 => 14:41:24.119877
[0m14:41:24.121872 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: ROLLBACK
[0m14:41:24.122869 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:24.124862 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_dim_tempo_data_id.7391507257: Close
[0m14:41:24.125860 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-e3b2-1e52-b062-d62630d19721
[0m14:41:24.354406 [info ] [Thread-1 (]: 20 of 21 PASS unique_dim_tempo_data_id ......................................... [[32mPASS[0m in 1.90s]
[0m14:41:24.356832 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:41:24.357859 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:41:24.358857 [info ] [Thread-1 (]: 21 of 21 START test unique_fato_nascimento_nascimento_id ....................... [RUN]
[0m14:41:24.360821 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m14:41:24.361818 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:41:24.372820 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:41:24.374790 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 14:41:24.362846 => 14:41:24.374790
[0m14:41:24.375811 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:41:24.381793 [debug] [Thread-1 (]: Writing runtime sql for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:41:24.383758 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:24.383758 [debug] [Thread-1 (]: Using databricks connection "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:41:24.384756 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "node_id": "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    nascimento_id as unique_field,
    count(*) as n_records

from `workspace`.`default`.`fato_nascimento`
where nascimento_id is not null
group by nascimento_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:41:24.385782 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:25.164335 [info ] [Thread-1 (]: databricks-sql-connector adapter: Successfully opened session 01f07935-e4cf-182c-87a7-efd6d652dc54
[0m14:41:26.234624 [debug] [Thread-1 (]: SQL status: OK in 1.850000023841858 seconds
[0m14:41:26.238613 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 14:41:24.376778 => 14:41:26.237616
[0m14:41:26.238613 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: ROLLBACK
[0m14:41:26.239610 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m14:41:26.240607 [debug] [Thread-1 (]: On test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e: Close
[0m14:41:26.240607 [info ] [Thread-1 (]: databricks-sql-connector adapter: Closing session 01f07935-e4cf-182c-87a7-efd6d652dc54
[0m14:41:26.476240 [info ] [Thread-1 (]: 21 of 21 PASS unique_fato_nascimento_nascimento_id ............................. [[32mPASS[0m in 2.12s]
[0m14:41:26.480228 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:41:26.486213 [debug] [MainThread]: On master: ROLLBACK
[0m14:41:26.488207 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:41:27.300559 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 01f07935-e617-1077-9c51-9db51b96577a
[0m14:41:27.301556 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:41:27.301556 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:27.302582 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:41:27.302582 [debug] [MainThread]: On master: ROLLBACK
[0m14:41:27.303579 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:41:27.303579 [debug] [MainThread]: On master: Close
[0m14:41:27.304551 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 01f07935-e617-1077-9c51-9db51b96577a
[0m14:41:27.534089 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:41:27.535086 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:41:27.535086 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m14:41:27.537081 [info ] [MainThread]: 
[0m14:41:27.538091 [info ] [MainThread]: Finished running 21 tests in 0 hours 0 minutes and 42.48 seconds (42.48s).
[0m14:41:27.543100 [debug] [MainThread]: Command end result
[0m14:41:27.563012 [info ] [MainThread]: 
[0m14:41:27.565006 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:41:27.567001 [info ] [MainThread]: 
[0m14:41:27.568016 [info ] [MainThread]: Done. PASS=21 WARN=0 ERROR=0 SKIP=0 TOTAL=21
[0m14:41:27.569060 [debug] [MainThread]: Command `dbt test` succeeded at 14:41:27.569060 after 44.30 seconds
[0m14:41:27.569996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209599FF410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020959EAC310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020953DBC910>]}
[0m14:41:27.569996 [debug] [MainThread]: Flushing usage events
[0m14:50:44.435622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4FD039690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4FCDB9190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4FD021AD0>]}


============================== 14:50:44.441636 | f276f44b-bec0-412a-b054-a2ce6d19f92c ==============================
[0m14:50:44.441636 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:50:44.442604 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:50:45.853661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f276f44b-bec0-412a-b054-a2ce6d19f92c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4FD0F9190>]}
[0m14:50:45.873607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f276f44b-bec0-412a-b054-a2ce6d19f92c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4FD097150>]}
[0m14:50:45.874605 [info ] [MainThread]: Registered adapter: databricks=1.5.7
[0m14:50:45.904255 [debug] [MainThread]: checksum: 2435fa7be9fcfa6ffe15da8f202ddc09867bf7d6308330064299ccb85c8646b5, vars: {}, profile: , target: , version: 1.5.2
[0m14:50:46.044907 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:50:46.045898 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:50:46.110732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f276f44b-bec0-412a-b054-a2ce6d19f92c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A487A4D850>]}
[0m14:50:46.114721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f276f44b-bec0-412a-b054-a2ce6d19f92c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4879EBE90>]}
[0m14:50:46.115690 [info ] [MainThread]: Found 12 models, 21 tests, 0 snapshots, 0 analyses, 578 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics, 0 groups
[0m14:50:46.116718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f276f44b-bec0-412a-b054-a2ce6d19f92c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4FCD5E490>]}
[0m14:50:46.120676 [info ] [MainThread]: 
[0m14:50:46.122701 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:50:46.125664 [debug] [ThreadPool]: Acquiring new databricks connection 'list_workspace_default'
[0m14:50:46.132673 [debug] [ThreadPool]: Using databricks connection "list_workspace_default"
[0m14:50:46.135637 [debug] [ThreadPool]: On list_workspace_default: GetTables(database=workspace, schema=default, identifier=None)
[0m14:50:46.136635 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:50:47.055507 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07937-33b7-10a4-86a2-74173926f2e7
[0m14:50:47.723366 [debug] [ThreadPool]: SQL status: OK in 1.590000033378601 seconds
[0m14:50:47.730380 [debug] [ThreadPool]: On list_workspace_default: Close
[0m14:50:47.731375 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07937-33b7-10a4-86a2-74173926f2e7
[0m14:50:47.981217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f276f44b-bec0-412a-b054-a2ce6d19f92c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4FCD262D0>]}
[0m14:50:47.983182 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:50:47.984179 [info ] [MainThread]: 
[0m14:50:47.993917 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_atendimento
[0m14:50:47.995947 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.projeto_health_insights.stg_atendimento'
[0m14:50:47.996943 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_atendimento
[0m14:50:48.002896 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_atendimento"
[0m14:50:48.004891 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (compile): 14:50:47.997910 => 14:50:48.004891
[0m14:50:48.005888 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_atendimento
[0m14:50:48.007883 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_atendimento (execute): 14:50:48.006886 => 14:50:48.006886
[0m14:50:48.008881 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_atendimento
[0m14:50:48.009878 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_doenca
[0m14:50:48.010875 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_atendimento, now model.projeto_health_insights.stg_doenca)
[0m14:50:48.011872 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_doenca
[0m14:50:48.015861 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_doenca"
[0m14:50:48.016886 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (compile): 14:50:48.011872 => 14:50:48.016886
[0m14:50:48.017887 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_doenca
[0m14:50:48.018854 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_doenca (execute): 14:50:48.017887 => 14:50:48.017887
[0m14:50:48.020849 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_doenca
[0m14:50:48.021857 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_localidade
[0m14:50:48.022843 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_doenca, now model.projeto_health_insights.stg_localidade)
[0m14:50:48.023854 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_localidade
[0m14:50:48.027857 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_localidade"
[0m14:50:48.029854 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (compile): 14:50:48.024837 => 14:50:48.029854
[0m14:50:48.030821 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_localidade
[0m14:50:48.031819 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_localidade (execute): 14:50:48.030821 => 14:50:48.030821
[0m14:50:48.032844 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_localidade
[0m14:50:48.032844 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:50:48.034811 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_localidade, now model.projeto_health_insights.stg_nascidos_vivos)
[0m14:50:48.034811 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_nascidos_vivos
[0m14:50:48.061752 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_nascidos_vivos"
[0m14:50:48.063733 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (compile): 14:50:48.035807 => 14:50:48.063733
[0m14:50:48.064731 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_nascidos_vivos
[0m14:50:48.065728 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_nascidos_vivos (execute): 14:50:48.064731 => 14:50:48.065728
[0m14:50:48.066725 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_nascidos_vivos
[0m14:50:48.067722 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_doenca
[0m14:50:48.069717 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_nascidos_vivos, now model.projeto_health_insights.dim_doenca)
[0m14:50:48.070715 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_doenca
[0m14:50:48.074734 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_doenca"
[0m14:50:48.075701 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (compile): 14:50:48.070715 => 14:50:48.075701
[0m14:50:48.075701 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_doenca
[0m14:50:48.076727 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_doenca (execute): 14:50:48.076727 => 14:50:48.076727
[0m14:50:48.077696 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_doenca
[0m14:50:48.078721 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_atendimento
[0m14:50:48.079698 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_doenca, now model.projeto_health_insights.int_atendimento)
[0m14:50:48.080716 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_atendimento
[0m14:50:48.084678 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_atendimento"
[0m14:50:48.086680 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (compile): 14:50:48.080716 => 14:50:48.085710
[0m14:50:48.087669 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_atendimento
[0m14:50:48.087669 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_atendimento (execute): 14:50:48.087669 => 14:50:48.087669
[0m14:50:48.089663 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_atendimento
[0m14:50:48.089663 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.int_nascimento
[0m14:50:48.090661 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_atendimento, now model.projeto_health_insights.int_nascimento)
[0m14:50:48.091686 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.int_nascimento
[0m14:50:48.095676 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.int_nascimento"
[0m14:50:48.096646 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (compile): 14:50:48.091686 => 14:50:48.096646
[0m14:50:48.097646 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.int_nascimento
[0m14:50:48.097646 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.int_nascimento (execute): 14:50:48.097646 => 14:50:48.097646
[0m14:50:48.099665 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.int_nascimento
[0m14:50:48.099665 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.stg_tempo
[0m14:50:48.101650 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.int_nascimento, now model.projeto_health_insights.stg_tempo)
[0m14:50:48.101650 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.stg_tempo
[0m14:50:48.105653 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.stg_tempo"
[0m14:50:48.107655 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (compile): 14:50:48.102657 => 14:50:48.107655
[0m14:50:48.108613 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.stg_tempo
[0m14:50:48.108613 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.stg_tempo (execute): 14:50:48.108613 => 14:50:48.108613
[0m14:50:48.110636 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.stg_tempo
[0m14:50:48.110636 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:50:48.111605 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.stg_tempo, now model.projeto_health_insights.fato_atendimento_hospitalar)
[0m14:50:48.112602 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:50:48.115622 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_atendimento_hospitalar"
[0m14:50:48.117603 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (compile): 14:50:48.112602 => 14:50:48.116619
[0m14:50:48.117603 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:50:48.118614 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_atendimento_hospitalar (execute): 14:50:48.118614 => 14:50:48.118614
[0m14:50:48.120582 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_atendimento_hospitalar
[0m14:50:48.121598 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_localidade
[0m14:50:48.122575 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_atendimento_hospitalar, now model.projeto_health_insights.dim_localidade)
[0m14:50:48.122575 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_localidade
[0m14:50:48.126593 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_localidade"
[0m14:50:48.127590 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (compile): 14:50:48.123601 => 14:50:48.127590
[0m14:50:48.128589 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_localidade
[0m14:50:48.129557 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_localidade (execute): 14:50:48.128589 => 14:50:48.128589
[0m14:50:48.130570 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_localidade
[0m14:50:48.130570 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.dim_tempo
[0m14:50:48.132070 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_localidade, now model.projeto_health_insights.dim_tempo)
[0m14:50:48.133070 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.dim_tempo
[0m14:50:48.137061 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.dim_tempo"
[0m14:50:48.138056 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (compile): 14:50:48.133070 => 14:50:48.138056
[0m14:50:48.139054 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.dim_tempo
[0m14:50:48.140065 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.dim_tempo (execute): 14:50:48.140065 => 14:50:48.140065
[0m14:50:48.141049 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.dim_tempo
[0m14:50:48.142045 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:50:48.143043 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.dim_tempo, now test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f)
[0m14:50:48.144040 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:50:48.158031 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f"
[0m14:50:48.159998 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (compile): 14:50:48.144040 => 14:50:48.159016
[0m14:50:48.159998 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:50:48.161023 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f (execute): 14:50:48.161023 => 14:50:48.161023
[0m14:50:48.161992 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f
[0m14:50:48.163035 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:50:48.164016 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_cod_municipio.f6c59a5e3f, now test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771)
[0m14:50:48.164016 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:50:48.169972 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771"
[0m14:50:48.171965 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (compile): 14:50:48.165015 => 14:50:48.170998
[0m14:50:48.171965 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:50:48.172991 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771 (execute): 14:50:48.172991 => 14:50:48.172991
[0m14:50:48.174986 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771
[0m14:50:48.175956 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:50:48.176981 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_localidade_municipio.b6a0aee771, now test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2)
[0m14:50:48.176981 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:50:48.187951 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2"
[0m14:50:48.188951 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (compile): 14:50:48.177953 => 14:50:48.188951
[0m14:50:48.189946 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:50:48.190943 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2 (execute): 14:50:48.189946 => 14:50:48.189946
[0m14:50:48.191940 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2
[0m14:50:48.191940 [debug] [Thread-1 (]: Began running node model.projeto_health_insights.fato_nascimento
[0m14:50:48.192938 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_localidade_cod_municipio.50f447eaa2, now model.projeto_health_insights.fato_nascimento)
[0m14:50:48.193936 [debug] [Thread-1 (]: Began compiling node model.projeto_health_insights.fato_nascimento
[0m14:50:48.198902 [debug] [Thread-1 (]: Writing injected SQL for node "model.projeto_health_insights.fato_nascimento"
[0m14:50:48.200889 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (compile): 14:50:48.193936 => 14:50:48.199892
[0m14:50:48.200889 [debug] [Thread-1 (]: Began executing node model.projeto_health_insights.fato_nascimento
[0m14:50:48.201886 [debug] [Thread-1 (]: Timing info for model.projeto_health_insights.fato_nascimento (execute): 14:50:48.201886 => 14:50:48.201886
[0m14:50:48.203881 [debug] [Thread-1 (]: Finished running node model.projeto_health_insights.fato_nascimento
[0m14:50:48.204878 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:50:48.205875 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.projeto_health_insights.fato_nascimento, now test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6)
[0m14:50:48.206872 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:50:48.212856 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6"
[0m14:50:48.213854 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (compile): 14:50:48.206872 => 14:50:48.213854
[0m14:50:48.213854 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:50:48.214851 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6 (execute): 14:50:48.214851 => 14:50:48.214851
[0m14:50:48.215848 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6
[0m14:50:48.216845 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:50:48.217843 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_ano.3f823cffe6, now test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252)
[0m14:50:48.217843 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:50:48.237790 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252"
[0m14:50:48.239785 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (compile): 14:50:48.218840 => 14:50:48.239785
[0m14:50:48.240782 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:50:48.240782 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252 (execute): 14:50:48.240782 => 14:50:48.240782
[0m14:50:48.242776 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252
[0m14:50:48.242776 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:50:48.243774 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data.85a3cce252, now test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e)
[0m14:50:48.244771 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:50:48.250755 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e"
[0m14:50:48.251752 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (compile): 14:50:48.244771 => 14:50:48.251752
[0m14:50:48.252754 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:50:48.253749 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e (execute): 14:50:48.252754 => 14:50:48.252754
[0m14:50:48.254745 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e
[0m14:50:48.255742 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:50:48.256740 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_data_id.7f65d2dc0e, now test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306)
[0m14:50:48.257737 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:50:48.263720 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306"
[0m14:50:48.264717 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (compile): 14:50:48.257737 => 14:50:48.264717
[0m14:50:48.265715 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:50:48.265715 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306 (execute): 14:50:48.265715 => 14:50:48.265715
[0m14:50:48.267711 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306
[0m14:50:48.267711 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:50:48.268707 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_dia.6eb7b46306, now test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972)
[0m14:50:48.269704 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:50:48.283689 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972"
[0m14:50:48.285661 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (compile): 14:50:48.277682 => 14:50:48.284664
[0m14:50:48.286659 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:50:48.287657 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972 (execute): 14:50:48.286659 => 14:50:48.286659
[0m14:50:48.289650 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972
[0m14:50:48.289650 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:50:48.290648 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_dim_tempo_mes.e5c1450972, now test.projeto_health_insights.unique_dim_tempo_data_id.7391507257)
[0m14:50:48.291646 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:50:48.296655 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_dim_tempo_data_id.7391507257"
[0m14:50:48.298649 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (compile): 14:50:48.291646 => 14:50:48.298649
[0m14:50:48.299656 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:50:48.300628 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_dim_tempo_data_id.7391507257 (execute): 14:50:48.299656 => 14:50:48.300628
[0m14:50:48.301633 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_dim_tempo_data_id.7391507257
[0m14:50:48.303614 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:50:48.304611 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.unique_dim_tempo_data_id.7391507257, now test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d)
[0m14:50:48.305608 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:50:48.318573 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d"
[0m14:50:48.320569 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (compile): 14:50:48.305608 => 14:50:48.319577
[0m14:50:48.321566 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:50:48.322563 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d (execute): 14:50:48.321566 => 14:50:48.321566
[0m14:50:48.323561 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d
[0m14:50:48.324557 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:50:48.325555 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.accepted_values_fato_nascimento_sexo__Masculino__Feminino__Ignorado.76bb16b46d, now test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074)
[0m14:50:48.326552 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:50:48.332558 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074"
[0m14:50:48.334531 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (compile): 14:50:48.326552 => 14:50:48.333534
[0m14:50:48.334531 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:50:48.335528 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074 (execute): 14:50:48.335528 => 14:50:48.335528
[0m14:50:48.337523 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074
[0m14:50:48.338520 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:50:48.339527 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR1.64ae15f074, now test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994)
[0m14:50:48.340515 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:50:48.348494 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994"
[0m14:50:48.349491 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (compile): 14:50:48.340515 => 14:50:48.349491
[0m14:50:48.350516 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:50:48.351486 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994 (execute): 14:50:48.350516 => 14:50:48.350516
[0m14:50:48.352482 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994
[0m14:50:48.353480 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:50:48.354478 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_APGAR5.904d49b994, now test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11)
[0m14:50:48.355474 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:50:48.360490 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11"
[0m14:50:48.362459 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (compile): 14:50:48.355474 => 14:50:48.362459
[0m14:50:48.363455 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:50:48.363455 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11 (execute): 14:50:48.363455 => 14:50:48.363455
[0m14:50:48.365471 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11
[0m14:50:48.365471 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:50:48.367465 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_cod_municipio.4ad53e4c11, now test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244)
[0m14:50:48.367465 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:50:48.374447 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244"
[0m14:50:48.375422 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (compile): 14:50:48.368465 => 14:50:48.375422
[0m14:50:48.376419 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:50:48.377429 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244 (execute): 14:50:48.377429 => 14:50:48.377429
[0m14:50:48.379411 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244
[0m14:50:48.380408 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:50:48.381421 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_data_id.9fa964f244, now test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2)
[0m14:50:48.382403 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:50:48.388411 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2"
[0m14:50:48.390404 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (compile): 14:50:48.382403 => 14:50:48.390404
[0m14:50:48.391403 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:50:48.392377 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2 (execute): 14:50:48.391403 => 14:50:48.391403
[0m14:50:48.393375 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2
[0m14:50:48.394370 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:50:48.395369 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_gestacao_semanas.33149734c2, now test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42)
[0m14:50:48.395369 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:50:48.401374 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42"
[0m14:50:48.403347 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (compile): 14:50:48.396393 => 14:50:48.402372
[0m14:50:48.404357 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:50:48.405342 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42 (execute): 14:50:48.404357 => 14:50:48.404357
[0m14:50:48.406361 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42
[0m14:50:48.407358 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:50:48.407358 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_idade_mae.393b6dfd42, now test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a)
[0m14:50:48.408359 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:50:48.414317 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a"
[0m14:50:48.415314 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (compile): 14:50:48.409330 => 14:50:48.415314
[0m14:50:48.415314 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:50:48.416312 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a (execute): 14:50:48.416312 => 14:50:48.416312
[0m14:50:48.417309 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a
[0m14:50:48.418306 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:50:48.419304 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_nascimento_id.c8a01ebe6a, now test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b)
[0m14:50:48.419304 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:50:48.425318 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b"
[0m14:50:48.427284 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (compile): 14:50:48.420302 => 14:50:48.426313
[0m14:50:48.427284 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:50:48.428308 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b (execute): 14:50:48.428308 => 14:50:48.428308
[0m14:50:48.429306 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b
[0m14:50:48.430274 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:50:48.431304 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_peso.4dd88ba66b, now test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156)
[0m14:50:48.431304 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:50:48.444238 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156"
[0m14:50:48.449224 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (compile): 14:50:48.432270 => 14:50:48.448226
[0m14:50:48.450222 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:50:48.451219 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156 (execute): 14:50:48.450222 => 14:50:48.450222
[0m14:50:48.452217 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156
[0m14:50:48.453213 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:50:48.455208 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_sexo.f35c128156, now test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e)
[0m14:50:48.455208 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:50:48.462213 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e"
[0m14:50:48.463209 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (compile): 14:50:48.456205 => 14:50:48.463209
[0m14:50:48.464184 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:50:48.464184 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e (execute): 14:50:48.464184 => 14:50:48.464184
[0m14:50:48.466200 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e
[0m14:50:48.466200 [debug] [Thread-1 (]: Began running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:50:48.467198 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.projeto_health_insights.not_null_fato_nascimento_tipo_parto.f5f2b5619e, now test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e)
[0m14:50:48.468195 [debug] [Thread-1 (]: Began compiling node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:50:48.476175 [debug] [Thread-1 (]: Writing injected SQL for node "test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e"
[0m14:50:48.477175 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (compile): 14:50:48.468195 => 14:50:48.477175
[0m14:50:48.478147 [debug] [Thread-1 (]: Began executing node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:50:48.478147 [debug] [Thread-1 (]: Timing info for test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e (execute): 14:50:48.478147 => 14:50:48.478147
[0m14:50:48.479161 [debug] [Thread-1 (]: Finished running node test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e
[0m14:50:48.481150 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:50:48.481150 [debug] [MainThread]: Connection 'list_workspace_default' was properly closed.
[0m14:50:48.482136 [debug] [MainThread]: Connection 'test.projeto_health_insights.unique_fato_nascimento_nascimento_id.6c4349305e' was properly closed.
[0m14:50:48.489116 [debug] [MainThread]: Command end result
[0m14:50:48.508079 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m14:50:48.509063 [info ] [MainThread]: Building catalog
[0m14:50:48.512319 [debug] [ThreadPool]: Acquiring new databricks connection 'raw'
[0m14:50:48.525310 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:50:48.526307 [debug] [ThreadPool]: Using databricks connection "raw"
[0m14:50:48.527288 [debug] [ThreadPool]: On raw: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "raw"} */

      select current_catalog()
  
[0m14:50:48.527288 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:50:49.319870 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07937-3511-17c3-8112-6b59f47d236d
[0m14:50:49.700807 [debug] [ThreadPool]: SQL status: OK in 1.1699999570846558 seconds
[0m14:50:49.713799 [debug] [ThreadPool]: Using databricks connection "raw"
[0m14:50:49.714769 [debug] [ThreadPool]: On raw: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "raw"} */
show table extended in `workspace`.`raw` like 'atendimento|procedimento|doenca|faixa_etaria_sexo|localidade'
  
[0m14:50:50.373536 [debug] [ThreadPool]: SQL status: OK in 0.6600000262260437 seconds
[0m14:50:50.376527 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`atendimento`
[0m14:50:50.377525 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`doenca`
[0m14:50:50.377525 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`raw`.`localidade`
[0m14:50:50.379519 [debug] [ThreadPool]: On raw: ROLLBACK
[0m14:50:50.380516 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:50:50.380516 [debug] [ThreadPool]: On raw: Close
[0m14:50:50.381514 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07937-3511-17c3-8112-6b59f47d236d
[0m14:50:50.624923 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly raw, now default)
[0m14:50:50.628881 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:50:50.629908 [debug] [ThreadPool]: Using databricks connection "default"
[0m14:50:50.630905 [debug] [ThreadPool]: On default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "default"} */

      select current_catalog()
  
[0m14:50:50.630905 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:50:51.436899 [info ] [ThreadPool]: databricks-sql-connector adapter: Successfully opened session 01f07937-3655-1089-ba06-245213820d78
[0m14:50:51.755440 [debug] [ThreadPool]: SQL status: OK in 1.1200000047683716 seconds
[0m14:50:51.762422 [debug] [ThreadPool]: Using databricks connection "default"
[0m14:50:51.763419 [debug] [ThreadPool]: On default: /* {"app": "dbt", "dbt_version": "1.5.2", "dbt_databricks_version": "1.5.7", "databricks_sql_connector_version": "2.9.6", "profile_name": "projeto_health_insights", "target_name": "dev", "connection_name": "default"} */
show table extended in `workspace`.`default` like 'fato_atendimento_hospitalar|dim_tempo|dim_localidade|stg_nascidos_vivos|sinasc_2022_sc_clean|stg_localidade|stg_atendimento|int_atendimento|dim_doenca|fato_nascimento|stg_tempo|stg_doenca|int_nascimento'
  
[0m14:50:52.445583 [debug] [ThreadPool]: SQL status: OK in 0.6800000071525574 seconds
[0m14:50:52.449581 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`dim_doenca`
[0m14:50:52.449581 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`dim_localidade`
[0m14:50:52.450588 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`dim_tempo`
[0m14:50:52.451563 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`fato_atendimento_hospitalar`
[0m14:50:52.451563 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`fato_nascimento`
[0m14:50:52.452589 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`int_atendimento`
[0m14:50:52.453558 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`int_nascimento`
[0m14:50:52.453558 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`sinasc_2022_sc_clean`
[0m14:50:52.454556 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_atendimento`
[0m14:50:52.455553 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_doenca`
[0m14:50:52.455553 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_localidade`
[0m14:50:52.456550 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_nascidos_vivos`
[0m14:50:52.456550 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `workspace`.`default`.`stg_tempo`
[0m14:50:52.463559 [debug] [ThreadPool]: On default: ROLLBACK
[0m14:50:52.464535 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:50:52.464535 [debug] [ThreadPool]: On default: Close
[0m14:50:52.465526 [info ] [ThreadPool]: databricks-sql-connector adapter: Closing session 01f07937-3655-1089-ba06-245213820d78
[0m14:50:52.728165 [info ] [MainThread]: Catalog written to C:\Users\Marisa\Desktop\projeto_health_insights\target\catalog.json
[0m14:50:52.729174 [debug] [MainThread]: Command `dbt docs generate` succeeded at 14:50:52.729174 after 8.31 seconds
[0m14:50:52.730188 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m14:50:52.730188 [debug] [MainThread]: Connection 'default' was properly closed.
[0m14:50:52.731158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4FA7EAE50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4FD038350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4FCC860D0>]}
[0m14:50:52.732155 [debug] [MainThread]: Flushing usage events
[0m14:51:21.386319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233CDF6A090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233CD596610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233CD9A1510>]}


============================== 14:51:21.391300 | 1701b83b-6cc7-423b-bb79-831af1f101e4 ==============================
[0m14:51:21.391300 [info ] [MainThread]: Running with dbt=1.5.2
[0m14:51:21.393297 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\Marisa\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\Marisa\\Desktop\\projeto_health_insights\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m14:51:22.792314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1701b83b-6cc7-423b-bb79-831af1f101e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233CD724A90>]}
[0m14:51:22.813258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1701b83b-6cc7-423b-bb79-831af1f101e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233D70FC4D0>]}
[0m16:44:57.236292 [error] [MainThread]: Encountered an error:

[0m16:44:57.283126 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 86, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 71, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 142, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\requires.py", line 215, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\cli\main.py", line 299, in docs_serve
    results = task.run()
              ^^^^^^^^^^
  File "C:\Users\Marisa\Desktop\projeto_health_insights\venv\Lib\site-packages\dbt\task\serve.py", line 28, in run
    httpd.serve_forever()
  File "C:\Users\Marisa\AppData\Local\Programs\Python\Python311\Lib\socketserver.py", line 233, in serve_forever
    ready = selector.select(poll_interval)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\AppData\Local\Programs\Python\Python311\Lib\selectors.py", line 323, in select
    r, w, _ = self._select(self._readers, self._writers, [], timeout)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Marisa\AppData\Local\Programs\Python\Python311\Lib\selectors.py", line 314, in _select
    r, w, x = select.select(r, w, w, timeout)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

[0m16:44:57.309055 [debug] [MainThread]: Command `dbt docs serve` failed at 16:44:57.308058 after 6818.31 seconds
[0m16:44:57.313044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233CDC25150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233CDF66490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233CD682450>]}
[0m16:44:57.317034 [debug] [MainThread]: Flushing usage events
